<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 55]
- [cs.NI](#cs.NI) [Total: 2]
- [cs.IT](#cs.IT) [Total: 33]
- [cs.LG](#cs.LG) [Total: 57]
- [eess.SY](#eess.SY) [Total: 8]
- [cs.AI](#cs.AI) [Total: 43]
- [eess.IV](#eess.IV) [Total: 2]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Explainable Deep Learning for Pediatric Pneumonia Detection in Chest X-Ray Images](https://arxiv.org/abs/2601.09814)
*Adil O. Khadidos,Aziida Nanyonga,Alaa O. Khadidos,Olfat M. Mirza,Mustafa Tahsin Yilmaz*

Main category: cs.CV

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Background: Pneumonia remains a leading cause of morbidity and mortality among children worldwide, emphasizing the need for accurate and efficient diagnostic support tools. Deep learning has shown strong potential in medical image analysis, particularly for chest X-ray interpretation. This study compares two state-of-the-art convolutional neural network (CNN) architectures for automated pediatric pneumonia detection. Methods: A publicly available dataset of 5,863 pediatric chest X-ray images was used. Images were preprocessed through normalization, resizing, and data augmentation to enhance generalization. DenseNet121 and EfficientNet-B0 were fine-tuned using pretrained ImageNet weights under identical training settings. Performance was evaluated using accuracy, F1-score, Matthews Correlation Coefficient (MCC), and recall. Model explainability was incorporated using Gradient-weighted Class Activation Mapping (Grad-CAM) and Local Interpretable Model-agnostic Explanations (LIME) to visualize image regions influencing predictions. Results: EfficientNet-B0 outperformed DenseNet121, achieving an accuracy of 84.6%, F1-score of 0.8899, and MCC of 0.6849. DenseNet121 achieved 79.7% accuracy, an F1-score of 0.8597, and MCC of 0.5852. Both models demonstrated high recall values above 0.99, indicating strong sensitivity to pneumonia detection. Grad-CAM and LIME visualizations showed consistent focus on clinically relevant lung regions, supporting the reliability of model decisions. Conclusions: EfficientNet-B0 provided a more balanced and computationally efficient performance compared to DenseNet121, making it a strong candidate for clinical deployment. The integration of explainability techniques enhances transparency and trustworthiness in AI-assisted pediatric pneumonia diagnosis.

</details>


### [2] [Optimizing Multimodal LLMs for Egocentric Video Understanding: A Solution for the HD-EPIC VQA Challenge](https://arxiv.org/abs/2601.10228)
*Sicheng Yang,Yukai Huang,Shitong Sun,Weitong Cai,Jiankang Deng,Jifei Song,Zhensong Zhang*

Main category: cs.CV

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Multimodal Large Language Models (MLLMs) struggle with complex video QA benchmarks like HD-EPIC VQA due to ambiguous queries/options, poor long-range temporal reasoning, and non-standardized outputs. We propose a framework integrating query/choice pre-processing, domain-specific Qwen2.5-VL fine-tuning, a novel Temporal Chain-of-Thought (T-CoT) prompting for multi-step reasoning, and robust post-processing. This system achieves 41.6% accuracy on HD-EPIC VQA, highlighting the need for holistic pipeline optimization in demanding video understanding. Our code, fine-tuned models are available at https://github.com/YoungSeng/Egocentric-Co-Pilot.

</details>


### [3] [ViSIL: Unified Evaluation of Information Loss in Multimodal Video Captioning](https://arxiv.org/abs/2601.09851)
*Po-han Li,Shenghui Chen,Ufuk Topcu,Sandeep Chinchali*

Main category: cs.CV

TL;DR: 本文针对传统评估指标无法量化多模态视频摘要信息覆盖的问题，提出了ViSIL评分框架，基于信息论衡量视频信息在摘要中的损失，统一评估不同结构的摘要，并在视频问答任务上验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 传统指标(如BLEU、ROUGE)无法跨模态(如文本段落与关键帧序列)量化视频摘要的信息覆盖度，需要一种统一框架来比较不同结构的多模态摘要的信息保留程度。

Method: 1. 提出视频摘要信息损失(ViSIL)评分：基于信息论，通过视觉语言模型推理量化视频信息未被摘要捕获的程度。2. 作为统一指标，可直接比较不同结构的多模态摘要。3. 通过视频问答任务验证ViSIL与人类和VLM性能的相关性。4. 基于ViSIL进行摘要选择，优化信息损失与处理速度的权衡。

Result: 1. ViSIL分数与视频问答任务上的人类和VLM性能呈现统计显著相关性。2. ViSIL支持摘要选择，在帕累托最优前沿上，相比纯文本摘要能在不增加处理负载的情况下将VQA准确率提升7%。

Conclusion: ViSIL为评估多模态视频摘要提供了一种统一且有效的信息论框架，能够量化信息覆盖度，实现跨结构比较，并支持在信息保留和处理效率之间做出优化权衡。

Abstract: Multimodal video captioning condenses dense footage into a structured format of keyframes and natural language. By creating a cohesive multimodal summary, this approach anchors generative AI in rich semantic evidence and serves as a lightweight proxy for high-efficiency retrieval. However, traditional metrics like BLEU or ROUGE fail to quantify information coverage across disparate modalities, such as comparing a paragraph of text to a sequence of keyframes. To address this, we propose the Video Summary Information Loss (ViSIL) score, an information-theoretic framework that quantifies the video information not captured by a summary via vision-language model (VLM) inference. By measuring the information loss, ViSIL is a unified metric that enables direct comparison across multimodal summary formats despite their structural discrepancies. Our results demonstrate that ViSIL scores show a statistically significant correlation with both human and VLM performance on Video Question Answering (VQA) tasks. ViSIL also enables summary selection to optimize the trade-off between information loss and processing speed, establishing a Pareto-optimal frontier that outperforms text summaries by $7\%$ in VQA accuracy without increasing processing load.

</details>


### [4] [Breaking the Limits of Open-Weight CLIP: An Optimization Framework for Self-supervised Fine-tuning of CLIP](https://arxiv.org/abs/2601.09859)
*Anant Mehta,Xiyuan Wei,Xingyu Chen,Tianbao Yang*

Main category: cs.CV

TL;DR: 摘要介绍了TuneCLIP——一个自监督微调框架，用于提升开源CLIP模型在下游任务上的泛化性能，避免传统微调导致的性能下降。


<details>
  <summary>Details</summary>
Motivation: 当前改进CLIP性能通常需要从零开始在海量数据上训练，成本极高。本文探索能否利用现有自监督数据集提升开源CLIP模型在各种下游任务上的表现，而传统微调方法往往导致性能退化。

Method: 提出TuneCLIP框架，包含两个关键组件：(1) 预热阶段：恢复优化统计量以减少冷启动偏差；(2) 微调阶段：优化新的对比损失以减少对假阴性样本的惩罚。

Result: 实验表明TuneCLIP能持续提升不同架构和规模的模型性能。例如在SigLIP（ViT-B/16）上，ImageNet及其相关领域外基准提升达+2.5%，在竞争激烈的DataComp基准上提升+1.2%。

Conclusion: TuneCLIP为高效的后预训练适应设定了新的强基线，通过自监督微调有效提升了开源CLIP模型的通用性能。

Abstract: CLIP has become a cornerstone of multimodal representation learning, yet improving its performance typically requires a prohibitively costly process of training from scratch on billions of samples. We ask a different question: Can we improve the performance of open-weight CLIP models across various downstream tasks using only existing self-supervised datasets? Unlike supervised fine-tuning, which adapts a pretrained model to a single downstream task, our setting seeks to improve general performance across various tasks. However, as both our experiments and prior studies reveal, simply applying standard training protocols starting from an open-weight CLIP model often fails, leading to performance degradation. In this paper, we introduce TuneCLIP, a self-supervised fine-tuning framework that overcomes the performance degradation. TuneCLIP has two key components: (1) a warm-up stage of recovering optimization statistics to reduce cold-start bias, inspired by theoretical analysis, and (2) a fine-tuning stage of optimizing a new contrastive loss to mitigate the penalization on false negative pairs. Our extensive experiments show that TuneCLIP consistently improves performance across model architectures and scales. Notably, it elevates leading open-weight models like SigLIP (ViT-B/16), achieving gains of up to +2.5% on ImageNet and related out-of-distribution benchmarks, and +1.2% on the highly competitive DataComp benchmark, setting a new strong baseline for efficient post-pretraining adaptation.

</details>


### [5] [VibrantSR: Sub-Meter Canopy Height Models from Sentinel-2 Using Generative Flow Matching](https://arxiv.org/abs/2601.09866)
*Kiarie Ndegwa,Andreas Gros,Tony Chang,David Diaz,Vincent A. Landau,Nathan E. Rutenbeck,Luke J. Zachmann,Guy Bayes,Scott Conway*

Main category: cs.CV

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: We present VibrantSR (Vibrant Super-Resolution), a generative super-resolution framework for estimating 0.5 meter canopy height models (CHMs) from 10 meter Sentinel-2 imagery. Unlike approaches based on aerial imagery that are constrained by infrequent and irregular acquisition schedules, VibrantSR leverages globally available Sentinel-2 seasonal composites, enabling consistent monitoring at a seasonal-to-annual cadence. Evaluated across 22 EPA Level 3 eco-regions in the western United States using spatially disjoint validation splits, VibrantSR achieves a Mean Absolute Error of 4.39 meters for canopy heights >= 2 m, outperforming Meta (4.83 m), LANDFIRE (5.96 m), and ETH (7.05 m) satellite-based benchmarks. While aerial-based VibrantVS (2.71 m MAE) retains an accuracy advantage, VibrantSR enables operational forest monitoring and carbon accounting at continental scales without reliance on costly and temporally infrequent aerial acquisitions.

</details>


### [6] [MedVL-SAM2: A unified 3D medical vision-language model for multimodal reasoning and prompt-driven segmentation](https://arxiv.org/abs/2601.09879)
*Yang Xing,Jiong Wu,Savas Ozdemir,Ying Zhang,Yang Yang,Wei Shao,Kuang Gong*

Main category: cs.CV

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Recent progress in medical vision-language models (VLMs) has achieved strong performance on image-level text-centric tasks such as report generation and visual question answering (VQA). However, achieving fine-grained visual grounding and volumetric spatial reasoning in 3D medical VLMs remains challenging, particularly when aiming to unify these capabilities within a single, generalizable framework. To address this challenge, we proposed MedVL-SAM2, a unified 3D medical multimodal model that concurrently supports report generation, VQA, and multi-paradigm segmentation, including semantic, referring, and interactive segmentation. MedVL-SAM2 integrates image-level reasoning and pixel-level perception through a cohesive architecture tailored for 3D medical imaging, and incorporates a SAM2-based volumetric segmentation module to enable precise multi-granular spatial reasoning. The model is trained in a multi-stage pipeline: it is first pre-trained on a large-scale corpus of 3D CT image-text pairs to align volumetric visual features with radiology-language embeddings. It is then jointly optimized with both language-understanding and segmentation objectives using a comprehensive 3D CT segmentation dataset. This joint training enables flexible interaction via language, point, or box prompts, thereby unifying high-level visual reasoning with spatially precise localization. Our unified architecture delivers state-of-the-art performance across report generation, VQA, and multiple 3D segmentation tasks. Extensive analyses further show that the model provides reliable 3D visual grounding, controllable interactive segmentation, and robust cross-modal reasoning, demonstrating that high-level semantic reasoning and precise 3D localization can be jointly achieved within a unified 3D medical VLM.

</details>


### [7] [OT-Drive: Out-of-Distribution Off-Road Traversable Area Segmentation via Optimal Transport](https://arxiv.org/abs/2601.09952)
*Zhihua Zhao,Guoqiang Li,Chen Min,Kangping Lu*

Main category: cs.CV

TL;DR: OT-Drive：一种基于最优传输的多模态融合框架，通过场景锚生成器和最优传输融合模块提升自动驾驶在非结构化环境中可通行区域分割的泛化性能。


<details>
  <summary>Details</summary>
Motivation: 现有数据驱动方法在分布外（OOD）场景下的可通行区域分割性能下降，影响下游驾驶任务，需要提高模型对未知场景的泛化能力。

Method: 1. 设计场景锚生成器（SAG）将场景信息分解为天气、时间和道路类型的联合分布，构建可泛化到未见场景的语义锚点；2. 设计基于最优传输的多模态融合模块（OT Fusion），将RGB和表面法线特征传输到语义锚点定义的流形上。

Result: 在ORFD OOD场景上达到95.16% mIoU，比现有方法提升6.35%；在跨数据集迁移任务上达到89.79% mIoU，比基线提升13.99%。

Conclusion: 该方法能够以有限训练数据实现强大的OOD泛化能力，显著提升了在现实世界部署的实用性和效率。

Abstract: Reliable traversable area segmentation in unstructured environments is critical for planning and decision-making in autonomous driving. However, existing data-driven approaches often suffer from degraded segmentation performance in out-of-distribution (OOD) scenarios, consequently impairing downstream driving tasks. To address this issue, we propose OT-Drive, an Optimal Transport--driven multi-modal fusion framework. The proposed method formulates RGB and surface normal fusion as a distribution transport problem. Specifically, we design a novel Scene Anchor Generator (SAG) to decompose scene information into the joint distribution of weather, time-of-day, and road type, thereby constructing semantic anchors that can generalize to unseen scenarios. Subsequently, we design an innovative Optimal Transport-based multi-modal fusion module (OT Fusion) to transport RGB and surface normal features onto the manifold defined by the semantic anchors, enabling robust traversable area segmentation under OOD scenarios. Experimental results demonstrate that our method achieves 95.16% mIoU on ORFD OOD scenarios, outperforming prior methods by 6.35%, and 89.79% mIoU on cross-dataset transfer tasks, surpassing baselines by 13.99%.These results indicate that the proposed model can attain strong OOD generalization with only limited training data, substantially enhancing its practicality and efficiency for real-world deployment.

</details>


### [8] [The Spatial Blindspot of Vision-Language Models](https://arxiv.org/abs/2601.09954)
*Nahid Alam,Leema Krishna Murali,Siddhant Bharadwaj,Patrick Liu,Timothy Chung,Drishti Sharma,Akshata A,Kranthi Kiran,Wesley Tam,Bala Krishna S Vegesna*

Main category: cs.CV

TL;DR: 当前视觉语言模型在空间关系理解上存在缺陷，本文通过改进图像编码器训练目标和引入2D位置编码来提升空间推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型通常采用CLIP风格的图像编码器，将图像展平为1D序列，丢失了2D结构信息，导致空间推理能力不足。这在机器人学和具身AI等需要空间感知的应用中成为瓶颈。

Method: 研究两种改进方法：(i) 采用替代训练目标的图像编码器 (ii) 引入2D位置编码，以保留图像的2D结构信息。

Result: 实验表明，这些架构上的改进在多个基准测试中带来了更好的空间推理表现。

Conclusion: 空间感知是视觉语言模型设计中缺失的重要维度，通过针对性的架构改进可以显著提升空间推理能力，对需要空间感知的应用具有重要意义。

Abstract: Vision-language models (VLMs) have advanced rapidly, but their ability to capture spatial relationships remains a blindspot. Current VLMs are typically built with contrastive language-image pretraining (CLIP) style image encoders. The training recipe often flattens images into 1D patch sequences, discarding the 2D structure necessary for spatial reasoning. We argue that this lack of spatial awareness is a missing dimension in VLM design and a bottleneck for applications requiring spatial grounding, such as robotics and embodied AI. To address this, we investigate (i) image encoders trained with alternative objectives and (ii) 2D positional encodings. Our experiments show that these architectural choices can lead to improved spatial reasoning on several benchmarks.

</details>


### [9] [DR$^2$Seg: Decomposed Two-Stage Rollouts for Efficient Reasoning Segmentation in Multimodal Large Language Models](https://arxiv.org/abs/2601.09981)
*Yulin He,Wei Chen,Zhikang Jian,Tianhang Guo,Wenjuan Zhou,Minglong Li*

Main category: cs.CV

TL;DR: 提出了DR2Seg框架，通过两阶段自我奖励机制解决推理分割中的过度思考问题，提升推理效率和分割精度


<details>
  <summary>Details</summary>
Motivation: 现有的推理分割方法存在过度思考问题，生成冗长的推理链会干扰多模态大语言模型中的物体定位

Method: 采用两阶段策略：第一阶段生成明确描述目标物体的自包含描述；第二阶段用该描述替代原始复杂查询验证其自包含性，引入两种自我奖励来增强目标导向推理并抑制冗余思考

Result: 在不同规模和分割模型的MLLMs上进行的广泛实验表明，DR2Seg能持续提升推理效率和整体分割性能

Conclusion: DR2Seg是一个无需额外思考监督的自我奖励框架，能有效提高推理分割任务的效率和准确性

Abstract: Reasoning segmentation is an emerging vision-language task that requires reasoning over intricate text queries to precisely segment objects. However, existing methods typically suffer from overthinking, generating verbose reasoning chains that interfere with object localization in multimodal large language models (MLLMs). To address this issue, we propose DR$^2$Seg, a self-rewarding framework that improves both reasoning efficiency and segmentation accuracy without requiring extra thinking supervision. DR$^2$Seg employs a two-stage rollout strategy that decomposes reasoning segmentation into multimodal reasoning and referring segmentation. In the first stage, the model generates a self-contained description that explicitly specifies the target object. In the second stage, this description replaces the original complex query to verify its self-containment. Based on this design, two self-rewards are introduced to strengthen goal-oriented reasoning and suppress redundant thinking. Extensive experiments across MLLMs of varying scales and segmentation models demonstrate that DR$^2$Seg consistently improves reasoning efficiency and overall segmentation performance.

</details>


### [10] [VERHallu: Evaluating and Mitigating Event Relation Hallucination in Video Large Language Models](https://arxiv.org/abs/2601.10010)
*Zefan Zhang,Kehua Zhu,Shijie Jiang,Hongyuan Lu,Shengkai Sun,Tian Bai*

Main category: cs.CV

TL;DR: 该论文提出了专门评估视频大语言模型事件关系幻觉的新基准VERHallu，涵盖因果、时序和子事件关系，并提出关键帧传播策略以缓解这类幻觉。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注视频中事件、物体和场景存在的幻觉问题，而忽视了事件关系幻觉。视频大语言模型在密集事件关系推理上存在困难，经常依赖先验知识而非帧级视觉线索，导致对事件关系的理解不完整不准确。

Method: 1）建立VERHallu基准测试，包含关系分类、问答和反事实问答三种任务；2）提出关键帧传播（KFP）策略，通过在中间层重新分配帧级注意力来增强多事件理解能力。

Result: 实验表明当前最先进的VideoLLMs在密集事件关系推理上表现不佳，过度依赖语言先验；而提出的KFP策略能有效缓解事件关系幻觉且不影响推理速度。

Conclusion: 事件关系幻觉是VideoLLMs中被忽视的重要问题，VERHallu基准为该领域提供了系统评估工具，KFP策略通过优化注意力机制改善了多事件关系理解。

Abstract: Video Large Language Models (VideoLLMs) exhibit various types of hallucinations. Existing research has primarily focused on hallucinations involving the presence of events, objects, and scenes in videos, while largely neglecting event relation hallucination. In this paper, we introduce a novel benchmark for evaluating the Video Event Relation Hallucination, named VERHallu. This benchmark focuses on causal, temporal, and subevent relations between events, encompassing three types of tasks: relation classification, question answering, and counterfactual question answering, for a comprehensive evaluation of event relation hallucination. Additionally, it features counterintuitive video scenarios that deviate from typical pretraining distributions, with each sample accompanied by human-annotated candidates covering both vision-language and pure language biases. Our analysis reveals that current state-of-the-art VideoLLMs struggle with dense-event relation reasoning, often relying on prior knowledge due to insufficient use of frame-level cues. Although these models demonstrate strong grounding capabilities for key events, they often overlook the surrounding subevents, leading to an incomplete and inaccurate understanding of event relations. To tackle this, we propose a Key-Frame Propagating (KFP) strategy, which reallocates frame-level attention within intermediate layers to enhance multi-event understanding. Experiments show it effectively mitigates the event relation hallucination without affecting inference speed.

</details>


### [11] [Disentangled Concept Representation for Text-to-image Person Re-identification](https://arxiv.org/abs/2601.10053)
*Giyeol Kim,Chanho Eom*

Main category: cs.CV

TL;DR: 该论文提出了DiCo框架，通过解耦概念表示解决文本到图像行人重识别中的模态差异与细粒度对应问题。


<details>
  <summary>Details</summary>
Motivation: 文本到图像行人重识别面临视觉外观与文本表达间的显著模态鸿沟，以及需要建模细粒度对应关系以区分具有相似属性（如服装颜色、纹理或款式）的个体。

Method: 提出DiCo框架，引入基于共享槽的表示方法，每个槽作为跨模态的部分级锚点，并进一步分解为多个概念块，实现互补属性的解耦，同时保持图像和文本之间一致的部分级对应。

Result: 在CUHK-PEDES、ICFG-PEDES和RSTPReid数据集上的实验表明，该框架达到了与最先进方法竞争的性能，同时通过显式的槽级和块级表示提高了可解释性，实现更细粒度的检索结果。

Conclusion: DiCo框架通过分层解耦的跨模态对齐，有效解决了文本到图像行人重识别的挑战，在保持高性能的同时增强了模型的可解释性。

Abstract: Text-to-image person re-identification (TIReID) aims to retrieve person images from a large gallery given free-form textual descriptions. TIReID is challenging due to the substantial modality gap between visual appearances and textual expressions, as well as the need to model fine-grained correspondences that distinguish individuals with similar attributes such as clothing color, texture, or outfit style. To address these issues, we propose DiCo (Disentangled Concept Representation), a novel framework that achieves hierarchical and disentangled cross-modal alignment. DiCo introduces a shared slot-based representation, where each slot acts as a part-level anchor across modalities and is further decomposed into multiple concept blocks. This design enables the disentanglement of complementary attributes (\textit{e.g.}, color, texture, shape) while maintaining consistent part-level correspondence between image and text. Extensive experiments on CUHK-PEDES, ICFG-PEDES, and RSTPReid demonstrate that our framework achieves competitive performance with state-of-the-art methods, while also enhancing interpretability through explicit slot- and block-level representations for more fine-grained retrieval results.

</details>


### [12] [UEOF: A Benchmark Dataset for Underwater Event-Based Optical Flow](https://arxiv.org/abs/2601.10054)
*Nick Truong,Pritam P. Karmokar,William J. Beksi*

Main category: cs.CV

TL;DR: 首次提出基于事件的合成水下光流数据集，通过物理光线追踪渲染视频生成包含真实水下光学效果的事件数据，为水下事件相机的感知算法提供基准。


<details>
  <summary>Details</summary>
Motivation: 水下成像面临波长依赖的光衰减、悬浮颗粒散射、浑浊度模糊和非均匀照明等挑战，传统相机难以获取真实运动信息。事件相机具有微秒级分辨率和宽动态范围，但缺乏同时包含真实水下光学效果和精确光流的地面实况数据集，限制了相关研究进展。

Method: 基于物理的光线追踪渲染 RGBD 序列，通过现代视频到事件的处理流水线生成真实水下光学环境下的事件数据流，包含密集的地面实况光流、深度和相机运动信息。对最先进的基于学习和模型的预测方法进行基准测试。

Result: 创建了首个基于事件的合成水下光流基准数据集，提供了配对真实水下光学效果与精确光流的事件数据，为水下事件感知算法的开发和评估建立了新的基线标准。

Conclusion: 该数据集填补了水下事件相机研究的数据空白，有望推动水下事件感知算法的发展。数据集和源代码已公开，为相关研究提供了重要资源。

Abstract: Underwater imaging is fundamentally challenging due to wavelength-dependent light attenuation, strong scattering from suspended particles, turbidity-induced blur, and non-uniform illumination. These effects impair standard cameras and make ground-truth motion nearly impossible to obtain. On the other hand, event cameras offer microsecond resolution and high dynamic range. Nonetheless, progress on investigating event cameras for underwater environments has been limited due to the lack of datasets that pair realistic underwater optics with accurate optical flow. To address this problem, we introduce the first synthetic underwater benchmark dataset for event-based optical flow derived from physically-based ray-traced RGBD sequences. Using a modern video-to-event pipeline applied to rendered underwater videos, we produce realistic event data streams with dense ground-truth flow, depth, and camera motion. Moreover, we benchmark state-of-the-art learning-based and model-based optical flow prediction methods to understand how underwater light transport affects event formation and motion estimation accuracy. Our dataset establishes a new baseline for future development and evaluation of underwater event-based perception algorithms. The source code and dataset for this project are publicly available at https://robotic-vision-lab.github.io/ueof.

</details>


### [13] [CoF-T2I: Video Models as Pure Visual Reasoners for Text-to-Image Generation](https://arxiv.org/abs/2601.10061)
*Chengzhuo Tong,Mingkun Chang,Shenglong Zhang,Yuran Wang,Cheng Liang,Zhizheng Zhao,Ruichuan An,Bohan Zeng,Yang Shi,Yifan Dai,Ziming Zhao,Guanbin Li,Pengfei Wan,Yuanxing Zhang,Wentao Zhang*

Main category: cs.CV

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Recent video generation models have revealed the emergence of Chain-of-Frame (CoF) reasoning, enabling frame-by-frame visual inference. With this capability, video models have been successfully applied to various visual tasks (e.g., maze solving, visual puzzles). However, their potential to enhance text-to-image (T2I) generation remains largely unexplored due to the absence of a clearly defined visual reasoning starting point and interpretable intermediate states in the T2I generation process. To bridge this gap, we propose CoF-T2I, a model that integrates CoF reasoning into T2I generation via progressive visual refinement, where intermediate frames act as explicit reasoning steps and the final frame is taken as output. To establish such an explicit generation process, we curate CoF-Evol-Instruct, a dataset of CoF trajectories that model the generation process from semantics to aesthetics. To further improve quality and avoid motion artifacts, we enable independent encoding operation for each frame. Experiments show that CoF-T2I significantly outperforms the base video model and achieves competitive performance on challenging benchmarks, reaching 0.86 on GenEval and 7.468 on Imagine-Bench. These results indicate the substantial promise of video models for advancing high-quality text-to-image generation.

</details>


### [14] [Thinking Like Van Gogh: Structure-Aware Style Transfer via Flow-Guided 3D Gaussian Splatting](https://arxiv.org/abs/2601.10075)
*Zhendong Wang,Lebin Zhou,Jingchuan Xiao,Rongduo Han,Nam Ling,Cihan Ruan*

Main category: cs.CV

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: In 1888, Vincent van Gogh wrote, "I am seeking exaggeration in the essential." This principle, amplifying structural form while suppressing photographic detail, lies at the core of Post-Impressionist art. However, most existing 3D style transfer methods invert this philosophy, treating geometry as a rigid substrate for surface-level texture projection. To authentically reproduce Post-Impressionist stylization, geometric abstraction must be embraced as the primary vehicle of expression.
  We propose a flow-guided geometric advection framework for 3D Gaussian Splatting (3DGS) that operationalizes this principle in a mesh-free setting. Our method extracts directional flow fields from 2D paintings and back-propagates them into 3D space, rectifying Gaussian primitives to form flow-aligned brushstrokes that conform to scene topology without relying on explicit mesh priors. This enables expressive structural deformation driven directly by painterly motion rather than photometric constraints.
  Our contributions are threefold: (1) a projection-based, mesh-free flow guidance mechanism that transfers 2D artistic motion into 3D Gaussian geometry; (2) a luminance-structure decoupling strategy that isolates geometric deformation from color optimization, mitigating artifacts during aggressive structural abstraction; and (3) a VLM-as-a-Judge evaluation framework that assesses artistic authenticity through aesthetic judgment instead of conventional pixel-level metrics, explicitly addressing the subjective nature of artistic stylization.

</details>


### [15] [Difficulty-guided Sampling: Bridging the Target Gap between Dataset Distillation and Downstream Tasks](https://arxiv.org/abs/2601.10090)
*Mingzhuo Li,Guang Li,Linfeng Ye,Jiafeng Mao,Takahiro Ogawa,Konstantinos N. Plataniotis,Miki Haseyama*

Main category: cs.CV

TL;DR: 本文提出了难度引导采样（DGS）方法，通过桥接蒸馏目标与下游任务之间的目标间隙，提升数据集蒸馏的性能。该方法引入难度概念，从现有方法生成的图像池中按特定难度分布采样，形成最终蒸馏数据集。


<details>
  <summary>Details</summary>
Motivation: 现有数据集蒸馏方法通常关注原始数据集提取的特征，忽视了任务特定信息，导致蒸馏目标与下游任务之间存在目标间隙，影响了蒸馏数据集在下游任务上的性能。

Method: 针对图像分类下游任务，引入难度概念并提出DGS作为可插拔的后处理采样模块。按照特定目标难度分布，从现有方法生成的图像池中采样形成最终蒸馏数据集。同时提出难度感知引导（DAG）探索难度在生成过程中的作用。

Result: 在多个设置下的广泛实验证明了所提方法的有效性，并凸显了难度概念在各种下游任务中的更广泛潜力。

Conclusion: 通过引入难度概念和DGS方法，成功桥接了数据集蒸馏目标与下游任务间的间隙，提升了数据集蒸馏在下游任务上的性能表现。

Abstract: In this paper, we propose difficulty-guided sampling (DGS) to bridge the target gap between the distillation objective and the downstream task, therefore improving the performance of dataset distillation. Deep neural networks achieve remarkable performance but have time and storage-consuming training processes. Dataset distillation is proposed to generate compact, high-quality distilled datasets, enabling effective model training while maintaining downstream performance. Existing approaches typically focus on features extracted from the original dataset, overlooking task-specific information, which leads to a target gap between the distillation objective and the downstream task. We propose leveraging characteristics that benefit the downstream training into data distillation to bridge this gap. Focusing on the downstream task of image classification, we introduce the concept of difficulty and propose DGS as a plug-in post-stage sampling module. Following the specific target difficulty distribution, the final distilled dataset is sampled from image pools generated by existing methods. We also propose difficulty-aware guidance (DAG) to explore the effect of difficulty in the generation process. Extensive experiments across multiple settings demonstrate the effectiveness of the proposed methods. It also highlights the broader potential of difficulty for diverse downstream tasks.

</details>


### [16] [V-Zero: Self-Improving Multimodal Reasoning with Zero Annotation](https://arxiv.org/abs/2601.10094)
*Han Wang,Yi Yang,Jingyuan Hu,Minfeng Zhu,Wei Chen*

Main category: cs.CV

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Recent advances in multimodal learning have significantly enhanced the reasoning capabilities of vision-language models (VLMs). However, state-of-the-art approaches rely heavily on large-scale human-annotated datasets, which are costly and time-consuming to acquire. To overcome this limitation, we introduce V-Zero, a general post-training framework that facilitates self-improvement using exclusively unlabeled images. V-Zero establishes a co-evolutionary loop by instantiating two distinct roles: a Questioner and a Solver. The Questioner learns to synthesize high-quality, challenging questions by leveraging a dual-track reasoning reward that contrasts intuitive guesses with reasoned results. The Solver is optimized using pseudo-labels derived from majority voting over its own sampled responses. Both roles are trained iteratively via Group Relative Policy Optimization (GRPO), driving a cycle of mutual enhancement. Remarkably, without a single human annotation, V-Zero achieves consistent performance gains on Qwen2.5-VL-7B-Instruct, improving visual mathematical reasoning by +1.7 and general vision-centric by +2.6, demonstrating the potential of self-improvement in multimodal systems. Code is available at https://github.com/SatonoDia/V-Zero

</details>


### [17] [InfoSculpt: Sculpting the Latent Space for Generalized Category Discovery](https://arxiv.org/abs/2601.10098)
*Wenwen Liao,Hang Ruan,Jianbo Yu,Yuansong Wang,Qingchao Jiang,Xiaofeng Yang*

Main category: cs.CV

TL;DR: InfoSculpt是一个基于信息瓶颈原则的广义类别发现框架，通过双条件互信息目标来解耦分类信号和实例噪声。


<details>
  <summary>Details</summary>
Motivation: 解决现有广义类别发现方法依赖伪标签或两阶段聚类、缺乏显式机制解耦类别定义信号与实例特定噪声的核心限制。

Method: 提出InfoSculpt框架，基于信息瓶颈原则，通过最小化双条件互信息目标：在标注数据上使用类别级CMI学习已知类别的紧凑判别表示，在所有数据上使用实例级CMI通过压缩增强诱导的噪声来蒸馏不变特征。

Result: 在8个基准测试上的大量实验表明InfoSculpt的有效性。

Conclusion: 信息论方法能够有效解决广义类别发现中的表示解耦问题，创造出既保留类别信息又丢弃实例特定噪声的鲁棒潜在空间。

Abstract: Generalized Category Discovery (GCD) aims to classify instances from both known and novel categories within a large-scale unlabeled dataset, a critical yet challenging task for real-world, open-world applications. However, existing methods often rely on pseudo-labeling, or two-stage clustering, which lack a principled mechanism to explicitly disentangle essential, category-defining signals from instance-specific noise. In this paper, we address this fundamental limitation by re-framing GCD from an information-theoretic perspective, grounded in the Information Bottleneck (IB) principle. We introduce InfoSculpt, a novel framework that systematically sculpts the representation space by minimizing a dual Conditional Mutual Information (CMI) objective. InfoSculpt uniquely combines a Category-Level CMI on labeled data to learn compact and discriminative representations for known classes, and a complementary Instance-Level CMI on all data to distill invariant features by compressing augmentation-induced noise. These two objectives work synergistically at different scales to produce a disentangled and robust latent space where categorical information is preserved while noisy, instance-specific details are discarded. Extensive experiments on 8 benchmarks demonstrate that InfoSculpt validating the effectiveness of our information-theoretic approach.

</details>


### [18] [MathDoc: Benchmarking Structured Extraction and Active Refusal on Noisy Mathematics Exam Papers](https://arxiv.org/abs/2601.10104)
*Chenyue Zhou,Jiayi Tuo,Shitong Qin,Wei Dai,Mingxuan Wang,Ziwei Zhao,Duoyang Li,Shiyang Su,Yanxi Lu,Yanbiao Ma*

Main category: cs.CV

TL;DR: 该论文提出了MathDoc基准，首个针对真实高中数学试卷文档级信息提取的基准，包含3,609个真实噪声样本，可评估模型在文档降级条件下的提取性能和主动拒绝能力。


<details>
  <summary>Details</summary>
Motivation: 现有基准主要关注干净文档或通用布局分析，忽略了数学问题的结构完整性以及模型主动拒绝不完整输入的能力。真实数学试卷常有严重视觉噪声，现有方法难以应对。

Method: 构建MathDoc基准，包含3,609个经过精心策划的高中数学试题，包含真实世界伪影和不可识别样本。提出多维评估框架，涵盖题干准确性、视觉相似度和拒绝能力。在多款SOTA MLLM上进行实验。

Result: 实验表明，虽然端到端模型在提取性能上表现强劲，但在拒绝难以辨认输入时持续失败，反而产生自信但无效的输出。这些结果凸显了当前MLLM在文档降级条件下的关键缺陷。

Conclusion: MathDoc基准为评估模型在降级文档条件下的可靠性提供了标准，揭示了当前MLLM在主动拒绝能力方面的不足，为未来研究工作提供了方向。

Abstract: The automated extraction of structured questions from paper-based mathematics exams is fundamental to intelligent education, yet remains challenging in real-world settings due to severe visual noise. Existing benchmarks mainly focus on clean documents or generic layout analysis, overlooking both the structural integrity of mathematical problems and the ability of models to actively reject incomplete inputs. We introduce MathDoc, the first benchmark for document-level information extraction from authentic high school mathematics exam papers. MathDoc contains \textbf{3,609} carefully curated questions with real-world artifacts and explicitly includes unrecognizable samples to evaluate active refusal behavior. We propose a multi-dimensional evaluation framework covering stem accuracy, visual similarity, and refusal capability. Experiments on SOTA MLLMs, including Qwen3-VL and Gemini-2.5-Pro, show that although end-to-end models achieve strong extraction performance, they consistently fail to refuse illegible inputs, instead producing confident but invalid outputs. These results highlight a critical gap in current MLLMs and establish MathDoc as a benchmark for assessing model reliability under degraded document conditions. Our project repository is available at \href{https://github.com/winnk123/papers/tree/master}{GitHub repository}

</details>


### [19] [Enhancing Visual In-Context Learning by Multi-Faceted Fusion](https://arxiv.org/abs/2601.10107)
*Wenwen Liao,Jianbo Yu,Yuansong Wang,Qingchao Jiang,Xiaofeng Yang*

Main category: cs.CV

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Visual In-Context Learning (VICL) has emerged as a powerful paradigm, enabling models to perform novel visual tasks by learning from in-context examples. The dominant "retrieve-then-prompt" approach typically relies on selecting the single best visual prompt, a practice that often discards valuable contextual information from other suitable candidates. While recent work has explored fusing the top-K prompts into a single, enhanced representation, this still simply collapses multiple rich signals into one, limiting the model's reasoning capability. We argue that a more multi-faceted, collaborative fusion is required to unlock the full potential of these diverse contexts. To address this limitation, we introduce a novel framework that moves beyond single-prompt fusion towards an multi-combination collaborative fusion. Instead of collapsing multiple prompts into one, our method generates three contextual representation branches, each formed by integrating information from different combinations of top-quality prompts. These complementary guidance signals are then fed into proposed MULTI-VQGAN architecture, which is designed to jointly interpret and utilize collaborative information from multiple sources. Extensive experiments on diverse tasks, including foreground segmentation, single-object detection, and image colorization, highlight its strong cross-task generalization, effective contextual fusion, and ability to produce more robust and accurate predictions than existing methods.

</details>


### [20] [Beyond Single Prompts: Synergistic Fusion and Arrangement for VICL](https://arxiv.org/abs/2601.10117)
*Wenwen Liao,Jianbo Yu,Yuansong Wang,Shifu Yan,Xiaofeng Yang*

Main category: cs.CV

TL;DR: Vision In-Context Learning (VICL) 框架通过自适应融合模块和多提示整合，解决了现有方法丢弃高质量提示互补信息、以及未能利用提示排列结构信息的问题，实现了更优的跨任务泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉上下文学习（VICL）方法存在两大关键问题：1)只选择最相似的提示，丢弃了其他高质量提示的互补信息；2)未能利用不同类型提示排列所蕴含的结构化信息，限制了模型的性能。

Method: 1) 自适应融合模块：聚合多个提示中的关键模式和标注，形成更精确的上下文提示；2) 排列特定轻量级MLP：将布局先验与核心模型解耦，同时最小化对整体模型的影响；3) 双向微调机制：交换查询和提示的角色，鼓励模型从融合的上下文重建原始提示，增强融合模块与修复模型的协作。

Result: 在前景分割、单目标检测和图像着色等任务上的实验表明，该方法取得了优异的结果，并表现出强大的跨任务泛化能力。

Conclusion: 提出的端到端VICL框架通过多提示融合和结构信息利用，克服了现有方法的局限性，在多个视觉任务上实现了显著性能提升和良好的泛化性。

Abstract: Vision In-Context Learning (VICL) enables inpainting models to quickly adapt to new visual tasks from only a few prompts. However, existing methods suffer from two key issues: (1) selecting only the most similar prompt discards complementary cues from other high-quality prompts; and (2) failing to exploit the structured information implied by different prompt arrangements.
  We propose an end-to-end VICL framework to overcome these limitations. Firstly, an adaptive Fusion Module aggregates critical patterns and annotations from multiple prompts to form more precise contextual prompts. Secondly, we introduce arrangement-specific lightweight MLPs to decouple layout priors from the core model, while minimally affecting the overall model. In addition, an bidirectional fine-tuning mechanism swaps the roles of query and prompt, encouraging the model to reconstruct the original prompt from fused context and thus enhancing collaboration between the fusion module and the inpainting model. Experiments on foreground segmentation, single-object detection, and image colorization demonstrate superior results and strong cross-task generalization of our method.

</details>


### [21] [VQ-Seg: Vector-Quantized Token Perturbation for Semi-Supervised Medical Image Segmentation](https://arxiv.org/abs/2601.10124)
*Sicheng Yang,Zhaohu Xing,Lei Zhu*

Main category: cs.CV

TL;DR: VQ-Seg提出了一种用于半监督医学图像分割的新方法，通过向量量化离散化特征空间并引入可控制的量化扰动模块来替代传统的dropout，解决了手动调整dropout率这一敏感超参数的问题。


<details>
  <summary>Details</summary>
Motivation: 传统的基于dropout的一致性学习在半监督医学图像分割中需要手动调整敏感的dropout率超参数，这往往难以优化并可能导致次优正则化效果。需要一种更有效且可控的正则化方法来解决这一局限性。

Method: 开发了VQ-Seg框架，首次将向量量化应用于离散化特征空间，设计了可控制的量化扰动模块替代dropout，通过打乱码书索引的空间位置进行扰动。采用双分支架构共享量化后特征空间用于图像重建和分割任务，并引入后VQ特征适配器整合基础模型的高层语义信息。

Result: 在包含828个CT扫描的大规模肺癌数据集以及其他公共基准测试上进行了广泛实验，结果表明该方法优于现有的最先进方法。

Conclusion: VQ-Seg通过向量量化和可控制的量化扰动提供了一种有效的半监督医学图像分割方法，解决了传统dropout方法的局限性，在多个数据集上表现出卓越性能。

Abstract: Consistency learning with feature perturbation is a widely used strategy in semi-supervised medical image segmentation. However, many existing perturbation methods rely on dropout, and thus require a careful manual tuning of the dropout rate, which is a sensitive hyperparameter and often difficult to optimize and may lead to suboptimal regularization. To overcome this limitation, we propose VQ-Seg, the first approach to employ vector quantization (VQ) to discretize the feature space and introduce a novel and controllable Quantized Perturbation Module (QPM) that replaces dropout. Our QPM perturbs discrete representations by shuffling the spatial locations of codebook indices, enabling effective and controllable regularization. To mitigate potential information loss caused by quantization, we design a dual-branch architecture where the post-quantization feature space is shared by both image reconstruction and segmentation tasks. Moreover, we introduce a Post-VQ Feature Adapter (PFA) to incorporate guidance from a foundation model (FM), supplementing the high-level semantic information lost during quantization. Furthermore, we collect a large-scale Lung Cancer (LC) dataset comprising 828 CT scans annotated for central-type lung carcinoma. Extensive experiments on the LC dataset and other public benchmarks demonstrate the effectiveness of our method, which outperforms state-of-the-art approaches. Code available at: https://github.com/script-Yang/VQ-Seg.

</details>


### [22] [LaViT: Aligning Latent Visual Thoughts for Multi-modal Reasoning](https://arxiv.org/abs/2601.10129)
*Linquan Wu,Tianxiang Jiang,Yifei Dong,Haoyu Yang,Fengji Zhang,Shichaang Meng,Ai Xuan,Linqi Song,Jacky Keung*

Main category: cs.CV

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Current multimodal latent reasoning often relies on external supervision (e.g., auxiliary images), ignoring intrinsic visual attention dynamics. In this work, we identify a critical Perception Gap in distillation: student models frequently mimic a teacher's textual output while attending to fundamentally divergent visual regions, effectively relying on language priors rather than grounded perception. To bridge this, we propose LaViT, a framework that aligns latent visual thoughts rather than static embeddings. LaViT compels the student to autoregressively reconstruct the teacher's visual semantics and attention trajectories prior to text generation, employing a curriculum sensory gating mechanism to prevent shortcut learning. Extensive experiments show that LaViT significantly enhances visual grounding, achieving up to +16.9% gains on complex reasoning tasks and enabling a compact 3B model to outperform larger open-source variants and proprietary models like GPT-4o.

</details>


### [23] [Advancing Adaptive Multi-Stage Video Anomaly Reasoning: A Benchmark Dataset and Method](https://arxiv.org/abs/2601.10165)
*Chao Huang,Benfeng Wang,Wei Wang,Jie Wen,Li Shen,Wenqi Ren,Yong Xu,Xiaochun Cao*

Main category: cs.CV

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Recent progress in reasoning capabilities of Multimodal Large Language Models(MLLMs) has highlighted their potential for performing complex video understanding tasks. However, in the domain of Video Anomaly Detection and Understanding (VAD&U), existing MLLM-based methods are largely limited to anomaly localization or post-hoc description, lacking explicit reasoning processes, risk awareness, and decision-oriented interpretation. To address this gap, we define a new task termed Video Anomaly Reasoning (VAR), which elevates video anomaly analysis from descriptive understanding to structured, multi-stage reasoning. VAR explicitly requires models to perform progressive reasoning over anomalous events before answering anomaly-related questions, encompassing visual perception, causal interpretation, and risk-aware decision making. To support this task, we present a new dataset with 8,641 videos, where each video is annotated with diverse question types corresponding to different reasoning depths, totaling more than 50,000 samples, making it one of the largest datasets for video anomaly. The annotations are based on a structured Perception-Cognition-Action Chain-of-Thought (PerCoAct-CoT), which formalizes domain-specific reasoning priors for video anomaly understanding. This design enables systematic evaluation of multi-stage and adaptive anomaly reasoning. In addition, we propose Anomaly-Aware Group Relative Policy Optimization to further enhance reasoning reliability under weak supervision. Building upon the proposed task and dataset, we develop an end-to-end MLLM-based VAR model termed Vad-R1-Plus, which supports adaptive hierarchical reasoning and risk-aware decision making. Extensive experiments demonstrate that the proposed benchmark and method effectively advance the reasoning capabilities of MLLMs on VAR tasks, outperforming both open-source and proprietary baselines.

</details>


### [24] [RAG-3DSG: Enhancing 3D Scene Graphs with Re-Shot Guided Retrieval-Augmented Generation](https://arxiv.org/abs/2601.10168)
*Yue Chang,Rufeng Chen,Zhaofan Zhang,Yi Chen,Sihong Xie*

Main category: cs.CV

TL;DR: 提出RAG-3DSG方法，通过重投影引导的不确定性估计和基于可靠低不确定性对象的检索增强生成，解决开放词汇3D场景图生成中物体识别精度低、速度慢的问题。


<details>
  <summary>Details</summary>
Motivation: 现有开放词汇3D场景图生成方法存在物体识别精度低、速度慢的问题，主要受限于观察视角约束、遮挡和冗余表面密度等挑战，影响了其在机器人操作、导航等下游任务中的应用。

Method: 1. 提出重投影引导的不确定性估计来减少聚合噪声；
2. 通过可靠低不确定性对象支持物体级检索增强生成；
3. 设计动态下采样映射策略，利用自适应粒度加速跨图像物体聚合。

Result: 在Replica数据集上的实验表明，RAG-3DSG显著提高了3D场景图生成的节点标注准确性，同时将映射时间相比原始版本减少了三分之二。

Conclusion: RAG-3DSG方法通过不确定性估计和检索增强生成有效提升了开放词汇3D场景图生成的准确性和效率，为机器人应用提供了更高质量的语义场景理解。

Abstract: Open-vocabulary 3D Scene Graph (3DSG) generation can enhance various downstream tasks in robotics, such as manipulation and navigation, by leveraging structured semantic representations. A 3DSG is constructed from multiple images of a scene, where objects are represented as nodes and relationships as edges. However, existing works for open-vocabulary 3DSG generation suffer from both low object-level recognition accuracy and speed, mainly due to constrained viewpoints, occlusions, and redundant surface density. To address these challenges, we propose RAG-3DSG to mitigate aggregation noise through re-shot guided uncertainty estimation and support object-level Retrieval-Augmented Generation (RAG) via reliable low-uncertainty objects. Furthermore, we propose a dynamic downsample-mapping strategy to accelerate cross-image object aggregation with adaptive granularity. Experiments on Replica dataset demonstrate that RAG-3DSG significantly improves node captioning accuracy in 3DSG generation while reducing the mapping time by two-thirds compared to the vanilla version.

</details>


### [25] [ELITE: Efficient Gaussian Head Avatar from a Monocular Video via Learned Initialization and TEst-time Generative Adaptation](https://arxiv.org/abs/2601.10200)
*Kim Youwang,Lee Hyoseok,Subin Park,Gerard Pons-Moll,Tae-Hyun Oh*

Main category: cs.CV

TL;DR: ELITE 提出了一种高效的高斯头部虚拟人合成方法，通过单目视频学习初始化和测试时生成适应来实现


<details>
  <summary>Details</summary>
Motivation: 现有方法要么依赖3D数据先验（泛化能力差），要么依赖2D生成先验（计算量大且易出现身份幻觉）。作者发现两种先验具有互补协同作用，旨在设计高效且泛化能力强的虚拟人合成系统。

Method: 1. 前馈式网格到高斯先验模型（MGPM）实现高斯虚拟人的快速初始化；2. 测试时生成适应阶段，使用真实和合成图像作为监督；3. 渲染引导的单步扩散增强器，基于高斯虚拟人渲染恢复缺失的视觉细节。

Result: ELITE 在视觉效果上优于现有方法，即使是挑战性表情也能处理，并且比2D生成先验方法快60倍。

Conclusion: ELITE 通过结合3D数据先验和2D生成先验的互补优势，实现了高效、高质量且泛化能力强的虚拟人合成。

Abstract: We introduce ELITE, an Efficient Gaussian head avatar synthesis from a monocular video via Learned Initialization and TEst-time generative adaptation. Prior works rely either on a 3D data prior or a 2D generative prior to compensate for missing visual cues in monocular videos. However, 3D data prior methods often struggle to generalize in-the-wild, while 2D generative prior methods are computationally heavy and prone to identity hallucination. We identify a complementary synergy between these two priors and design an efficient system that achieves high-fidelity animatable avatar synthesis with strong in-the-wild generalization. Specifically, we introduce a feed-forward Mesh2Gaussian Prior Model (MGPM) that enables fast initialization of a Gaussian avatar. To further bridge the domain gap at test time, we design a test-time generative adaptation stage, leveraging both real and synthetic images as supervision. Unlike previous full diffusion denoising strategies that are slow and hallucination-prone, we propose a rendering-guided single-step diffusion enhancer that restores missing visual details, grounded on Gaussian avatar renderings. Our experiments demonstrate that ELITE produces visually superior avatars to prior works, even for challenging expressions, while achieving 60x faster synthesis than the 2D generative prior method.

</details>


### [26] [Beyond Inpainting: Unleash 3D Understanding for Precise Camera-Controlled Video Generation](https://arxiv.org/abs/2601.10214)
*Dong-Yu Chen,Yixin Guo,Shuojin Yang,Tai-Jiang Mu,Shi-Min Hu*

Main category: cs.CV

TL;DR: DepthDirector使用深度视频作为摄像机控制引导，通过双流条件机制和轻量级LoRA适配器，实现了精确的摄像机轨迹改变下的视频重渲染，解决了现有方法中的内容不一致和生成质量下降问题。


<details>
  <summary>Details</summary>
Motivation: 当前的视频生成方法在进行精确摄像机控制时，往往无法充分利用视频扩散模型的3D先验知识，导致内容一致性差和生成质量下降（'修复陷阱'）。需要一种能精确控制摄像机轨迹同时忠实保持视频内容的方法。

Method: 1. 设计了视图-内容双流条件机制，将源视频和目标视点下渲染的扭曲深度序列注入预训练视频生成模型
2. 使用轻量级LoRA适配器进行训练，完全保留VDMs的先验知识
3. 构建了MultiCam-WarpData大规模多摄像机同步数据集（包含1K动态场景的8K视频）

Result: DepthDirector在摄像机可控性和视觉质量方面优于现有方法。实验证明该方法能够精确控制摄像机运动并保持内容一致性。

Conclusion: DepthDirector通过在视频生成中引入深度视频作为几何引导，成功实现了精确的摄像机轨迹控制，同时保持了视频内容的忠实再现，解决了现有方法中的关键问题。

Abstract: Camera control has been extensively studied in conditioned video generation; however, performing precisely altering the camera trajectories while faithfully preserving the video content remains a challenging task. The mainstream approach to achieving precise camera control is warping a 3D representation according to the target trajectory. However, such methods fail to fully leverage the 3D priors of video diffusion models (VDMs) and often fall into the Inpainting Trap, resulting in subject inconsistency and degraded generation quality. To address this problem, we propose DepthDirector, a video re-rendering framework with precise camera controllability. By leveraging the depth video from explicit 3D representation as camera-control guidance, our method can faithfully reproduce the dynamic scene of an input video under novel camera trajectories. Specifically, we design a View-Content Dual-Stream Condition mechanism that injects both the source video and the warped depth sequence rendered under the target viewpoint into the pretrained video generation model. This geometric guidance signal enables VDMs to comprehend camera movements and leverage their 3D understanding capabilities, thereby facilitating precise camera control and consistent content generation. Next, we introduce a lightweight LoRA-based video diffusion adapter to train our framework, fully preserving the knowledge priors of VDMs. Additionally, we construct a large-scale multi-camera synchronized dataset named MultiCam-WarpData using Unreal Engine 5, containing 8K videos across 1K dynamic scenes. Extensive experiments show that DepthDirector outperforms existing methods in both camera controllability and visual quality. Our code and dataset will be publicly available.

</details>


### [27] [Attend to what I say: Highlighting relevant content on slides](https://arxiv.org/abs/2601.10244)
*Megha Mariam K M,C. V. Jawahar*

Main category: cs.CV

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Imagine sitting in a presentation, trying to follow the speaker while simultaneously scanning the slides for relevant information. While the entire slide is visible, identifying the relevant regions can be challenging. As you focus on one part of the slide, the speaker moves on to a new sentence, leaving you scrambling to catch up visually. This constant back-and-forth creates a disconnect between what is being said and the most important visual elements, making it hard to absorb key details, especially in fast-paced or content-heavy presentations such as conference talks. This requires an understanding of slides, including text, graphics, and layout. We introduce a method that automatically identifies and highlights the most relevant slide regions based on the speaker's narrative. By analyzing spoken content and matching it with textual or graphical elements in the slides, our approach ensures better synchronization between what listeners hear and what they need to attend to. We explore different ways of solving this problem and assess their success and failure cases. Analyzing multimedia documents is emerging as a key requirement for seamless understanding of content-rich videos, such as educational videos and conference talks, by reducing cognitive strain and improving comprehension. Code and dataset are available at: https://github.com/meghamariamkm2002/Slide_Highlight

</details>


### [28] [Hierarchical Refinement of Universal Multimodal Attacks on Vision-Language Models](https://arxiv.org/abs/2601.10313)
*Peng-Fei Zhang,Zi Huang*

Main category: cs.CV

TL;DR: 作者提出了一个分层精炼攻击框架，用于视觉-语言预训练模型，可生成高效的多模态通用对抗扰动。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉-语言预训练模型对抗攻击大多是样本特定的，在大规模数据集或新场景中计算开销大。需要开发可扩展的通用攻击方法。

Method: HRA框架在样本层级和优化层级对通用对抗扰动进行精炼。图像模态：将对抗样本解耦为干净图像和扰动，引入ScMix增强策略。文本模态：通过句子内和句子间重要性度量识别全局有影响的词作为通用扰动。优化路径：利用历史和未来梯度的时间层次结构避免局部极小值。

Result: 在各种下游任务、视觉-语言预训练模型和数据集上的广泛实验证明了所提通用多模态攻击的优越性。

Conclusion: HRA是一个有效的视觉-语言预训练模型通用对抗攻击框架，具有可扩展性和高效性。

Abstract: Existing adversarial attacks for VLP models are mostly sample-specific, resulting in substantial computational overhead when scaled to large datasets or new scenarios. To overcome this limitation, we propose Hierarchical Refinement Attack (HRA), a multimodal universal attack framework for VLP models. HRA refines universal adversarial perturbations (UAPs) at both the sample level and the optimization level. For the image modality, we disentangle adversarial examples into clean images and perturbations, allowing each component to be handled independently for more effective disruption of cross-modal alignment. We further introduce a ScMix augmentation strategy that diversifies visual contexts and strengthens both global and local utility of UAPs, thereby reducing reliance on spurious features. In addition, we refine the optimization path by leveraging a temporal hierarchy of historical and estimated future gradients to avoid local minima and stabilize universal perturbation learning. For the text modality, HRA identifies globally influential words by combining intra-sentence and inter-sentence importance measures, and subsequently utilizes these words as universal text perturbations. Extensive experiments across various downstream tasks, VLP models, and datasets demonstrate the superiority of the proposed universal multimodal attacks.

</details>


### [29] [ROMA: Real-time Omni-Multimodal Assistant with Interactive Streaming Understanding](https://arxiv.org/abs/2601.10323)
*Xueyun Tian,Wei Li,Bingbing Xu,Heng Dong,Yuanzhuo Wang,Huawei Shen*

Main category: cs.CV

TL;DR: 本文提出ROMA，一个实时全多模态助手，用于统一反应式与主动式交互，通过同步多模态单元处理连续输入并解决细粒度不匹配问题，在主动任务上达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有全多模态大模型在流式音视频理解方面存在挑战：通常模态支持不完整或缺乏自主主动监控能力，能力分散。

Method: ROMA处理连续输入为同步多模态单元，对齐密集音频与离散视频帧以处理粒度不匹配；引入轻量级说话头解耦响应触发与生成；使用流式数据集和两阶段课程训练策略。

Result: 在12个基准测试上，ROMA在主动任务上达到SOTA，在反应式设置中具有竞争力，证明了其在统一实时全多模态理解中的鲁棒性。

Conclusion: ROMA解决了流式音视频理解中的现有问题，实现了实时统一反应与主动交互，为全多模态理解提供了有效解决方案。

Abstract: Recent Omni-multimodal Large Language Models show promise in unified audio, vision, and text modeling. However, streaming audio-video understanding remains challenging, as existing approaches suffer from disjointed capabilities: they typically exhibit incomplete modality support or lack autonomous proactive monitoring. To address this, we present ROMA, a real-time omni-multimodal assistant for unified reactive and proactive interaction. ROMA processes continuous inputs as synchronized multimodal units, aligning dense audio with discrete video frames to handle granularity mismatches. For online decision-making, we introduce a lightweight speak head that decouples response initiation from generation to ensure precise triggering without task conflict. We train ROMA with a curated streaming dataset and a two-stage curriculum that progressively optimizes for streaming format adaptation and proactive responsiveness. To standardize the fragmented evaluation landscape, we reorganize diverse benchmarks into a unified suite covering both proactive (alert, narration) and reactive (QA) settings. Extensive experiments across 12 benchmarks demonstrate ROMA achieves state-of-the-art performance on proactive tasks while competitive in reactive settings, validating its robustness in unified real-time omni-multimodal understanding.

</details>


### [30] [Think-Then-Generate: Reasoning-Aware Text-to-Image Diffusion with LLM Encoders](https://arxiv.org/abs/2601.10332)
*Siqi Kou,Jiachun Jin,Zetong Zhou,Ye Ma,Yugang Wang,Quan Chen,Peng Jiang,Xiao Yang,Jun Zhu,Kai Yu,Zhijie Deng*

Main category: cs.CV

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Recent progress in text-to-image (T2I) diffusion models (DMs) has enabled high-quality visual synthesis from diverse textual prompts. Yet, most existing T2I DMs, even those equipped with large language model (LLM)-based text encoders, remain text-pixel mappers -- they employ LLMs merely as text encoders, without leveraging their inherent reasoning capabilities to infer what should be visually depicted given the textual prompt. To move beyond such literal generation, we propose the think-then-generate (T2G) paradigm, where the LLM-based text encoder is encouraged to reason about and rewrite raw user prompts; the states of the rewritten prompts then serve as diffusion conditioning. To achieve this, we first activate the think-then-rewrite pattern of the LLM encoder with a lightweight supervised fine-tuning process. Subsequently, the LLM encoder and diffusion backbone are co-optimized to ensure faithful reasoning about the context and accurate rendering of the semantics via Dual-GRPO. In particular, the text encoder is reinforced using image-grounded rewards to infer and recall world knowledge, while the diffusion backbone is pushed to produce semantically consistent and visually coherent images. Experiments show substantial improvements in factual consistency, semantic alignment, and visual realism across reasoning-based image generation and editing benchmarks, achieving 0.79 on WISE score, nearly on par with GPT-4. Our results constitute a promising step toward next-generation unified models with reasoning, expression, and demonstration capacities.

</details>


### [31] [Fine-Grained Human Pose Editing Assessment via Layer-Selective MLLMs](https://arxiv.org/abs/2601.10369)
*Ningyu Sun,Zhaolin Cai,Zitong Xu,Peihang Chen,Huiyu Duan,Yichao Yan,Xiongkuo Min,Xiaokang Yang*

Main category: cs.CV

TL;DR: 针对文本引导人体姿态编辑中结构异常和生成伪影问题，作者提出HPE-Bench评估基准和基于MLLM的统一评估框架，通过对比LoRA调优和层敏感分析机制实现对姿态异常的多维度质量评估。


<details>
  <summary>Details</summary>
Motivation: 文本引导人体姿态编辑存在结构异常和生成伪影问题，现有评估方法往往将真实性检测与质量评估割裂，无法提供针对姿态不一致性的细粒度分析。

Method: 1) 构建包含1700个样本的HPE-Bench基准数据集；2) 开发基于多层选择多模态大语言模型的统一框架，采用对比LoRA调优和层敏感分析机制确定姿态评估的最佳特征层。

Result: 该框架在真实性检测和多维度质量回归方面均取得优越性能，有效弥合了法医级检测与质量评估之间的鸿沟。

Conclusion: HPE-Bench基准和所提出的MLLM框架为文本引导人体姿态编辑提供了细粒度的评估解决方案，为该领域的模型优化提供了可靠基准。

Abstract: Text-guided human pose editing has gained significant traction in AIGC applications. However,it remains plagued by structural anomalies and generative artifacts. Existing evaluation metrics often isolate authenticity detection from quality assessment, failing to provide fine-grained insights into pose-specific inconsistencies. To address these limitations, we introduce HPE-Bench, a specialized benchmark comprising 1,700 standardized samples from 17 state-of-the-art editing models, offering both authenticity labels and multi-dimensional quality scores. Furthermore, we propose a unified framework based on layer-selective multimodal large language models (MLLMs). By employing contrastive LoRA tuning and a novel layer sensitivity analysis (LSA) mechanism, we identify the optimal feature layer for pose evaluation. Our framework achieves superior performance in both authenticity detection and multi-dimensional quality regression, effectively bridging the gap between forensic detection and quality assessment.

</details>


### [32] [Towards Efficient Low-rate Image Compression with Frequency-aware Diffusion Prior Refinement](https://arxiv.org/abs/2601.10373)
*Yichong Xia,Yimin Zhou,Jinpeng Wang,Bin Chen*

Main category: cs.CV

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Recent advancements in diffusion-based generative priors have enabled visually plausible image compression at extremely low bit rates. However, existing approaches suffer from slow sampling processes and suboptimal bit allocation due to fragmented training paradigms. In this work, we propose Accelerate \textbf{Diff}usion-based Image Compression via \textbf{C}onsistency Prior \textbf{R}efinement (DiffCR), a novel compression framework for efficient and high-fidelity image reconstruction. At the heart of DiffCR is a Frequency-aware Skip Estimation (FaSE) module that refines the $ε$-prediction prior from a pre-trained latent diffusion model and aligns it with compressed latents at different timesteps via Frequency Decoupling Attention (FDA). Furthermore, a lightweight consistency estimator enables fast \textbf{two-step decoding} by preserving the semantic trajectory of diffusion sampling. Without updating the backbone diffusion model, DiffCR achieves substantial bitrate savings (27.2\% BD-rate (LPIPS) and 65.1\% BD-rate (PSNR)) and over $10\times$ speed-up compared to SOTA diffusion-based compression baselines.

</details>


### [33] [Global Context Compression with Interleaved Vision-Text Transformation](https://arxiv.org/abs/2601.10378)
*Dian Jiao,Jiaxin Duan,Shuai Zhao,Jiabing Leng,Yiran Zhang,Feng Huang*

Main category: cs.CV

TL;DR: VIST2是一种新型Transformer模型，通过将文本块转换为视觉编码来全局压缩上下文，在预填充和推理阶段都能节省token数量，显著提升长文本生成效率。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型在OCR中的成功表明低损失文本压缩的新途径，但部分压缩方法无法在token级推理中节省计算或内存成本。本文研究全局上下文压缩，以同时在预填充和推理阶段节省token。

Method: 提出VIST2模型，将输入文本块与其视觉编码交错排列，在预上下文中仅依赖视觉token预测下一个文本token分布。将文本块渲染为草图图像，并通过多阶段训练：从课程调度的预训练开始进行光学语言建模，然后进行模态交错指令微调。使用0.6B到8B不同规模的VIST2模型进行实验。

Result: 在4倍压缩比下，模型在长文本写作任务上显著优于基线，平均实现首次token生成速度提升3倍，内存使用减少77%，FLOPs减少74%。

Conclusion: VIST2通过视觉编码全局压缩上下文的方法，在长文本生成任务中实现了效率和性能的显著提升，为文本压缩和高效语言模型架构提供了新思路。

Abstract: Recent achievements of vision-language models in end-to-end OCR point to a new avenue for low-loss compression of textual information. This motivates earlier works that render the Transformer's input into images for prefilling, which effectively reduces the number of tokens through visual encoding, thereby alleviating the quadratically increased Attention computations. However, this partial compression fails to save computational or memory costs at token-by-token inference. In this paper, we investigate global context compression, which saves tokens at both prefilling and inference stages. Consequently, we propose VIST2, a novel Transformer that interleaves input text chunks alongside their visual encoding, while depending exclusively on visual tokens in the pre-context to predict the next text token distribution. Around this idea, we render text chunks into sketch images and train VIST2 in multiple stages, starting from curriculum-scheduled pretraining for optical language modeling, followed by modal-interleaved instruction tuning. We conduct extensive experiments using VIST2 families scaled from 0.6B to 8B to explore the training recipe and hyperparameters. With a 4$\times$ compression ratio, the resulting models demonstrate significant superiority over baselines on long writing tasks, achieving, on average, a 3$\times$ speedup in first-token generation, 77% reduction in memory usage, and 74% reduction in FLOPS. Our codes and datasets will be public to support further studies.

</details>


### [34] [Handling Missing Modalities in Multimodal Survival Prediction for Non-Small Cell Lung Cancer](https://arxiv.org/abs/2601.10386)
*Filippo Ruffini,Camillo Maria Caruso,Claudia Tacconi,Lorenzo Nibid,Francesca Miccolis,Marta Lovino,Carlo Greco,Edy Ippolito,Michele Fiore,Alessio Cortellini,Bruno Beomonte Zobel,Giuseppe Perrone,Bruno Vincenzi,Claudio Marrocco,Alessandro Bria,Elisa Ficarra,Sara Ramella,Valerio Guarrasi,Paolo Soda*

Main category: cs.CV

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Accurate survival prediction in Non-Small Cell Lung Cancer (NSCLC) requires the integration of heterogeneous clinical, radiological, and histopathological information. While Multimodal Deep Learning (MDL) offers a promises for precision prognosis and survival prediction, its clinical applicability is severely limited by small cohort sizes and the presence of missing modalities, often forcing complete-case filtering or aggressive imputation. In this work, we present a missing-aware multimodal survival framework that integrates Computed Tomography (CT), Whole-Slide Histopathology (WSI) Images, and structured clinical variables for overall survival modeling in unresectable stage II-III NSCLC. By leveraging Foundation Models (FM) for modality-specific feature extraction and a missing-aware encoding strategy, the proposed approach enables intermediate multimodal fusion under naturally incomplete modality profiles. The proposed architecture is resilient to missing modalities by design, allowing the model to utilize all available data without being forced to drop patients during training or inference. Experimental results demonstrate that intermediate fusion consistently outperforms unimodal baselines as well as early and late fusion strategies, with the strongest performance achieved by the fusion of WSI and clinical modalities (73.30 C-index). Further analyses of modality importance reveal an adaptive behavior in which less informative modalities, i.e., CT modality, are automatically down-weighted and contribute less to the final survival prediction.

</details>


### [35] [Multi-Temporal Frames Projection for Dynamic Processes Fusion in Fluorescence Microscopy](https://arxiv.org/abs/2601.10392)
*Hassan Eshkiki,Sarah Costa,Mostafa Mohammadpour,Farinaz Tanhaei,Christopher H. George,Fabio Caraffini*

Main category: cs.CV

TL;DR: 提出了一种新颖的计算框架，将多帧时序荧光显微镜图像融合为单张高质量图像，在保持原始生物学内容的同时提升信息质量。


<details>
  <summary>Details</summary>
Motivation: 荧光显微镜在分析活体生物样本时面临噪声、时间变异性和信号振荡可视化不一致的问题，限制了记录数据的实用性。需要一种方法来整合多个时间分辨帧的信息，生成既能保持原始视频生物学内容又具备高质量的单张图像。

Method: 开发了一个独特的计算框架，结合了来自不同计算机视觉应用领域可解释技术的组合。该框架将多时序图像堆栈融合成高质量的2D图像，方法通过111种配置在包含动态、异质且形态复杂的心肌细胞2D单层挑战性数据集上进行了评估。

Result: 结果显示该框架能够生成保持并增强单个显微镜帧质量和信息的合成图像，与传统方法相比，细胞计数平均提高了44%。该框架适用于需要将多时序图像堆栈融合为高质量2D图像的其他成像领域。

Conclusion: 提出的计算框架有效解决了荧光显微镜分析中的噪声和时间变异性问题，通过融合多时序帧信息生成高质量单张图像，显著提升了细胞检测性能，并具有向其他成像领域推广的潜力，有助于注释和下游分割任务的便利性。

Abstract: Fluorescence microscopy is widely employed for the analysis of living biological samples; however, the utility of the resulting recordings is frequently constrained by noise, temporal variability, and inconsistent visualisation of signals that oscillate over time. We present a unique computational framework that integrates information from multiple time-resolved frames into a single high-quality image, while preserving the underlying biological content of the original video. We evaluate the proposed method through an extensive number of configurations (n = 111) and on a challenging dataset comprising dynamic, heterogeneous, and morphologically complex 2D monolayers of cardiac cells. Results show that our framework, which consists of a combination of explainable techniques from different computer vision application fields, is capable of generating composite images that preserve and enhance the quality and information of individual microscopy frames, yielding 44% average increase in cell count compared to previous methods. The proposed pipeline is applicable to other imaging domains that require the fusion of multi-temporal image stacks into high-quality 2D images, thereby facilitating annotation and downstream segmentation.

</details>


### [36] [Lunar-G2R: Geometry-to-Reflectance Learning for High-Fidelity Lunar BRDF Estimation](https://arxiv.org/abs/2601.10449)
*Clementine Grethen,Nicolas Menga,Roland Brochard,Geraldine Morin,Simone Gasparini,Jeremy Lebreton,Manuel Sanchez Gestido*

Main category: cs.CV

TL;DR: 论文提出的Lunar-G2R方法，解决了如何从单数字高程模型（DEM）直接推断出具有空间变化反射率的BRDF参数的问题，从而提升了月球表面渲染的真实感。


<details>
  <summary>Details</summary>
Motivation: 现有的月球渲染流程依赖于过于简化或空间均匀的BRDF模型。这些模型的参数难以估计，且无法捕捉局部的反射率变化，限制了渲染的光度真实感。本文旨在开发一种无需多视图图像或专用反射率采集硬件的方法，从几何数据中直接推断反射率。

Method: 提出了Lunar-G2R框架。该方法利用U-Net架构，结合可微分渲染进行训练。它从数字高程模型（DEM）出发，在已知观测和光照几何条件下，通过最小化真实轨道图像与物理渲染图像之间的光度差异，直接预测空间变化的BRDF参数。

Result: 在Tycho陨石坑的地理数据保留区域进行的实验表明，该方法将光度误差降低了38%（相较最先进的基线）。同时，它取得了更高的PSNR和SSIM指标，以及对细粒度反射率变化有更好的感知相似性，这是空间均匀模型无法捕捉的。

Conclusion: 这是首个能够直接从地形几何推断出具有空间变化反射率模型的方法。Lunar-G2R显著提升了渲染的光度真实感，并可以应用于需要高保真渲染（如视觉导航）的场景，而无需复杂的图像采集或专用硬件。

Abstract: We address the problem of estimating realistic, spatially varying reflectance for complex planetary surfaces such as the lunar regolith, which is critical for high-fidelity rendering and vision-based navigation. Existing lunar rendering pipelines rely on simplified or spatially uniform BRDF models whose parameters are difficult to estimate and fail to capture local reflectance variations, limiting photometric realism. We propose Lunar-G2R, a geometry-to-reflectance learning framework that predicts spatially varying BRDF parameters directly from a lunar digital elevation model (DEM), without requiring multi-view imagery, controlled illumination, or dedicated reflectance-capture hardware at inference time. The method leverages a U-Net trained with differentiable rendering to minimize photometric discrepancies between real orbital images and physically based renderings under known viewing and illumination geometry. Experiments on a geographically held-out region of the Tycho crater show that our approach reduces photometric error by 38 % compared to a state-of-the-art baseline, while achieving higher PSNR and SSIM and improved perceptual similarity, capturing fine-scale reflectance variations absent from spatially uniform models. To our knowledge, this is the first method to infer a spatially varying reflectance model directly from terrain geometry.

</details>


### [37] [Urban Socio-Semantic Segmentation with Vision-Language Reasoning](https://arxiv.org/abs/2601.10477)
*Yu Wang,Yi Wang,Rui Dai,Yujie Wang,Kaikui Liu,Xiangxiang Chu,Yansheng Li*

Main category: cs.CV

TL;DR: 本文介绍了SocioSeg数据集和SocioReasoner框架，用于解决卫星图像中社会语义实体（如学校、公园）的分割难题。


<details>
  <summary>Details</summary>
Motivation: 当前分割模型能可靠分割物理属性定义的实体（如建筑物、水体），但在处理社会定义类别（如学校、公园）时仍存在困难。城市表面包含丰富的语义实体，分割这些实体对下游应用至关重要。

Method: 提出SocioReasoner视觉-语言推理框架，模拟人类识别和标注社会语义实体的过程，通过跨模态识别和多阶段推理实现。使用强化学习优化这一非可微分过程，激发视觉-语言模型的推理能力。

Result: 实验表明该方法优于现有最先进模型，并展示了强大的零样本泛化能力。

Conclusion: 通过视觉-语言模型推理实现了城市社会语义分割，提出的SocioSeg数据集和SocioReasoner框架为解决社会定义实体分割问题提供了有效方案。

Abstract: As hubs of human activity, urban surfaces consist of a wealth of semantic entities. Segmenting these various entities from satellite imagery is crucial for a range of downstream applications. Current advanced segmentation models can reliably segment entities defined by physical attributes (e.g., buildings, water bodies) but still struggle with socially defined categories (e.g., schools, parks). In this work, we achieve socio-semantic segmentation by vision-language model reasoning. To facilitate this, we introduce the Urban Socio-Semantic Segmentation dataset named SocioSeg, a new resource comprising satellite imagery, digital maps, and pixel-level labels of social semantic entities organized in a hierarchical structure. Additionally, we propose a novel vision-language reasoning framework called SocioReasoner that simulates the human process of identifying and annotating social semantic entities via cross-modal recognition and multi-stage reasoning. We employ reinforcement learning to optimize this non-differentiable process and elicit the reasoning capabilities of the vision-language model. Experiments demonstrate our approach's gains over state-of-the-art models and strong zero-shot generalization. Our dataset and code are available in https://github.com/AMAP-ML/SocioReasoner.

</details>


### [38] [mergetune: Continued fine-tuning of vision-language models](https://arxiv.org/abs/2601.10497)
*Wenqing Wang,Da Li,Xiatian Zhu,Josef Kittler*

Main category: cs.CV

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Fine-tuning vision-language models (VLMs) such as CLIP often leads to catastrophic forgetting of pretrained knowledge. Prior work primarily aims to mitigate forgetting during adaptation; however, forgetting often remains inevitable during this process. We introduce a novel paradigm, \emph{continued fine-tuning (CFT)}, which seeks to recover pretrained knowledge after a zero-shot model has already been adapted. We propose a simple, model-agnostic CFT strategy (named MERGETUNE) guided by linear mode connectivity (LMC), which can be applied post hoc to existing fine-tuned models without requiring architectural changes. Given a fine-tuned model, we continue fine-tuning its trainable parameters (e.g., soft prompts or linear heads) to search for a continued model which has two low-loss paths to the zero-shot (e.g., CLIP) and the fine-tuned (e.g., CoOp) solutions. By exploiting the geometry of the loss landscape, the continued model implicitly merges the two solutions, restoring pretrained knowledge lost in the fine-tuned counterpart. A challenge is that the vanilla LMC constraint requires data replay from the pretraining task. We approximate this constraint for the zero-shot model via a second-order surrogate, eliminating the need for large-scale data replay. Experiments show that MERGETUNE improves the harmonic mean of CoOp by +5.6\% on base-novel generalisation without adding parameters. % We show \emph{the first time} superior performance than CLIP on both DTD and EuroSAT, on cross-dataset transfer. On robust fine-tuning evaluations, the LMC-merged model from MERGETUNE surpasses ensemble baselines with lower inference cost, achieving further gains and state-of-the-art results when ensembled with the zero-shot model. Our code is available at \href{https://github.com/Surrey-UP-Lab/MERGETUNE}{https://github.com/Surrey-UP-Lab/MERGETUNE}.

</details>


### [39] [SatMap: Revisiting Satellite Maps as Prior for Online HD Map Construction](https://arxiv.org/abs/2601.10512)
*Kanak Mazumder,Fabian B. Flohr*

Main category: cs.CV

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Online high-definition (HD) map construction is an essential part of a safe and robust end-to-end autonomous driving (AD) pipeline. Onboard camera-based approaches suffer from limited depth perception and degraded accuracy due to occlusion. In this work, we propose SatMap, an online vectorized HD map estimation method that integrates satellite maps with multi-view camera observations and directly predicts a vectorized HD map for downstream prediction and planning modules. Our method leverages lane-level semantics and texture from satellite imagery captured from a Bird's Eye View (BEV) perspective as a global prior, effectively mitigating depth ambiguity and occlusion. In our experiments on the nuScenes dataset, SatMap achieves 34.8% mAP performance improvement over the camera-only baseline and 8.5% mAP improvement over the camera-LiDAR fusion baseline. Moreover, we evaluate our model in long-range and adverse weather conditions to demonstrate the advantages of using a satellite prior map. Source code will be available at https://iv.ee.hm.edu/satmap/.

</details>


### [40] [BikeActions: An Open Platform and Benchmark for Cyclist-Centric VRU Action Recognition](https://arxiv.org/abs/2601.10521)
*Max A. Buettner,Kanak Mazumder,Luca Koecher,Mario Finkbeiner,Sebastian Niebler,Fabian B. Flohr*

Main category: cs.CV

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Anticipating the intentions of Vulnerable Road Users (VRUs) is a critical challenge for safe autonomous driving (AD) and mobile robotics. While current research predominantly focuses on pedestrian crossing behaviors from a vehicle's perspective, interactions within dense shared spaces remain underexplored. To bridge this gap, we introduce FUSE-Bike, the first fully open perception platform of its kind. Equipped with two LiDARs, a camera, and GNSS, it facilitates high-fidelity, close-range data capture directly from a cyclist's viewpoint. Leveraging this platform, we present BikeActions, a novel multi-modal dataset comprising 852 annotated samples across 5 distinct action classes, specifically tailored to improve VRU behavior modeling. We establish a rigorous benchmark by evaluating state-of-the-art graph convolution and transformer-based models on our publicly released data splits, establishing the first performance baselines for this challenging task. We release the full dataset together with data curation tools, the open hardware design, and the benchmark code to foster future research in VRU action understanding under https://iv.ee.hm.edu/bikeactions/.

</details>


### [41] [SVII-3D: Advancing Roadside Infrastructure Inventory with Decimeter-level 3D Localization and Comprehension from Sparse Street Imagery](https://arxiv.org/abs/2601.10535)
*Chong Liu,Luxuan Fu,Yang Jia,Zhen Dong,Bisheng Yang*

Main category: cs.CV

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: The automated creation of digital twins and precise asset inventories is a critical task in smart city construction and facility lifecycle management. However, utilizing cost-effective sparse imagery remains challenging due to limited robustness, inaccurate localization, and a lack of fine-grained state understanding. To address these limitations, SVII-3D, a unified framework for holistic asset digitization, is proposed. First, LoRA fine-tuned open-set detection is fused with a spatial-attention matching network to robustly associate observations across sparse views. Second, a geometry-guided refinement mechanism is introduced to resolve structural errors, achieving precise decimeter-level 3D localization. Third, transcending static geometric mapping, a Vision-Language Model agent leveraging multi-modal prompting is incorporated to automatically diagnose fine-grained operational states. Experiments demonstrate that SVII-3D significantly improves identification accuracy and minimizes localization errors. Consequently, this framework offers a scalable, cost-effective solution for high-fidelity infrastructure digitization, effectively bridging the gap between sparse perception and automated intelligent maintenance.

</details>


### [42] [Enhancing the quality of gauge images captured in smoke and haze scenes through deep learning](https://arxiv.org/abs/2601.10537)
*Oscar H. Ramírez-Agudelo,Akshay N. Shewatkar,Edoardo Milana,Roland C. Aydin,Kai Franke*

Main category: cs.CV

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Images captured in hazy and smoky environments suffer from reduced visibility, posing a challenge when monitoring infrastructures and hindering emergency services during critical situations. The proposed work investigates the use of the deep learning models to enhance the automatic, machine-based readability of gauge in smoky environments, with accurate gauge data interpretation serving as a valuable tool for first responders. The study utilizes two deep learning architectures, FFA-Net and AECR-Net, to improve the visibility of gauge images, corrupted with light up to dense haze and smoke. Since benchmark datasets of analog gauge images are unavailable, a new synthetic dataset, containing over 14,000 images, was generated using the Unreal Engine. The models were trained with an 80\% train, 10\% validation, and 10\% test split for the haze and smoke dataset, respectively. For the synthetic haze dataset, the SSIM and PSNR metrics are about 0.98 and 43\,dB, respectively, comparing well to state-of-the art results. Additionally, more robust results are retrieved from the AECR-Net, when compared to the FFA-Net. Although the results from the synthetic smoke dataset are poorer, the trained models achieve interesting results. In general, imaging in the presence of smoke are more difficult to enhance given the inhomogeneity and high density. Secondly, FFA-Net and AECR-Net are implemented to dehaze and not to desmoke images. This work shows that use of deep learning architectures can improve the quality of analog gauge images captured in smoke and haze scenes immensely. Finally, the enhanced output images can be successfully post-processed for automatic autonomous reading of gauges

</details>


### [43] [Inference-time Physics Alignment of Video Generative Models with Latent World Models](https://arxiv.org/abs/2601.10553)
*Jianhao Yuan,Xiaofeng Zhang,Felix Friedrich,Nicolas Beltran-Velez,Melissa Hall,Reyhane Askari-Hemmat,Xiaochuang Han,Nicolas Ballas,Michal Drozdzal,Adriana Romero-Soriano*

Main category: cs.CV

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: State-of-the-art video generative models produce promising visual content yet often violate basic physics principles, limiting their utility. While some attribute this deficiency to insufficient physics understanding from pre-training, we find that the shortfall in physics plausibility also stems from suboptimal inference strategies. We therefore introduce WMReward and treat improving physics plausibility of video generation as an inference-time alignment problem. In particular, we leverage the strong physics prior of a latent world model (here, VJEPA-2) as a reward to search and steer multiple candidate denoising trajectories, enabling scaling test-time compute for better generation performance. Empirically, our approach substantially improves physics plausibility across image-conditioned, multiframe-conditioned, and text-conditioned generation settings, with validation from human preference study. Notably, in the ICCV 2025 Perception Test PhysicsIQ Challenge, we achieve a final score of 62.64%, winning first place and outperforming the previous state of the art by 7.42%. Our work demonstrates the viability of using latent world models to improve physics plausibility of video generation, beyond this specific instantiation or parameterization.

</details>


### [44] [DeepUrban: Interaction-Aware Trajectory Prediction and Planning for Automated Driving by Aerial Imagery](https://arxiv.org/abs/2601.10554)
*Constantin Selzer,Fabian B. Flohr*

Main category: cs.CV

TL;DR: DeepUrban数据集通过无人机采集密集城市交通场景，丰富现有自动驾驶预测和规划基准，显著提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 当前自动驾驶预测和规划基准缺乏密集交通场景，限制了复杂交互建模能力，需要更具挑战性的数据集来推动技术进步。

Method: 与DeepScenario合作，使用无人机在约100米高度采集城市交叉口高分辨率图像，构建包含3D交通对象、地图和场景信息的DeepUrban数据集。

Result: 在nuScenes基础上添加DeepUrban数据后，车辆预测和规划准确性显著提升，ADE/FDE指标改进高达44.1%/44.3%。

Conclusion: DeepUrban为密集城市交通场景提供了更全面的基准，证明了丰富数据对提升自动驾驶系统预测和规划能力的重要性。

Abstract: The efficacy of autonomous driving systems hinges critically on robust prediction and planning capabilities. However, current benchmarks are impeded by a notable scarcity of scenarios featuring dense traffic, which is essential for understanding and modeling complex interactions among road users. To address this gap, we collaborated with our industrial partner, DeepScenario, to develop DeepUrban-a new drone dataset designed to enhance trajectory prediction and planning benchmarks focusing on dense urban settings. DeepUrban provides a rich collection of 3D traffic objects, extracted from high-resolution images captured over urban intersections at approximately 100 meters altitude. The dataset is further enriched with comprehensive map and scene information to support advanced modeling and simulation tasks. We evaluate state-of-the-art (SOTA) prediction and planning methods, and conducted experiments on generalization capabilities. Our findings demonstrate that adding DeepUrban to nuScenes can boost the accuracy of vehicle predictions and planning, achieving improvements up to 44.1 % / 44.3% on the ADE / FDE metrics. Website: https://iv.ee.hm.edu/deepurban

</details>


### [45] [Jordan-Segmentable Masks: A Topology-Aware definition for characterizing Binary Image Segmentation](https://arxiv.org/abs/2601.10577)
*Serena Grazia De Benedictis,Amedeo Altavilla,Nicoletta Del Buono*

Main category: cs.CV

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Image segmentation plays a central role in computer vision. However, widely used evaluation metrics, whether pixel-wise, region-based, or boundary-focused, often struggle to capture the structural and topological coherence of a segmentation. In many practical scenarios, such as medical imaging or object delineation, small inaccuracies in boundary, holes, or fragmented predictions can result in high metric scores, despite the fact that the resulting masks fail to preserve the object global shape or connectivity. This highlights a limitation of conventional metrics: they are unable to assess whether a predicted segmentation partitions the image into meaningful interior and exterior regions.
  In this work, we introduce a topology-aware notion of segmentation based on the Jordan Curve Theorem, and adapted for use in digital planes. We define the concept of a \emph{Jordan-segmentatable mask}, which is a binary segmentation whose structure ensures a topological separation of the image domain into two connected components. We analyze segmentation masks through the lens of digital topology and homology theory, extracting a $4$-curve candidate from the mask, verifying its topological validity using Betti numbers. A mask is considered Jordan-segmentatable when this candidate forms a digital 4-curve with $β_0 = β_1 = 1$, or equivalently when its complement splits into exactly two $8$-connected components.
  This framework provides a mathematically rigorous, unsupervised criterion with which to assess the structural coherence of segmentation masks. By combining digital Jordan theory and homological invariants, our approach provides a valuable alternative to standard evaluation metrics, especially in applications where topological correctness must be preserved.

</details>


### [46] [Action100M: A Large-scale Video Action Dataset](https://arxiv.org/abs/2601.10592)
*Delong Chen,Tejaswi Kasarla,Yejin Bang,Mustafa Shukor,Willy Chung,Jade Yu,Allen Bolourchi,Theo Moutakanni,Pascale Fung*

Main category: cs.CV

TL;DR: Action100M 是一个基于 120 万互联网教学视频构建的大规模开放词汇视频动作数据集，包含约 1 亿个时序分割段，通过全自动流水线生成结构化标注，用于视频理解和世界建模研究。


<details>
  <summary>Details</summary>
Motivation: 从视觉观察中推理物理动作是推动机器智能在物理世界中发展的关键能力，这需要大规模、开放词汇的视频动作数据集来覆盖广泛领域。

Method: 采用全自动流水线：(1) 使用 V-JEPA 2 嵌入进行分层时序分割，(2) 生成组织为树状结构的帧层和段层多级字幕，(3) 利用推理模型（GPT-OSS-120B）通过多轮自我精炼聚合证据，输出结构化标注（简要/详细动作、执行者、简要/详细字幕）。

Result: 在 Action100M 上训练 VL-JEPA 显示出持续的数据规模提升效益，并在多种动作识别基准测试中实现了强大的零样本性能。

Conclusion: Action100M 为视频理解和世界建模的可扩展研究建立了新的基础。

Abstract: Inferring physical actions from visual observations is a fundamental capability for advancing machine intelligence in the physical world. Achieving this requires large-scale, open-vocabulary video action datasets that span broad domains. We introduce Action100M, a large-scale dataset constructed from 1.2M Internet instructional videos (14.6 years of duration), yielding O(100 million) temporally localized segments with open-vocabulary action supervision and rich captions. Action100M is generated by a fully automated pipeline that (i) performs hierarchical temporal segmentation using V-JEPA 2 embeddings, (ii) produces multi-level frame and segment captions organized as a Tree-of-Captions, and (iii) aggregates evidence with a reasoning model (GPT-OSS-120B) under a multi-round Self-Refine procedure to output structured annotations (brief/detailed action, actor, brief/detailed caption). Training VL-JEPA on Action100M demonstrates consistent data-scaling improvements and strong zero-shot performance across diverse action recognition benchmarks, establishing Action100M as a new foundation for scalable research in video understanding and world modeling.

</details>


### [47] [RSATalker: Realistic Socially-Aware Talking Head Generation for Multi-Turn Conversation](https://arxiv.org/abs/2601.10606)
*Peng Chen,Xiaobao Wei,Yi Yang,Naiming Yao,Hui Chen,Feng Tian*

Main category: cs.CV

TL;DR: RSATalker：首个基于3D高斯溅射的社交感知对话头像生成框架，支持多轮对话，在真实感和社交意识方面表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有方法存在局限：基于网格的3D方法能处理双人对话但缺少真实纹理；基于大模型的2D方法纹理自然但计算成本过高；3DGS方法效率高但仅限于说话者且忽略社交关系。虚拟现实中社交场景需要既高效又自然的多轮对话生成方案。

Method: 首先从语音驱动网格3D面部运动，然后将3D高斯绑定到网格面来渲染2D头像视频。提出社交感知模块，通过可学习查询机制将社会关系（血缘/非血缘、平等/不平等）编码为高层嵌入。设计三阶段训练范式并构建带社会关系标注的语音-网格-图像三元组数据集。

Result: 大量实验证明RSATalker在真实感和社交意识方面达到最优性能。

Conclusion: RSATalker是首个利用3DGS实现真实社交感知对话头像生成的框架，优于现有方法。代码和数据集将开源。

Abstract: Talking head generation is increasingly important in virtual reality (VR), especially for social scenarios involving multi-turn conversation. Existing approaches face notable limitations: mesh-based 3D methods can model dual-person dialogue but lack realistic textures, while large-model-based 2D methods produce natural appearances but incur prohibitive computational costs. Recently, 3D Gaussian Splatting (3DGS) based methods achieve efficient and realistic rendering but remain speaker-only and ignore social relationships. We introduce RSATalker, the first framework that leverages 3DGS for realistic and socially-aware talking head generation with support for multi-turn conversation. Our method first drives mesh-based 3D facial motion from speech, then binds 3D Gaussians to mesh facets to render high-fidelity 2D avatar videos. To capture interpersonal dynamics, we propose a socially-aware module that encodes social relationships, including blood and non-blood as well as equal and unequal, into high-level embeddings through a learnable query mechanism. We design a three-stage training paradigm and construct the RSATalker dataset with speech-mesh-image triplets annotated with social relationships. Extensive experiments demonstrate that RSATalker achieves state-of-the-art performance in both realism and social awareness. The code and dataset will be released.

</details>


### [48] [Molmo2: Open Weights and Data for Vision-Language Models with Video Understanding and Grounding](https://arxiv.org/abs/2601.10611)
*Christopher Clark,Jieyu Zhang,Zixian Ma,Jae Sung Park,Mohammadreza Salehi,Rohun Tripathi,Sangho Lee,Zhongzheng Ren,Chris Dongjoo Kim,Yinuo Yang,Vincent Shao,Yue Yang,Weikai Huang,Ziqi Gao,Taira Anderson,Jianrui Zhang,Jitesh Jain,George Stoica,Winson Han,Ali Farhadi,Ranjay Krishna*

Main category: cs.CV

TL;DR: Molmo2是一个开源视频语言模型家族，在视频理解与像素级定位任务上达到SOTA，通过构建无闭源模型污染的数据集和创新的训练方法实现突破。


<details>
  <summary>Details</summary>
Motivation: 当前最强的视频语言模型都是闭源的，开源模型要么依赖闭源模型蒸馏的合成数据，要么不公开训练数据和方法，导致开源社区缺乏改进视频/图像语言模型的基础。同时，现有模型缺乏像素级定位能力。

Method: 构建了7个新的视频数据集和2个多图像数据集（包括高清视频描述、自由形式视频问答、复杂查询对象跟踪、创新视频指向数据集）。提出高效打包和消息树编码的训练方案，使用双向视觉token注意力和新颖的token权重策略。

Result: 8B模型在短视频、计数和描述任务上优于同类开源模型，在长视频任务上有竞争力。在视频定位任务上显著优于Qwen3-VL等开源模型（视频计数准确率35.5 vs 29.6），在某些任务上甚至超越Gemini 3 Pro等闭源模型（视频指向F1 38.4 vs 20.0，视频跟踪J&F 56.2 vs 41.1）。

Conclusion: Molmo2系列模型展示了通过高质量开源数据集和创新训练方法，可以在视频语言理解特别是像素级定位任务上达到甚至超越闭源模型的性能，为开源社区提供了重要的基础工具。

Abstract: Today's strongest video-language models (VLMs) remain proprietary. The strongest open-weight models either rely on synthetic data from proprietary VLMs, effectively distilling from them, or do not disclose their training data or recipe. As a result, the open-source community lacks the foundations needed to improve on the state-of-the-art video (and image) language models. Crucially, many downstream applications require more than just high-level video understanding; they require grounding -- either by pointing or by tracking in pixels. Even proprietary models lack this capability. We present Molmo2, a new family of VLMs that are state-of-the-art among open-source models and demonstrate exceptional new capabilities in point-driven grounding in single image, multi-image, and video tasks. Our key contribution is a collection of 7 new video datasets and 2 multi-image datasets, including a dataset of highly detailed video captions for pre-training, a free-form video Q&A dataset for fine-tuning, a new object tracking dataset with complex queries, and an innovative new video pointing dataset, all collected without the use of closed VLMs. We also present a training recipe for this data utilizing an efficient packing and message-tree encoding scheme, and show bi-directional attention on vision tokens and a novel token-weight strategy improves performance. Our best-in-class 8B model outperforms others in the class of open weight and data models on short videos, counting, and captioning, and is competitive on long-videos. On video-grounding Molmo2 significantly outperforms existing open-weight models like Qwen3-VL (35.5 vs 29.6 accuracy on video counting) and surpasses proprietary models like Gemini 3 Pro on some tasks (38.4 vs 20.0 F1 on video pointing and 56.2 vs 41.1 J&F on video tracking).

</details>


### [49] [CoMoVi: Co-Generation of 3D Human Motions and Realistic Videos](https://arxiv.org/abs/2601.10632)
*Chengfeng Zhao,Jiazhi Shu,Yubo Zhao,Tianyu Huang,Jiahao Lu,Zekai Gu,Chengwei Ren,Zhiyang Dou,Qing Shuai,Yuan Liu*

Main category: cs.CV

TL;DR: CoMoVi框架通过耦合两个视频扩散模型，在单个扩散去噪循环中同步生成3D人体运动和2D视频，实现了运动与视频生成的协同优化。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常独立生成3D人体运动和2D视频，忽视了它们之间的内在耦合关系：3D运动提供空间结构先验确保合理性，而预训练视频模型提供强大的泛化能力用于运动生成。

Method: 1) 提出有效的2D人体运动表示以继承预训练VDMs的先验；2) 设计双分支扩散模型，通过特征交互和3D-2D交叉注意力耦合运动与视频生成过程；3) 构建CoMoVi数据集，包含带文本和运动标注的大规模真实世界人体视频。

Result: 广泛的实验表明，该方法在3D人体运动和视频生成任务中均取得了优异性能。

Conclusion: 3D人体运动与2D视频生成是内在耦合的，CoMoVi框架通过同步生成这两个要素，实现了两者相互增强的协同效果，在两项任务上均表现优越。

Abstract: In this paper, we find that the generation of 3D human motions and 2D human videos is intrinsically coupled. 3D motions provide the structural prior for plausibility and consistency in videos, while pre-trained video models offer strong generalization capabilities for motions, which necessitate coupling their generation processes. Based on this, we present CoMoVi, a co-generative framework that couples two video diffusion models (VDMs) to generate 3D human motions and videos synchronously within a single diffusion denoising loop. To achieve this, we first propose an effective 2D human motion representation that can inherit the powerful prior of pre-trained VDMs. Then, we design a dual-branch diffusion model to couple human motion and video generation process with mutual feature interaction and 3D-2D cross attentions. Moreover, we curate CoMoVi Dataset, a large-scale real-world human video dataset with text and motion annotations, covering diverse and challenging human motions. Extensive experiments demonstrate the effectiveness of our method in both 3D human motion and video generation tasks.

</details>


### [50] [CURVE: A Benchmark for Cultural and Multilingual Long Video Reasoning](https://arxiv.org/abs/2601.10649)
*Darshan Singh,Arsha Nagrani,Kawshik Manikantan,Harman Singh,Dinesh Tewari,Tobias Weyand,Cordelia Schmid,Anelia Angelova,Shachi Dave*

Main category: cs.CV

TL;DR: 提出CURVE基准，聚焦文化和语言多样性挑战，评估视频模型跨文化理解能力，发现现有模型因文化感知不足显著落后于人类水平。


<details>
  <summary>Details</summary>
Motivation: 现有视频理解基准以西方为中心且多为英语，存在显著偏见，需建立多元文化、多语言的评估体系以推动模型公平发展。

Method: 构建CURVE基准，包含18个地区的文化视频及全人工标注的问题、答案与多步推理链；利用推理轨迹构建证据图，提出迭代策略以识别细粒度推理错误。

Result: 当前最先进的视频-大语言模型在CURVE上表现显著低于人类水平，主要错误源于对文化元素的视觉感知不足。

Conclusion: CURVE揭示了模型在跨文化理解上的局限性，强调需提升视觉文化感知能力，为公平评估视频模型提供重要基准。

Abstract: Recent advancements in video models have shown tremendous progress, particularly in long video understanding. However, current benchmarks predominantly feature western-centric data and English as the dominant language, introducing significant biases in evaluation. To address this, we introduce CURVE (Cultural Understanding and Reasoning in Video Evaluation), a challenging benchmark for multicultural and multilingual video reasoning. CURVE comprises high-quality, entirely human-generated annotations from diverse, region-specific cultural videos across 18 global locales. Unlike prior work that relies on automatic translations, CURVE provides complex questions, answers, and multi-step reasoning steps, all crafted in native languages. Making progress on CURVE requires a deeply situated understanding of visual cultural context. Furthermore, we leverage CURVE's reasoning traces to construct evidence-based graphs and propose a novel iterative strategy using these graphs to identify fine-grained errors in reasoning. Our evaluations reveal that SoTA Video-LLMs struggle significantly, performing substantially below human-level accuracy, with errors primarily stemming from the visual perception of cultural elements. CURVE will be publicly available under https://github.com/google-deepmind/neptune?tab=readme-ov-file\#minerva-cultural

</details>


### [51] [A continental-scale dataset of ground beetles with high-resolution images and validated morphological trait measurements](https://arxiv.org/abs/2601.10687)
*S M Rayeed,Mridul Khurana,Alyson East,Isadora E. Fluck,Elizabeth G. Campolongo,Samuel Stevens,Iuliia Zarubiieva,Scott C. Lowe,Michael W. Denslow,Evan D. Donoso,Jiaman Wu,Michelle Ramirez,Benjamin Baiser,Charles V. Stewart,Paula Mabee,Tanya Berger-Wolf,Anuj Karpatne,Hilmar Lapp,Robert P. Guralnick,Graham W. Taylor,Sydne Record*

Main category: cs.CV

TL;DR: 本研究通过数字化NEON收集的13200多个步甲标本，填补了全球性状数据库中无脊椎动物代表性不足的空白，为AI驱动的自动性状提取和生物多样性监测提供基础。


<details>
  <summary>Details</summary>
Motivation: 无脊椎动物在全球性状数据库中代表性严重不足，限制了高多样性类群如步甲的全面生态分析。NEON虽有大量步甲标本收藏，但主要作为实体收藏存在，限制了广泛的研究访问和大规模分析。

Method: 数字化来自美国大陆和夏威夷30个站点的13200多个NEON步甲标本，包括高分辨率成像和鞘翅长度与宽度的数字化测量，建立了基于AI的自动性状提取基础。

Result: 数字性状提取达到了亚毫米精度，并与人工测量结果进行了验证，确保了生态和计算研究的可靠性。

Conclusion: 这项研究通过填补无脊椎动物在性状数据库中的代表性空白，支持AI驱动的自动物种识别和基于性状的研究，推动了生物多样性监测和保护的进展。

Abstract: Despite the ecological significance of invertebrates, global trait databases remain heavily biased toward vertebrates and plants, limiting comprehensive ecological analyses of high-diversity groups like ground beetles. Ground beetles (Coleoptera: Carabidae) serve as critical bioindicators of ecosystem health, providing valuable insights into biodiversity shifts driven by environmental changes. While the National Ecological Observatory Network (NEON) maintains an extensive collection of carabid specimens from across the United States, these primarily exist as physical collections, restricting widespread research access and large-scale analysis. To address these gaps, we present a multimodal dataset digitizing over 13,200 NEON carabids from 30 sites spanning the continental US and Hawaii through high-resolution imaging, enabling broader access and computational analysis. The dataset includes digitally measured elytra length and width of each specimen, establishing a foundation for automated trait extraction using AI. Validated against manual measurements, our digital trait extraction achieves sub-millimeter precision, ensuring reliability for ecological and computational studies. By addressing invertebrate under-representation in trait databases, this work supports AI-driven tools for automated species identification and trait-based research, fostering advancements in biodiversity monitoring and conservation.

</details>


### [52] [See Less, Drive Better: Generalizable End-to-End Autonomous Driving via Foundation Models Stochastic Patch Selection](https://arxiv.org/abs/2601.10707)
*Amir Mallak,Erfan Aasi,Shiva Sreeram,Tsun-Hsuan Wang,Daniela Rus,Alaa Maalouf*

Main category: cs.CV

TL;DR: 本文提出了一种名为随机补丁选择（SPS）的新方法，通过随机屏蔽部分图像补丁特征来减轻基础模型特征冗余导致的过拟合，从而提升端到端自动驾驶策略在分布外（OOD）场景下的鲁棒性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有方法使用基础模型提取的图像补丁特征存在高度冗余（自注意力机制导致补丁间信息重叠），这会使策略学习到虚假相关性，在分布外场景中泛化能力较差。作者旨在通过减少特征冗余来提升自动驾驶策略的鲁棒性。

Method: 提出的SPS方法在每个训练帧中随机屏蔽一部分补丁描述符（同时保留剩余补丁的空间布局），迫使策略基于不同的随机但完整的场景视图进行决策。这样策略学习到的特征对具体哪些标记保留具有不变性。

Result: 实验表明SPS在所有OOD场景中都优于现有最佳方法：平均提升6.2%，闭环仿真最高提升20.4%，推理速度快2.4倍。9个系统中有8个超越先前SOTA，且学习到的策略无需调整即可迁移到真实物理汽车上。

Conclusion: 随机补丁选择能有效减轻特征冗余导致的过拟合，显著提升端到端自动驾驶策略的OOD鲁棒性、泛化能力和效率，证明该方法具有实际部署价值。

Abstract: Recent advances in end-to-end autonomous driving show that policies trained on patch-aligned features extracted from foundation models generalize better to Out-of-Distribution (OOD). We hypothesize that due to the self-attention mechanism, each patch feature implicitly embeds/contains information from all other patches, represented in a different way and intensity, making these descriptors highly redundant. We quantify redundancy in such (BLIP2) features via PCA and cross-patch similarity: $90$% of variance is captured by $17/64$ principal components, and strong inter-token correlations are pervasive. Training on such overlapping information leads the policy to overfit spurious correlations, hurting OOD robustness. We present Stochastic-Patch-Selection (SPS), a simple yet effective approach for learning policies that are more robust, generalizable, and efficient. For every frame, SPS randomly masks a fraction of patch descriptors, not feeding them to the policy model, while preserving the spatial layout of the remaining patches. Thus, the policy is provided with different stochastic but complete views of the (same) scene: every random subset of patches acts like a different, yet still sensible, coherent projection of the world. The policy thus bases its decisions on features that are invariant to which specific tokens survive. Extensive experiments confirm that across all OOD scenarios, our method outperforms the state of the art (SOTA), achieving a $6.2$% average improvement and up to $20.4$% in closed-loop simulations, while being $2.4\times$ faster. We conduct ablations over masking rates and patch-feature reorganization, training and evaluating 9 systems, with 8 of them surpassing prior SOTA. Finally, we show that the same learned policy transfers to a physical, real-world car without any tuning.

</details>


### [53] [From One-to-One to Many-to-Many: Dynamic Cross-Layer Injection for Deep Vision-Language Fusion](https://arxiv.org/abs/2601.10710)
*Cheng Chen,Yuyu Guo,Pengpeng Zeng,Jingkuan Song,Peng Di,Hang Yu,Lianli Gao*

Main category: cs.CV

TL;DR: 本文提出Cross-Layer Injection（CLI）框架，通过动态多对多视觉-语言连接解决传统VLM中视觉特征瓶颈问题，显著提升多模态理解性能。


<details>
  <summary>Details</summary>
Motivation: 传统视觉语言模型（VLMs）存在严重的视觉特征瓶颈，它们采用简单的非对称连接，仅将视觉编码器输出链接到大语言模型输入。这种静态架构限制了LLM与分层视觉知识的全面对齐能力，无法准确整合局部细节和全局语义进行连贯推理。

Method: 提出Cross-Layer Injection（CLI）框架，包含两个参数高效的协同组件：1）Adaptive Multi-Projection（AMP）模块，协调来自不同视觉层的特征；2）Adaptive Gating Fusion（AGF）机制，使LLM能基于实时解码上下文选择性地注入最相关的视觉信息。

Result: 将CLI集成到LLaVA-OneVision和LLaVA-1.5中，在18个多样化基准测试上进行了广泛实验，展示了显著的性能提升。

Conclusion: CLI作为一种可扩展的范式，通过赋予LLM按需访问完整视觉层次的能力，解锁了更深层次的多模态理解。

Abstract: Vision-Language Models (VLMs) create a severe visual feature bottleneck by using a crude, asymmetric connection that links only the output of the vision encoder to the input of the large language model (LLM). This static architecture fundamentally limits the ability of LLMs to achieve comprehensive alignment with hierarchical visual knowledge, compromising their capacity to accurately integrate local details with global semantics into coherent reasoning. To resolve this, we introduce Cross-Layer Injection (CLI), a novel and lightweight framework that forges a dynamic many-to-many bridge between the two modalities. CLI consists of two synergistic, parameter-efficient components: an Adaptive Multi-Projection (AMP) module that harmonizes features from diverse vision layers, and an Adaptive Gating Fusion (AGF) mechanism that empowers the LLM to selectively inject the most relevant visual information based on its real-time decoding context. We validate the effectiveness and versatility of CLI by integrating it into LLaVA-OneVision and LLaVA-1.5. Extensive experiments on 18 diverse benchmarks demonstrate significant performance improvements, establishing CLI as a scalable paradigm that unlocks deeper multimodal understanding by granting LLMs on-demand access to the full visual hierarchy.

</details>


### [54] [Alterbute: Editing Intrinsic Attributes of Objects in Images](https://arxiv.org/abs/2601.10714)
*Tal Reiss,Daniel Winter,Matan Cohen,Alex Rav-Acha,Yael Pritch,Ariel Shamir,Yedid Hoshen*

Main category: cs.CV

TL;DR: 提出了Alterbute方法，使用扩散模型编辑图像中物体的内在属性（颜色、纹理、材质、形状），同时保持物体身份和场景上下文。


<details>
  <summary>Details</summary>
Motivation: 现有方法要么依赖无监督先验但难以保持物体身份，要么使用限制性监督阻碍内在属性变化。需要能在保持物体身份的同时改变其内在属性的编辑方法。

Method: 1) 使用宽松训练目标：模型可根据身份参考图像、目标属性文本描述、背景图像和对象掩码改变内在和外在属性，推理时通过重用原始背景和掩码限制外在变化。2) 引入视觉命名实体（VNEs）作为细粒度视觉身份类别，使用视觉语言模型从大图像数据集中自动提取VNE标签和属性描述进行可扩展的监督学习。

Result: Alterbute在保持身份的同时进行物体内在属性编辑方面优于现有方法。

Conclusion: 通过结合宽松训练目标和VNE监督，实现了在保持物体身份和场景上下文的同时有效编辑物体内在属性的方法。

Abstract: We introduce Alterbute, a diffusion-based method for editing an object's intrinsic attributes in an image. We allow changing color, texture, material, and even the shape of an object, while preserving its perceived identity and scene context. Existing approaches either rely on unsupervised priors that often fail to preserve identity or use overly restrictive supervision that prevents meaningful intrinsic variations. Our method relies on: (i) a relaxed training objective that allows the model to change both intrinsic and extrinsic attributes conditioned on an identity reference image, a textual prompt describing the target intrinsic attributes, and a background image and object mask defining the extrinsic context. At inference, we restrict extrinsic changes by reusing the original background and object mask, thereby ensuring that only the desired intrinsic attributes are altered; (ii) Visual Named Entities (VNEs) - fine-grained visual identity categories (e.g., ''Porsche 911 Carrera'') that group objects sharing identity-defining features while allowing variation in intrinsic attributes. We use a vision-language model to automatically extract VNE labels and intrinsic attribute descriptions from a large public image dataset, enabling scalable, identity-preserving supervision. Alterbute outperforms existing methods on identity-preserving object intrinsic attribute editing.

</details>


### [55] [WildRayZer: Self-supervised Large View Synthesis in Dynamic Environments](https://arxiv.org/abs/2601.10716)
*Xuweiyi Chen,Wentao Zhou,Zezhou Cheng*

Main category: cs.CV

TL;DR: WildRayZer是一个自监督框架，用于解决动态环境下的新视角合成问题。通过分析合成测试识别并处理动态区域，利用残差构建伪运动掩码，并通过掩码输入令牌和门控损失梯度来专注于背景补全。


<details>
  <summary>Details</summary>
Motivation: 动态环境打破了静态新视角合成模型依赖的多视角一致性，导致鬼影、虚幻几何和不稳定的姿态估计。当前方法难以有效处理相机和物体同时运动的情况。

Method: 1) 使用仅相机静态渲染器解释刚性结构，从其残差中识别瞬态区域；2) 从残差构建伪运动掩码；3) 蒸馏运动估计器，用其掩码输入令牌并门控损失梯度，使监督专注于跨视角背景补全。还整理了动态数据集D-RE10K用于训练和评估。

Result: WildRayZer在瞬态区域去除和全帧新视角合成质量方面持续优于基于优化的和前馈基线方法，仅需单次前馈传递即可完成。

Conclusion: 该方法通过分析合成测试有效处理动态新视角合成，提出的自监督框架在真实世界动态场景中表现优越，为动态环境下的新视角合成提供了实用解决方案。

Abstract: We present WildRayZer, a self-supervised framework for novel view synthesis (NVS) in dynamic environments where both the camera and objects move. Dynamic content breaks the multi-view consistency that static NVS models rely on, leading to ghosting, hallucinated geometry, and unstable pose estimation. WildRayZer addresses this by performing an analysis-by-synthesis test: a camera-only static renderer explains rigid structure, and its residuals reveal transient regions. From these residuals, we construct pseudo motion masks, distill a motion estimator, and use it to mask input tokens and gate loss gradients so supervision focuses on cross-view background completion. To enable large-scale training and evaluation, we curate Dynamic RealEstate10K (D-RE10K), a real-world dataset of 15K casually captured dynamic sequences, and D-RE10K-iPhone, a paired transient and clean benchmark for sparse-view transient-aware NVS. Experiments show that WildRayZer consistently outperforms optimization-based and feed-forward baselines in both transient-region removal and full-frame NVS quality with a single feed-forward pass.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [56] [Starfield: Demand-Aware Satellite Topology Design for Low-Earth Orbit Mega Constellations](https://arxiv.org/abs/2601.10083)
*Shayan Hamidi Dehshali,Tzu-Hsuan Liao,Shaileshh Bojja Venkatakrishnan*

Main category: cs.NI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Low-Earth orbit (LEO) mega-constellations are emerging as high-capacity backbones for next-generation Internet. Deployment of laser terminals enables high-bandwidth, low-latency inter-satellite links (ISLs); however, their limited number, slow acquisition, and instability make forming a stable satellite topology difficult. Existing patterns like +Grid and Motif ignore regional traffic, ground station placement, and constellation geometry. Given sparse population distribution on Earth and the isolation of rural areas, traffic patterns are inherently non-uniform, providing an opportunity to orient inter-satellite links (ISLs) according to these traffic patterns. In this paper, we propose Starfield, a novel demand-aware satellite topology design heuristic algorithm supported by mathematical analysis. We first formulate a vector field on the constellation's shell according to traffic flows and define a corresponding Riemannian metric on the spherical manifold of the shell. The metric, combined with the spatial geometry, is used to assign a distance to each potential ISL, which we then aggregate over all demand flows to generate a heuristic for each satellite's link selection. Inspired by +Grid, each satellite selects the link with the minimum Riemannian heuristic along with its corresponding angular links. To evaluate Starfield, we developed a custom, link-aware, and link-configurable packet-level simulator, comparing it against +Grid and Random topologies. For the Phase 1 Starlink, simulation results show up to a 30% reduction in hop count and a 15% improvement in stretch factor across multiple traffic distributions. Moreover, static Starfield, an inter-orbital link matching modification of Starfield, achieves a 20% improvement in stretch factor under realistic traffic patterns compared to +Grid. Experiments further demonstrate Starfield's robustness under traffic demand perturbations.

</details>


### [57] [SDN-Driven Innovations in MANETs and IoT: A Path to Smarter Networks](https://arxiv.org/abs/2601.10544)
*Andrea Piroddi,Riccardo Fonti*

Main category: cs.NI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Mobile Ad Hoc Networks (MANETs) and Internet of Things (IoT) networks operate in decentralized and dynamic environments, making them ideal for scenarios lacking traditional infrastructure. However, these networks face challenges such as inefficient routing, limited scalability, and security vulnerabilities due to their decentralized nature and resource constraints. This paper explores the integration of Software-Defined Networking (SDN) as a unified solution that leverages its centralized control and network programmability to improve routing, resource management, and security. A mathematical model evaluates the impact of SDN integration on Capital Expenditure (CAPEX), Operational Expenditure (OPEX), and performance metrics. Results demonstrate that SDN-enhanced MANETs and IoT networks offer superior scalability, reduced latency, increased throughput, and lower packet loss, especially in dynamic and large-scale environments. While SDN introduces computational overhead, it significantly enhances routing efficiency, resource optimization, and adaptability. The proposed framework provides a robust and scalable solution, enabling the development of network architectures that efficiently manage growing node densities, dynamic topologies, and high data traffic. This approach ensures resilience, making it well-suited to meet the performance and reliability demands of modern, large-scale applications.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [58] [High signal-to-noise ratio asymptotics of entropy-constrained Gaussian channel capacity](https://arxiv.org/abs/2601.09864)
*Adway Girish,Shlomo Shamai,Emre Telatar*

Main category: cs.IT

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: We study the input-entropy-constrained Gaussian channel capacity problem in the asymptotic high signal-to-noise ratio (SNR) regime. We show that the capacity-achieving distribution as SNR goes to infinity is given by a discrete Gaussian distribution supported on a scaled integer lattice. Further, we show that the gap between the input entropy and the capacity decreases to zero exponentially in SNR, and characterize this exponent.

</details>


### [59] [One-Cold Poisson Channel: A Simple Continuous-Time Channel with Zero Dispersion](https://arxiv.org/abs/2601.09894)
*Cheuk Ting Li*

Main category: cs.IT

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: We introduce the one-cold Poisson channel (OCPC), where the transmitter chooses one of several frequency bands to attenuate at a time. In particular, the perfect OCPC, where the number of bands is unlimited, is an extremely simple continuous-time memoryless channel. It has a capacity 1, zero channel dispersion, and an information spectrum being the degenerate distribution at 1. It is the only known nontrivial (discrete or continuous-time) memoryless channel with a closed-form formula for its optimal non-asymptotic error probability, making it the simplest channel in this sense. A potential application is optical communication with a tunable band rejection filter. Due to its simplicity, we may use it as a basic currency of information that is infinitely divisible, as an alternative to bits which are not infinitely divisible. OCPC with perfect feedback gives a generalization of prefix codes. We also study non-asymptotic coding and channel simulation results for the general OCPC.

</details>


### [60] [Reconstructing Reed-Solomon Codes from Multiple Noisy Channel Outputs](https://arxiv.org/abs/2601.09947)
*Shubhransh Singhvi,Han Mao Kiah,Eitan Yaakobi*

Main category: cs.IT

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: The sequence reconstruction problem, introduced by Levenshtein in 2001, considers a communication setting in which a sender transmits a codeword and the receiver observes K independent noisy versions of this codeword. In this work, we study the problem of efficient reconstruction when each of the $K$ outputs is corrupted by a $q$-ary discrete memoryless symmetric (DMS) substitution channel with substitution probability $p$. Focusing on Reed-Solomon (RS) codes, we adapt the Koetter-Vardy soft-decision decoding algorithm to obtain an efficient reconstruction algorithm. For sufficiently large blocklength and alphabet size, we derive an explicit rate threshold, depending only on $(p, K)$, such that the transmitted codeword can be reconstructed with arbitrarily small probability of error whenever the code rate $R$ lies below this threshold.

</details>


### [61] [Private Information Retrieval for Graph-based Replication with Minimal Subpacketization](https://arxiv.org/abs/2601.09957)
*Vayur Shanbhag,Prasad Krishnan*

Main category: cs.IT

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: We design new minimal-subpacketization schemes for information-theoretic private information retrieval on graph-based replicated databases. In graph-based replication, the system consists of $K$ files replicated across $N$ servers according to a graph with $N$ vertices and $K$ edges. The client wants to retrieve one desired file, while keeping the index of the desired file private from each server via a query-response protocol. We seek PIR protocols that have (a) high rate, which is the ratio of the file-size to the total download cost, and (b) low subpacketization, which acts as a constraint on the size of the files for executing the protocol. We report two new schemes which have unit-subpacketization (which is minimal): (i) for a special class of graphs known as star graphs, and (ii) for general graphs. Our star-graph scheme has a better rate than previously known schemes with low subpacketization for general star graphs. Our scheme for general graphs uses a decomposition of the graph via independent sets. This scheme achieves a rate lower than prior schemes for the complete graph, however it can achieve higher rates than known for some specific graph classes. An extension of our scheme to the case of multigraphs achieves a higher rate than previous schemes for the complete multi-graph.

</details>


### [62] [On the Leaky Private Information Retrieval with Side Information](https://arxiv.org/abs/2601.09960)
*Yingying Huangfu,Tian Bai*

Main category: cs.IT

TL;DR: 本文研究了带有边信息的泄漏隐私的私有信息检索（L-PIR-SI）问题，通过放松完美隐私要求以提高通信效率。作者提出了统一的概率框架来构建隐私泄漏量化方案，量化了可实现的下载成本，并建立了泄漏、边信息和检索效率之间的权衡关系。


<details>
  <summary>Details</summary>
Motivation: 在私有信息检索（PIR）领域，虽然对带有边信息的PIR-SI在不同隐私定义下的容量已经有了部分探索，但控制信息泄漏在这些设置中的影响尚未得到解决。研究人员希望通过放松完美隐私要求来改善通信效率，在存在边信息的场景下实现更高效率的信息检索。

Method: 研究人员提出了一个统一的概率框架来构建L-PIR-SI方案，其中隐私泄漏通过参数ε进行量化，与差分隐私标准保持一致。该框架能够系统地评估在各种隐私泄漏水平下的性能表现。

Result: 该文量化了可实现的下载成本，并展示了研究结果的广泛适用性：当ε趋近于0时，结果恢复了PIR-SI的容量；当没有边信息时，结果退化为已知的泄漏PIR界。这些结果首次揭示了泄漏、边信息和检索效率之间的权衡关系。

Conclusion: 这项研究首次探讨了在带有边信息的私有信息检索中，隐私泄漏、边信息和检索效率之间的权衡关系。通过引入差分隐私风格的量化方法，建立了统一的框架，为未来在非完美隐私条件下的高效信息检索系统设计提供了理论基础。

Abstract: This paper investigates the problem of leaky-private Private Information Retrieval with Side Information (L-PIR-SI), which relaxes the requirement of perfect privacy to achieve improved communication efficiency in the presence of side information. While the capacities of PIR-SI under both $W$-privacy and $(W,S)$-privacy have been partially explored, the impact of controlled information leakage in these settings remains unaddressed. We propose a unified probabilistic framework to construct L-PIR-SI schemes where the privacy leakage is quantified by a parameter $\varepsilon$, consistent with differential privacy standards. We characterize the achievable download costs and show that our results generalize several landmark results in the PIR literature: they recover the capacity of PIR-SI when $\varepsilon \to 0$, and reduce to the known bounds for leaky-PIR when side information is absent. This work provides the first look at the trade-offs between leakage, side information, and retrieval efficiency.

</details>


### [63] [Fundamental Limits of Coded Polynomial Aggregation](https://arxiv.org/abs/2601.10028)
*Xi Zhong,Jörg Kliewer,Mingyue Ji*

Main category: cs.IT

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Coded polynomial aggregation (CPA) enables the master to directly recover a weighted aggregation of polynomial evaluations without individually decoding each term, thereby reducing the number of required worker responses. In this paper, we extend CPA to straggler-aware distributed computing systems and introduce a straggler-aware CPA framework with pre-specified non-straggler patterns, where exact recovery is required only for a given collection of admissible non-straggler sets. Our main result shows that exact recovery of the desired aggregation is achievable with fewer worker responses than required by polynomial coded computing based on individual decoding, and that feasibility is fundamentally characterized by the intersection structure of the non-straggler patterns. In particular, we establish necessary and sufficient conditions for exact recovery in straggler-aware CPA and identify an intersection-size threshold that is sufficient to guarantee exact recovery. We further prove that this threshold becomes both necessary and sufficient when the number of admissible non-straggler sets is sufficiently large. We also provide an explicit construction of feasible CPA schemes whenever the intersection size exceeds the derived threshold. Finally, simulations reveal a sharp feasibility transition at the predicted threshold, providing empirical evidence that the bound is tight in practice.

</details>


### [64] [Breaking the Storage-Bandwidth Tradeoff in Distributed Storage with Quantum Entanglement](https://arxiv.org/abs/2601.10676)
*Lei Hu,Mohamed Nomeir,Alptug Aytekin,Sennur Ulukus*

Main category: cs.IT

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: This work investigates the use of quantum resources in distributed storage systems. Consider an $(n,k,d)$ distributed storage system in which a file is stored across $n$ nodes such that any $k$ nodes suffice to reconstruct the file. When a node fails, any $d$ helper nodes transmit information to a newcomer to rebuild the system. In contrast to the classical repair, where helper nodes transmit classical bits, we allow them to send classical information over quantum channels to the newcomer. The newcomer then generates its storage by performing appropriate measurements on the received quantum states. In this setting, we fully characterize the fundamental tradeoff between storage and repair bandwidth (total communication cost). Compared to classical systems, the optimal storage--bandwidth tradeoff can be significantly improved with the enhancement of quantum entanglement shared only among the surviving nodes, particularly at the minimum-storage regenerating point. Remarkably, we show that when $d \geq 2k-2$, there exists an operating point at which \textit{both storage and repair bandwidth are simultaneously minimized}. This phenomenon breaks the tradeoff in the classical setting and reveals a fundamentally new regime enabled by quantum communication.

</details>


### [65] [Function Correcting Codes for Maximally-Unbalanced Boolean Functions](https://arxiv.org/abs/2601.10135)
*Rajlaxmi Pandey,Shiven Bajpai,Anjana A Mahesh,B. Sundar Rajan*

Main category: cs.IT

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Function-Correcting Codes (FCCs) enable reliable computation of a function of a $k$-bit message over noisy channels without requiring full message recovery. In this work, we study optimal single-error correcting FCCs (SEFCCs) for maximally-unbalanced Boolean functions, where $k$ denotes the message length and $t$ denotes the error-correction capability. We analyze the structure of optimal SEFCC constructions through their associated codeword distance matrices and identify distinct FCC classes based on this structure. We then examine the impact of these structural differences on error performance by evaluating representative FCCs over the additive white Gaussian noise (AWGN) channel using both soft-decision and hard-decision decoding. The results show that FCCs with different distance-matrix structures can exhibit markedly different Data BER and function error behavior, and that the influence of code structure depends strongly on the decoding strategy.

</details>


### [66] [On Existence of Girth-8 QC-LDPC Code with Large Column Weight: Combining Mirror-sequence with Classification Modulo Ten](https://arxiv.org/abs/2601.10170)
*Guohua Zhang,Xiangya Liu,Jianhua Zhang,Yi Fang*

Main category: cs.IT

TL;DR: 本研究基于GCD框架，通过引入镜像序列和新行重组方案，代数构造了列重7和8、码长极短且围长为8的QC-LDPC码，显著缩小了循环子矩阵尺寸。


<details>
  <summary>Details</summary>
Motivation: 大围长准循环LDPC码在信道编码、压缩感知和分布式存储等应用中至关重要，但现有构造方法难以用纯代数方式获得最短码长。

Method: 在之前提出的GCD框架基础上，引入镜像序列概念和新行重组方案，以代数方法构造列重7和8的QC-LDPC码。

Result: 相对于现有基准，新构造将列重7和8的连续循环子矩阵尺寸下界改善了约20%，实际构造的循环子矩阵尺寸比新颖下界缩小约25%。

Conclusion: 所提代数构造方法成功获得了列重7和8、码长极短且围长为8的QC-LDPC码，显著改善了循环子矩阵尺寸的下界和实际构造性能。

Abstract: Quasi-cyclic (QC) LDPC codes with large girths play a crucial role in several research and application fields, including channel coding, compressed sensing and distributed storage systems. A major challenge in respect of the code construction is how to obtain such codes with the shortest possible length (or equivalently, the smallest possible circulant size) using algebraic methods instead of search methods. The greatest-common-divisor (GCD) framework we previously proposed has algebraically constructed QC-LDPC codes with column weights of 5 and 6, very short lengths, and a girth of 8. By introducing the concept of a mirror sequence and adopting a new row-regrouping scheme, QC-LDPC codes with column weights of 7 and 8, very short lengths, and a girth of 8 are proposed for arbitrary row weights in this article via an algebraic manner under the GCD framework. Thanks to these novel algebraic methods, the lower bounds (for column weights 7 and 8) on consecutive circulant sizes are both improved by asymptotically about 20%, compared with the existing benchmarks. Furthermore, these new constructions can also offer circulant sizes asymptotically about 25% smaller than the novel bounds.

</details>


### [67] [Error-Correcting Codes for the Sum Channel](https://arxiv.org/abs/2601.10256)
*Lyan Abboud,Eitan Yaakobi*

Main category: cs.IT

TL;DR: 提出了称为求和通道的新信道模型，适用于分布式存储和DNA数据存储。针对$
$输入行，构建了冗余为$2\lceil\log_2\log_2 n\rceil + O(\ell^2)$的双删除纠错码。当$
=2$时证明了冗余至多差两倍的最优性。同时还提出了可在单替换纠错时接近最优的编码方案。


<details>
  <summary>Details</summary>
Motivation: 从分布式存储和DNA数据存储应用出发，研究一种新的信道模型——求和通道。该模型在输入$
$行二进制矩阵时，会输出一个$(
+1)$行矩阵，其前$
$行与输入相同，最后一行是前$
$行的奇偶校验行（求和行）。这种模型的纠错编码问题具有实际应用价值。

Method: 针对求和通道模型，设计了专门的双删除纠错码，其冗余度为$2\lceil\log_2\log_2 n\rceil + O(\ell^2)$。对于$
=2$的特殊情况，建立了$
\lceil\log_2\log_2 n\rceil + O(1)$的上界。同时还提出了针对单替换错误的编码方案，冗余仅为$\lceil \log_2(\ell+1)\rceil$位。

Result: 当$
=2$时，证明所提出的双删除纠错码的冗余度是最优的，最多只差两倍。对于单替换纠错码，证明了其冗余度与最优解最多只差一位。这些结果在理论上有重要意义，为实际应用提供了高效的纠错方案。

Conclusion: 求和通道模型在分布式存储和DNA数据存储中具有重要应用价值。本文提出了针对该模型的高效纠错码，并在理论上证明了其接近最优的性能。特别是双删除纠错码在$
=2$时达到了接近最优的冗余度，单替换纠错码也基本达到了最优。这些结果为相关应用提供了理论支持和技术基础。

Abstract: We introduce the sum channel, a new channel model motivated by applications in distributed storage and DNA data storage. In the error-free case, it takes as input an $\ell$-row binary matrix and outputs an $(\ell+1)$-row matrix whose first $\ell$ rows equal the input and whose last row is their parity (sum) row. We construct a two-deletion-correcting code with redundancy $2\lceil\log_2\log_2 n\rceil + O(\ell^2)$ for $\ell$-row inputs. When $\ell=2$, we establish an upper bound of $\lceil\log_2\log_2 n\rceil + O(1)$, implying that our redundancy is optimal up to a factor of 2. We also present a code correcting a single substitution with $\lceil \log_2(\ell+1)\rceil$ redundant bits and prove that it is within one bit of optimality.

</details>


### [68] [Algebraic Properties of PAC Codes](https://arxiv.org/abs/2601.10262)
*Vlad-Florin Dragoi,Mohammad Rowshan*

Main category: cs.IT

TL;DR: 对极化调整卷积码进行代数分析，定义广义多项式极化码大类，包含PAC码和反向PAC码，推导其结构特性如对偶性和最小距离，以及最小重量码字数量、单项式子码维度等结构限制。


<details>
  <summary>Details</summary>
Motivation: 研究极化调整卷积码的代数结构，通过极化码和里德-穆勒码的代数表示，建立更广泛的理论框架，统一处理PAC码及其变体，为分析这类码的结构性质奠定基础。

Method: 利用极化码和里德-穆勒码的代数表示方法，定义广义多项式极化码这一大类代码，系统分析其对偶性、最小距离等结构特性，并推导最小重量码字数量和单项式子码维度等理论极限。

Result: 成功定义了包含PAC码和反向PAC码的广义多项式极化码类，证明了这类码的结构性质，包括对偶关系、最小距离特征，并在最小重量码字数量和单项式子码维度方面得到了理论界限。

Conclusion: 该研究为极化调整卷积码提供了统一的代数框架，揭示了PAC码及其变体在广义多项式极化码下的共性结构特征，为后续理论和应用研究奠定了基础。

Abstract: We analyze polarization-adjusted convolutional codes using the algebraic representation of polar and Reed-Muller codes. We define a large class of codes, called generalized polynomial polar codes which include PAC codes and Reverse PAC codes. We derive structural properties of generalized polynomial polar codes, such as duality, minimum distance. We also deduce some structural limits in terms of number of minimum weight codewords, and dimension of monomial sub-code.

</details>


### [69] [On the Capacity of Noisy Frequency-based Channels](https://arxiv.org/abs/2601.10329)
*Yuval Gerzon,Ilan Shomorony,Nir Weinberger*

Main category: cs.IT

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: We investigate the capacity of noisy frequency-based channels, motivated by DNA data storage in the short-molecule regime, where information is encoded in the frequency of items types rather than their order. The channel output is a histogram formed by random sampling of items, followed by noisy item identification. While the capacity of the noiseless frequency-based channel has been previously addressed, the effect of identification noise has not been fully characterized. We present a converse bound on the channel capacity that follows from stochastic degradation and the data processing inequality. We then establish an achievable bound, which is based on a Poissonization of the multinomial sampling process, and an analysis of the resulting vector Poisson channel with inter-symbol interference. This analysis refines concentration inequalities for the information density used in Feinstein bound, and explicitly characterizes an additive loss in the mutual information due to identification noise. We apply our results to a DNA storage channel in the short-molecule regime, and quantify the resulting loss in the scaling of the total number of reliably stored bits.

</details>


### [70] [Convertible Codes for Data and Device Heterogeneity](https://arxiv.org/abs/2601.10341)
*Anina Gruica,Benjamin Jany,Stanislav Kruglik*

Main category: cs.IT

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Distributed storage systems must handle both data heterogeneity, arising from non-uniform access demands, and device heterogeneity, caused by time-varying node reliability. In this paper, we study convertible codes, which enable the transformation of one code into another with minimum cost in the merge regime, addressing the latter. We derive general lower bounds on the read and write costs of linear code conversion, applicable to arbitrary linear codes. We then focus on Reed-Muller codes, which efficiently handle data heterogeneity, addressing the former issue, and construct explicit conversion procedures that, for the first time, combine both forms of heterogeneity for distributed data storage.

</details>


### [71] [A New Construction Structure on MISO Coded Caching with Linear Subpacketization: Half-Sum Disjoint Packing](https://arxiv.org/abs/2601.10353)
*Bowen Zheng,Minquan Cheng,Kai Wan,Giuseppe Caire*

Main category: cs.IT

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: In the $(L,K,M,N)$ cache-aided multiple-input single-output (MISO) broadcast channel (BC) system, the server is equipped with $L$ antennas and communicates with $K$ single-antenna users through a wireless broadcast channel where the server has a library containing $N$ files, and each user is equipped with a cache of size $M$ files. Under the constraints of uncoded placement and one-shot linear delivery strategies, many schemes achieve the maximum sum Degree-of-Freedom (sum-DoF). However, for general parameters $L$, $M$, and $N$, their subpacketizations increase exponentially with the number of users. We aim to design a MISO coded caching scheme that achieves a large sum-DoF with low subpacketization $F$. An interesting combinatorial structure, called the multiple-antenna placement delivery array (MAPDA), can be used to generate MISO coded caching schemes under these two strategies; moreover, all existing schemes with these strategies can be represented by the corresponding MAPDAs. In this paper, we study the case with $F=K$ (i.e., $F$ grows linearly with $K$) by investigating MAPDAs. Specifically, based on the framework of Latin squares, we transform the design of MAPDA with $F=K$ into the construction of a combinatorial structure called the $L$-half-sum disjoint packing (HSDP). It is worth noting that a $1$-HSDP is exactly the concept of NHSDP, which is used to generate the shared-link coded caching scheme with $F=K$. By constructing $L$-HSDPs, we obtain a class of new schemes with $F=K$. Finally, theoretical and numerical analyses show that our $L$-HSDP schemes significantly reduce subpacketization compared to existing schemes with exponential subpacketization, while only slightly sacrificing sum-DoF, and achieve both a higher sum-DoF and lower subpacketization than the existing schemes with linear subpacketization.

</details>


### [72] [Generalized Weight Structure of Polar Codes: Selected Template Polynomials](https://arxiv.org/abs/2601.10362)
*Mohammad Rowshan,Vlad-Florin Dragoi*

Main category: cs.IT

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Polar codes can be viewed as decreasing monomial codes, revealing a rich algebraic structure governed by the lower-triangular affine (LTA) group. We develop a general framework to compute the Hamming weight of codewords generated by sums of monomials, express these weights in a canonical dyadic form, and derive closed expressions for key structural templates (disjoint sums, nested blocks, complementary flips) that generate the low and intermediate weight spectrum. Combining these templates with the LTA group action, we obtain explicit multiplicity formulas, yielding a unified algebraic method to characterize and enumerate codewords.

</details>


### [73] [A Hybrid Reliability--Weight Framework for Construction of Polar Codes](https://arxiv.org/abs/2601.10376)
*Mohammad Rowshan,Vlad-Florin Dragoi*

Main category: cs.IT

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Polar codes are usually constructed by ranking synthetic bit-channels according to reliability, which guarantees capacity-achieving behavior but can yield poor low-weight spectra at short and moderate lengths. Recent algebraic results express the contribution of individual bit-channels to the multiplicities of minimum and near-minimum weight codewords in closed form. In this work we combine these insights into a mixed (reliability--weight) bit-channel ordering. We define a per-bit cost whose distance term is derived from orbit enumeration of minimum-weight codewords and scaled by a Bhattacharyya-type factor, and show that the resulting mixed construction minimises a truncated SC/ML union-bound surrogate within a class of decreasing monomial codes. We relate the mixed metric to error events in SCL decoding via a pruning/ML decomposition, and prove that mixed designs act as local perturbations of reliability-based constructions whose asymptotic impact vanishes as code-length approaches infinity. Numerical results for short and moderate lengths on BPSK-AWGN, implemented via Gaussian approximation and closed-form weight contributions, illustrate the trade-off between pure reliability-based and mixed constructions in terms of minimum distance, multiplicity, and union-bound approximations. All proofs are deferred to the appendices.

</details>


### [74] [Codebook Design for Limited Feedback in Near-Field XL-MIMO Systems](https://arxiv.org/abs/2601.10391)
*Liujia Yao,Changsheng You,Zixuan Huang,Chao Zhou,Zhaohui Yang,Xiaoyang Li*

Main category: cs.IT

TL;DR: 该论文提出了一种针对用户分布优化的高效码本设计方案，用于超大规模MIMO FDD系统的有限反馈场景，通过角度采样与距离采样的联合优化显著降低了反馈开销。


<details>
  <summary>Details</summary>
Motivation: 现有面向XL-MIMO的码本设计（如极域码本）未充分考虑实际用户分布特点，导致反馈开销过大。为了解决这个问题，本文致力于设计一种针对用户分布的高效反馈码本。

Method: 首先考虑用户均匀分布于特定极域区域的典型场景，建立和速率最大化问题，联合优化角度-距离样本和比特分配。利用Voronoi分区证明均匀角度采样是最优的；对于更具挑战性的距离采样设计，推导出接收功率的紧致下界，并证明几何采样（相邻样本比例恒定）能最大化该下界。还扩展到非均匀用户分布的通用场景，采用交替采样法优化码本设计。

Result: 理论分析表明，随着天线阵列规模增大，最优反馈比特分配逐渐倾向于距离采样而非角度采样。数值结果验证了所提码本方案在各种系统设置下具有优异的速率性能和鲁棒性，相比包括广泛使用的极域码本在内的基准方案获得了显著增益。

Conclusion: 本文提出了有效解决超大规模MIMO FDD系统有限反馈挑战的码本设计新方法，通过充分考虑用户分布特性，在保持高性能的同时显著降低了反馈开销，为解决实际系统中的反馈效率问题提供了有力方案。

Abstract: In this paper, we study efficient codebook design for limited feedback in extremely large-scale multiple-input-multiple-output (XL-MIMO) frequency division duplexing (FDD) systems. It is worth noting that existing codebook designs for XL-MIMO, such as polar-domain codebook, have not well taken into account user (location) distribution in practice, thereby incurring excessive feedback overhead. To address this issue, we propose in this paper a novel and efficient feedback codebook tailored to user distribution. To this end, we first consider a typical scenario where users are uniformly distributed within a specific polar-region, based on which a sum-rate maximization problem is formulated to jointly optimize angle-range samples and bit allocation among angle/range feedback. This problem is challenging to solve due to the lack of a closed-form expression for the received power in terms of angle and range samples. By leveraging a Voronoi partitioning approach, we show that uniform angle sampling is optimal for received power maximization. For more challenging range sampling design, we obtain a tight lower-bound on the received power and show that geometric sampling, where the ratio between adjacent samples is constant, can maximize the lower bound and thus serves as a high-quality suboptimal solution. We then extend the proposed framework to accommodate more general non-uniform user distribution via an alternating sampling method. Furthermore, theoretical analysis reveals that as the array size increases, the optimal allocation of feedback bits increasingly favors range samples at the expense of angle samples. Finally, numerical results validate the superior rate performance and robustness of the proposed codebook design under various system setups, achieving significant gains over benchmark schemes, including the widely used polar-domain codebook.

</details>


### [75] [Multiaccess Coded Caching with Heterogeneous Retrieval Costs](https://arxiv.org/abs/2601.10394)
*Wenbo Huang,Minquan Cheng,Kai Wan,Xiaojun Li,Robert Caiming Qiu,Giuseppe Caire*

Main category: cs.IT

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: The multiaccess coded caching (MACC) system, as formulated by Hachem {\it et al.}, consists of a central server with a library of $N$ files, connected to $K$ cache-less users via an error-free shared link, and $K$ cache nodes, each equipped with cache memory of size $M$ files. Each user can access $L$ neighboring cache nodes under a cyclic wrap-around topology. Most existing studies operate under the strong assumption that users can retrieve content from their connected cache nodes at no communication cost. In practice, each user retrieves content from its $L$ different connected cache nodes at varying costs. Additionally, the server also incurs certain costs to transmit the content to the users. In this paper, we focus on a cost-aware MACC system and aim to minimize the total system cost, which includes cache-access costs and broadcast costs. Firstly, we propose a novel coded caching framework based on superposition coding, where the MACC schemes of Cheng \textit{et al.} are layered. Then, a cost-aware optimization problem is derived that optimizes cache placement and minimizes system cost. By identifying a sparsity property of the optimal solution, we propose a structure-aware algorithm with reduced complexity. Simulation results demonstrate that our proposed scheme consistently outperforms the scheme of Cheng {\it et al.} in scenarios with heterogeneous retrieval costs.

</details>


### [76] [Placement Delivery Array for Cache-Aided MIMO Systems](https://arxiv.org/abs/2601.10422)
*Yifei Huang,Kai Wan,Minquan Cheng,Jinyan Wang,Giuseppe Caire*

Main category: cs.IT

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: We consider a $(G,L,K,M,N)$ cache-aided multiple-input multiple-output (MIMO) network, where a server equipped with $L$ antennas and a library of $N$ equal-size files communicates with $K$ users, each equipped with $G$ antennas and a cache of size $M$ files, over a wireless interference channel. Each user requests an arbitrary file from the library. The goal is to design coded caching schemes that simultaneously achieve the maximum sum degrees of freedom (sum-DoF) and low subpacketization. In this paper, we first introduce a unified combinatorial structure, termed the MIMO placement delivery array (MIMO-PDA), which characterizes uncoded placement and one-shot zero-forcing delivery. By analyzing the combinatorial properties of MIMO-PDAs, we derive a sum-DoF upper bound of $\min\{KG, Gt+G\lceil L/G \rceil\}$, where $t=KM/N$, which coincides with the optimal DoF characterization in prior work by Tehrani \emph{et al.}. Based on this upper bound, we present two novel constructions of MIMO-PDAs that achieve the maximum sum-DoF. The first construction achieves linear subpacketization under stringent parameter constraints, while the second achieves ordered exponential subpacketization under substantially milder constraints. Theoretical analysis and numerical comparisons demonstrate that the second construction exponentially reduces subpacketization compared to existing schemes while preserving the maximum sum-DoF.

</details>


### [77] [A Construction Framework of Coded Caching Scheme for Multi-Access MIMO Systems via Knapsack Problem](https://arxiv.org/abs/2601.10484)
*Siying Luo,Youlong Wu,Mingming Zhang,Minquan Cheng,Dianhua Wu*

Main category: cs.IT

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: This paper investigates the coded caching problem in a multi-access multiple-input single-output (MAMISO) network with the combinatorial topology. The considered system consists of a server containing $N$ files, $Λ$ cache nodes, and $K$ cache-less users, where each user can access a unique subset of $r$ cache nodes. The server is equipped with $L$ transmit antennas. Our objective is to design a caching scheme that simultaneously achieves a high sum Degree of Freedom (sum-DoF) and low subpacketization complexity. To address this challenge, we formulate the design of multi-antenna placement delivery arrays (MAPDA) as a $0$--$1$ knapsack problem to maximize the achievable DoF, thereby transforming the complex combinatorial caching structure into a tractable optimization framework that yields efficient cache placement and flexible delivery strategies. Theoretical and numerical analyses demonstrate that: for networks with combinatorial topologies, the proposed scheme achieves a higher sum-DoF than existing schemes. Under identical cache size constraints, the subpacketization level remains comparable to existing linear subpacketization schemes. Moreover, under specific system conditions, the proposed scheme attains the theoretical maximum sum-DoF of $\min\{L+KM/N, K\}$ while achieving further reductions subpacketization. For particular combinatorial structures, we further derive optimized constructions that achieve even higher sum-DoF with lower subpacketization. ```

</details>


### [78] [Coded Caching for Combinatorial Multi-Access Hotplug Networks from $t$-Designs](https://arxiv.org/abs/2601.10503)
*Dhruv Pratap Singh,Anjana A. Mahesh,B. Sundar Rajan*

Main category: cs.IT

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: We study hotplug coded caching in combinatorial multi-access networks, which generalizes existing hotplug coded caching models by allowing users to access multiple caches, while only a subset of caches is online during the delivery phase. We first generalize the Hotplug Placement Delivery Array (HpPDA) framework to the combinatorial multi-access setting. Based on this generalized framework, we propose a t-design-based coded caching scheme for combinatorial multi-access networks. We characterize a class of design parameters under which every active user has access to a sufficient number of coded subfiles to decode its requested file, and show that appropriate parameter choices allow for the elimination of redundant multicast transmissions. As a result, the proposed scheme achieves a family of rate memory trade offs with flexible subpacketization. We present numerical comparisons illustrating that the proposed t-scheme outperforms existing hotplug coded caching schemes in certain memory regimes.

</details>


### [79] [A New Construction Structure on Coded Caching with Linear Subpacketization: Non-Half-Sum Latin Rectangle](https://arxiv.org/abs/2601.10505)
*Yongcheng Yang,Minquan Cheng,Kai Wan,Giuseppe Caire*

Main category: cs.IT

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Coded caching is recognized as an effective method for alleviating network congestion during peak periods by leveraging local caching and coded multicasting gains. The key challenge in designing coded caching schemes lies in simultaneously achieving low subpacketization and low transmission load. Most existing schemes require exponential or polynomial subpacketization levels, while some linear subpacketization schemes often result in excessive transmission load. Recently, Cheng et al. proposed a construction framework for linear coded caching schemes called Non-Half-Sum Disjoint Packing (NHSDP), where the subpacketization equals the number of users $K$. This paper introduces a novel combinatorial structure, termed the Non-Half-Sum Latin Rectangle (NHSLR), which extends the framework of linear coded caching schemes from $F=K$ (i.e., the construction via NHSDP) to a broader scenario with $F=\mathcal{O}(K)$. By constructing NHSLR, we have obtained a new class of coded caching schemes that achieves linearly scalable subpacketization, while further reducing the transmission load compared with the NHSDP scheme. Theoretical and numerical analyses demonstrate that the proposed schemes not only achieves lower transmission load than existing linear subpacketization schemes but also approaches the performance of certain exponential subpacketization schemes.

</details>


### [80] [A New Construction Structure on Multi-access Coded Caching with Linear Subpacketization: Cyclic Multi-Access Non-Half-Sum Disjoint Packing](https://arxiv.org/abs/2601.10510)
*Mengyuan Li,Minquan Cheng,Kai Wan,Giuseppe Caire*

Main category: cs.IT

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: We consider the $(K,L,M,N)$ multi-access coded caching system introduced by Hachem et al., which consists of a central server with $N$ files and $K$ cache nodes, each of memory size $M$, where each user can access $L$ cache nodes in a cyclic wrap-around fashion. At present, several existing schemes achieve competitive transmission performance, but their subpacketization levels grow exponentially with the number of users. In contrast, schemes with linear or polynomial subpacketization always incur higher transmission loads. We aim to design a multi-access coded caching scheme with linear subpacketization $F$ while maintaining low transmission load. Recently, Cheng et al. proposed a construction framework for coded caching schemes with linear subpacketization (i.e., $F=K$) called non-half-sum disjoint packing (NHSDP). Inspired by this structure, we introduce a novel combinatorial structure named cyclic multi-access non-half-sum disjoint packing (CMA-NHSDP) by extending NHSDP to MACC system. By constructing CMA-NHSDP, we obtain a new class of multi-access coded caching schemes. Theoretical and numerical analyses show that our scheme achieves lower transmission loads than some existing schemes with linear subpacketization. Moreover, the proposed schemes achieves lower transmission load compared to existing schemes with exponential subpacketization in some case.

</details>


### [81] [Network Integrated Sensing and Communication](https://arxiv.org/abs/2601.10538)
*Edward Andrews,Lawrence Ong,Duy T. Ngo,Yao Liu,Min Li*

Main category: cs.IT

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Integrated sensing and communication (ISAC) is a cornerstone technology for 6G networks, offering unified support for high-rate communication and high-accuracy sensing. While existing literature extensively covers link-level designs, the transition toward large-scale deployment necessitates a fundamental understanding of network-level performance. This paper investigates a network ISAC model where a source node communicates with a destination via a relay network, while intermediate nodes concurrently perform cooperative sensing over specific spatial regions. We formulate a novel optimization framework that captures the interplay between multi-node routing and sensing coverage. For a one-dimensional path network, we provide an analytical characterization of the complete sensing-throughput region. Extending this to general network topologies, we establish that the sensing-throughput Pareto boundary is piecewise linear and provide physical interpretations for each segment. Our results reveal the fundamental trade-offs between sensing coverage and communication routing, offering key insights for the design of future 6G heterogeneous networks.

</details>


### [82] [Error-Correcting Codes for Two Bursts of t1-Deletion-t2-Insertion with Low Computational Complexity](https://arxiv.org/abs/2601.10540)
*Yajuan Liu,Tolga M. Duman*

Main category: cs.IT

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Burst errors involving simultaneous insertions, deletions, and substitutions occur in practical scenarios, including DNA data storage and document synchronization, motivating developments of channel codes that can correct such errors. In this paper, we address the problem of constructing error-correcting codes (ECCs) capable of handling multiple bursts of $t_1$-deletion-$t_2$-insertion ($(t_1,t_2)$-DI) errors, where each burst consists of $t_1$ deletions followed by $t_2$ insertions in a binary sequence. We make three key contributions: Firstly, we establish the fundamental equivalence of (1) two bursts of $(t_1,t_2)$-DI ECCs, (2) two bursts of $(t_2,t_1)$-DI ECCs, and (3) one burst each of $(t_1,t_2)$-DI and $(t_2,t_1)$-DI ECCs. Then, we derive lower and upper bounds on the code size of two bursts of $(t_1,t_2)$-DI ECCs, which can naturally be extended to the case of multiple bursts. Finally, we present constructions of two bursts of $(t_1,t_2)$-DI ECCs. Compared to the codes obtained by the syndrome compression technique, the resulting codes achieve significantly lower computational complexity.

</details>


### [83] [Sparse Signal Recovery from Random Measurements](https://arxiv.org/abs/2601.10569)
*Siu-Wing Cheng,Man Ting Wong*

Main category: cs.IT

TL;DR: 该文提出一种无需优化或解线性系统即可从压缩感知测量中恢复未知稀疏向量z的简单方法


<details>
  <summary>Details</summary>
Motivation: 压缩感知通常需要求解复杂的优化问题，本文旨在开发一种更简单快速的方法来恢复稀疏信号

Method: 使用Θ(log n)个随机传感矩阵，通过简单计算而非优化或线性系统求解来恢复向量z及其支撑集，时间复杂度为O(kn log n)

Result: 该方法能在合理时间内恢复稀疏信号，并且在二进制信号上与基于优化的方法进行了实验比较

Conclusion: 提出了一种简单高效的压缩感知信号恢复方法，避免了传统复杂优化过程

Abstract: Given the compressed sensing measurements of an unknown vector $z \in \mathbb{R}^n$ using random matrices, we present a simple method to determine $z$ without solving any optimization problem or linear system. Our method uses $Θ(\log n)$ random sensing matrices in $\mathbb{R}^{k \times n}$ and runs in $O(kn\log n)$ time, where $k = Θ(s\log n)$ and $s$ is the number of nonzero coordinates in $z$. We adapt our method to determine the support set of $z$ and experimentally compare with some optimization-based methods on binary signals.

</details>


### [84] [Fundamental Limits of Multi-User Distributed Computing of Linearly Separable Functions](https://arxiv.org/abs/2601.10603)
*K. K. Krishnan Namboodiri,Elizabath Peter,Derya Malak,Petros Elia*

Main category: cs.IT

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: This work establishes the fundamental limits of the classical problem of multi-user distributed computing of linearly separable functions. In particular, we consider a distributed computing setting involving $L$ users, each requesting a linearly separable function over $K$ basis subfunctions from a master node, who is assisted by $N$ distributed servers. At the core of this problem lies a fundamental tradeoff between communication and computation: each server can compute up to $M$ subfunctions, and each server can communicate linear combinations of their locally computed subfunctions outputs to at most $Δ$ users. The objective is to design a distributed computing scheme that reduces the communication cost (total amount of data from servers to users), and towards this, for any given $K$, $L$, $M$, and $Δ$, we propose a distributed computing scheme that jointly designs the task assignment and transmissions, and shows that the scheme achieves optimal performance in the real field under various conditions using a novel converse. We also characterize the performance of the scheme in the finite field using another converse based on counting arguments.

</details>


### [85] [Basis-Spline Assisted Coded Computing: Strategies and Error Bounds](https://arxiv.org/abs/2601.10616)
*Rimpi Borah,J. Harshan,V. Lalitha*

Main category: cs.IT

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Coded computing has become a key framework for reliable distributed computation over decentralized networks, effectively mitigating the impact of stragglers. Although there exists a wide range of coded computing methods to handle both polynomial and non-polynomial functions, computing methods for the latter class have received traction due its inherent challenges in reconstructing non-polynomial functions using a finite number of evaluations. Among them, the state-of-the-art method is Berrut Approximated coded computing, wherein Berrut interpolants, are used for approximating the non-polynomial function. However, since Berrut interpolants have global support characteristics, such methods are known to offer degraded accuracy when the number of stragglers is large. To address this challenge, we propose a coded computing framework based on cubic B-spline interpolation. In our approach, server-side function evaluations are reconstructed at the master node using B-splines, exploiting their local support and smoothness properties to enhance stability and accuracy. We provide a systematic methodology for integrating B-spline interpolation into coded computing and derive theoretical bounds on approximation error in terms of the number of servers and stragglers. Comparative analysis demonstrates that our framework significantly outperforms Berrut-based methods for various non-polynomial functions.

</details>


### [86] [Converse Bounds for Sun-Jafar-type Weak Private Information Retrieval](https://arxiv.org/abs/2601.10643)
*Chandan Anand,Jayesh Seshadri,Prasad Krishnan,Gowtham R. Kurri*

Main category: cs.IT

TL;DR: 本文证明了Chandan等人提出的弱隐私信息检索（WPIR）方案在复制存储无串通设置中的速率-隐私权衡是最优的，并在参数阈值约束下证明了MDS编码无串通设置和T-串通设置的类最优性；当不满足阈值约束时，提出了可达到更高速率的反例。


<details>
  <summary>Details</summary>
Motivation: Chandan等人最近提出了针对无串通（复制和MDS编码）设置以及T-串通场景的新类别弱隐私信息检索（WPIR）方案，并给出了这些方案在互信息泄露和最大泄露度量下的速率-隐私权衡表达式，但其报告的交易权衡的类最优性未知。

Method: 在参数阈值约束下，对非串通复制设置、Banawan-Ulukus型MDS-WPIR和Sun-Jafar型T-串通WPIR方案的速率-隐私权衡进行理论证明；当不满足阈值约束时，通过构造反例展示可获得更高速率。

Result: 证明了Chandan等人报告的Sun-Jafar型方案在非串通复制设置下的速率-隐私权衡是最优的；在参数阈值约束下证明了MDS-WPIR和T-串通WPIR方案的类最优性；当不满足阈值约束时，发现了可达到比先前报告更高速率的反例。

Conclusion: 本研究确立了特定弱隐私信息检索方案在特定条件下的最优性，并揭示了当系统参数不满足阈值约束时存在改进空间，为WPIR方案的性能边界提供了更清晰的理解。

Abstract: Building on the well-established capacity-achieving schemes of Sun-Jafar (for replicated storage) and the closely related scheme of Banawan-Ulukus (for MDS-coded setting), a recent work by Chandan et al. proposed new classes of weak private information retrieval (WPIR) schemes for the collusion-free (replication and MDS-coded) setting, as well as for the $T$-colluding scenario. In their work, Chandan et al. characterized the expressions for the rate-privacy trade-offs for these classes of WPIR schemes, under the mutual information leakage and maximal leakage metrics. Explicit achievable trade-offs for the same were also presented, which were shown to be competitive or better than prior WPIR schemes. However, the class-wise optimality of the reported trade-offs were unknown. In this work, we show that the explicit rate-privacy trade-offs reported for the Sun-Jafar-type schemes by Chandan et al. are optimal for the non-colluding and replicated setting. Furthermore, we prove the class-wise optimality for Banawan-Ulukus-type MDS-WPIR and Sun-Jafar-type $T$-colluding WPIR schemes, under threshold-constraints on the system parameters. When these threshold-constraints do not hold, we present counter-examples which show that even higher rates than those reported before can be achieved.

</details>


### [87] [One-Shot Broadcast Joint Source-Channel Coding with Codebook Diversity](https://arxiv.org/abs/2601.10648)
*Joseph Rowan,Buu Phan,Ashish Khisti*

Main category: cs.IT

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: We study a one-shot joint source-channel coding setting where the source is encoded once and broadcast to $K$ decoders through independent channels. Success is predicated on at least one decoder recovering the source within a maximum distortion constraint. We find that in the one-shot regime, utilizing disjoint codebooks at each decoder yields a codebook diversity gain, distinct from the channel diversity gain that may be expected when several decoders observe independent realizations of the channel's output but share the same codebook. Coding schemes are introduced that leverage this phenomenon, where first- and second-order achievability bounds are derived via an adaptation of the Poisson matching lemma (Li and Anantharam, 2021) which allows for multiple decoders using disjoint codebooks. We further propose a hybrid coding scheme that partitions decoders into groups to optimally balance codebook and channel diversity. Numerical results on the binary symmetric channel demonstrate that the hybrid approach outperforms strategies where the decoders' codebooks are either fully shared or disjoint.

</details>


### [88] [Synchronizing Probabilities in Model-Driven Lossless Compression](https://arxiv.org/abs/2601.10678)
*Aviv Adler,Jennifer Tang*

Main category: cs.IT

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: It is well-known in the field of lossless data compression that probabilistic next-symbol prediction can be used to compress sequences of symbols. Deep neural networks are able to capture rich dependencies in data, offering a powerful means of estimating these probabilities and hence an avenue towards more effective compression algorithms. However, both compressor and decompressor must have exactly matching predictions; even small non-deterministic differences (which often happen with learned models due to hardware, software, or computation order) can lead to cascading decoding failures. In this paper, we formalize the problem of prediction mismatch in model-driven compression, and introduce Probability Matching Interval Coding (PMATIC), a model-agnostic algorithm that tolerates bounded prediction mismatch with low overhead. PMATIC works with the predicted probabilities, making it compatible as a drop-in replacement for the arithmetic encoder in model-driven compression tools. We show theoretical correctness and performance bounds for PMATIC, and validate these results on text data. These results confirm that, when paired an advanced prediction model, PMATIC is robust to prediction mismatch while achieving compression rates that out-perform standard modern compression tools.

</details>


### [89] [Implementation of Oblivious Transfer over Binary-Input AWGN Channels by Polar Codes](https://arxiv.org/abs/2601.10682)
*Pin-Hsun Lin,Hadi Aghaee,Christian Deppe,Eduard A. Jorswieck,Holger Boche*

Main category: cs.IT

TL;DR: 该论文提出了基于二进制输入加性高斯白噪声通道的二选一不经意传输协议，利用极化码技术实现接收者的完全隐私和发送者的渐近隐私。


<details>
  <summary>Details</summary>
Motivation: 为在二进制输入加性高斯白噪声通道上构建安全的不经意传输协议，需要同时保证接收者隐私（任何有限码长下完美）和发送者隐私，并优化有限码长性能。

Method: 使用极化码，通过极化变换的自同构链接两个解码器视图，公开随机选择编码器；结合信道极化与隐私放大实现发送者隐私；在差比特通道中注入随机性，推导松弛可靠性准则。

Result: 实现了接收者在任何有限码长下的完美隐私（通过公开编码器分布独立于接收者选择比特），发送者隐私通过极化渐近获得；表征了极化变换自同构作为比特通道索引的比特级置换，并优化了可实现的有限码长OT速率。

Conclusion: 该工作成功构造了一个基于极化码的二选一不经意传输协议，在有限码长下实现了接收者完美隐私和优化性能，为高斯噪声通道上的安全通信提供了有效方案。

Abstract: We develop a one-out-of-two-oblivious transfer protocol over the binary-input additive white Gaussian noise channel using polar codes. The scheme uses two decoder views linked by automorphisms of the polar transform and publicly draws the encoder at random from the corresponding automorphism group. This yields perfect receiver privacy at any finite blocklength, since the public encoder distribution is independent of the receiver's choice bit. Sender privacy is obtained asymptotically via channel polarization combined with privacy amplification. Because the construction deliberately injects randomness on selected bad bit-channels, we derive a relaxed reliability criterion and evaluate finite-blocklength performance. Finally, we characterize the polar-transform automorphisms as bit-level permutations of bit-channel indices, and exploit this structure to derive and optimize an achievable finite-blocklength OT rate.

</details>


### [90] [Improved Constructions of Reed-Solomon Codes with Optimal Repair Bandwidth](https://arxiv.org/abs/2601.10685)
*Jing Qiu,Weijun Fang,Shu-Tao Xia,Fang-Wei Fu*

Main category: cs.IT

TL;DR: 论文提出了改进的RS-MSR码构造方法，消除了素数同余限制条件，显著降低了子分组化水平并扩展了参数范围。


<details>
  <summary>Details</summary>
Motivation: 分布式存储中MDS码的单节点擦除修复通常需要下载k个节点的全部内容，修复带宽开销大。MSR码能最小化修复带宽，但RS码能否达到MSR点（RS-MSR码）长期存在。Tamo等人的突破性构造虽然实现了RS-MSR码，但其子分组化水平较高且参数选择受限。

Method: 通过消除原构造中素数必须满足p_i ≡ 1 mod s的同余条件，改进了RS-MSR码的构造方法。新构造不限制素数与修复参数s之间的同余关系，从而扩展了可用素数范围。

Result: 新构造将子分组化水平降低了φ(s)^n倍（φ为欧拉函数），大幅减少了存储开销。同时拓展了RS-MSR码的可行参数范围，使构造更加灵活实用。

Conclusion: 改进的RS-MSR码构造通过消除素数同余限制，显著降低了子分组化复杂度并扩展了参数空间，使RS码在实际分布式存储系统中实现最小存储修复更具可行性。

Abstract: Maximum-distance-separable (MDS) codes are widely used in distributed storage, yet naive repair of a single erasure in an $[n,k]$ MDS code downloads the entire contents of $k$ nodes. Minimum Storage Regenerating (MSR) codes (Dimakis et al., 2010) minimize repair bandwidth by contacting $d>k$ helpers and downloading only a fraction of data from each. Guruswami and Wootters first proposed a linear repair scheme for Reed-Solomon (RS) codes, showing that they can be repaired with lower bandwidth than the naive approach. The existence of RS codes achieving the MSR point (RS-MSR codes) nevertheless remained open until the breakthrough construction of Tamo, Barg, and Ye, which yields RS-MSR codes with subpacketization $\ell = s \prod_{i=1}^n p_i$, where $p_i$ are distinct primes satisfying $p_i \equiv 1 \pmod{s}$ and $s=d+1-k$.
  In this paper, we present an improved construction of RS-MSR codes by eliminating the congruence condition $p_i \equiv 1 \pmod{s}$. Consequently, our construction reduces the subpacketization by a multiplicative factor of $φ(s)^n$ ( $φ(\cdot)$ is Euler's totient function) and broadens the range of feasible parameters for RS-MSR codes.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [91] [Social Determinants of Health Prediction for ICD-9 Code with Reasoning Models](https://arxiv.org/abs/2601.09709)
*Sharim Khan,Paul Landes,Adam Cross,Jimeng Sun*

Main category: cs.LG

TL;DR: 该研究探讨使用大语言模型从医院入院记录中提取社会健康决定因素并映射到ICD-9代码，在MIMIC-III数据集上达到89%的F1分数。


<details>
  <summary>Details</summary>
Motivation: 社会健康决定因素对患者预后有重要影响，但很少被结构化的医疗数据所记录。现有的自动提取方法在长文本（如入院记录）中面临长距离依赖的挑战。

Method: 我们使用了推理模型和传统大语言模型，在MIMIC-III数据集上进行多标签SDoH ICD-9代码分类，并利用现有的ICD-9代码进行预测。

Result: 在入院记录上的预测达到了89%的F1分数，同时发现了139份入院记录中缺少的SDoH代码，并提供了可复现结果的代码。

Conclusion: 大型语言模型能够有效从临床文本中提取社会健康决定因素并映射到标准诊断代码，这在补充结构化医疗数据方面具有重要价值。

Abstract: Social Determinants of Health correlate with patient outcomes but are rarely captured in structured data. Recent attention has been given to automatically extracting these markers from clinical text to supplement diagnostic systems with knowledge of patients' social circumstances. Large language models demonstrate strong performance in identifying Social Determinants of Health labels from sentences. However, prediction in large admissions or longitudinal notes is challenging given long distance dependencies. In this paper, we explore hospital admission multi-label Social Determinants of Health ICD-9 code classification on the MIMIC-III dataset using reasoning models and traditional large language models. We exploit existing ICD-9 codes for prediction on admissions, which achieved an 89% F1. Our contributions include our findings, missing SDoH codes in 139 admissions, and code to reproduce the results.

</details>


### [92] [TimeSAE: Sparse Decoding for Faithful Explanations of Black-Box Time Series Models](https://arxiv.org/abs/2601.09776)
*Khalid Oublal,Quentin Bouniot,Qi Gan,Stephan Clémençon,Zeynep Akata*

Main category: cs.LG

TL;DR: 本文针对时序黑盒模型在分布外场景下解释性不足的问题，提出了结合稀疏自编码器和因果视角的TimeSAE框架，显著提升了解释的忠诚度和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 随着黑盒模型和预训练模型在时间序列应用中日益普及，尤其是在高风险领域，理解和解释其预测变得至关重要。然而，现有方法大多只适用于训练分布内的解释，无法泛化到训练支持之外，这限制了它们在真实场景中的有效性。

Method: 基于稀疏自编码器（SAEs）和因果关系的双重视角，作者提出了TimeSAE框架来为时间序列数据的黑盒模型提供解释。该方法不仅关注传统的特征重要性，还考虑了因果结构，从而提升对分布变化（distributional shifts）的鲁棒性。

Result: 在合成和真实世界的时间序列数据集上对TimeSAE进行了广泛评估，与领先的基线方法进行了比较。定量指标和定性分析均表明，TimeSAE能够提供更忠实且更鲁棒的解释。

Conclusion: TimeSAE通过结合稀疏自编码器和因果关系，成功地解决了现有解释方法对分布变化敏感的问题，为黑盒时序模型提供了一种可泛化的、更可靠的解释框架，并发布了易于使用的开源代码库。

Abstract: As black box models and pretrained models gain traction in time series applications, understanding and explaining their predictions becomes increasingly vital, especially in high-stakes domains where interpretability and trust are essential. However, most of the existing methods involve only in-distribution explanation, and do not generalize outside the training support, which requires the learning capability of generalization. In this work, we aim to provide a framework to explain black-box models for time series data through the dual lenses of Sparse Autoencoders (SAEs) and causality. We show that many current explanation methods are sensitive to distributional shifts, limiting their effectiveness in real-world scenarios. Building on the concept of Sparse Autoencoder, we introduce TimeSAE, a framework for black-box model explanation. We conduct extensive evaluations of TimeSAE on both synthetic and real-world time series datasets, comparing it to leading baselines. The results, supported by both quantitative metrics and qualitative insights, show that TimeSAE provides more faithful and robust explanations. Our code is available in an easy-to-use library TimeSAE-Lib: https://anonymous.4open.science/w/TimeSAE-571D/.

</details>


### [93] [Comparative Evaluation of Deep Learning-Based and WHO-Informed Approaches for Sperm Morphology Assessment](https://arxiv.org/abs/2601.10070)
*Mohammad Abbadi*

Main category: cs.LG

TL;DR: 本研究比较了基于图像的深度学习模型（HuSHeM）与基于WHO标准并加入系统性炎症反应指数（SIRI）的传统方法，在精子形态质量评估中的性能。深度学习模型在判别性能、校准性和临床效用方面均优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 传统的精子形态质量评估存在主观性强、观察者间变异大、资源受限等问题。本研究旨在探索基于人工智能的方法是否能提供更客观、可靠的评估。

Method: 1. 开发了基于高分辨率精子形态图像的深度学习模型（HuSHeM）；2. 建立了基于WHO标准并加入系统性炎症反应指数（SIRI）的临床基线方法（WHO(+SIRI)）；3. 使用独立的临床队列进行评估；4. 采用判别性能（ROC曲线下面积）、校准分析和临床效用（决策曲线分析）等多维度评估指标。

Result: 1. HuSHeM模型在ROC曲线下面积方面表现更优，置信区间较窄；2. 在类别不平衡情况下，precision-recall分析显示HuSHeM性能更好；3. 校准分析表明HuSHeM的预测概率与实际结果更一致；4. 决策曲线分析显示HuSHeM在临床相关阈值范围内具有更大的净临床效益。

Conclusion: 基于图像的深度学习模型相较于传统的基于规则和炎症增强的标准，能够提供更高的预测可靠性和临床效用。该框架支持精子形态的客观、可重复评估，可作为生育筛查和转诊工作流程中的决策支持工具，但并非旨在替代临床判断或实验室评估。

Abstract: Assessment of sperm morphological quality remains a critical yet subjective component of male fertility evaluation, often limited by inter-observer variability and resource constraints. This study presents a comparative biomedical artificial intelligence framework evaluating an image-based deep learning model (HuSHeM) alongside a clinically grounded baseline derived from World Health Organization criteria augmented with the Systemic Inflammation Response Index (WHO(+SIRI)).
  The HuSHeM model was trained on high-resolution sperm morphology images and evaluated using an independent clinical cohort. Model performance was assessed using discrimination, calibration, and clinical utility analyses. The HuSHeM model demonstrated higher discriminative performance, as reflected by an increased area under the receiver operating characteristic curve with relatively narrow confidence intervals compared to WHO(+SIRI). Precision-recall analysis further indicated improved performance under class imbalance, with higher precision-recall area values across evaluated thresholds. Calibration analysis indicated closer agreement between predicted probabilities and observed outcomes for HuSHeM, while decision curve analysis suggested greater net clinical benefit across clinically relevant threshold probabilities.
  These findings suggest that image-based deep learning may offer improved predictive reliability and clinical utility compared with traditional rule-based and inflammation-augmented criteria. The proposed framework supports objective and reproducible assessment of sperm morphology and may serve as a decision-support tool within fertility screening and referral workflows. The proposed models are intended as decision-support or referral tools and are not designed to replace clinical judgment or laboratory assessment.

</details>


### [94] [QFed: Parameter-Compact Quantum-Classical Federated Learning](https://arxiv.org/abs/2601.09809)
*Samar Abdelghani,Soumaya Cherkaoui*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Organizations and enterprises across domains such as healthcare, finance, and scientific research are increasingly required to extract collective intelligence from distributed, siloed datasets while adhering to strict privacy, regulatory, and sovereignty requirements. Federated Learning (FL) enables collaborative model building without sharing sensitive raw data, but faces growing challenges posed by statistical heterogeneity, system diversity, and the computational burden from complex models. This study examines the potential of quantum-assisted federated learning, which could cut the number of parameters in classical models by polylogarithmic factors and thus lessen training overhead. Accordingly, we introduce QFed, a quantum-enabled federated learning framework aimed at boosting computational efficiency across edge device networks. We evaluate the proposed framework using the widely adopted FashionMNIST dataset. Experimental results show that QFed achieves a 77.6% reduction in the parameter count of a VGG-like model while maintaining an accuracy comparable to classical approaches in a scalable environment. These results point to the potential of leveraging quantum computing within a federated learning context to strengthen FL capabilities of edge devices.

</details>


### [95] [Eluder dimension: localise it!](https://arxiv.org/abs/2601.09825)
*Alireza Bakhtiari,Alex Ayoub,Samuel Robertson,David Janz,Csaba Szepesvári*

Main category: cs.LG

TL;DR: 本文建立了广义线性模型类的eluder维度的下界，表明基于标准eluder维度的分析无法获得一阶遗憾界。为此，引入了eluder维度的局部化方法，该方法不仅恢复并改进了伯努利多臂赌博机的经典结果，还首次为有限时域强化学习任务提供了真正的一阶有界累积回报界。


<details>
  <summary>Details</summary>
Motivation: 标准的eluder维度分析无法获得一阶遗憾界，这限制了其在广义线性模型类和强化学习中的性能保证。本文旨在突破这一限制，提供能够实现一阶性能保证的新分析方法。

Method: 提出了eluder维度的局部化方法，这是一种能够适应局部复杂度特征的改进分析框架，适用于广义线性模型类和有限时域强化学习任务。

Result: 1. 建立了广义线性模型类eluder维度的下界
2. 局部化方法能够恢复并改进伯努利多臂赌博机的经典结果
3. 首次为有限时域强化学习任务提供了真正的一阶有界累积回报界

Conclusion: 通过引入eluder维度的局部化方法，本文突破了标准分析框架的局限性，为广义线性模型类和强化学习任务提供了一阶性能保证，是该领域的重要理论进展。

Abstract: We establish a lower bound on the eluder dimension of generalised linear model classes, showing that standard eluder dimension-based analysis cannot lead to first-order regret bounds. To address this, we introduce a localisation method for the eluder dimension; our analysis immediately recovers and improves on classic results for Bernoulli bandits, and allows for the first genuine first-order bounds for finite-horizon reinforcement learning tasks with bounded cumulative returns.

</details>


### [96] [Queueing-Aware Optimization of Reasoning Tokens for Accuracy-Latency Trade-offs in LLM Servers](https://arxiv.org/abs/2601.10274)
*Emre Ozbas,Melih Bastopcu*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: We consider a single large language model (LLM) server that serves a heterogeneous stream of queries belonging to $N$ distinct task types. Queries arrive according to a Poisson process, and each type occurs with a known prior probability. For each task type, the server allocates a fixed number of internal thinking tokens, which determines the computational effort devoted to that query. The token allocation induces an accuracy-latency trade-off: the service time follows an approximately affine function of the allocated tokens, while the probability of a correct response exhibits diminishing returns. Under a first-in, first-out (FIFO) service discipline, the system operates as an $M/G/1$ queue, and the mean system time depends on the first and second moments of the resulting service-time distribution. We formulate a constrained optimization problem that maximizes a weighted average accuracy objective penalized by the mean system time, subject to architectural token-budget constraints and queue-stability conditions. The objective function is shown to be strictly concave over the stability region, which ensures existence and uniqueness of the optimal token allocation. The first-order optimality conditions yield a coupled projected fixed-point characterization of the optimum, together with an iterative solution and an explicit sufficient condition for contraction. Moreover, a projected gradient method with a computable global step-size bound is developed to guarantee convergence beyond the contractive regime. Finally, integer-valued token allocations are attained via rounding of the continuous solution, and the resulting performance loss is evaluated in simulation results.

</details>


### [97] [A New Convergence Analysis of Plug-and-Play Proximal Gradient Descent Under Prior Mismatch](https://arxiv.org/abs/2601.09831)
*Guixian Xu,Jinglai Li,Junqi Tang*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: In this work, we provide a new convergence theory for plug-and-play proximal gradient descent (PnP-PGD) under prior mismatch where the denoiser is trained on a different data distribution to the inference task at hand. To the best of our knowledge, this is the first convergence proof of PnP-PGD under prior mismatch. Compared with the existing theoretical results for PnP algorithms, our new results removed the need for several restrictive and unverifiable assumptions.

</details>


### [98] [A pipeline for enabling path-specific causal fairness in observational health data](https://arxiv.org/abs/2601.09841)
*Aparajita Kashyap,Sara Matijevic,Noémie Elhadad,Steven A. Kushner,Shalmali Joshi*

Main category: cs.LG

TL;DR: 该论文提出了一个模型无关的流程，用于训练因果公平的机器学习模型，专门针对医疗保健环境中的直接和间接偏见问题。


<details>
  <summary>Details</summary>
Motivation: 在医疗保健环境中部署机器学习模型时，需要确保模型不会复制或加剧现有的医疗偏见。现有的公平性定义虽然多，但需要更好地考虑偏见发生的社会和医疗背景。

Method: 将结构性公平模型映射到观测性医疗环境中，创建一个可通用的管道来训练因果公平模型。该管道明确考虑特定的医疗背景和差异来定义目标公平模型。还展示了如何在已知社会和医疗差异的任务中，利用未受公平约束的基础模型生成因果公平的下游预测。

Result: 该工作填补了两个主要空白：1）通过解构直接和间接的偏见来源，扩展了'公平性-准确性'权衡的特征描述；2）展示了如何利用观测性健康数据上未受公平约束训练的基础模型，在已知社会和医疗差异的任务中生成因果公平的下游预测。

Conclusion: 该工作为训练因果公平的机器学习模型提供了一个模型无关的管道，能够同时解决医疗偏见中的直接和间接形式，为医疗健康领域的公平性研究提供了实用的方法论。

Abstract: When training machine learning (ML) models for potential deployment in a healthcare setting, it is essential to ensure that they do not replicate or exacerbate existing healthcare biases. Although many definitions of fairness exist, we focus on path-specific causal fairness, which allows us to better consider the social and medical contexts in which biases occur (e.g., direct discrimination by a clinician or model versus bias due to differential access to the healthcare system) and to characterize how these biases may appear in learned models. In this work, we map the structural fairness model to the observational healthcare setting and create a generalizable pipeline for training causally fair models. The pipeline explicitly considers specific healthcare context and disparities to define a target "fair" model. Our work fills two major gaps: first, we expand on characterizations of the "fairness-accuracy" tradeoff by detangling direct and indirect sources of bias and jointly presenting these fairness considerations alongside considerations of accuracy in the context of broadly known biases. Second, we demonstrate how a foundation model trained without fairness constraints on observational health data can be leveraged to generate causally fair downstream predictions in tasks with known social and medical disparities. This work presents a model-agnostic pipeline for training causally fair machine learning models that address both direct and indirect forms of healthcare bias.

</details>


### [99] [Advancing Model Refinement: Muon-Optimized Distillation and Quantization for LLM Deployment](https://arxiv.org/abs/2601.09865)
*Jacob Sander,Brian Jalaian,Venkat R. Dasari*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Large Language Models (LLMs) enable advanced natural language processing but face deployment challenges on resource-constrained edge devices due to high computational, memory, and energy demands. Optimizing these models requires addressing three key challenges: acquiring task-specific data, fine-tuning for performance, and compressing models to accelerate inference while reducing resource demands. We propose an integrated framework combining GPTQ-based quantization, low-rank adaptation (LoRA), and a specialized data distillation process to significantly reduce model size and complexity while preserving or enhancing task-specific performance. By leveraging data distillation, knowledge distillation via Kullback-Leibler divergence, Bayesian hyperparameter optimization, and the Muon optimizer, our pipeline achieves up to 2x memory compression (e.g., reducing a 6GB model to 3GB) and enables efficient inference for specialized tasks. Empirical results demonstrate superior performance on standard LLM benchmarks compared to GPTQ quantization alone, with the Muon optimizer notably enhancing fine-tuned models' resistance to accuracy decay during quantization.

</details>


### [100] [The PROPER Approach to Proactivity: Benchmarking and Advancing Knowledge Gap Navigation](https://arxiv.org/abs/2601.09926)
*Kirandeep Kaur,Vinayak Gupta,Aditya Gupta,Chirag Shah*

Main category: cs.LG

TL;DR: ProPer是一个双智能体架构，通过生成隐性维度来增强语言助手的主动性，实现个性化的主动干预，在多个领域显著提升了响应质量。


<details>
  <summary>Details</summary>
Motivation: 当前语言助手大多遵循被动问答模式，用户需明确表达需求，导致未表达的需求无法得到满足。现有主动智能体要么需要用户额外澄清而增加负担，要么仅从上下文推断未来需求，常导致不必要或时机不当的干预。

Method: 提出ProPer双智能体架构：维度生成智能体(DGA)利用用户数据生成多个隐性维度（用户未考虑但与任务相关的潜在方面）或知识缺口，通过基于质量、多样性和任务相关性的重排序器进行筛选；响应生成智能体(RGA)平衡显性和隐性维度，生成个性化响应并进行及时主动的干预。

Result: 使用结构化、缺口感知的评估标准（覆盖度、主动性适当性、意图对齐）在多个领域评估ProPer。结果显示ProPer在所有领域均提高了质量分数和胜率，单轮评估最高提升84%，多轮交互中也保持持续优势。

Conclusion: ProPer通过双智能体架构有效解决了语言助手被动性的局限，能够识别和响应用户未表达的需求，实现更加个性化、主动且适时的干预，显著提升了用户体验。

Abstract: Most language-based assistants follow a reactive ask-and-respond paradigm, requiring users to explicitly state their needs. As a result, relevant but unexpressed needs often go unmet. Existing proactive agents attempt to address this gap either by eliciting further clarification, preserving this burden, or by extrapolating future needs from context, often leading to unnecessary or mistimed interventions. We introduce ProPer, Proactivity-driven Personalized agents, a novel two-agent architecture consisting of a Dimension Generating Agent (DGA) and a Response Generating Agent (RGA). DGA, a fine-tuned LLM agent, leverages explicit user data to generate multiple implicit dimensions (latent aspects relevant to the user's task but not considered by the user) or knowledge gaps. These dimensions are selectively filtered using a reranker based on quality, diversity, and task relevance. RGA then balances explicit and implicit dimensions to tailor personalized responses with timely and proactive interventions. We evaluate ProPer across multiple domains using a structured, gap-aware rubric that measures coverage, initiative appropriateness, and intent alignment. Our results show that ProPer improves quality scores and win rates across all domains, achieving up to 84% gains in single-turn evaluation and consistent dominance in multi-turn interactions.

</details>


### [101] [Interpolation-Based Optimization for Enforcing lp-Norm Metric Differential Privacy in Continuous and Fine-Grained Domains](https://arxiv.org/abs/2601.09946)
*Chenxi Qiu*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Metric Differential Privacy (mDP) generalizes Local Differential Privacy (LDP) by adapting privacy guarantees based on pairwise distances, enabling context-aware protection and improved utility. While existing optimization-based methods reduce utility loss effectively in coarse-grained domains, optimizing mDP in fine-grained or continuous settings remains challenging due to the computational cost of constructing dense perterubation matrices and satisfying pointwise constraints.
  In this paper, we propose an interpolation-based framework for optimizing lp-norm mDP in such domains. Our approach optimizes perturbation distributions at a sparse set of anchor points and interpolates distributions at non-anchor locations via log-convex combinations, which provably preserve mDP. To address privacy violations caused by naive interpolation in high-dimensional spaces, we decompose the interpolation process into a sequence of one-dimensional steps and derive a corrected formulation that enforces lp-norm mDP by design. We further explore joint optimization over perturbation distributions and privacy budget allocation across dimensions. Experiments on real-world location datasets demonstrate that our method offers rigorous privacy guarantees and competitive utility in fine-grained domains, outperforming baseline mechanisms. in high-dimensional spaces, we decompose the interpolation process into a sequence of one-dimensional steps and derive a corrected formulation that enforces lp-norm mDP by design. We further explore joint optimization over perturbation distributions and privacy budget allocation across dimensions. Experiments on real-world location datasets demonstrate that our method offers rigorous privacy guarantees and competitive utility in fine-grained domains, outperforming baseline mechanisms.

</details>


### [102] [Kinematic Tokenization: Optimization-Based Continuous-Time Tokens for Learnable Decision Policies in Noisy Time Series](https://arxiv.org/abs/2601.09949)
*Griffin Kearney*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Transformers are designed for discrete tokens, yet many real-world signals are continuous processes observed through noisy sampling. Discrete tokenizations (raw values, patches, finite differences) can be brittle in low signal-to-noise regimes, especially when downstream objectives impose asymmetric penalties that rationally encourage abstention. We introduce Kinematic Tokenization, an optimization-based continuous-time representation that reconstructs an explicit spline from noisy measurements and tokenizes local spline coefficients (position, velocity, acceleration, jerk). This is applied to financial time series data in the form of asset prices in conjunction with trading volume profiles. Across a multi-asset daily-equity testbed, we use a risk-averse asymmetric classification objective as a stress test for learnability. Under this objective, several discrete baselines collapse to an absorbing cash policy (the Liquidation Equilibrium), whereas the continuous spline tokens sustain calibrated, non-trivial action distributions and stable policies. These results suggest that explicit continuous-time tokens can improve the learnability and calibration of selective decision policies in noisy time series under abstention-inducing losses.

</details>


### [103] [A Sustainable AI Economy Needs Data Deals That Work for Generators](https://arxiv.org/abs/2601.09966)
*Ruoxi Jia,Luis Oala,Wenjie Xiong,Suqin Ge,Jiachen T. Wang,Feiyang Kang,Dawn Song*

Main category: cs.LG

TL;DR: 论文论证机器学习价值链结构不可持续，因经济数据处理不平等导致数据生成者权益被剥夺，提出公平数据价值交换框架解决此问题。


<details>
  <summary>Details</summary>
Motivation: 当前机器学习数据价值链存在结构性缺陷，数据从输入到合成输出的过程中技术信号不断被提炼，但经济权益从数据生成者剥离，导致价值主要流向聚合者而创作者报酬近乎为零。这不仅是经济福利问题，更威胁到支撑当前学习算法的反馈循环的可持续性。

Method: 分析了73个公开数据交易案例，识别出三个结构性缺陷：（1）数据来源缺失，（2）议价能力不对称，（3）定价机制非动态。在此基础上，提出公平数据价值交换（EDVEX）框架，以建立使所有参与者受益的最小化市场机制。

Result: 研究发现大多数价值被聚合者获取，创作者版税记录近似为零，交易条款普遍不透明。这些结构性缺陷在整个价值链中造成了数据经济权益的分配不均。

Conclusion: 机器学习数据价值链的当前结构不可持续，需要建立公平的数据价值交换机制。EDVEX框架为解决数据来源、议价能力和动态定价问题提供了基础，并指出了社区可在数据交易方面做出具体贡献的研究方向。

Abstract: We argue that the machine learning value chain is structurally unsustainable due to an economic data processing inequality: each state in the data cycle from inputs to model weights to synthetic outputs refines technical signal but strips economic equity from data generators. We show, by analyzing seventy-three public data deals, that the majority of value accrues to aggregators, with documented creator royalties rounding to zero and widespread opacity of deal terms. This is not just an economic welfare concern: as data and its derivatives become economic assets, the feedback loop that sustains current learning algorithms is at risk. We identify three structural faults - missing provenance, asymmetric bargaining power, and non-dynamic pricing - as the operational machinery of this inequality. In our analysis, we trace these problems along the machine learning value chain and propose an Equitable Data-Value Exchange (EDVEX) Framework to enable a minimal market that benefits all participants. Finally, we outline research directions where our community can make concrete contributions to data deals and contextualize our position with related and orthogonal viewpoints.

</details>


### [104] [An Exploratory Study to Repurpose LLMs to a Unified Architecture for Time Series Classification](https://arxiv.org/abs/2601.09971)
*Hansen He,Shuheng Li*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Time series classification (TSC) is a core machine learning problem with broad applications. Recently there has been growing interest in repurposing large language models (LLMs) for TSC, motivated by their strong reasoning and generalization ability. Prior work has primarily focused on alignment strategies that explicitly map time series data into the textual domain; however, the choice of time series encoder architecture remains underexplored. In this work, we conduct an exploratory study of hybrid architectures that combine specialized time series encoders with a frozen LLM backbone. We evaluate a diverse set of encoder families, including Inception, convolutional, residual, transformer-based, and multilayer perceptron architectures, among which the Inception model is the only encoder architecture that consistently yields positive performance gains when integrated with an LLM backbone. Overall, this study highlights the impact of time series encoder choice in hybrid LLM architectures and points to Inception-based models as a promising direction for future LLM-driven time series learning.

</details>


### [105] [In-Context Operator Learning on the Space of Probability Measures](https://arxiv.org/abs/2601.09979)
*Frank Cole,Dixi Wang,Yineng Chen,Yulong Lu,Rongjie Lai*

Main category: cs.LG

TL;DR: 该论文提出了针对最优传输的概率测度空间内的上下文算子学习方法，通过少量样本提示学习从分布对到OT映射的算子，无需推理时梯度更新。


<details>
  <summary>Details</summary>
Motivation: 传统最优传输计算方法需要为每个新分布对重新求解，计算成本高。本文旨在开发一个通用算子，能够通过少量样本提示直接预测OT映射，实现高效的一次性学习。

Method: 提出in-context operator learning框架，参数化解算子。在两个体系开展研究：非参数设置下建立泛化界理论；参数设置下给出显式架构。使用缩放律理论分析性能与提示大小、内在维度、模型容量的关系。

Result: 理论分析表明模型能够从上下文中准确恢复OT映射，数值实验在合成传输和生成建模基准上验证了框架有效性。非参数设置下建立了泛化界，参数设置下证明了有限样本超额风险界。

Conclusion: 上下文算子学习为最优传输提供了一种高效、适应性强的解决方案，通过少量样本提示即可预测OT映射，避免了重复计算，在理论和实验上都展示了优良性能。

Abstract: We introduce \emph{in-context operator learning on probability measure spaces} for optimal transport (OT). The goal is to learn a single solution operator that maps a pair of distributions to the OT map, using only few-shot samples from each distribution as a prompt and \emph{without} gradient updates at inference. We parameterize the solution operator and develop scaling-law theory in two regimes. In the \emph{nonparametric} setting, when tasks concentrate on a low-intrinsic-dimension manifold of source--target pairs, we establish generalization bounds that quantify how in-context accuracy scales with prompt size, intrinsic task dimension, and model capacity. In the \emph{parametric} setting (e.g., Gaussian families), we give an explicit architecture that recovers the exact OT map in context and provide finite-sample excess-risk bounds. Our numerical experiments on synthetic transports and generative-modeling benchmarks validate the framework.

</details>


### [106] [FaTRQ: Tiered Residual Quantization for LLM Vector Search in Far-Memory-Aware ANNS Systems](https://arxiv.org/abs/2601.09985)
*Tianqi Zhang,Flavio Ponzina,Tajana Rosing*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Approximate Nearest-Neighbor Search (ANNS) is a key technique in retrieval-augmented generation (RAG), enabling rapid identification of the most relevant high-dimensional embeddings from massive vector databases. Modern ANNS engines accelerate this process using prebuilt indexes and store compressed vector-quantized representations in fast memory. However, they still rely on a costly second-pass refinement stage that reads full-precision vectors from slower storage like SSDs. For modern text and multimodal embeddings, these reads now dominate the latency of the entire query. We propose FaTRQ, a far-memory-aware refinement system using tiered memory that eliminates the need to fetch full vectors from storage. It introduces a progressive distance estimator that refines coarse scores using compact residuals streamed from far memory. Refinement stops early once a candidate is provably outside the top-k. To support this, we propose tiered residual quantization, which encodes residuals as ternary values stored efficiently in far memory. A custom accelerator is deployed in a CXL Type-2 device to perform low-latency refinement locally. Together, FaTRQ improves the storage efficiency by 2.4$\times$ and improves the throughput by up to 9$ \times$ than SOTA GPU ANNS system.

</details>


### [107] [Continuous-Depth Transformers with Learned Control Dynamics](https://arxiv.org/abs/2601.10007)
*Peter Jemley*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: We present a hybrid transformer architecture that replaces discrete middle layers with a continuous-depth Neural Ordinary Differential Equation (ODE) block, enabling inference-time control over generation attributes via a learned steering signal. Unlike standard transformers that process representations through fixed discrete layers, our approach treats depth as a continuous variable governed by a learned vector field $F_θ(H, τ, u)$, where $u$ is a low-dimensional control signal injected via explicit concatenation. We validate the architecture through four experiments: (1) gradient flow stability with zero exploding/vanishing gradient events, (2) semantic steering achieving 98\%/88\% accuracy for positive/negative sentiment control, (3) continuous interpolation validated by a negligible 0.068\% trajectory divergence between fixed and adaptive solvers, and (4) efficiency benchmarking demonstrating latency parity with standard discrete baselines. Additionally, we show that adaptive ODE solvers reveal geometric structure in the learned dynamics: the control signal partitions the vector field into distinct dynamical regimes with different curvature characteristics. The adjoint method enables $O(1)$ memory training regardless of integration depth. Our results demonstrate that continuous-depth dynamics with learned control signals provide a viable, efficient mechanism for steerable language generation.

</details>


### [108] [PID-Guided Partial Alignment for Multimodal Decentralized Federated Learning](https://arxiv.org/abs/2601.10012)
*Yanhang Shi,Xiaoyu Wang,Houwei Cao,Jian Li,Yong Liu*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Multimodal decentralized federated learning (DFL) is challenging because agents differ in available modalities and model architectures, yet must collaborate over peer-to-peer (P2P) networks without a central coordinator. Standard multimodal pipelines learn a single shared embedding across all modalities. In DFL, such a monolithic representation induces gradient misalignment between uni- and multimodal agents; as a result, it suppresses heterogeneous sharing and cross-modal interaction. We present PARSE, a multimodal DFL framework that operationalizes partial information decomposition (PID) in a server-free setting. Each agent performs feature fission to factorize its latent representation into redundant, unique, and synergistic slices. P2P knowledge sharing among heterogeneous agents is enabled by slice-level partial alignment: only semantically shareable branches are exchanged among agents that possess the corresponding modality. By removing the need for central coordination and gradient surgery, PARSE resolves uni-/multimodal gradient conflicts, thereby overcoming the multimodal DFL dilemma while remaining compatible with standard DFL constraints. Across benchmarks and agent mixes, PARSE yields consistent gains over task-, modality-, and hybrid-sharing DFL baselines. Ablations on fusion operators and split ratios, together with qualitative visualizations, further demonstrate the efficiency and robustness of the proposed design.

</details>


### [109] [CAFEDistill: Learning Personalized and Dynamic Models through Federated Early-Exit Network Distillation](https://arxiv.org/abs/2601.10015)
*Boyi Liu,Zimu Zhou,Yongxin Tong*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Personalized Federated Learning (PFL) enables collaboratively model training on decentralized, heterogeneous data while tailoring them to each client's unique distribution. However, existing PFL methods produce static models with a fixed tradeoff between accuracy and efficiency, limiting their applicability in environments where inference requirements vary with contexts and resource availability. Early-exit networks (EENs) offer adaptive inference by attaching intermediate classifiers. Yet integrating them into PFL is challenging due to client-wise heterogeneity and depth-wise interference arising from conflicting exit objectives. Prior studies fail to resolve both conflicts simultaneously, leading to suboptimal performance. In this paper, we propose CAFEDistill, a Conflict-Aware Federated Exit Distillation framework that jointly addresses these conflicts and extends PFL to early-exit networks. Through a progressive, depth-prioritized student coordination mechanism, CAFEDistill mitigates interference among shallow and deep exits while allowing effective personalized knowledge transfer across clients. Furthermore, it reduces communication overhead via a client-decoupled formulation. Extensive evaluations show that CAFEDistill outperforms the state-of-the-arts, achieving higher accuracy and reducing inference costs by 30.79%-46.86%.

</details>


### [110] [Time Aggregation Features for XGBoost Models](https://arxiv.org/abs/2601.10019)
*Mykola Pinchuk*

Main category: cs.LG

TL;DR: XGBoost模型在CTR预测中的时间聚合特征研究，发现简单的时间窗口设计与事件计数窗口最有效


<details>
  <summary>Details</summary>
Motivation: 研究CTR预测中时间聚合特征的有效性，在严格的时间约束下，比较不同时间窗口设计与目标编码baseline的性能差异

Method: 使用Avazu数据集，严格执行out-of-time划分且不允未来泄漏特征，比较了：时间感知目标编码baseline vs 实体历史的多种时间聚合方案（滚动尾窗口、事件计数窗口、间隔窗口、分桶窗口）

Result: 在10%确定样本的两个滚动尾fold上，滑动窗口相对于单纯目标编码使ROC AUC提升0.0066-0.0082，PR AUC提升0.0084-0.0094；事件计数窗口提供小幅稳定改进，间隔窗口和分桶窗口表现不佳

Conclusion: 实践上推荐使用滑动窗口作为默认方案，当追求边际ROC AUC收益时可考虑事件计数窗口，间隔窗口和分桶窗口在该数据集和协议下效果较差

Abstract: This paper studies time aggregation features for XGBoost models in click-through rate prediction. The setting is the Avazu click-through rate prediction dataset with strict out-of-time splits and a no-lookahead feature constraint. Features for hour H use only impressions from hours strictly before H. This paper compares a strong time-aware target encoding baseline to models augmented with entity history time aggregation under several window designs. Across two rolling-tail folds on a deterministic ten percent sample, a trailing window specification improves ROC AUC by about 0.0066 to 0.0082 and PR AUC by about 0.0084 to 0.0094 relative to target encoding alone. Within the time aggregation design grid, event count windows provide the only consistent improvement over trailing windows, and the gain is small. Gap windows and bucketized windows underperform simple trailing windows in this dataset and protocol. These results support a practical default of trailing windows, with an optional event count window when marginal ROC AUC gains matter.

</details>


### [111] [BPE: Behavioral Profiling Ensemble](https://arxiv.org/abs/2601.10024)
*Yanxin Liu,Yunqi Zhang*

Main category: cs.LG

TL;DR: 提出了行为画像集成（BPE）框架，通过为每个模型构建内在的“行为画像”，基于模型对具体测试实例的响应与其画像的偏差来确定集成权重，显著优于传统静态和动态集成方法。


<details>
  <summary>Details</summary>
Motivation: 传统静态集成方法将每个基学习器视为整体分配权重，忽略了不同模型在实例空间不同区域的能力差异；动态集成选择（DES）主要依赖模型间差异，忽视了模型自身内在特性，且严重依赖验证集进行能力评估。

Method: 提出行为画像集成（BPE）框架：1）为每个模型构建内在的行为画像；2）基于模型对具体测试实例的响应与其行为画像的偏差来计算集成权重；3）取代传统的基于模型间分歧或验证集能力评估的方法。

Result: 在合成和真实数据集上的大量实验显示，基于BPE框架的算法在预测准确率、计算效率和存储资源利用方面均显著优于最先进的集成基线方法。

Conclusion: BPE框架通过关注模型内在行为特性而非模型间差异，为集成学习提供了新的范式，在实际应用中展现出优越的性能和效率优势。

Abstract: Ensemble learning is widely recognized as a pivotal strategy for pushing the boundaries of predictive performance. Traditional static ensemble methods, such as Stacking, typically assign weights by treating each base learner as a holistic entity, thereby overlooking the fact that individual models exhibit varying degrees of competence across different regions of the instance space. To address this limitation, Dynamic Ensemble Selection (DES) was introduced. However, both static and dynamic approaches predominantly rely on the divergence among different models as the basis for integration. This inter-model perspective neglects the intrinsic characteristics of the models themselves and necessitates a heavy reliance on validation sets for competence estimation. In this paper, we propose the Behavioral Profiling Ensemble (BPE) framework, which introduces a novel paradigm shift. Unlike traditional methods, BPE constructs a ``behavioral profile'' intrinsic to each model and derives integration weights based on the deviation between the model's response to a specific test instance and its established behavioral profile. Extensive experiments on both synthetic and real-world datasets demonstrate that the algorithm derived from the BPE framework achieves significant improvements over state-of-the-art ensemble baselines. These gains are evident not only in predictive accuracy but also in computational efficiency and storage resource utilization across various scenarios.

</details>


### [112] [Unlabeled Data Can Provably Enhance In-Context Learning of Transformers](https://arxiv.org/abs/2601.10058)
*Renpu Liu,Jing Yang*

Main category: cs.LG

TL;DR: 本文提出了一种增强型情境学习框架，通过结合少量标记样本和大量未标记数据来提升大语言模型的分类性能


<details>
  <summary>Details</summary>
Motivation: 大语言模型的情境学习能力受限于提示中能容纳的少量标记示例，而实际应用中存在大量未标记数据。如何理论性地利用这些未标记数据来提升ICL性能成为一个重要研究问题

Method: 提出增强型ICL框架，在提示中同时包含少量标记示例和一个未标记输入块。在多类线性分类任务中，通过思维链提示让多层transformer模拟期望最大化算法，隐式地从标记和未标记数据中提取信息

Result: 理论证明transformer能够有效模拟EM算法，通过教师强制训练以线性速率收敛到期望解。实验表明增强ICL框架始终优于传统少样本ICL

Conclusion: 这是首个关于未标记数据对transformer情境学习性能影响的理论研究，证明了结合未标记数据的增强ICL能够理论性地提升模型性能

Abstract: Large language models (LLMs) exhibit impressive in-context learning (ICL) capabilities, yet the quality of their predictions is fundamentally limited by the few costly labeled demonstrations that can fit into a prompt. Meanwhile, there exist vast and continuously growing amounts of unlabeled data that may be closely related to the ICL task. How to utilize such unlabeled data to provably enhance the performance of ICL thus becomes an emerging fundamental question. In this work, we propose a novel augmented ICL framework, in which the prompt includes a small set of labeled examples alongside a block of unlabeled inputs. We focus on the multi-class linear classification setting and demonstrate that, with chain-of-thought (CoT) prompting, a multi-layer transformer can effectively emulate an expectation-maximization (EM) algorithm. This enables the transformer to implicitly extract useful information from both labeled and unlabeled data, leading to provable improvements in ICL accuracy. Moreover, we show that such a transformer can be trained via teacher forcing, with its parameters converging to the desired solution at a linear rate. Experiments demonstrate that the augmented ICL framework consistently outperforms conventional few-shot ICL, providing empirical support for our theoretical findings. To the best of our knowledge, this is the first theoretical study on the impact of unlabeled data on the ICL performance of transformers.

</details>


### [113] [Efficient Content-based Recommendation Model Training via Noise-aware Coreset Selection](https://arxiv.org/abs/2601.10067)
*Hung Vinh Tran,Tong Chen,Hechuan Wen,Quoc Viet Hung Nguyen,Bin Cui,Hongzhi Yin*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Content-based recommendation systems (CRSs) utilize content features to predict user-item interactions, serving as essential tools for helping users navigate information-rich web services. However, ensuring the effectiveness of CRSs requires large-scale and even continuous model training to accommodate diverse user preferences, resulting in significant computational costs and resource demands. A promising approach to this challenge is coreset selection, which identifies a small but representative subset of data samples that preserves model quality while reducing training overhead. Yet, the selected coreset is vulnerable to the pervasive noise in user-item interactions, particularly when it is minimally sized. To this end, we propose Noise-aware Coreset Selection (NaCS), a specialized framework for CRSs. NaCS constructs coresets through submodular optimization based on training gradients, while simultaneously correcting noisy labels using a progressively trained model. Meanwhile, we refine the selected coreset by filtering out low-confidence samples through uncertainty quantification, thereby avoid training with unreliable interactions. Through extensive experiments, we show that NaCS produces higher-quality coresets for CRSs while achieving better efficiency than existing coreset selection techniques. Notably, NaCS recovers 93-95\% of full-dataset training performance using merely 1\% of the training data. The source code is available at \href{https://github.com/chenxing1999/nacs}{https://github.com/chenxing1999/nacs}.

</details>


### [114] [Sparse-RL: Breaking the Memory Wall in LLM Reinforcement Learning via Stable Sparse Rollouts](https://arxiv.org/abs/2601.10079)
*Sijia Luo,Xiaokang Zhang,Yuxuan Hu,Bohan Zhang,Ke Wang,Jinbo Su,Mengshu Sun,Lei Liang,Jing Zhang*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Reinforcement Learning (RL) has become essential for eliciting complex reasoning capabilities in Large Language Models (LLMs). However, the substantial memory overhead of storing Key-Value (KV) caches during long-horizon rollouts acts as a critical bottleneck, often prohibiting efficient training on limited hardware. While existing KV compression techniques offer a remedy for inference, directly applying them to RL training induces a severe policy mismatch, leading to catastrophic performance collapse. To address this, we introduce Sparse-RL empowers stable RL training under sparse rollouts. We show that instability arises from a fundamental policy mismatch among the dense old policy, the sparse sampler policy, and the learner policy. To mitigate this issue, Sparse-RL incorporates Sparsity-Aware Rejection Sampling and Importance-based Reweighting to correct the off-policy bias introduced by compression-induced information loss. Experimental results show that Sparse-RL reduces rollout overhead compared to dense baselines while preserving the performance. Furthermore, Sparse-RL inherently implements sparsity-aware training, significantly enhancing model robustness during sparse inference deployment.

</details>


### [115] [Bayesian Meta-Analyses Could Be More: A Case Study in Trial of Labor After a Cesarean-section Outcomes and Complications](https://arxiv.org/abs/2601.10089)
*Ashley Klein,Edward Raff,Marcia DesJardin*

Main category: cs.LG

TL;DR: 本文提出一种贝叶斯方法来处理医学元分析中关键决策变量缺失的问题，通过TOLAC案例证明了该方法能为医生提供决策支持


<details>
  <summary>Details</summary>
Motivation: 传统医学元分析的可靠性受到关键决策变量缺失的影响，导致效应大小不明确、结论不可靠。为此需要新方法来评估阳性效应主张是否成立

Method: 构建贝叶斯方法来处理医学研究中常见的关键变量缺失场景，将其应用于剖宫产后试产(TOLAC)的评估，协助产科医生在干预措施有限的情况下做出决策

Result: 该方法显示了实用性，能为TOLAC情况下提供必要的支持，帮助医生推进患者护理

Conclusion: 贝叶斯方法能够解决医学元分析中关键决策变量缺失的问题，为临床决策提供更可靠的依据，在TOLAC等复杂医疗场景中具有重要应用价值

Abstract: The meta-analysis's utility is dependent on previous studies having accurately captured the variables of interest, but in medical studies, a key decision variable that impacts a physician's decisions was not captured. This results in an unknown effect size and unreliable conclusions. A Bayesian approach may allow analysis to determine if the claim of a positive effect is still warranted, and we build a Bayesian approach to this common medical scenario. To demonstrate its utility, we assist professional OBGYNs in evaluating Trial of Labor After a Cesarean-section (TOLAC) situations where few interventions are available for patients and find the support needed for physicians to advance patient care.

</details>


### [116] [LeMoF: Level-guided Multimodal Fusion for Heterogeneous Clinical Data](https://arxiv.org/abs/2601.10092)
*Jongseok Kim,Seongae Kang,Jonghwan Shin,Yuhan Lee,Ohyun Jo*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Multimodal clinical prediction is widely used to integrate heterogeneous data such as Electronic Health Records (EHR) and biosignals. However, existing methods tend to rely on static modality integration schemes and simple fusion strategies. As a result, they fail to fully exploit modality-specific representations. In this paper, we propose Level-guided Modal Fusion (LeMoF), a novel framework that selectively integrates level-guided representations within each modality. Each level refers to a representation extracted from a different layer of the encoder. LeMoF explicitly separates and learns global modality-level predictions from level-specific discriminative representations. This design enables LeMoF to achieve a balanced performance between prediction stability and discriminative capability even in heterogeneous clinical environments. Experiments on length of stay prediction using Intensive Care Unit (ICU) data demonstrate that LeMoF consistently outperforms existing state-of-the-art multimodal fusion techniques across various encoder configurations. We also confirmed that level-wise integration is a key factor in achieving robust predictive performance across various clinical conditions.

</details>


### [117] [Multilingual-To-Multimodal (M2M): Unlocking New Languages with Monolingual Text](https://arxiv.org/abs/2601.10096)
*Piyush Singh Pasi*

Main category: cs.LG

TL;DR: METAL是一个轻量级的视觉-语言对齐方法，只学习少数线性层就能将多语言文本嵌入映射到多模态空间，实现了英语保持94.9% Recall@10，并在11种未见语言上平均达到89.5% Recall@10的零样本传输性能。方法不仅适用于图像-文本检索，还泛化到音频-文本检索和跨语言文本到图像生成。


<details>
  <summary>Details</summary>
Motivation: 当前多模态模型在英语上表现出色，但在其他语言上性能大幅下降，主要因为缺乏多语言多模态数据。现有解决方案过度依赖机器翻译，且多语言文本建模的进展未被充分利用。

Method: METAL方法仅使用英语文本学习少量线性层，将这些层学习到的对齐关系应用于将多语言文本嵌入映射到多模态空间。方法轻量且不需要多语言对齐数据。

Result: 在XTD文本到图像检索任务中，METAL在英语上匹配基线性能(94.9% Recall@10)，在11种未见语言上平均达到89.5% Recall@10。t-SNE可视化显示多语言嵌入与多模态表征紧密对齐，权重分析表明变换重塑了嵌入几何结构而非简单旋转。方法也能泛化到音频-文本检索和跨语言文本到图像生成。

Conclusion: METAL是一个简单有效的多语言多模态对齐方法，仅需学习少量参数就能实现强大的零样本跨语言传输。作者发布了代码、检查点和多语言评测数据集，为推动多语言多模态研究提供了资源基础。

Abstract: Multimodal models excel in English, supported by abundant image-text and audio-text data, but performance drops sharply for other languages due to limited multilingual multimodal resources. Existing solutions rely heavily on machine translation, while advances in multilingual text modeling remain underutilized. We introduce METAL, a lightweight alignment method that learns only a few linear layers using English text alone to map multilingual text embeddings into a multimodal space. Despite its simplicity, METAL matches baseline performance in English (94.9 percent Recall at 10) and achieves strong zero-shot transfer (89.5 percent Recall at 10 averaged across 11 languages, 10 unseen) on XTD text-to-image retrieval. Qualitative t-SNE visualizations show that multilingual embeddings align tightly with multimodal representations, while weight analysis reveals that the transformation reshapes embedding geometry rather than performing trivial rotations. Beyond image-text retrieval, METAL generalizes to audio-text retrieval and cross-lingual text-to-image generation. We release code and checkpoints at https://github.com/m2m-codebase/M2M , as well as multilingual evaluation datasets including MSCOCO Multilingual 30K (https://huggingface.co/datasets/piyushsinghpasi/mscoco-multilingual-30k ), AudioCaps Multilingual (https://huggingface.co/datasets/piyushsinghpasi/audiocaps-multilingual ), and Clotho Multilingual (https://huggingface.co/datasets/piyushsinghpasi/clotho-multilingual ), to facilitate further research.

</details>


### [118] [Step-by-Step Causality: Transparent Causal Discovery with Multi-Agent Tree-Query and Adversarial Confidence Estimation](https://arxiv.org/abs/2601.10137)
*Ziyi Ding,Chenfei Ye-Hao,Zheyuan Wang,Xiao-Ping Zhang*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Causal discovery aims to recover ``what causes what'', but classical constraint-based methods (e.g., PC, FCI) suffer from error propagation, and recent LLM-based causal oracles often behave as opaque, confidence-free black boxes. This paper introduces Tree-Query, a tree-structured, multi-expert LLM framework that reduces pairwise causal discovery to a short sequence of queries about backdoor paths, (in)dependence, latent confounding, and causal direction, yielding interpretable judgments with robustness-aware confidence scores. Theoretical guarantees are provided for asymptotic identifiability of four pairwise relations. On data-free benchmarks derived from Mooij et al. and UCI causal graphs, Tree-Query improves structural metrics over direct LLM baselines, and a diet--weight case study illustrates confounder screening and stable, high-confidence causal conclusions. Tree-Query thus offers a principled way to obtain data-free causal priors from LLMs that can complement downstream data-driven causal discovery. Code is available at https://anonymous.4open.science/r/Repo-9B3E-4F96.

</details>


### [119] [Understanding and Preserving Safety in Fine-Tuned LLMs](https://arxiv.org/abs/2601.10141)
*Jiawen Zhang,Yangfan Hu,Kejia Chen,Lipeng He,Jiachen Ma,Jian Lou,Dan Li,Jian Liu,Xiaohu Yang,Ruoxi Jia*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Fine-tuning is an essential and pervasive functionality for applying large language models (LLMs) to downstream tasks. However, it has the potential to substantially degrade safety alignment, e.g., by greatly increasing susceptibility to jailbreak attacks, even when the fine-tuning data is entirely harmless. Despite garnering growing attention in defense efforts during the fine-tuning stage, existing methods struggle with a persistent safety-utility dilemma: emphasizing safety compromises task performance, whereas prioritizing utility typically requires deep fine-tuning that inevitably leads to steep safety declination.
  In this work, we address this dilemma by shedding new light on the geometric interaction between safety- and utility-oriented gradients in safety-aligned LLMs. Through systematic empirical analysis, we uncover three key insights: (I) safety gradients lie in a low-rank subspace, while utility gradients span a broader high-dimensional space; (II) these subspaces are often negatively correlated, causing directional conflicts during fine-tuning; and (III) the dominant safety direction can be efficiently estimated from a single sample. Building upon these novel insights, we propose safety-preserving fine-tuning (SPF), a lightweight approach that explicitly removes gradient components conflicting with the low-rank safety subspace. Theoretically, we show that SPF guarantees utility convergence while bounding safety drift. Empirically, SPF consistently maintains downstream task performance and recovers nearly all pre-trained safety alignment, even under adversarial fine-tuning scenarios. Furthermore, SPF exhibits robust resistance to both deep fine-tuning and dynamic jailbreak attacks. Together, our findings provide new mechanistic understanding and practical guidance toward always-aligned LLM fine-tuning.

</details>


### [120] [Simple Network Graph Comparative Learning](https://arxiv.org/abs/2601.10150)
*Qiang Yu,Xinran Cheng,Shiqiang Xu,Chuanyi Liu*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: The effectiveness of contrastive learning methods has been widely recognized in the field of graph learning, especially in contexts where graph data often lack labels or are difficult to label. However, the application of these methods to node classification tasks still faces a number of challenges. First, existing data enhancement techniques may lead to significant differences from the original view when generating new views, which may weaken the relevance of the view and affect the efficiency of model training. Second, the vast majority of existing graph comparison learning algorithms rely on the use of a large number of negative samples. To address the above challenges, this study proposes a novel node classification contrast learning method called Simple Network Graph Comparative Learning (SNGCL). Specifically, SNGCL employs a superimposed multilayer Laplace smoothing filter as a step in processing the data to obtain global and local feature smoothing matrices, respectively, which are thus passed into the target and online networks of the siamese network, and finally employs an improved triple recombination loss function to bring the intra-class distance closer and the inter-class distance farther. We have compared SNGCL with state-of-the-art models in node classification tasks, and the experimental results show that SNGCL is strongly competitive in most tasks.

</details>


### [121] [LOOKAT: Lookup-Optimized Key-Attention for Memory-Efficient Transformers](https://arxiv.org/abs/2601.10155)
*Aryan Karmore*

Main category: cs.LG

TL;DR: LOOKAT方法在不修改架构或重新训练的情况下，通过对KV缓存应用乘积量化和非对称距离计算，将注意力从内存密集型转为计算密集型，实现了高压缩比下的输出保真度。


<details>
  <summary>Details</summary>
Motivation: 当前的量化方法在压缩KV缓存存储时，仍需要在注意力计算时将INT4/INT8密钥反量化为FP16，未能有效减少带宽。注意力评分与向量数据库中的内积相似性搜索在数学上等价，因此可以利用向量数据库的压缩技术来更好地压缩KV缓存。

Method: 提出LOOKAT方法，将乘积量化和非对称距离计算应用于Transformer架构：1. 将关键向量分解到子空间；2. 学习码本；3. 通过查找表计算注意力表。这种方法将注意力从内存密集型转为计算密集型。

Result: 在GPT-2上测试：1. 64倍压缩时达到95.7%输出保真度；2. 32倍压缩时达到95.0%输出保真度；3. 无需架构修改或重新训练；4. 保持秩相关性ρ>0.95；5. 理论分析证实秩相关性按O(d_k/mK)下降，在长达1024个token的序列长度上得到验证。

Conclusion: LOOKAT方法通过将向量数据库压缩技术应用于KV缓存，实现了高压缩比下的注意力计算优化，为在边缘设备上部署大型语言模型提供了有效的解决方案。

Abstract: Compressing the KV cache is a required step to deploy large language models on edge devices. Current quantization methods compress storage but fail to reduce bandwidth as attention calculation requires dequantizing keys from INT4/INT8 to FP16 before use. We observe that attention scoring is mathematically equivalent to the inner product similarity search and we can apply some compression techniques from vector databases to compress KV-cache better. We propose LOOKAT, which applies product quantization and asymmetric distance computation, to transformer architecture by decomposing key vectors into subspaces, learning codebooks and computing attention tables via lookup tables. This transforms attention from memory-bound to compute-bound. LOOKAT achieves 64 $\times$ compression at 95.7\% output fidelity and 32 $\times$ compression at 95.0\% fidelity when tested on GPT-2. LOOKAT requires no architecture changes or training while maintaining rank correlation $ρ> 0.95$. Theoretical analysis confirms that rank correlation degrades as $O(d_k/mK)$, with guarantees validated across sequence lengths up to 1024 tokens.

</details>


### [122] [CC-OR-Net: A Unified Framework for LTV Prediction through Structural Decoupling](https://arxiv.org/abs/2601.10176)
*Mingyu Zhao,Haoran Bai,Yu Tian,Bing Zhu,Hengliang Luo*

Main category: cs.LG

TL;DR: 针对客户生命周期价值（LTV）预测中零膨胀和长尾数据分布的挑战，本文提出了CC-OR-Net框架，通过结构分解实现更稳健的排序与回归解耦，在真实数据集上取得了优于现有方法的性能。


<details>
  <summary>Details</summary>
Motivation: LTV预测面临两大挑战：绝大多数低价值用户在数量上压倒少数但极其重要的'鲸鱼用户'；即使在低价值用户群体内部也存在显著的异质性。现有方法要么依赖严格的统计假设，要么试图通过排序桶来解耦排序和回归，但通常通过损失约束而非内在架构设计来保证有序性，难以平衡全局准确性和高价值用户的预测精度。

Method: 提出了**CC-OR-Net**框架，该框架通过**结构分解**实现排序与回归的稳健解耦。包含三个核心组件：1) **结构有序分解模块**，确保排序在架构层面得到保证；2) **桶内残差模块**，实现细粒度的回归；3) **目标高价值增强模块**，专门优化顶层用户的预测精度。

Result: 在包含超过3亿用户的真实世界数据集上进行评估，CC-OR-Net在所有关键业务指标上都取得了更好的权衡，超越了现有最先进的方法。

Conclusion: CC-OR-Net框架为LTV预测提供了一个统一且商业价值高的解决方案，通过结构分解有效解决了数据分布带来的挑战，并在大规模真实数据中验证了其优越性。

Abstract: Customer Lifetime Value (LTV) prediction, a central problem in modern marketing, is characterized by a unique zero-inflated and long-tail data distribution. This distribution presents two fundamental challenges: (1) the vast majority of low-to-medium value users numerically overwhelm the small but critically important segment of high-value "whale" users, and (2) significant value heterogeneity exists even within the low-to-medium value user base. Common approaches either rely on rigid statistical assumptions or attempt to decouple ranking and regression using ordered buckets; however, they often enforce ordinality through loss-based constraints rather than inherent architectural design, failing to balance global accuracy with high-value precision. To address this gap, we propose \textbf{C}onditional \textbf{C}ascaded \textbf{O}rdinal-\textbf{R}esidual Networks \textbf{(CC-OR-Net)}, a novel unified framework that achieves a more robust decoupling through \textbf{structural decomposition}, where ranking is architecturally guaranteed. CC-OR-Net integrates three specialized components: a \textit{structural ordinal decomposition module} for robust ranking, an \textit{intra-bucket residual module} for fine-grained regression, and a \textit{targeted high-value augmentation module} for precision on top-tier users. Evaluated on real-world datasets with over 300M users, CC-OR-Net achieves a superior trade-off across all key business metrics, outperforming state-of-the-art methods in creating a holistic and commercially valuable LTV prediction solution.

</details>


### [123] [Graph Regularized PCA](https://arxiv.org/abs/2601.10199)
*Antonio Briola,Marwin Schmidt,Fabio Caccioli,Carlos Ros Perez,James Singleton,Christian Michler,Tomaso Aste*

Main category: cs.LG

TL;DR: 针对高维数据中变量间的依赖关系违反各向同性噪声假设的问题，本文提出了图正则化主成分分析（GR-PCA），通过引入图拉普拉斯惩罚来学习稀疏精度图并偏置载荷向量，抑制高频噪声同时保留图相干低频信号。


<details>
  <summary>Details</summary>
Motivation: 传统PCA假设噪声在各特征间独立同分布于球形协方差，但高维数据常存在特征间依赖，违反该假设。需要开发能利用数据特征依赖结构的降维方法，提高结果的解释性和结构保真度。

Method: GR-PCA结合图正则化：1）学习特征间的稀疏精度图；2）通过图拉普拉斯惩罚将载荷向量偏向低频傅里叶模式；3）抑制高频信号，保留与条件关系一致的图相干低频信号。方法具有模块化、易实现和可扩展的特点。

Result: 实验表明GR-PCA在合成数据上：1）将方差集中在目标支撑上；2）产生较低图拉普拉斯能量的载荷；3）保持样本外重建的竞争力；4）当存在高频信号时，防止过拟合并提高结构保真度。当高频信号与图相关时，GR-PCA相比PCA优势明显；当高频信号接近旋转不变时，PCA仍具竞争力。

Conclusion: GR-PCA为结构感知的降维提供了实用途径，在不牺牲预测性能的前提下提高了结构保真度，特别适用于特征间存在依赖关系且高频信号与图结构相关的高维数据分析场景。

Abstract: High-dimensional data often exhibit dependencies among variables that violate the isotropic-noise assumption under which principal component analysis (PCA) is optimal. For cases where the noise is not independent and identically distributed across features (i.e., the covariance is not spherical) we introduce Graph Regularized PCA (GR-PCA). It is a graph-based regularization of PCA that incorporates the dependency structure of the data features by learning a sparse precision graph and biasing loadings toward the low-frequency Fourier modes of the corresponding graph Laplacian. Consequently, high-frequency signals are suppressed, while graph-coherent low-frequency ones are preserved, yielding interpretable principal components aligned with conditional relationships. We evaluate GR-PCA on synthetic data spanning diverse graph topologies, signal-to-noise ratios, and sparsity levels. Compared to mainstream alternatives, it concentrates variance on the intended support, produces loadings with lower graph-Laplacian energy, and remains competitive in out-of-sample reconstruction. When high-frequency signals are present, the graph Laplacian penalty prevents overfitting, reducing the reconstruction accuracy but improving structural fidelity. The advantage over PCA is most pronounced when high-frequency signals are graph-correlated, whereas PCA remains competitive when such signals are nearly rotationally invariant. The procedure is simple to implement, modular with respect to the precision estimator, and scalable, providing a practical route to structure-aware dimensionality reduction that improves structural fidelity without sacrificing predictive performance.

</details>


### [124] [PRL: Process Reward Learning Improves LLMs' Reasoning Ability and Broadens the Reasoning Boundary](https://arxiv.org/abs/2601.10201)
*Jiarui Yao,Ruida Wang,Tong Zhang*

Main category: cs.LG

TL;DR: 提出了Process Reward Learning (PRL)方法，解决现有LLM推理训练中的过程监督不足、效率低下和理论支撑薄弱问题


<details>
  <summary>Details</summary>
Motivation: 当前LLM推理能力提升工作大多依赖轨迹级别的结果奖励，缺乏推理过程中的细粒度监督。现有结合过程信号的训练框架通常需要繁琐的额外步骤（如MCTS、训练单独奖励模型等），效率低下，且过程信号设计缺乏严谨理论支持

Method: 提出过程奖励学习(PRL)，将熵正则化强化学习目标分解为中间步骤，分配严谨的过程奖励。从理论动机出发，推导出与奖励最大化加策略模型和参考模型间KL散度惩罚项等价的PRL公式设计，将结果奖励转化为过程监督信号

Result: 实验结果表明，PRL不仅改善了LLM推理能力的平均性能(由average @ n衡量)，还通过提高pass @ n指标拓宽了推理边界

Conclusion: 大量实验验证了PRL的有效性和泛化能力，该方法能更好地指导强化学习优化中的探索过程

Abstract: Improving the reasoning abilities of Large Language Models (LLMs) has been a continuous topic recently. But most relevant works are based on outcome rewards at the trajectory level, missing fine-grained supervision during the reasoning process. Other existing training frameworks that try to combine process signals together to optimize LLMs also rely heavily on tedious additional steps like MCTS, training a separate reward model, etc., doing harm to the training efficiency. Moreover, the intuition behind the process signals design lacks rigorous theoretical support, leaving the understanding of the optimization mechanism opaque. In this paper, we propose Process Reward Learning (PRL), which decomposes the entropy regularized reinforcement learning objective into intermediate steps, with rigorous process rewards that could be assigned to models accordingly. Starting from theoretical motivation, we derive the formulation of PRL that is essentially equivalent to the objective of reward maximization plus a KL-divergence penalty term between the policy model and a reference model. However, PRL could turn the outcome reward into process supervision signals, which helps better guide the exploration during RL optimization. From our experiment results, we demonstrate that PRL not only improves the average performance for LLMs' reasoning ability measured by average @ n, but also broadens the reasoning boundary by improving the pass @ n metric. Extensive experiments show the effectiveness of PRL could be verified and generalized.

</details>


### [125] [Fundamental Limitations of Favorable Privacy-Utility Guarantees for DP-SGD](https://arxiv.org/abs/2601.10237)
*Murat Bilgehan Ertan,Marten van Dijk*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Differentially Private Stochastic Gradient Descent (DP-SGD) is the dominant paradigm for private training, but its fundamental limitations under worst-case adversarial privacy definitions remain poorly understood. We analyze DP-SGD in the $f$-differential privacy framework, which characterizes privacy via hypothesis-testing trade-off curves, and study shuffled sampling over a single epoch with $M$ gradient updates. We derive an explicit suboptimal upper bound on the achievable trade-off curve. This result induces a geometric lower bound on the separation $κ$ which is the maximum distance between the mechanism's trade-off curve and the ideal random-guessing line. Because a large separation implies significant adversarial advantage, meaningful privacy requires small $κ$. However, we prove that enforcing a small separation imposes a strict lower bound on the Gaussian noise multiplier $σ$, which directly limits the achievable utility. In particular, under the standard worst-case adversarial model, shuffled DP-SGD must satisfy
  $σ\ge \frac{1}{\sqrt{2\ln M}}$ $\quad\text{or}\quad$ $κ\ge\ \frac{1}{\sqrt{8}}\!\left(1-\frac{1}{\sqrt{4π\ln M}}\right)$,
  and thus cannot simultaneously achieve strong privacy and high utility. Although this bound vanishes asymptotically as $M \to \infty$, the convergence is extremely slow: even for practically relevant numbers of updates the required noise magnitude remains substantial. We further show that the same limitation extends to Poisson subsampling up to constant factors. Our experiments confirm that the noise levels implied by this bound leads to significant accuracy degradation at realistic training settings, thus showing a critical bottleneck in DP-SGD under standard worst-case adversarial assumptions.

</details>


### [126] [X-SAM: Boosting Sharpness-Aware Minimization with Dominant-Eigenvector Gradient Correction](https://arxiv.org/abs/2601.10251)
*Hongru Duan,Yongle Chen,Lei Guan*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Sharpness-Aware Minimization (SAM) aims to improve generalization by minimizing a worst-case perturbed loss over a small neighborhood of model parameters. However, during training, its optimization behavior does not always align with theoretical expectations, since both sharp and flat regions may yield a small perturbed loss. In such cases, the gradient may still point toward sharp regions, failing to achieve the intended effect of SAM. To address this issue, we investigate SAM from a spectral and geometric perspective: specifically, we utilize the angle between the gradient and the leading eigenvector of the Hessian as a measure of sharpness. Our analysis illustrates that when this angle is less than or equal to ninety degrees, the effect of SAM's sharpness regularization can be weakened. Furthermore, we propose an explicit eigenvector-aligned SAM (X-SAM), which corrects the gradient via orthogonal decomposition along the top eigenvector, enabling more direct and efficient regularization of the Hessian's maximum eigenvalue. We prove X-SAM's convergence and superior generalization, with extensive experimental evaluations confirming both theoretical and practical advantages.

</details>


### [127] [Early Fault Detection on CMAPSS with Unsupervised LSTM Autoencoders](https://arxiv.org/abs/2601.10269)
*P. Sánchez,K. Reyes,B. Radu,E. Fernández*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: This paper introduces an unsupervised health-monitoring framework for turbofan engines that does not require run-to-failure labels. First, operating-condition effects in NASA CMAPSS sensor streams are removed via regression-based normalisation; then a Long Short-Term Memory (LSTM) autoencoder is trained only on the healthy portion of each trajectory. Persistent reconstruction error, estimated using an adaptive data-driven threshold, triggers real-time alerts without hand-tuned rules. Benchmark results show high recall and low false-alarm rates across multiple operating regimes, demonstrating that the method can be deployed quickly, scale to diverse fleets, and serve as a complementary early-warning layer to Remaining Useful Life models.

</details>


### [128] [SPIKE: Sparse Koopman Regularization for Physics-Informed Neural Networks](https://arxiv.org/abs/2601.10282)
*Jose Marie Antonio Minoza*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Physics-Informed Neural Networks (PINNs) provide a mesh-free approach for solving differential equations by embedding physical constraints into neural network training. However, PINNs tend to overfit within the training domain, leading to poor generalization when extrapolating beyond trained spatiotemporal regions. This work presents SPIKE (Sparse Physics-Informed Koopman-Enhanced), a framework that regularizes PINNs with continuous-time Koopman operators to learn parsimonious dynamics representations. By enforcing linear dynamics $dz/dt = Az$ in a learned observable space, both PIKE (without explicit sparsity) and SPIKE (with L1 regularization on $A$) learn sparse generator matrices, embodying the parsimony principle that complex dynamics admit low-dimensional structure. Experiments across parabolic, hyperbolic, dispersive, and stiff PDEs, including fluid dynamics (Navier-Stokes) and chaotic ODEs (Lorenz), demonstrate consistent improvements in temporal extrapolation, spatial generalization, and long-term prediction accuracy. The continuous-time formulation with matrix exponential integration provides unconditional stability for stiff systems while avoiding diagonal dominance issues inherent in discrete-time Koopman operators.

</details>


### [129] [We Need a More Robust Classifier: Dual Causal Learning Empowers Domain-Incremental Time Series Classification](https://arxiv.org/abs/2601.10312)
*Zhipeng Liu,Peibo Duan,Xuan Tang,Haodong Jing,Mingyang Geng,Yongsheng Huang,Jialu Xu,Bin Zhang,Binwu Wang*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: The World Wide Web thrives on intelligent services that rely on accurate time series classification, which has recently witnessed significant progress driven by advances in deep learning. However, existing studies face challenges in domain incremental learning. In this paper, we propose a lightweight and robust dual-causal disentanglement framework (DualCD) to enhance the robustness of models under domain incremental scenarios, which can be seamlessly integrated into time series classification models. Specifically, DualCD first introduces a temporal feature disentanglement module to capture class-causal features and spurious features. The causal features can offer sufficient predictive power to support the classifier in domain incremental learning settings. To accurately capture these causal features, we further design a dual-causal intervention mechanism to eliminate the influence of both intra-class and inter-class confounding features. This mechanism constructs variant samples by combining the current class's causal features with intra-class spurious features and with causal features from other classes. The causal intervention loss encourages the model to accurately predict the labels of these variant samples based solely on the causal features. Extensive experiments on multiple datasets and models demonstrate that DualCD effectively improves performance in domain incremental scenarios. We summarize our rich experiments into a comprehensive benchmark to facilitate research in domain incremental time series classification.

</details>


### [130] [Meta Dynamic Graph for Traffic Flow Prediction](https://arxiv.org/abs/2601.10328)
*Yiqing Zou,Hanning Yuan,Qianyu Yang,Ziqiang Yuan,Shuliang Wang,Sijie Ruan*

Main category: cs.LG

TL;DR: 论文提出了名为MetaDG的新框架来改进交通流预测，通过动态图结构统一建模时空动力学和异质性，超越传统仅考虑拓扑动态的方法。


<details>
  <summary>Details</summary>
Motivation: 交通流预测的核心挑战是建模复杂的时空依赖关系。现有方法分离处理空间和时间依赖，且动态建模常限于拓扑结构变化，时空异质性也被分割处理。需要更全面的动态建模来统一这些方面。

Method: 提出MetaDG框架，利用节点表示的动态图结构显式建模时空动力学。该方法生成动态邻接矩阵和元参数，扩展动态建模范围，并将时空异质性捕获统一到一个维度中。

Result: 在四个真实世界数据集上的广泛实验验证了MetaDG的有效性。

Conclusion: MetaDG通过动态图结构统一建模时空动力学和异质性，超越了传统仅关注拓扑动态的方法，为交通预测提供了更全面的解决方案。

Abstract: Traffic flow prediction is a typical spatio-temporal prediction problem and has a wide range of applications. The core challenge lies in modeling the underlying complex spatio-temporal dependencies. Various methods have been proposed, and recent studies show that the modeling of dynamics is useful to meet the core challenge. While handling spatial dependencies and temporal dependencies using separate base model structures may hinder the modeling of spatio-temporal correlations, the modeling of dynamics can bridge this gap. Incorporating spatio-temporal heterogeneity also advances the main goal, since it can extend the parameter space and allow more flexibility. Despite these advances, two limitations persist: 1) the modeling of dynamics is often limited to the dynamics of spatial topology (e.g., adjacency matrix changes), which, however, can be extended to a broader scope; 2) the modeling of heterogeneity is often separated for spatial and temporal dimensions, but this gap can also be bridged by the modeling of dynamics. To address the above limitations, we propose a novel framework for traffic prediction, called Meta Dynamic Graph (MetaDG). MetaDG leverages dynamic graph structures of node representations to explicitly model spatio-temporal dynamics. This generates both dynamic adjacency matrices and meta-parameters, extending dynamic modeling beyond topology while unifying the capture of spatio-temporal heterogeneity into a single dimension. Extensive experiments on four real-world datasets validate the effectiveness of MetaDG.

</details>


### [131] [SuS: Strategy-aware Surprise for Intrinsic Exploration](https://arxiv.org/abs/2601.10349)
*Mark Kashirskiy,Ilya Makarov*

Main category: cs.LG

TL;DR: 提出一个新的内在激励框架 Strategy-aware Surprise (SuS)，通过策略稳定性和策略意外度信号来增强强化学习中的探索性能，在数学推理任务上显著提升准确性和解决方案多样性。


<details>
  <summary>Details</summary>
Motivation: 传统基于好奇心的探索方法仅依赖于状态预测误差，这在大语言模型的数学推理任务中限制了策略的多样化探索效果。

Method: 提出了 Strategy-aware Surprise (SuS) 框架，包含两个互补组件：Strategy Stability (SS) 和 Strategy Surprise (SuS)。SS 衡量行为策略在时间步上的稳定性，SuS 捕捉相对于当前策略表示的意外结果。通过学习的加权系数组整合这两个信号进行奖励计算。

Result: 在数学推理任务上使用大语言模型进行评估，相比基线方法实现了 17.4% 的 Pass@1 提升和 26.4% 的 Pass@5 提升，同时在整个训练过程中保持了更高的策略多样性。

Conclusion: SuS 框架通过策略稳定性和策略意外度的组合有效地促进了多样化的策略探索，为强化学习在复杂推理任务中的应用提供了新方法。

Abstract: We propose Strategy-aware Surprise (SuS), a novel intrinsic motivation framework that uses pre-post prediction mismatch as a novelty signal for exploration in reinforcement learning. Unlike traditional curiosity-driven methods that rely solely on state prediction error, SuS introduces two complementary components: Strategy Stability (SS) and Strategy Surprise (SuS). SS measures consistency in behavioral strategy across temporal steps, while SuS captures unexpected outcomes relative to the agent's current strategy representation. Our combined reward formulation leverages both signals through learned weighting coefficients. We evaluate SuS on mathematical reasoning tasks using large language models, demonstrating significant improvements in both accuracy and solution diversity. Ablation studies confirm that removing either component results in at least 10% performance degradation, validating the synergistic nature of our approach. SuS achieves 17.4% improvement in Pass@1 and 26.4% improvement in Pass@5 compared to baseline methods, while maintaining higher strategy diversity throughout training.

</details>


### [132] [EvoMorph: Counterfactual Explanations for Continuous Time-Series Extrinsic Regression Applied to Photoplethysmography](https://arxiv.org/abs/2601.10356)
*Mesut Ceylan,Alexis Tabin,Patrick Langer,Elgar Fleisch,Filipe Barata*

Main category: cs.LG

TL;DR: EvoMorph: 用于时间序列外因回归的生理可信及多样化反事实解释生成的多目标演化框架


<details>
  <summary>Details</summary>
Motivation: 目前时间序列反事实解释方法主要限于分类任务、忽略波形形态学且常生成生理不可信信号，限制了其在连续生物医学时间序列中的应用，临床需要理解模型预测的稳定性及信号变化对预测的影响

Method: EvoMorph采用多目标演化框架，优化基于可解释信号描述子的形态学感知目标，应用变换以保持波形结构，生成生理可信的多样化反事实解释

Result: 在三个PPG数据集（心率、呼吸率、血氧饱和度）上评估EvoMorph，优于近邻基线；案例研究表明其可作为不确定性量化工具，将反事实敏感性与自举集成不确定性及数据密度度量相关联

Conclusion: EvoMorph能够生成连续生物医学信号的生理感知反事实，支持不确定性感知的解释性，推进临床时间序列应用的可信模型分析

Abstract: Wearable devices enable continuous, population-scale monitoring of physiological signals, such as photoplethysmography (PPG), creating new opportunities for data-driven clinical assessment. Time-series extrinsic regression (TSER) models increasingly leverage PPG signals to estimate clinically relevant outcomes, including heart rate, respiratory rate, and oxygen saturation. For clinical reasoning and trust, however, single point estimates alone are insufficient: clinicians must also understand whether predictions are stable under physiologically plausible variations and to what extent realistic, attainable changes in physiological signals would meaningfully alter a model's prediction. Counterfactual explanations (CFE) address these "what-if" questions, yet existing time series CFE generation methods are largely restricted to classification, overlook waveform morphology, and often produce physiologically implausible signals, limiting their applicability to continuous biomedical time series. To address these limitations, we introduce EvoMorph, a multi-objective evolutionary framework for generating physiologically plausible and diverse CFE for TSER applications. EvoMorph optimizes morphology-aware objectives defined on interpretable signal descriptors and applies transformations to preserve the waveform structure. We evaluated EvoMorph on three PPG datasets (heart rate, respiratory rate, and oxygen saturation) against a nearest-unlike-neighbor baseline. In addition, in a case study, we evaluated EvoMorph as a tool for uncertainty quantification by relating counterfactual sensitivity to bootstrap-ensemble uncertainty and data-density measures. Overall, EvoMorph enables the generation of physiologically-aware counterfactuals for continuous biomedical signals and supports uncertainty-aware interpretability, advancing trustworthy model analysis for clinical time-series applications.

</details>


### [133] [Reinforcement Learning with Multi-Step Lookahead Information Via Adaptive Batching](https://arxiv.org/abs/2601.10418)
*Nadav Merlis*

Main category: cs.LG

TL;DR: 该论文研究具有多步骤前瞻信息的表格强化学习问题，提出自适应批量策略来解决传统启发式方法的局限性，并设计了乐观遗憾最小化算法来学习最优策略。


<details>
  <summary>Details</summary>
Motivation: 针对具有前瞻信息的强化学习问题，现有启发式方法（固定批量策略和模型预测控制）存在局限性，需要开发更优的策略利用前瞻信息。

Method: 提出自适应批量策略，根据状态动态调整前瞻信息处理方式；推导最优贝尔曼方程；设计了与环境交互时可学习最优ABP的乐观遗憾最小化算法。

Result: 提出的算法遗憾界在最优阶上，仅受前瞻视界ℓ的潜在影响，通常ℓ可视为小常数。

Conclusion: 自适应批量策略能更有效地利用前瞻信息，算法能学习最优策略且理论性能具有保证，为前瞻强化学习提供了新方法。

Abstract: We study tabular reinforcement learning problems with multiple steps of lookahead information. Before acting, the learner observes $\ell$ steps of future transition and reward realizations: the exact state the agent would reach and the rewards it would collect under any possible course of action. While it has been shown that such information can drastically boost the value, finding the optimal policy is NP-hard, and it is common to apply one of two tractable heuristics: processing the lookahead in chunks of predefined sizes ('fixed batching policies'), and model predictive control. We first illustrate the problems with these two approaches and propose utilizing the lookahead in adaptive (state-dependent) batches; we refer to such policies as adaptive batching policies (ABPs). We derive the optimal Bellman equations for these strategies and design an optimistic regret-minimizing algorithm that enables learning the optimal ABP when interacting with unknown environments. Our regret bounds are order-optimal up to a potential factor of the lookahead horizon $\ell$, which can usually be considered a small constant.

</details>


### [134] [DeFlow: Decoupling Manifold Modeling and Value Maximization for Offline Policy Extraction](https://arxiv.org/abs/2601.10471)
*Zhancun Mu*

Main category: cs.LG

TL;DR: DeFlow是一个解耦的离线强化学习框架，利用流匹配技术来准确捕捉复杂的行为流形，通过轻量级精炼模块在数据驱动的信任区域内学习，避免了求解器微分和损失项平衡的问题。


<details>
  <summary>Details</summary>
Motivation: 生成策略优化计算成本高昂，通常需要通过ODE求解器进行反向传播，现有方法常因单步蒸馏而牺牲迭代生成能力。DeFlow旨在解决这一核心挑战，在保持流模型迭代表达能力的同时实现高效稳定的策略优化。

Method: 1. 基于流匹配技术构建行为流形；2. 在流形上定义数据驱动的信任区域；3. 学习轻量级精炼模块而非对整个生成过程进行优化；4. 避免使用ODE求解器微分；5. 无需平衡多个损失项。

Result: 1. 在具有挑战性的OGBench基准测试中取得优越性能；2. 实现了高效的离线到在线适应；3. 保持了流的迭代表达能力；4. 实现了稳定的策略改进。

Conclusion: DeFlow通过解耦的精炼方法在离线强化学习中取得了突破，既保持了生成模型的表达能力，又显著降低了计算成本，为复杂行为建模和策略优化提供了新的有效范式。

Abstract: We present DeFlow, a decoupled offline RL framework that leverages flow matching to faithfully capture complex behavior manifolds. Optimizing generative policies is computationally prohibitive, typically necessitating backpropagation through ODE solvers. We address this by learning a lightweight refinement module within an explicit, data-derived trust region of the flow manifold, rather than sacrificing the iterative generation capability via single-step distillation. This way, we bypass solver differentiation and eliminate the need for balancing loss terms, ensuring stable improvement while fully preserving the flow's iterative expressivity. Empirically, DeFlow achieves superior performance on the challenging OGBench benchmark and demonstrates efficient offline-to-online adaptation.

</details>


### [135] [Communication-Efficient Federated Learning by Exploiting Spatio-Temporal Correlations of Gradients](https://arxiv.org/abs/2601.10491)
*Shenlong Zheng,Zhen Zhang,Yuhui Deng,Geyong Min,Lin Cui*

Main category: cs.LG

TL;DR: GradESTC是一种利用梯度的时空相关性来降低联邦学习通信开销的压缩技术


<details>
  <summary>Details</summary>
Motivation: 现有方法通常只关注单轮梯度的压缩，忽略了梯度在时间维度上的相关性。研究发现梯度不仅具有空间相关性（低秩性），还在相邻轮次间存在强烈的时序相关性。

Method: GradESTC利用空间相关性将完整梯度分解为少量基向量及其组合系数，通过时序相关性仅动态更新部分基向量。每轮通信只需传输轻量级的组合系数和少量更新的基向量，而非完整梯度。

Result: 实验表明，在达到接近收敛的目标准确率时，GradESTC相比最强基线平均减少上行通信39.79%，同时保持与未压缩FedAvg相当的收敛速度和最终准确率。

Conclusion: GradESTC通过有效利用梯度的时空结构，为通信高效的联邦学习提供了一个实用且可扩展的解决方案。

Abstract: Communication overhead is a critical challenge in federated learning, particularly in bandwidth-constrained networks. Although many methods have been proposed to reduce communication overhead, most focus solely on compressing individual gradients, overlooking the temporal correlations among them. Prior studies have shown that gradients exhibit spatial correlations, typically reflected in low-rank structures. Through empirical analysis, we further observe a strong temporal correlation between client gradients across adjacent rounds. Based on these observations, we propose GradESTC, a compression technique that exploits both spatial and temporal gradient correlations. GradESTC exploits spatial correlations to decompose each full gradient into a compact set of basis vectors and corresponding combination coefficients. By exploiting temporal correlations, only a small portion of the basis vectors need to be dynamically updated in each round. GradESTC significantly reduces communication overhead by transmitting lightweight combination coefficients and a limited number of updated basis vectors instead of the full gradients. Extensive experiments show that, upon reaching a target accuracy level near convergence, GradESTC reduces uplink communication by an average of 39.79% compared to the strongest baseline, while maintaining comparable convergence speed and final accuracy to uncompressed FedAvg. By effectively leveraging spatio-temporal gradient structures, GradESTC offers a practical and scalable solution for communication-efficient federated learning.

</details>


### [136] [Projected Microbatch Accumulation yields reference-free proximal policy updates for reinforcement learning](https://arxiv.org/abs/2601.10498)
*Nilin Abrahamsen*

Main category: cs.LG

TL;DR: PROMA是一种用于大语言模型微调的近似策略更新方法，通过投影去除序列梯度分量在微批次间累积策略梯度，实现了更稳定的策略学习。


<details>
  <summary>Details</summary>
Motivation: 现有方法如PPO和GRPO在策略更新中可能引发熵崩溃或过度依赖参考策略，需要一种更稳定、无需复杂调整的近似更新方法。

Method: 在反向传播过程中逐层投影去除序列梯度分量，在微批次间累积策略梯度，无需额外前向/反向传播即可高效实现。

Result: 相比GRPO能更严格控制局部KL散度，实现更稳定的策略学习，且不引发熵崩溃，不依赖参考策略或似然比裁剪。

Conclusion: PROMA提供了一种有效的近似策略更新方法，在大语言模型微调中表现出更好的稳定性，简化了策略优化过程。

Abstract: This note introduces Projected Microbatch Accumulation (PROMA), a proximal policy update method for large language model fine-tuning. PROMA accumulates policy gradients across microbatches by projecting out sequence-wise gradient components before microbatch aggregation. The projection is applied layer-wise during the backward pass, enabling efficient implementation without additional forward or backward passes. Empirically, PROMA enforces tighter control of local KL divergence than GRPO, resulting in more stable policy learning. Unlike PPO and GRPO, PROMA achieves proximal updates without inducing entropy collapse and does not rely on a reference policy or likelihood-ratio clipping.

</details>


### [137] [Transformer-Based Cognitive Radio: Adaptive Modulation Strategies Using Transformer Models](https://arxiv.org/abs/2601.10519)
*Andrea Melis,Andrea Piroddi,Roberto Girau*

Main category: cs.LG

TL;DR: 研究了使用Transformer模型（特别是GPT-2架构）为无线通信生成新型调制方案的方法，通过与传统调制方案在SNR和PSD等指标上的对比，证明Transformer生成的调制方案具有可比甚至更优的性能。


<details>
  <summary>Details</summary>
Motivation: 认知无线电系统可以通过机器学习技术提升频谱效率、鲁棒性和安全性，特别是利用Transformer模型生成新颖的调制方案来增强无线通信系统。

Method: 使用GPT-2模型在现有调制公式数据集上进行训练，生成新的调制方案，并与传统方法在信号噪声比和功率谱密度等关键性能指标上进行比较。

Result: Transformer生成的调制方案在性能上与传统方法相当，在某些情况下甚至更优，表明先进的认知无线电系统可以从Transformer模型的实施中获益。

Conclusion: Transformer模型能够为认知无线电系统生成有效的调制方案，有助于构建更高效、鲁棒和安全的通信系统。

Abstract: Cognitive Radio (CR) systems, which dynamically adapt to changing spectrum environments, could benefit significantly from advancements in machine learning technologies. These systems can be enhanced in terms of spectral efficiency, robustness, and security through innovative approaches such as the use of Transformer models. This work investigates the application of Transformer models, specifically the GPT-2 architecture, to generate novel modulation schemes for wireless communications. By training a GPT-2 model on a dataset of existing modulation formulas, new modulation schemes has been created. These generated schemes are then compared to traditional methods using key performance metrics such as Signal-to-Noise Ratio (SNR) and Power Spectrum Density (PSD). The results show that Transformer-generated modulation schemes can achieve performance comparable to, and in some cases outperforming, traditional methods. This demonstrates that advanced CR systems could greatly benefit from the implementation of Transformer models, leading to more efficient, robust, and secure communication systems.

</details>


### [138] [Mixtures of Transparent Local Models](https://arxiv.org/abs/2601.10541)
*Niffa Cheick Oumar Diaby,Thierry Duchesne,Mario Marchand*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: The predominance of machine learning models in many spheres of human activity has led to a growing demand for their transparency. The transparency of models makes it possible to discern some factors, such as security or non-discrimination. In this paper, we propose a mixture of transparent local models as an alternative solution for designing interpretable (or transparent) models. Our approach is designed for the situations where a simple and transparent function is suitable for modeling the label of instances in some localities/regions of the input space, but may change abruptly as we move from one locality to another. Consequently, the proposed algorithm is to learn both the transparent labeling function and the locality of the input space where the labeling function achieves a small risk in its assigned locality. By using a new multi-predictor (and multi-locality) loss function, we established rigorous PAC-Bayesian risk bounds for the case of binary linear classification problem and that of linear regression. In both cases, synthetic data sets were used to illustrate how the learning algorithms work. The results obtained from real data sets highlight the competitiveness of our approach compared to other existing methods as well as certain opaque models. Keywords: PAC-Bayes, risk bounds, local models, transparent models, mixtures of local transparent models.

</details>


### [139] [Process-Guided Concept Bottleneck Model](https://arxiv.org/abs/2601.10562)
*Reza M. Asiyabi,SEOSAW Partnership,Steven Hancock,Casey Ryan*

Main category: cs.LG

TL;DR: 本文提出PG-CBM模型，在传统概念瓶颈模型中引入领域定义的因果机制限制，提高了生物量估计任务的准确性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 传统概念瓶颈模型忽略领域特定关系和因果机制，且依赖于完全的概念标注，在科学领域应用受限，因为科学监督数据稀疏但过程定义明确。需要开发既能利用领域知识，又能处理稀疏监督的模型。

Method: 提出过程引导概念瓶颈模型（PG-CBM），通过生物物理上有意义的中间概念来约束学习遵循领域定义的因果机制。使用地球观测数据进行地上生物量密度估计作为案例研究进行验证。

Result: PG-CBM相比多个基准模型减少了误差和偏见，同时利用多源异质训练数据并生成可解释的中间输出。提高透明度的同时能够检测虚妄学习并提供科学见解。

Conclusion: PG-CBM代表向科学应用中更可信AI系统迈出的重要一步，在提高准确性的同时增强了透明度和可解释性。

Abstract: Concept Bottleneck Models (CBMs) improve the explainability of black-box Deep Learning (DL) by introducing intermediate semantic concepts. However, standard CBMs often overlook domain-specific relationships and causal mechanisms, and their dependence on complete concept labels limits applicability in scientific domains where supervision is sparse but processes are well defined. To address this, we propose the Process-Guided Concept Bottleneck Model (PG-CBM), an extension of CBMs which constrains learning to follow domain-defined causal mechanisms through biophysically meaningful intermediate concepts. Using above ground biomass density estimation from Earth Observation data as a case study, we show that PG-CBM reduces error and bias compared to multiple benchmarks, whilst leveraging multi-source heterogeneous training data and producing interpretable intermediate outputs. Beyond improved accuracy, PG-CBM enhances transparency, enables detection of spurious learning, and provides scientific insights, representing a step toward more trustworthy AI systems in scientific applications.

</details>


### [140] [Combinatorial Optimization Augmented Machine Learning](https://arxiv.org/abs/2601.10583)
*Maximilian Schiffer,Heiko Hoppe,Yue Su,Louis Bouvier,Axel Parmentier*

Main category: cs.LG

TL;DR: 本文对组合优化增强机器学习进行了全面综述，提出了统一框架并建立分类学，涵盖领域应用和研究前沿


<details>
  <summary>Details</summary>
Motivation: 随着COAML成为整合预测模型与组合决策的重要范式，需要一个系统性综述来统一不同的方法论框架，为这一跨学科领域提供清晰的研究路线图

Method: 通过引入COAML的统一框架，建立基于不确定性和决策结构的分类学体系，系统分析静态和动态问题的算法方法，并在经验成本最小化、模仿学习和强化学习框架下进行综合梳理

Result: 为COAML领域提供了全面的技术概览和分类体系，总结了调度、车辆路径、随机规划和强化学习等跨领域应用，识别了关键研究前沿

Conclusion: COAML作为机器学习与组合优化的交叉领域具有重要价值，本文的综述旨在为这一领域提供入门教程，并为未来的跨学科研究提供路线图指引

Abstract: Combinatorial optimization augmented machine learning (COAML) has recently emerged as a powerful paradigm for integrating predictive models with combinatorial decision-making. By embedding combinatorial optimization oracles into learning pipelines, COAML enables the construction of policies that are both data-driven and feasibility-preserving, bridging the traditions of machine learning, operations research, and stochastic optimization. This paper provides a comprehensive overview of the state of the art in COAML. We introduce a unifying framework for COAML pipelines, describe their methodological building blocks, and formalize their connection to empirical cost minimization. We then develop a taxonomy of problem settings based on the form of uncertainty and decision structure. Using this taxonomy, we review algorithmic approaches for static and dynamic problems, survey applications across domains such as scheduling, vehicle routing, stochastic programming, and reinforcement learning, and synthesize methodological contributions in terms of empirical cost minimization, imitation learning, and reinforcement learning. Finally, we identify key research frontiers. This survey aims to serve both as a tutorial introduction to the field and as a roadmap for future research at the interface of combinatorial optimization and machine learning.

</details>


### [141] [ProbFM: Probabilistic Time Series Foundation Model with Uncertainty Decomposition](https://arxiv.org/abs/2601.10591)
*Arundeep Chinta,Lucas Vinh Tran,Jay Katukuri*

Main category: cs.LG

TL;DR: ProbFM是一个基于transformer的概率框架，首次利用深度证据回归（DER）进行理论基础的不确定性量化，显式分解认知和偶然不确定性，相比现有方法在保持预测准确性的同时提供更好的不确定性分解。


<details>
  <summary>Details</summary>
Motivation: 时间序列基础模型在零样本金融预测中表现出色，但现有方法在不确定性量化方面存在根本限制：要么依赖限制性分布假设，要么混淆不同来源的不确定性，或缺乏理论依据的校准机制，阻碍了金融应用中的采用。

Method: 提出ProbFM框架，利用深度证据回归（DER）进行理论基础的不确定性量化，显式分解认知和偶然不确定性。为独立评估DER方法，使用一致的LSTM架构对比五种概率方法：DER、高斯NLL、学生t分布NLL、分位数损失和一致性预测。

Result: 在加密货币收益率预测评估中，DER在保持竞争性预测准确性的同时，提供了显式的认知-偶然不确定性分解。实证研究证实了DER在金融应用中的有效性。

Conclusion: 本工作建立了一个可扩展的框架，为基础模型中的理论依据不确定性量化提供了基础，并为DER在金融应用中的有效性提供了经验证据，填补了现有方法在不确定性分解方面的空白。

Abstract: Time Series Foundation Models (TSFMs) have emerged as a promising approach for zero-shot financial forecasting, demonstrating strong transferability and data efficiency gains. However, their adoption in financial applications is hindered by fundamental limitations in uncertainty quantification: current approaches either rely on restrictive distributional assumptions, conflate different sources of uncertainty, or lack principled calibration mechanisms. While recent TSFMs employ sophisticated techniques such as mixture models, Student's t-distributions, or conformal prediction, they fail to address the core challenge of providing theoretically-grounded uncertainty decomposition. For the very first time, we present a novel transformer-based probabilistic framework, ProbFM (probabilistic foundation model), that leverages Deep Evidential Regression (DER) to provide principled uncertainty quantification with explicit epistemic-aleatoric decomposition. Unlike existing approaches that pre-specify distributional forms or require sampling-based inference, ProbFM learns optimal uncertainty representations through higher-order evidence learning while maintaining single-pass computational efficiency. To rigorously evaluate the core DER uncertainty quantification approach independent of architectural complexity, we conduct an extensive controlled comparison study using a consistent LSTM architecture across five probabilistic methods: DER, Gaussian NLL, Student's-t NLL, Quantile Loss, and Conformal Prediction. Evaluation on cryptocurrency return forecasting demonstrates that DER maintains competitive forecasting accuracy while providing explicit epistemic-aleatoric uncertainty decomposition. This work establishes both an extensible framework for principled uncertainty quantification in foundation models and empirical evidence for DER's effectiveness in financial applications.

</details>


### [142] [Single-Stage Huffman Encoder for ML Compression](https://arxiv.org/abs/2601.10673)
*Aditya Agrawal,Albert Magyar,Hiteshwar Eswaraiah,Patrick Sheridan,Pradeep Janedula,Ravi Krishnan Venkatesan,Krishna Nair,Ravi Iyer*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Training and serving Large Language Models (LLMs) require partitioning data across multiple accelerators, where collective operations are frequently bottlenecked by network bandwidth. Lossless compression using Huffman codes is an effective way to alleviate the issue, however, its three-stage design requiring on-the-fly frequency analysis, codebook generation and transmission of codebook along with data introduces computational, latency and data overheads which are prohibitive for latency-sensitive scenarios such as die-to-die communication. This paper proposes a single-stage Huffman encoder that eliminates these overheads by using fixed codebooks derived from the average probability distribution of previous data batches. Through our analysis of the Gemma 2B model, we demonstrate that tensors exhibit high statistical similarity across layers and shards. Using this approach we achieve compression within 0.5% of per-shard Huffman coding and within 1% of the ideal Shannon compressibility, enabling efficient on-the-fly compression.

</details>


### [143] [Data-driven stochastic reduced-order modeling of parametrized dynamical systems](https://arxiv.org/abs/2601.10690)
*Andrew F. Ilersich,Kevin Course,Prasanth B. Nair*

Main category: cs.LG

TL;DR: 提出一种基于摊销随机变分推断的数据驱动框架，用于学习连续时间随机降阶模型，该模型能在参数空间和激励条件下泛化，且训练成本与数据集大小和系统刚度无关。


<details>
  <summary>Details</summary>
Motivation: 复杂动力系统在变化条件下的建模计算量大，高保真模拟往往难以实现。现有降阶模型方法在处理随机动态和量化预测不确定性方面存在困难，限制了其在鲁棒决策中的应用。

Method: 采用摊销随机变分推断，利用马尔可夫高斯过程的重参数化技巧，避免训练过程中使用计算昂贵的前向求解器，联合学习概率自编码器和控制潜在动态的随机微分方程。

Result: 在三个具有挑战性的测试问题上进行数值研究，结果表明该方法对未见过的参数组合和激励条件具有优秀的泛化能力，且与现有方法相比显著提高了计算效率。

Conclusion: 所提出的框架能够有效学习连续时间随机降阶模型，在保持计算效率的同时实现了良好的泛化性能，为复杂动力系统的建模提供了一种有前景的解决方案。

Abstract: Modeling complex dynamical systems under varying conditions is computationally intensive, often rendering high-fidelity simulations intractable. Although reduced-order models (ROMs) offer a promising solution, current methods often struggle with stochastic dynamics and fail to quantify prediction uncertainty, limiting their utility in robust decision-making contexts. To address these challenges, we introduce a data-driven framework for learning continuous-time stochastic ROMs that generalize across parameter spaces and forcing conditions. Our approach, based on amortized stochastic variational inference, leverages a reparametrization trick for Markov Gaussian processes to eliminate the need for computationally expensive forward solvers during training. This enables us to jointly learn a probabilistic autoencoder and stochastic differential equations governing the latent dynamics, at a computational cost that is independent of the dataset size and system stiffness. Additionally, our approach offers the flexibility of incorporating physics-informed priors if available. Numerical studies are presented for three challenging test problems, where we demonstrate excellent generalization to unseen parameter combinations and forcings, and significant efficiency gains compared to existing approaches.

</details>


### [144] [Communication-Efficient and Privacy-Adaptable Mechanism -- a Federated Learning Scheme with Convergence Analysis](https://arxiv.org/abs/2601.10701)
*Chun Hei Michael Shiu,Chih Wei Ling*

Main category: cs.LG

TL;DR: 分析了CEPAM这一联邦学习量化机制的隐私保证与收敛性质，并通过实验评估其有效性


<details>
  <summary>Details</summary>
Motivation: 联邦学习需要在数据治理约束下实现隐私保护协作，但面临通信效率和隐私保护的关键挑战。CEPAM作为一种能够同时实现这两个目标的新方法，需要从理论和实验两方面进行深入分析

Method: 使用拒绝采样通用量化器（RSUQ），这是一种随机向量量化器，其量化误差相当于规定的噪声，可通过调节噪声水平自定义参与方之间的隐私保护级别，并理论分析了CEPAM的隐私保证和收敛性质

Result: 实验评估了CEPAM的效用性能，包括与其他基准方法的收敛曲线对比，以及不同参与方之间的准确度-隐私权衡关系

Conclusion: CEPAM提供了一种在联邦学习中同时保障通信效率和隐私保护的有效机制，其理论分析和实验验证支持了这一方法的可行性

Abstract: Federated learning enables multiple parties to jointly train learning models without sharing their own underlying data, offering a practical pathway to privacy-preserving collaboration under data-governance constraints. Continued study of federated learning is essential to address key challenges in it, including communication efficiency and privacy protection between parties. A recent line of work introduced a novel approach called the Communication-Efficient and Privacy-Adaptable Mechanism (CEPAM), which achieves both objectives simultaneously. CEPAM leverages the rejection-sampled universal quantizer (RSUQ), a randomized vector quantizer whose quantization error is equivalent to a prescribed noise, which can be tuned to customize privacy protection between parties. In this work, we theoretically analyze the privacy guarantees and convergence properties of CEPAM. Moreover, we assess CEPAM's utility performance through experimental evaluations, including convergence profiles compared with other baselines, and accuracy-privacy trade-offs between different parties.

</details>


### [145] [Distributed Perceptron under Bounded Staleness, Partial Participation, and Noisy Communication](https://arxiv.org/abs/2601.10705)
*Keval Jain,Anant Raj,Saurav Prakash,Girish Varma*

Main category: cs.LG

TL;DR: 本文研究了采用迭代参数混合的半异步客户端-服务器感知机，解决了联邦部署中的三个系统效应：版本滞后、部分参与和通信噪声，提出了基于时滞分组的聚合规则并获得了有限轮次下的错误边界。


<details>
  <summary>Details</summary>
Motivation: 联邦学习和分布式部署在实际应用中面临三个关键挑战：客户端计算的延迟更新导致模型版本陈旧、客户端的间歇性可用性带来部分参与问题、以及通信链路的噪声干扰。传统方法通常假设理想同步或特定随机模型，本文旨在设计一个在非理想条件下仍能保证性能收敛的聚合框架。

Method: 提出了一种服务器端聚合规则——带填充的时滞分组聚合，该方法不需要假设延迟或参与的随机模型，而是确定性强制规定的更新时滞分布。在满足边界可分性和数据半径有界的条件下，通过迭代参数混合方式训练半异步客户端-服务器感知机。

Result: 在给定服务器轮次内，证明了累积加权感知机错误数的有限时域期望边界：延迟的影响仅通过强制时滞的均值体现，而通信噪声则贡献了一个随噪声能量增长且与时域平方根成正比的附加项。在无噪声情况下，通过有限的期望错误预算证明了在温和的新鲜参与条件下存在明确的有限轮次稳定边界。

Conclusion: 本文提出的时滞分组聚合方法能够有效处理联邦学习中的系统非理想条件，为半异步训练提供了理论保证，特别是在存在延迟、部分参与和通信噪声的实际情况中实现了可证明的错误收敛性能。

Abstract: We study a semi-asynchronous client-server perceptron trained via iterative parameter mixing (IPM-style averaging): clients run local perceptron updates and a server forms a global model by aggregating the updates that arrive in each communication round. The setting captures three system effects in federated and distributed deployments: (i) stale updates due to delayed model delivery and delayed application of client computations (two-sided version lag), (ii) partial participation (intermittent client availability), and (iii) imperfect communication on both downlink and uplink, modeled as effective zero-mean additive noise with bounded second moment. We introduce a server-side aggregation rule called staleness-bucket aggregation with padding that deterministically enforces a prescribed staleness profile over update ages without assuming any stochastic model for delays or participation. Under margin separability and bounded data radius, we prove a finite-horizon expected bound on the cumulative weighted number of perceptron mistakes over a given number of server rounds: the impact of delay appears only through the mean enforced staleness, whereas communication noise contributes an additional term that grows on the order of the square root of the horizon with the total noise energy. In the noiseless case, we show how a finite expected mistake budget yields an explicit finite-round stabilization bound under a mild fresh-participation condition.

</details>


### [146] [High-accuracy and dimension-free sampling with diffusions](https://arxiv.org/abs/2601.10708)
*Khashayar Gatmiry,Sitan Chen,Adil Salim*

Main category: cs.LG

TL;DR: 本文提出了一种新的扩散模型求解器，结合低阶近似和配置方法，实现对数级迭代复杂度提升，为基于分数的采样器提供了首个高精度理论保证。


<details>
  <summary>Details</summary>
Motivation: 现有的扩散模型求解器需要通过数值方法求解微分方程，通常需要大量小步长迭代才能生成高质量样本，迭代复杂度与维度呈多项式关系且依赖准确度倒数。

Method: 提出新的扩散模型求解器，结合低阶近似和配置方法，通过分数近似访问数据分布来减少迭代需求。

Result: 证明了新求解器的迭代复杂度在准确度倒数上呈多对数缩放，且维度依赖仅通过目标分布支撑的有效半径显现，为基于分数的采样器提供了首个高精度理论保证。

Conclusion: 该方法首次实现了只使用数据分布分数近似访问的扩散基采样器的高精度理论保证，极大提升了样本生成效率，为扩散模型的应用开辟了新可能。

Abstract: Diffusion models have shown remarkable empirical success in sampling from rich multi-modal distributions. Their inference relies on numerically solving a certain differential equation. This differential equation cannot be solved in closed form, and its resolution via discretization typically requires many small iterations to produce \emph{high-quality} samples.
  More precisely, prior works have shown that the iteration complexity of discretization methods for diffusion models scales polynomially in the ambient dimension and the inverse accuracy $1/\varepsilon$. In this work, we propose a new solver for diffusion models relying on a subtle interplay between low-degree approximation and the collocation method (Lee, Song, Vempala 2018), and we prove that its iteration complexity scales \emph{polylogarithmically} in $1/\varepsilon$, yielding the first ``high-accuracy'' guarantee for a diffusion-based sampler that only uses (approximate) access to the scores of the data distribution. In addition, our bound does not depend explicitly on the ambient dimension; more precisely, the dimension affects the complexity of our solver through the \emph{effective radius} of the support of the target distribution only.

</details>


### [147] [DInf-Grid: A Neural Differential Equation Solver with Differentiable Feature Grids](https://arxiv.org/abs/2601.10715)
*Navami Kairanda,Shanthika Naik,Marc Habermann,Avinash Sharma,Christian Theobalt,Vladislav Golyanik*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: We present a novel differentiable grid-based representation for efficiently solving differential equations (DEs). Widely used architectures for neural solvers, such as sinusoidal neural networks, are coordinate-based MLPs that are both computationally intensive and slow to train. Although grid-based alternatives for implicit representations (e.g., Instant-NGP and K-Planes) train faster by exploiting signal structure, their reliance on linear interpolation restricts their ability to compute higher-order derivatives, rendering them unsuitable for solving DEs. Our approach overcomes these limitations by combining the efficiency of feature grids with radial basis function interpolation, which is infinitely differentiable. To effectively capture high-frequency solutions and enable stable and faster computation of global gradients, we introduce a multi-resolution decomposition with co-located grids. Our proposed representation, DInf-Grid, is trained implicitly using the differential equations as loss functions, enabling accurate modelling of physical fields. We validate DInf-Grid on a variety of tasks, including the Poisson equation for image reconstruction, the Helmholtz equation for wave fields, and the Kirchhoff-Love boundary value problem for cloth simulation. Our results demonstrate a 5-20x speed-up over coordinate-based MLP-based methods, solving differential equations in seconds or minutes while maintaining comparable accuracy and compactness.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [148] [Collision Avoidance for Non-Cooperative Multi-Swarm Coverage Control with Bounded Disturbance Measurements](https://arxiv.org/abs/2601.09917)
*Karolina Schmidt,Luis Rodrigues*

Main category: eess.SY

TL;DR: 提出了一种新的多群体非合作式覆盖控制算法，能够在有界扰动下实现无碰撞运动，特别考虑了扰动测量中的不确定性。


<details>
  <summary>Details</summary>
Motivation: 现有的多群体覆盖控制算法在面对有界扰动和扰动测量不确定性的情况下，难以保证无碰撞运动，需要提出一种能够处理这些实际约束的新方法。

Method: 引入一种新方法学，考虑了扰动测量的不确定性，并基于此开发了一个算法，确保在多群体覆盖控制中实现无碰撞运动。

Result: 理论结果通过模拟验证，多个群体在存在扰动和测量不确定性的环境中能够独立完成区域覆盖任务且避免碰撞。

Conclusion: 所提出的算法有效解决了有界扰动和测量不确定性下的多群体无碰撞覆盖控制问题，为实际应用提供了理论基础。

Abstract: This paper proposes a new algorithm for collision-free coverage control of multiple non-cooperating swarms in the presence of bounded disturbances. A new methodology is introduced that accounts for uncertainties in disturbance measurements. The proposed methodology is used to develop an algorithm that ensures collision-free motion in multi-swarm coverage control, specifically for cases where disturbances are present and their measurements are subject to bounded uncertainty. The theoretical results are validated through simulations of multiple swarms that independently aim to cover a given region in an environment with disturbances.

</details>


### [149] [Extremum Seeking Nonovershooting Control of Strict-Feedback Systems Under Unknown Control Direction](https://arxiv.org/abs/2601.09998)
*Kaixin Lu,Ziliang Lyu,Yanfang Mo,Yiguang Hong,Haoyong Yu*

Main category: eess.SY

TL;DR: 为具有未知控制方向的严格反馈非线性系统提出了非超调控制方法，结合极值搜索和Lie括号设计实现近似非超调跟踪。


<details>
  <summary>Details</summary>
Motivation: 传统非线性系统控制方法在处理未知控制方向时难以保证非超调性能，特别是在安全关键场景中需要满足高阶非超调约束。

Method: 集成极值搜索(extremum seeking)与Lie括号设计，通过参数调节将超调减少到任意小水平，确保从下方跟踪任意参考轨迹。

Result: 方法能够在任意初始条件下实现近似非超调跟踪，为涉及未知控制方向的安全关键场景提供高阶非超调约束执行机制。

Conclusion: 所提方法有效解决了未知控制方向下严格反馈非线性系统的非超调控制问题，具有实际安全应用价值。

Abstract: This paper addresses the nonovershooting control problem for strict-feedback nonlinear systems with unknown control direction. We propose a method that integrates extremum seeking with Lie bracket-based design to achieve approximately nonovershooting tracking. The approach ensures that arbitrary reference trajectories can be tracked from below for any initial condition, with the overshoot reducible to arbitrarily small levels through parameter tuning. The method further provides a mechanism for enforcing high-relative-degree nonovershooting constraints in safety-critical scenarios involving unknown control directions.

</details>


### [150] [On the Computation and Approximation of Backward Reachable Sets for Max-Plus Linear Systems using Polyhedras](https://arxiv.org/abs/2601.10095)
*Yuda Li,Shaoyuan Li,Xiang Yin*

Main category: eess.SY

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: This paper investigates reachability analysis for max-plus linear systems (MPLS), an important class of dynamical systems that model synchronization and delay phenomena in timed discrete-event systems. We specifically focus on backward reachability analysis, i.e., determining the set of states that can reach a given target set within a certain number of steps. Computing backward reachable sets presents significant challenges due to the non-convexity of max-plus dynamics and the complexity of set complement operations. To address these challenges, we propose a novel approximation framework that efficiently computes backward reachable sets by exploiting the structure of tropical polyhedra. Our approach reformulates the problem as a sequence of symbolic operations and approximates non-convex target sets through closure operations on unions of tropical polyhedra. We develop a systematic algorithm that constructs both outer (M-form) and inner (V-form) representations of the resulting sets, incorporating extremal filtering to reduce computational complexity. The proposed method offers a scalable alternative to traditional DBM-based approaches, enabling reliable approximate backward reachability analysis for general target regions in MPLS.

</details>


### [151] [HyMGP: A Customized MILP-Based Tool for Techno-Economic Planning of Islanded Microgrids](https://arxiv.org/abs/2601.10178)
*Andres Intriago,Rongxing Hu,Nabil Mohammed,S. Gokul Krishnan,Konstantinos Kotsovos,Issam Gereige,Nesren Attiah,Ali Basaheeh,Sarah Aqeel,Hamad A. Saiari,Shehab Ahmed,Charalambos Konstantinou*

Main category: eess.SY

TL;DR: 本文提出了针对干旱地区远程站点的定制化微电网规划算法和工具HyMGP，它被构建为混合整数线性规划问题，并与HOMER Pro进行比较，发现在光伏、垂直轴风力涡轮机和电池储能系统的优化配置方面具有优势。


<details>
  <summary>Details</summary>
Motivation: 为远程和离网应用开发更优化的微电网规划工具，特别是在沙特阿拉伯等干旱地区，这些地区太阳能辐照度高但风力资源有限，需要满足连续阴极保护和白天冷却的恒定负载需求。

Method: 使用混合整数线性规划方法构建HyMGP算法，并与商业软件HOMER Pro进行对比分析，考虑光伏、垂直轴风力发电机和电池储能系统的优化配置，分析不同电池类型（锂铁磷酸盐vs铅酸）的成本效益。

Result: HyMGP相比HOMER Pro提供更优且更灵活的解决方案；加入风力涡轮机可降低净现成本；增加电池储能容量会提高净现成本；锂铁磷酸盐电池比铅酸电池更具成本效益，净现成本更低。

Conclusion: HyMGP是一种有效的微电网规划工具，特别适用于干旱地区远程站点，能够提供优化的混合能源系统配置方案，且锂铁磷酸盐电池在经济性上优于传统铅酸电池。

Abstract: This paper presents a customized microgrid planning algorithm and tool, HyMGP, for remote sites in arid regions, which is formulated as a Mixed Integer Linear Programming (MILP) problem. HyMGP is compared with HOMER Pro to evaluate its performance in optimizing the sizing of microgrid components, including photovoltaic panels (PVs), vertical axis wind turbines (VAWTs), and battery energy storage systems (BESS), for remote and off-grid applications. The study focuses on a standalone microgrid in the Saudi Arabia, considering high solar irradiance, limited wind availability, and a constant load profile composed of continuous cathodic protection and daytime cooling. In the simulation environment, comparisons with HOMER solutions demonstrate the advantages of HyMGP, which provides optimal and more flexible solutions by allowing user-defined component specifications and strictly enforcing all constraints. Further analysis shows that incorporating wind turbines reduces the Net Present Cost (NPC) by decreasing the required PV and battery capacities. Increasing battery autonomy leads to a higher NPC in both PV-only and hybrid systems due to the need for larger storage. Finally, lithium iron phosphate (Li-ion LFP) batteries are found to be more cost effective than lead acid, offering lower NPCs due to their longer lifespan, deeper discharge capability, and fewer replacement cycles.

</details>


### [152] [Model Predictive Control of Thermo-Hydraulic Systems Using Primal Decomposition](https://arxiv.org/abs/2601.10189)
*Jonathan Vieth,Annika Eichler,Arne Speerforck*

Main category: eess.SY

TL;DR: 通过自动化框架和原始分解方法生成可扩展的模型预测控制器，提高热力系统运行效率


<details>
  <summary>Details</summary>
Motivation: 全球能源供应脱碳需要更高效的供暖和制冷系统。模型预测控制能提升系统运行效率，但依赖于基于控制容积的精确系统模型

Method: 提出了一个自动化框架，包括时间离散化技术，来为控制容积模型生成模型预测控制器。为确保可扩展性，应用了对模型结构进行原始分解的方法

Result: 方法在地下加热系统上进行了验证，涉及不同数量的状态，展示了原始分解方法在可扩展性方面的优势

Conclusion: 提出的自动化框架和原始分解方法能够有效地为热力系统生成可扩展的模型预测控制器，有助于提升能源效率

Abstract: Decarbonizing the global energy supply requires more efficient heating and cooling systems. Model predictive control enhances the operation of cooling and heating systems but depends on accurate system models, often based on control volumes. We present an automated framework including time discretization to generate model predictive controllers for such models. To ensure scalability, a primal decomposition exploiting the model structure is applied. The approach is validated on an underground heating system with varying numbers of states, demonstrating the primal decomposition's advantage regarding scalability.

</details>


### [153] [Single-Feed Circularly Polarized Super Realized Gain Antenna](https://arxiv.org/abs/2601.10292)
*Georgia Psychogiou,Donal P. Lynch,Spyridon N. Daskalakis,Manos M. Tentzeris,George Goussetis,Stylianos D. Asimonis*

Main category: eess.SY

TL;DR: 该论文提出了一种工作在3.5GHz的超高实现增益圆极化交叉条带偶极子天线，通过结构优化实现了超指向性和圆极化特性。


<details>
  <summary>Details</summary>
Motivation: 传统天线设计在实现高指向性（超指向性）时通常需要复杂的阵列结构或较大的物理尺寸。本文旨在通过创新的结构设计，在一个简单的低剖面天线中同时实现圆极化和超指向性特性，以满足紧凑型无线和传感平台的需求。

Method: 采用交叉条带偶极子天线设计，利用强烈的单元间互耦效应实现超指向性。天线包含一个驱动单元和一个带无功阻抗的被动加载单元，通过精心调整条带尺寸来优化性能。结构针对最大化左旋圆极化实现增益进行了优化设计。

Result: 优化设计获得50Ω阻抗带宽3.29-4.17GHz（23.75%），轴比带宽3.43-3.57GHz（4%）。在3.5GHz处实现峰值增益6.1dB（ka≈1.65），轴比为1.4dB，具有高极化纯度和良好阻抗匹配。天线剖面高度仅为0.15λ。

Conclusion: 研究表明，通过巧妙的交叉条带偶极子设计和互耦效应利用，可以在几何简单、低剖面的天线中同时实现圆极化和超指向性，该设计适合集成到紧凑型亚6GHz无线和传感平台中。

Abstract: This paper presents a super realized gain, circularly polarized strip-crossed dipole antenna operating at 3.5 GHz. Superdirective behavior is achieved by leveraging strong inter-element mutual coupling through careful adjustment of the strip dimensions. The antenna features a single driven element, with the other element passively loaded with a reactive impedance. The structure is optimized to maximize left-hand circularly polarized (LHCP) realized gain, ensuring high polarization purity and good impedance matching. The optimized design exhibits a 50 $Ω$ impedance bandwidth of 3.29 - 4.17 GHz (23.75%) and an axial-ratio bandwidth of 3.43 - 3.57 GHz (4%). At 3.5 GHz, the antenna achieves a peak realized gain of 6.1 dB ($ka \approx 1.65$), with an axial ratio of 1.4 dB. These results demonstrate that circular polarization and superdirectivity can be simultaneously realized in a geometrically simple, low-profile ($0.15λ$) antenna, rendering it suitable for integration into compact sub-6~GHz wireless and sensing platforms.

</details>


### [154] [Safe Trajectory Gradient Flow Control of a Grid-Interfacing Inverter](https://arxiv.org/abs/2601.10671)
*Trager Joswig-Jones,Baosen Zhang*

Main category: eess.SY

TL;DR: 本文提出了一种直接考虑硬件约束的电压源逆变器控制框架，该框架基于安全轨迹梯度流方法，确保状态在安全集内并导向最优平衡点。


<details>
  <summary>Details</summary>
Motivation: 现有并网逆变器控制方法常在设计时忽略电流幅值等硬件约束，而通过临时限幅器处理约束可能导致系统不稳定或性能下降，因此需要一种能直接约束的控制方法。

Method: 提出安全轨迹梯度流控制器，将安全梯度流方法应用于滚动时域轨迹优化问题，保证状态不违反约束并收敛至非线性规划的最优平衡点。

Result: 仿真结果表明，该方法能驱动逆变器输出至最优值并维持状态约束，即使在每个控制周期只进行有限步优化计算的情况下也有效。

Conclusion: 所提控制框架成功地将约束直接整合到逆变器控制中，实现了在硬件限制下的安全稳定运行，为约束逆变器控制提供了系统化解决方案。

Abstract: Grid-interfacing inverters serve as the interface between renewable energy resources and the electric power grid, offering fast, programmable control capabilities. However, their operation is constrained by hardware limitations, such as bounds on the current magnitude. Existing control methods for these systems often neglect these constraints during controller design and instead rely on ad hoc limiters, which can introduce instability or degrade performance. In this work, we present a control framework that directly incorporates constraints into the control of a voltage-source inverter. We propose a safe trajectory gradient flow controller, which applies the safe gradient flow method to a rolling horizon trajectory optimization problem to ensure that the states remain within a safe set defined by the constraints while directing the trajectory towards an optimal equilibrium point of a nonlinear program. Simulation results demonstrate that our approach can drive the outputs of a simulated inverter system to optimal values and maintain state constraints, even when using a limited number of optimization steps per control cycle.

</details>


### [155] [Self-Organizing Dual-Buffer Adaptive Clustering Experience Replay (SODASER) for Safe Reinforcement Learning in Optimal Control](https://arxiv.org/abs/2601.06540)
*Roya Khalili Amirabadi,Mohsen Jalaeian Farimani,Omid Solaymani Fard*

Main category: eess.SY

TL;DR: SODACER是一个新颖的强化学习框架，通过双缓冲区和自适应聚类实现高效经验回放，结合控制屏障函数保证安全，使用Sophia优化器提升收敛性，在非线性系统控制中表现出优越性能。


<details>
  <summary>Details</summary>
Motivation: 传统经验回放方法在非线性系统控制中存在内存效率低、样本冗余、收敛慢等问题，且难以在安全关键环境中保证约束满足，需要一种能够兼顾安全性、样本效率和收敛性能的通用解决方案。

Method: 1. 自组织双缓冲自适应聚类经验回放（SODACER）：由快速缓冲区（适应近期经验）和慢速缓冲区（自适应聚类机制保持多样非冗余历史经验）组成，动态剪枝冗余样本
2. 结合控制屏障函数（CBFs）保证学习过程中的状态和输入约束安全
3. 集成Sophia优化器实现自适应二阶梯度更新，提升收敛稳定性

Result: 1. 在非线性HPV传播模型（多控制输入和安全约束）上验证
2. 相比随机和基于聚类的经验回放方法：收敛更快、样本效率更高、偏置-方差权衡更优
3. 始终保持安全的系统轨迹
4. 通过Friedman检验统计验证

Conclusion: SODACER-Sophia架构为动态安全关键环境提供了可靠、高效、鲁棒的学习解决方案，在机器人、医疗和大规模系统优化等领域具有通用性。该框架在保证安全性的同时显著提升了学习效率和性能。

Abstract: This paper proposes a novel reinforcement learning framework, named Self-Organizing Dual-buffer Adaptive Clustering Experience Replay (SODACER), designed to achieve safe and scalable optimal control of nonlinear systems. The proposed SODACER mechanism consisting of a Fast-Buffer for rapid adaptation to recent experiences and a Slow-Buffer equipped with a self-organizing adaptive clustering mechanism to maintain diverse and non-redundant historical experiences. The adaptive clustering mechanism dynamically prunes redundant samples, optimizing memory efficiency while retaining critical environmental patterns. The approach integrates SODASER with Control Barrier Functions (CBFs) to guarantee safety by enforcing state and input constraints throughout the learning process. To enhance convergence and stability, the framework is combined with the Sophia optimizer, enabling adaptive second-order gradient updates. The proposed SODACER-Sophia's architecture ensures reliable, effective, and robust learning in dynamic, safety-critical environments, offering a generalizable solution for applications in robotics, healthcare, and large-scale system optimization. The proposed approach is validated on a nonlinear Human Papillomavirus (HPV) transmission model with multiple control inputs and safety constraints. Comparative evaluations against random and clustering-based experience replay methods demonstrate that SODACER achieves faster convergence, improved sample efficiency, and a superior bias-variance trade-off, while maintaining safe system trajectories, validated via the Friedman test.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [156] [AI Survival Stories: a Taxonomic Analysis of AI Existential Risk](https://arxiv.org/abs/2601.09765)
*Herman Cappelen,Simon Goldstein,John Hawthorne*

Main category: cs.AI

TL;DR: 该论文提出了一个分析AI存在性风险的一般框架，基于两个前提来构建人类生存情景的分类体系。


<details>
  <summary>Details</summary>
Motivation: 自ChatGPT发布以来，关于AI是否会对人类构成存在性风险的争论持续不断。作者旨在建立一个系统性的框架来思考这一问题，为这一复杂的辩论提供结构和清晰度。

Method: 提出了一个基于两个关键前提的分析框架：1) AI系统将变得极其强大；2) 如果AI变得极其强大，将会毁灭人类。基于这两个前提的真假组合，构建了一个'生存故事'的分类学，展示了人类能够在每种情景中生存下来的可能性。

Result: 建立了四种主要的生存情景：1) 科学障碍阻止AI变得极其强大；2) 人类禁止AI研究；3) 极其强大的AI不毁灭人类，因为它们的目标阻止这样做；4) 人类能够可靠检测并禁用有毁灭目标的AI系统。论文分析了每种情景面临的挑战和相应的应对策略。

Conclusion: 使用这个分类学框架，作者能够生成对'厄运概率'的粗略估计，即AI毁灭人类的概率。该框架为理解AI存在性风险的不同维度提供了结构化方法，并为制定相应的风险缓解策略提供了理论基础。

Abstract: Since the release of ChatGPT, there has been a lot of debate about whether AI systems pose an existential risk to humanity. This paper develops a general framework for thinking about the existential risk of AI systems. We analyze a two premise argument that AI systems pose a threat to humanity. Premise one: AI systems will become extremely powerful. Premise two: if AI systems become extremely powerful, they will destroy humanity. We use these two premises to construct a taxonomy of survival stories, in which humanity survives into the far future. In each survival story, one of the two premises fails. Either scientific barriers prevent AI systems from becoming extremely powerful; or humanity bans research into AI systems, thereby preventing them from becoming extremely powerful; or extremely powerful AI systems do not destroy humanity, because their goals prevent them from doing so; or extremely powerful AI systems do not destroy humanity, because we can reliably detect and disable systems that have the goal of doing so. We argue that different survival stories face different challenges. We also argue that different survival stories motivate different responses to the threats from AI. Finally, we use our taxonomy to produce rough estimates of P(doom), the probability that humanity will be destroyed by AI.

</details>


### [157] [GUI-Eyes: Tool-Augmented Perception for Visual Grounding in GUI Agents](https://arxiv.org/abs/2601.09770)
*Chen Chen,Jiawei Shao,Dakuan Lu,Haoyi Hu,Xiangcheng Liu,Hantao Yao,Wu Liu*

Main category: cs.AI

TL;DR: GUI自动化领域的研究进展：提出GUI-Eyes框架，通过强化学习实现界面任务中的主动视觉感知，通过两阶段策略决策控制视觉工具使用，解决现有方法静态被动感知的局限性。


<details>
  <summary>Details</summary>
Motivation: 当前基于视觉语言模型和强化学习的GUI自动化方法依赖静态的、一次性视觉输入和被动感知，缺乏自适应决定何时、是否以及如何观察界面的能力，这限制了智能体在复杂GUI环境中的表现。

Method: GUI-Eyes是一个用于GUI任务的主动视觉感知强化学习框架：1) 引入渐进感知策略，将决策分解为粗粒度探索和细粒度定位两个阶段；2) 构建双层次策略协调这两阶段；3) 设计空间连续奖励函数，结合位置邻近性和区域重叠度来缓解GUI环境中常见的奖励稀疏问题；4) 智能体在两阶段推理过程中学习是否以及如何调用裁剪、缩放等视觉工具。

Result: 在ScreenSpot-Pro基准测试中，GUI-Eyes-3B仅使用3k标记样本就达到44.8%的定位准确率，显著优于监督学习和强化学习基线方法。

Conclusion: 研究发现，通过阶段性策略推理和细粒度奖励反馈实现的工具感知主动视觉感知，对于构建鲁棒且数据高效的GUI智能体至关重要。GUI-Eyes框架有效提升了GUI环境中智能体的视觉感知能力。

Abstract: Recent advances in vision-language models (VLMs) and reinforcement learning (RL) have driven progress in GUI automation. However, most existing methods rely on static, one-shot visual inputs and passive perception, lacking the ability to adaptively determine when, whether, and how to observe the interface. We present GUI-Eyes, a reinforcement learning framework for active visual perception in GUI tasks. To acquire more informative observations, the agent learns to make strategic decisions on both whether and how to invoke visual tools, such as cropping or zooming, within a two-stage reasoning process. To support this behavior, we introduce a progressive perception strategy that decomposes decision-making into coarse exploration and fine-grained grounding, coordinated by a two-level policy. In addition, we design a spatially continuous reward function tailored to tool usage, which integrates both location proximity and region overlap to provide dense supervision and alleviate the reward sparsity common in GUI environments. On the ScreenSpot-Pro benchmark, GUI-Eyes-3B achieves 44.8% grounding accuracy using only 3k labeled samples, significantly outperforming both supervised and RL-based baselines. These results highlight that tool-aware active perception, enabled by staged policy reasoning and fine-grained reward feedback, is critical for building robust and data-efficient GUI agents.

</details>


### [158] [Antisocial behavior towards large language model users: experimental evidence](https://arxiv.org/abs/2601.09772)
*Paweł Niszczota,Cassandra Grützner*

Main category: cs.AI

TL;DR: 研究表明，LLM用户会遭受实际的经济惩罚，惩罚程度与AI使用量正相关，且存在'可信度鸿沟'：自我报告未使用反而比实际未使用受罚更重


<details>
  <summary>Details</summary>
Motivation: 探究大语言模型(LLMs)带来的效率提升是否会引发社会制裁的实际成本行为，而不仅仅是负面态度

Method: 两阶段在线实验(N=491)，参与者可花费自己的资金来减少那些使用或不使用LLM支持的同伴的收入，实验测量了惩罚行为与LLM实际使用/自我报告使用的关系

Result: 参与者平均销毁了完全依赖LLM的同伴36%的收入；惩罚随着LLM实际使用量单调增加；自我报告未使用者比实际未使用者受到更严厉惩罚；在高使用水平时，实际依赖比自我报告依赖受到更强惩罚

Conclusion: LLMs的效率提升是以社会制裁为代价的，人们不仅对LLM用户持负面态度，还会采取实际的惩罚行为，且存在对LLM使用声明的怀疑现象

Abstract: The rapid spread of large language models (LLMs) has raised concerns about the social reactions they provoke. Prior research documents negative attitudes toward AI users, but it remains unclear whether such disapproval translates into costly action. We address this question in a two-phase online experiment (N = 491 Phase II participants; Phase I provided targets) where participants could spend part of their own endowment to reduce the earnings of peers who had previously completed a real-effort task with or without LLM support. On average, participants destroyed 36% of the earnings of those who relied exclusively on the model, with punishment increasing monotonically with actual LLM use. Disclosure about LLM use created a credibility gap: self-reported null use was punished more harshly than actual null use, suggesting that declarations of "no use" are treated with suspicion. Conversely, at high levels of use, actual reliance on the model was punished more strongly than self-reported reliance. Taken together, these findings provide the first behavioral evidence that the efficiency gains of LLMs come at the cost of social sanctions.

</details>


### [159] [Improving Chain-of-Thought for Logical Reasoning via Attention-Aware Intervention](https://arxiv.org/abs/2601.09805)
*Nguyen Minh Phuong,Dang Huu Tien,Naoya Inoue*

Main category: cs.AI

TL;DR: 论文提出了一种端到端、非交互的注意力感知干预方法AAI，通过在推理时重新加权注意力分数来提升大语言模型的逻辑推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖复杂的交互式框架或外部符号求解器，这带来了额外开销且限制了可扩展性，需要一种能够在大模型内部实现推理且无需外部资源的端到端方法。

Method: 通过向few-shot提示中添加结构信息激活特定注意力头，并提出AAI方法在推理时根据逻辑模式重新加权所选注意力头的注意力分数。

Result: 实验表明AAI在各种基准测试和模型架构中增强了逻辑推理性能，同时仅带来可忽略的额外计算开销。

Conclusion: AAI通过注意力调制提供了一种高效的非交互式推理方法，使模型能够有效利用先验知识进行逻辑推理，同时在多个任务和模型上展现出良好的泛化能力。

Abstract: Modern logical reasoning with LLMs primarily relies on employing complex interactive frameworks that decompose the reasoning process into subtasks solved through carefully designed prompts or requiring external resources (e.g., symbolic solvers) to exploit their strong logical structures. While interactive approaches introduce additional overhead, hybrid approaches depend on external components, which limit their scalability. A non-interactive, end-to-end framework enables reasoning to emerge within the model itself -- improving generalization while preserving analyzability without any external resources. In this work, we introduce a non-interactive, end-to-end framework for reasoning tasks. We show that introducing structural information into the few-shot prompt activates a subset of attention heads that patterns aligned with logical reasoning operators. Building on this insight, we propose Attention-Aware Intervention (AAI), an inference-time intervention method that reweights attention scores across selected heads identified by their logical patterns. AAI offers an efficient way to steer the model's reasoning toward leveraging prior knowledge through attention modulation. Extensive experiments show that AAI enhances logical reasoning performance across diverse benchmarks and model architectures, while incurring negligible additional computational overhead. Code is available at https://github.com/phuongnm94/aai_for_logical_reasoning.

</details>


### [160] [Thinking Long, but Short: Stable Sequential Test-Time Scaling for Large Reasoning Models](https://arxiv.org/abs/2601.09855)
*Michael R. Metel,Yufei Cui,Boxing Chen,Prasanna Parthasarathi*

Main category: cs.AI

TL;DR: 本文提出了一种新的顺序测试时缩放方法Min-Seek，能大幅提升推理模型的准确性，稳定顺序缩放过程，无需推理长度调优，且具有计算高效性。


<details>
  <summary>Details</summary>
Motivation: 现有的顺序测试时缩放方法虽然能通过延长模型思考时间提高准确性，但存在推理长度进一步延伸时会出现准确性下降和模型不稳定的问题，且需要复杂的推理长度微调。

Method: 提出Min-Seek方法，通过自定义KV缓存（存储不带位置编码的键），在推理过程中仅保留一个额外诱导思想的KV对，并在每个新生成的思想前对键进行动态连续编码，从而实现在模型最大上下文长度之外的持续有效推理，并在温和条件下保持线性计算复杂度。

Result: 该方法在多种推理任务上显著提升了模型准确性，稳定了顺序缩放过程的准确性表现，同时消除了对推理长度微调的需求，且计算效率高。

Conclusion: Min-Seek是一种有效的顺序测试时缩放方法，既提升了推理准确性，又保持了计算效率，为解决大推理模型在延长思考时的稳定性和准确性下降问题提供了实用解决方案。

Abstract: Sequential test-time scaling is a promising training-free method to improve large reasoning model accuracy, but as currently implemented, significant limitations have been observed. Inducing models to think for longer can increase their accuracy, but as the length of reasoning is further extended, it has also been shown to result in accuracy degradation and model instability. This work presents a novel sequential test-time scaling method, Min-Seek, which improves model accuracy significantly over a wide range of induced thoughts, stabilizing the accuracy of sequential scaling, and removing the need for reasoning length fine-tuning. Beyond improving model accuracy over a variety of reasoning tasks, our method is inherently efficient, as only the KV pairs of one additional induced thought are kept in the KV cache during reasoning. With a custom KV cache which stores keys without position embeddings, by dynamically encoding them contiguously before each new generated thought, our method can continue to reason well beyond a model's maximum context length, and under mild conditions has linear computational complexity.

</details>


### [161] [Epistemology gives a Future to Complementarity in Human-AI Interactions](https://arxiv.org/abs/2601.09871)
*Andrea Ferrario,Alessandro Facchini,Juan M. Durán*

Main category: cs.AI

TL;DR: 该文批评了人机互补性这一概念的理论局限性，并提出基于认识论和计算可靠性主义的新框架，将其重新定位为人机交互过程可靠性的证据。


<details>
  <summary>Details</summary>
Motivation: 人机互补性概念虽然在HCI领域流行，但面临理论挑战：缺乏精确的理论基础、仅作为预测精度的后验指标、忽视人机交互的其他理想特性、忽略性能增益的成本效益分析，导致在实际应用中难以实现。作者希望通过认识论框架解决这些问题。

Method: 借鉴认识论中的计算可靠性主义理论，将历史互补性实例重新解释为人机交互过程在特定预测任务中可靠性的证据。结合其他评估人机团队与认识论标准及社会技术实践一致性的可靠性指标，构建完整性评估框架。

Result: 提出了一种新的理论框架，将互补性的作用和价值从预测精度的相对测量，重新定位为有助于校准决策过程可靠性的工具，支持患者、管理者、监管者等受AI输出影响主体的实践推理。

Conclusion: 互补性不应当仅仅用作衡量预测准确性的相对指标，而应作为评估人机协作过程可靠性的重要组成部分，帮助决策者更好地理解和信任日益影响日常生活的AI支持流程。

Abstract: Human-AI complementarity is the claim that a human supported by an AI system can outperform either alone in a decision-making process. Since its introduction in the human-AI interaction literature, it has gained traction by generalizing the reliance paradigm and by offering a more practical alternative to the contested construct of 'trust in AI.' Yet complementarity faces key theoretical challenges: it lacks precise theoretical anchoring, it is formalized just as a post hoc indicator of relative predictive accuracy, it remains silent about other desiderata of human-AI interactions and it abstracts away from the magnitude-cost profile of its performance gain. As a result, complementarity is difficult to obtain in empirical settings. In this work, we leverage epistemology to address these challenges by reframing complementarity within the discourse on justificatory AI. Drawing on computational reliabilism, we argue that historical instances of complementarity function as evidence that a given human-AI interaction is a reliable epistemic process for a given predictive task. Together with other reliability indicators assessing the alignment of the human-AI team with the epistemic standards and socio-technical practices, complementarity contributes to the degree of reliability of human-AI teams when generating predictions. This supports the practical reasoning of those affected by these outputs -- patients, managers, regulators, and others. In summary, our approach suggests that the role and value of complementarity lies not in providing a relative measure of predictive accuracy, but in helping calibrate decision-making to the reliability of AI-supported processes that increasingly shape everyday life.

</details>


### [162] [Beyond Rule-Based Workflows: An Information-Flow-Orchestrated Multi-Agents Paradigm via Agent-to-Agent Communication from CORAL](https://arxiv.org/abs/2601.09883)
*Xinxing Ren,Quagmire Zang,Caelum Forder,Suman Deb,Ahsen Tahir,Roman J. Georgio,Peter Carroll,Zekun Guo*

Main category: cs.AI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Most existing Large Language Model (LLM)-based Multi-Agent Systems (MAS) rely on predefined workflows, where human engineers enumerate task states in advance and specify routing rules and contextual injections accordingly. Such workflow-driven designs are essentially rule-based decision trees, which suffer from two fundamental limitations: they require substantial manual effort to anticipate and encode possible task states, and they cannot exhaustively cover the state space of complex real-world tasks. To address these issues, we propose an Information-Flow-Orchestrated Multi-Agent Paradigm via Agent-to-Agent (A2A) Communication from CORAL, in which a dedicated information flow orchestrator continuously monitors task progress and dynamically coordinates other agents through the A2A toolkit using natural language, without relying on predefined workflows. We evaluate our approach on the general-purpose benchmark GAIA, using the representative workflow-based MAS OWL as the baseline while controlling for agent roles and underlying models. Under the pass@1 setting, our method achieves 63.64% accuracy, outperforming OWL's 55.15% by 8.49 percentage points with comparable token consumption. Further case-level analysis shows that our paradigm enables more flexible task monitoring and more robust handling of edge cases. Our implementation is publicly available at: https://github.com/Coral-Protocol/Beyond-Rule-Based-Workflows

</details>


### [163] [Continuum Memory Architectures for Long-Horizon LLM Agents](https://arxiv.org/abs/2601.09913)
*Joe Logan*

Main category: cs.AI

TL;DR: 本文提出连续记忆架构(CMA)作为检索增强生成(RAG)的替代方案，以解决RAG在处理记忆时存在的状态缺失、只读检索和时间连续性缺失等局限性。


<details>
  <summary>Details</summary>
Motivation: 当前基于RAG的LLM智能体将记忆视为静态查找表，存在信息永久保留、检索只读且缺乏时间连续性等问题，限制了智能体在长期交互中的表现。

Method: 设计了连续记忆架构(CMA)，通过持久化存储、选择性保留、关联路由、时间链化和高阶抽象整合等机制，实现跨交互的内部状态维护和更新。

Result: 在知识更新、时间关联、关联回忆和上下文消歧等任务上的实验表明，CMA相对于RAG在行为表现上有显著优势，能有效解决RAG的结构性缺陷。

Conclusion: CMA是构建长期智能体的必要架构基础，但仍面临延迟、漂移和可解释性等开放挑战。

Abstract: Retrieval-augmented generation (RAG) has become the default strategy for providing large language model (LLM) agents with contextual knowledge. Yet RAG treats memory as a stateless lookup table: information persists indefinitely, retrieval is read-only, and temporal continuity is absent. We define the \textit{Continuum Memory Architecture} (CMA), a class of systems that maintain and update internal state across interactions through persistent storage, selective retention, associative routing, temporal chaining, and consolidation into higher-order abstractions. Rather than disclosing implementation specifics, we specify the architectural requirements CMA imposes and show consistent behavioral advantages on tasks that expose RAG's structural inability to accumulate, mutate, or disambiguate memory. The empirical probes (knowledge updates, temporal association, associative recall, contextual disambiguation) demonstrate that CMA is a necessary architectural primitive for long-horizon agents while highlighting open challenges around latency, drift, and interpretability.

</details>


### [164] [Hallucination Detection and Mitigation in Large Language Models](https://arxiv.org/abs/2601.09929)
*Ahmad Pesaranghader,Erin Li*

Main category: cs.AI

TL;DR: 提出了一个综合的操作框架，用于管理LLM/LRM的幻觉问题，基于根因意识的持续改进循环，并通过应用案例展示了可靠性提升效果。


<details>
  <summary>Details</summary>
Motivation: LLM/LRM在金融、法律等高风险领域有变革性潜力，但其生成不准确或无依据内容的幻觉倾向带来严重可靠性风险，需要系统化管理方法。

Method: 引入综合的操作框架：1）将幻觉根源分类为模型、数据和情境相关因素 2）集成多面检测方法（如不确定性估计、推理一致性）3）分层缓释策略（如知识基奠、置信度校准）4）通过分层架构和金融数据提取案例形成闭环反馈循环

Result: 通过金融数据提取案例展示了框架的有效性，模型、情境和数据层形成了渐进式可靠性增强的闭环反馈循环。

Conclusion: 该方法为在受监管环境中构建可信赖的生成式AI系统提供了系统化、可扩展的方法论，能够有针对性地处理幻觉问题而非依赖通用修复方案。

Abstract: Large Language Models (LLMs) and Large Reasoning Models (LRMs) offer transformative potential for high-stakes domains like finance and law, but their tendency to hallucinate, generating factually incorrect or unsupported content, poses a critical reliability risk. This paper introduces a comprehensive operational framework for hallucination management, built on a continuous improvement cycle driven by root cause awareness. We categorize hallucination sources into model, data, and context-related factors, allowing targeted interventions over generic fixes. The framework integrates multi-faceted detection methods (e.g., uncertainty estimation, reasoning consistency) with stratified mitigation strategies (e.g., knowledge grounding, confidence calibration). We demonstrate its application through a tiered architecture and a financial data extraction case study, where model, context, and data tiers form a closed feedback loop for progressive reliability enhancement. This approach provides a systematic, scalable methodology for building trustworthy generative AI systems in regulated environments.

</details>


### [165] [Chinese Labor Law Large Language Model Benchmark](https://arxiv.org/abs/2601.09972)
*Zixun Lan,Maochun Xu,Yifan Ren,Rui Wu,Jianghui Zhou,Xueyang Cheng,Jianan Ding Ding,Xinheng Wang,Mingmin Chi,Fei Ma*

Main category: cs.AI

TL;DR: 该论文提出了针对中国劳动法领域的专用大语言模型LabourLawLLM和配套基准LabourLawBench，旨在解决通用模型在法律专业领域表现不足的问题。


<details>
  <summary>Details</summary>
Motivation: 通用大语言模型（如GPT-4）在需要精确法律知识、复杂推理和上下文敏感性的法律专业子领域表现不佳，因此需要开发专业化的法律大语言模型。

Method: 1) 开发专门针对中国劳动法的专用大语言模型LabourLawLLM；2) 构建涵盖多种劳动法任务的综合基准LabourLawBench，包括法律条文引用、知识问答、案例分类、赔偿计算、命名实体识别和法律案例分析；3) 结合客观指标（如ROUGE-L、准确率、F1分数和soft-F1）和基于GPT-4评分的主观评估建立评价框架。

Result: 实验表明LabourLawLLM在所有任务类别中始终优于通用模型和现有的法律专用大语言模型。

Conclusion: 该模型不仅提升了劳动法领域的AI应用准确性、可靠性和社会价值，而且其方法论可以为其他法律子领域的专用大语言模型构建提供可扩展的途径。

Abstract: Recent advances in large language models (LLMs) have led to substantial progress in domain-specific applications, particularly within the legal domain. However, general-purpose models such as GPT-4 often struggle with specialized subdomains that require precise legal knowledge, complex reasoning, and contextual sensitivity. To address these limitations, we present LabourLawLLM, a legal large language model tailored to Chinese labor law. We also introduce LabourLawBench, a comprehensive benchmark covering diverse labor-law tasks, including legal provision citation, knowledge-based question answering, case classification, compensation computation, named entity recognition, and legal case analysis. Our evaluation framework combines objective metrics (e.g., ROUGE-L, accuracy, F1, and soft-F1) with subjective assessment based on GPT-4 scoring. Experiments show that LabourLawLLM consistently outperforms general-purpose and existing legal-specific LLMs across task categories. Beyond labor law, our methodology provides a scalable approach for building specialized LLMs in other legal subfields, improving accuracy, reliability, and societal value of legal AI applications.

</details>


### [166] [Memo-SQL: Structured Decomposition and Experience-Driven Self-Correction for Training-Free NL2SQL](https://arxiv.org/abs/2601.10011)
*Zerui Yang,Weichuan Wang,Yanwei Xu,Linqi Song,Yudai Matsuda,Wei Han,Bo Bai*

Main category: cs.AI

TL;DR: Memo-SQL是一个无训练框架，通过结构化分解和经验感知自校正解决NL2SQL系统的现有局限性，在BIRD数据集上以低十倍的资源消耗达到68.5%执行准确率。


<details>
  <summary>Details</summary>
Motivation: 现有NL2SQL系统存在两个关键限制：1)只使用正确示例进行上下文学习，忽略了历史错误修复对中蕴含的丰富信号；2)测试时缩放方法通常任意分解问题，产生几乎相同的SQL候选，削弱了集成收益。这些方法还存在精度-效率权衡：高性能需要过度计算，而快速变体又会牺牲质量。

Method: 提出Memo-SQL训练免费框架，采用两个核心思想：结构化分解（实体层面、层次化、原子顺序三种策略促进多样推理）和经验感知自校正（构建动态内存存储成功查询和历史错误修复对，在推理时使用检索增强提示将相关示例纳入上下文）。

Result: 在BIRD数据集上，Memo-SQL达到68.5%的执行准确率，在开放、零微调方法中创下新纪录，同时使用的资源比之前测试时缩放方法少10倍以上。

Conclusion: Memo-SQL框架通过有效的结构化分解和经验感知自校正机制，显著提升了NL2SQL系统的性能和效率，解决了现有方法的不足。

Abstract: Existing NL2SQL systems face two critical limitations: (1) they rely on in-context learning with only correct examples, overlooking the rich signal in historical error-fix pairs that could guide more robust self-correction; and (2) test-time scaling approaches often decompose questions arbitrarily, producing near-identical SQL candidates across runs and diminishing ensemble gains. Moreover, these methods suffer from a stark accuracy-efficiency trade-off: high performance demands excessive computation, while fast variants compromise quality. We present Memo-SQL, a training-free framework that addresses these issues through two simple ideas: structured decomposition and experience-aware self-correction. Instead of leaving decomposition to chance, we apply three clear strategies, entity-wise, hierarchical, and atomic sequential, to encourage diverse reasoning. For correction, we build a dynamic memory of both successful queries and historical error-fix pairs, and use retrieval-augmented prompting to bring relevant examples into context at inference time, no fine-tuning or external APIs required. On BIRD, Memo-SQL achieves 68.5% execution accuracy, setting a new state of the art among open, zero-fine-tuning methods, while using over 10 times fewer resources than prior TTS approaches.

</details>


### [167] [PaperScout: An Autonomous Agent for Academic Paper Search with Process-Aware Sequence-Level Policy Optimization](https://arxiv.org/abs/2601.10029)
*Tingyue Pan,Jie Ouyang,Mingyue Cheng,Qingchuan Li,Zirui Liu,Mingfan Pan,Shuo Yu,Qi Liu*

Main category: cs.AI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Academic paper search is a fundamental task in scientific research, yet most existing approaches rely on rigid, predefined workflows that struggle with complex, conditional queries. To address this limitation, we propose PaperScout, an autonomous agent that reformulates paper search as a sequential decision-making process. Unlike static workflows, PaperScout dynamically decides whether, when, and how to invoke search and expand tools based on accumulated retrieval context. However, training such agents presents a fundamental challenge: standard reinforcement learning methods, typically designed for single-turn tasks, suffer from a granularity mismatch when applied to multi-turn agentic tasks, where token-level optimization diverges from the granularity of sequence-level interactions, leading to noisy credit assignment. We introduce Proximal Sequence Policy Optimization (PSPO), a process-aware, sequence-level policy optimization method that aligns optimization with agent-environment interaction. Comprehensive experiments on both synthetic and real-world benchmarks demonstrate that PaperScout significantly outperforms strong workflow-driven and RL baselines in both recall and relevance, validating the effectiveness of our adaptive agentic framework and optimization strategy.

</details>


### [168] [FilDeep: Learning Large Deformations of Elastic-Plastic Solids with Multi-Fidelity Data](https://arxiv.org/abs/2601.10031)
*Jianheng Tang,Shilong Tao,Zhe Feng,Haonan Sun,Menglu Wang,Zhanxing Zhu,Yunhuai Liu*

Main category: cs.AI

TL;DR: 该研究提出了一种基于保真度的深度学习框架FilDeep，用于解决弹性塑性固体大变形的数量-精度困境，通过同时使用低保真度和高保真度数据进行训练，并在大变形问题中设计了跨保真度模块来捕获长程物理相互作用。


<details>
  <summary>Details</summary>
Motivation: 传统数值方法在计算弹性塑性固体大变形时存在局限性，而当前深度学习技术依赖于难以获取的高数量、高精度数据集，数据构建过程中面临数量与精度的矛盾，导致模型性能不理想。

Method: 提出FilDeep框架：1) 同时使用低保真度（数量多、精度低）和高保真度（精度高、数量少）数据进行训练；2) 引入注意力机制的跨保真度模块来捕获多保真度数据间的长程物理相互作用；3) 针对拉伸弯曲这一代表性大变形问题进行专门设计。

Result: 大量实验表明，FilDeep在弹性塑性固体大变形问题上能够持续达到最先进的性能，并且能够在制造应用中高效部署。

Conclusion: FilDeep是首个使用多保真度数据解决大变形问题的深度学习框架，有效解决了数据数量与精度之间的困境，为大变形计算提供了新的解决方案。

Abstract: The scientific computation of large deformations in elastic-plastic solids is crucial in various manufacturing applications. Traditional numerical methods exhibit several inherent limitations, prompting Deep Learning (DL) as a promising alternative. The effectiveness of current DL techniques typically depends on the availability of high-quantity and high-accuracy datasets, which are yet difficult to obtain in large deformation problems. During the dataset construction process, a dilemma stands between data quantity and data accuracy, leading to suboptimal performance in the DL models. To address this challenge, we focus on a representative application of large deformations, the stretch bending problem, and propose FilDeep, a Fidelity-based Deep Learning framework for large Deformation of elastic-plastic solids. Our FilDeep aims to resolve the quantity-accuracy dilemma by simultaneously training with both low-fidelity and high-fidelity data, where the former provides greater quantity but lower accuracy, while the latter offers higher accuracy but in less quantity. In FilDeep, we provide meticulous designs for the practical large deformation problem. Particularly, we propose attention-enabled cross-fidelity modules to effectively capture long-range physical interactions across MF data. To the best of our knowledge, our FilDeep presents the first DL framework for large deformation problems using MF data. Extensive experiments demonstrate that our FilDeep consistently achieves state-of-the-art performance and can be efficiently deployed in manufacturing.

</details>


### [169] [State of AI: An Empirical 100 Trillion Token Study with OpenRouter](https://arxiv.org/abs/2601.10088)
*Malika Aubakirova,Alex Atallah,Chris Clark,Justin Summerville,Anjney Midha*

Main category: cs.AI

TL;DR: 这篇论文通过OpenRouter平台分析了超过100万亿tokens的真实大语言模型使用数据，揭示了实际使用中的复杂模式：开放权重模型的广泛采用、创意角色扮演和编程辅助类别的突出受欢迎程度、自主推断的兴起，以及早期用户更持久参与度的'玻璃鞋效应'。


<details>
  <summary>Details</summary>
Motivation: 随着o1等推理模型的发布，大语言模型从单次模式生成转向多步深思推断，加速了实际应用。然而，对这些模型实际使用情况的实证理解滞后于技术发展速度。研究者希望通过真实使用数据填补这一空白。

Method: 利用OpenRouter平台（提供多种大语言模型的AI推理服务），分析了超过100万亿tokens的真实世界大语言模型交互数据，涵盖不同任务、地域和时间维度。

Result: 研究发现：1）开放权重模型获得实质性采用；2）创意角色扮演和编程助理类别的受欢迎程度超出预期（不仅是通常认为的生产力任务）；3）自主推断兴起；4）早期用户群体的参与度持久性远超后期用户，这被称为'玻璃鞋效应'。

Conclusion: 研究结果表明开发者和最终用户在实际环境中使用大语言模型的方式复杂多样。这些数据驱动的洞察可为模型构建者、AI开发者和基础设施提供商提供参考，帮助更好地设计和部署大语言模型系统。

Abstract: The past year has marked a turning point in the evolution and real-world use of large language models (LLMs). With the release of the first widely adopted reasoning model, o1, on December 5th, 2024, the field shifted from single-pass pattern generation to multi-step deliberation inference, accelerating deployment, experimentation, and new classes of applications. As this shift unfolded at a rapid pace, our empirical understanding of how these models have actually been used in practice has lagged behind. In this work, we leverage the OpenRouter platform, which is an AI inference provider across a wide variety of LLMs, to analyze over 100 trillion tokens of real-world LLM interactions across tasks, geographies, and time. In our empirical study, we observe substantial adoption of open-weight models, the outsized popularity of creative roleplay (beyond just the productivity tasks many assume dominate) and coding assistance categories, plus the rise of agentic inference. Furthermore, our retention analysis identifies foundational cohorts: early users whose engagement persists far longer than later cohorts. We term this phenomenon the Cinderella "Glass Slipper" effect. These findings underscore that the way developers and end-users engage with LLMs "in the wild" is complex and multifaceted. We discuss implications for model builders, AI developers, and infrastructure providers, and outline how a data-driven understanding of usage can inform better design and deployment of LLM systems.

</details>


### [170] [MATRIX AS PLAN: Structured Logical Reasoning with Feedback-Driven Replanning](https://arxiv.org/abs/2601.10101)
*Ke Chen,Jiandian Zeng,Zihao Peng,Guo Li,Guangxue Zhang,Tian Wang*

Main category: cs.AI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: As knowledge and semantics on the web grow increasingly complex, enhancing Large Language Models (LLMs) comprehension and reasoning capabilities has become particularly important. Chain-of-Thought (CoT) prompting has been shown to enhance the reasoning capabilities of LLMs. However, it still falls short on logical reasoning tasks that rely on symbolic expressions and strict deductive rules. Neuro-symbolic methods address this gap by enforcing formal correctness through external solvers. Yet these solvers are highly format-sensitive, and small instabilities in model outputs can lead to frequent processing failures. LLM-driven approaches avoid parsing brittleness, but they lack structured representations and process-level error-correction mechanisms. To further enhance the logical reasoning capabilities of LLMs, we propose MatrixCoT, a structured CoT framework with a matrix-based plan. Specifically, we normalize and type natural language expressions, attach explicit citation fields, and introduce a matrix-based planning method to preserve global relations among steps. The plan becomes a verifiable artifact, making execution more stable. For verification, we also add a feedback-driven replanning mechanism. Under semantic-equivalence constraints, it identifies omissions and defects, rewrites and compresses the dependency matrix, and produces a more trustworthy final answer. Experiments on five logical-reasoning benchmarks and five LLMs show that, without relying on external solvers, MatrixCoT enhances both robustness and interpretability when tackling complex symbolic reasoning tasks, while maintaining competitive performance.

</details>


### [171] [M^4olGen: Multi-Agent, Multi-Stage Molecular Generation under Precise Multi-Property Constraints](https://arxiv.org/abs/2601.10131)
*Yizhan Li,Florence Cloutier,Sifan Wu,Ali Parviz,Boris Knyazev,Yan Zhang,Glen Berseth,Bang Liu*

Main category: cs.AI

TL;DR: MolGen：两阶段分子生成框架，利用片段级检索增强和强化学习优化，实现多属性精确控制


<details>
  <summary>Details</summary>
Motivation: 生成满足多个物理化学属性精确数值约束的分子是具有挑战性的任务。尽管大语言模型表达能力强大，但在没有外部结构和反馈的情况下，难以实现精确的多目标控制和数值推理。

Method: MolGen是一个片段级、检索增强的两阶段框架：1）原型生成阶段：多智能体推理器执行检索锚定的片段级编辑，生成接近可行区域的候选分子；2）基于RL的细粒度优化阶段：使用组相对策略优化训练的片段级优化器，应用单跳或多跳细化，明确最小化属性误差，同时调控编辑复杂度和与原型偏差。使用大型自动策划的数据集支持两阶段训练，该数据集包含片段编辑推理链和测量的属性变化。

Result: 在两个属性约束集（QED、LogP、分子量和HOMO、LUMO）上的实验表明，该框架在有效性和精确满足多属性目标方面持续改进，优于强基线的大语言模型和图基算法。

Conclusion: MolGen框架通过利用片段和支持可控的数值目标细化，更好地推理分子结构，实现了对多属性约束的精确控制，在分子生成任务上表现出色。

Abstract: Generating molecules that satisfy precise numeric constraints over multiple physicochemical properties is critical and challenging. Although large language models (LLMs) are expressive, they struggle with precise multi-objective control and numeric reasoning without external structure and feedback. We introduce \textbf{M olGen}, a fragment-level, retrieval-augmented, two-stage framework for molecule generation under multi-property constraints. Stage I : Prototype generation: a multi-agent reasoner performs retrieval-anchored, fragment-level edits to produce a candidate near the feasible region. Stage II : RL-based fine-grained optimization: a fragment-level optimizer trained with Group Relative Policy Optimization (GRPO) applies one- or multi-hop refinements to explicitly minimize the property errors toward our target while regulating edit complexity and deviation from the prototype. A large, automatically curated dataset with reasoning chains of fragment edits and measured property deltas underpins both stages, enabling deterministic, reproducible supervision and controllable multi-hop reasoning. Unlike prior work, our framework better reasons about molecules by leveraging fragments and supports controllable refinement toward numeric targets. Experiments on generation under two sets of property constraints (QED, LogP, Molecular Weight and HOMO, LUMO) show consistent gains in validity and precise satisfaction of multi-property targets, outperforming strong LLMs and graph-based algorithms.

</details>


### [172] [Is More Context Always Better? Examining LLM Reasoning Capability for Time Interval Prediction](https://arxiv.org/abs/2601.10132)
*Yanan Cao,Farnaz Fallahi,Murali Mohana Krishna Dandu,Lalitesh Morishetti,Kai Zhao,Luyi Ma,Sinduja Subramaniam,Jianpeng Xu,Evren Korpeoglu,Kaushiki Nag,Sushant Kumar,Kannan Achan*

Main category: cs.AI

TL;DR: LLM在行为时序预测任务中表现有限，不及专用机器学习模型，过多用户上下文反而降低预测准确率。


<details>
  <summary>Details</summary>
Motivation: 探索LLM能否从结构化行为数据中推断时间规律性，特别是预测重复行为（如复购）之间的时间间隔，并研究不同层次上下文信息如何影响其预测行为。

Method: 使用回购场景作为代表性任务，在零样本设置下对最先进的LLM进行基准测试，与统计模型和机器学习模型进行比较，分析不同程度上下文信息（从无到详细用户级信息）对LLM预测性能的影响。

Result: 1. LLM仅能超越轻量级统计基线，但始终不如专用机器学习模型；2. 适度上下文能提升LLM准确性，但添加更详细的用户级信息会降低性能。

Conclusion: 当前LLM在结构化时序推理方面存在根本性局限，挑战了'更多上下文带来更好推理'的假设，为未来设计结合统计精度与语言灵活性的上下文感知混合模型提供指导。

Abstract: Large Language Models (LLMs) have demonstrated impressive capabilities in reasoning and prediction across different domains. Yet, their ability to infer temporal regularities from structured behavioral data remains underexplored. This paper presents a systematic study investigating whether LLMs can predict time intervals between recurring user actions, such as repeated purchases, and how different levels of contextual information shape their predictive behavior. Using a simple but representative repurchase scenario, we benchmark state-of-the-art LLMs in zero-shot settings against both statistical and machine-learning models. Two key findings emerge. First, while LLMs surpass lightweight statistical baselines, they consistently underperform dedicated machine-learning models, showing their limited ability to capture quantitative temporal structure. Second, although moderate context can improve LLM accuracy, adding further user-level detail degrades performance. These results challenge the assumption that "more context leads to better reasoning". Our study highlights fundamental limitations of today's LLMs in structured temporal inference and offers guidance for designing future context-aware hybrid models that integrate statistical precision with linguistic flexibility.

</details>


### [173] [History Is Not Enough: An Adaptive Dataflow System for Financial Time-Series Synthesis](https://arxiv.org/abs/2601.10143)
*Haochong Xia,Yao Long Teng,Regan Tan,Molei Qin,Xinrun Wang,Bo An*

Main category: cs.AI

TL;DR: 针对量化金融中训练与实盘性能差距问题（概念漂移和分布非平稳性），本文提出了一种漂移感知的数据流系统，通过集成机器学习自适应控制到数据管理流程中来解决历史数据不足的问题。


<details>
  <summary>Details</summary>
Motivation: 量化金融中，基于静态历史数据训练的模型往往过拟合，在动态市场中泛化能力差。训练与实盘性能之间的差距由概念漂移和分布非平稳性驱动，成为构建可靠数据驱动系统的关键障碍。'历史不足'这一格言强调了需要能够与市场共同演化的自适应数据生成方法，而非仅依赖历史观测。

Method: 提出了一个漂移感知数据流系统：1）参数化数据操作模块：包含单股转换、多股混合和筛选操作；2）自适应规划器-调度器：采用基于梯度的双层优化控制整个系统。该设计将数据增强、课程学习和数据工作流管理统一到单个可微分框架中，实现溯源感知重放和连续数据质量监控。

Result: 在预测和强化学习交易任务上的广泛实验表明，该框架提高了模型鲁棒性，改善了风险调整后收益。

Conclusion: 该系统为金融数据的自适应数据管理和学习引导的工作流自动化提供了一种可推广的方法，能够有效应对金融市场中的动态变化和分布漂移问题。

Abstract: In quantitative finance, the gap between training and real-world performance-driven by concept drift and distributional non-stationarity-remains a critical obstacle for building reliable data-driven systems. Models trained on static historical data often overfit, resulting in poor generalization in dynamic markets. The mantra "History Is Not Enough" underscores the need for adaptive data generation that learns to evolve with the market rather than relying solely on past observations. We present a drift-aware dataflow system that integrates machine learning-based adaptive control into the data curation process. The system couples a parameterized data manipulation module comprising single-stock transformations, multi-stock mix-ups, and curation operations, with an adaptive planner-scheduler that employs gradient-based bi-level optimization to control the system. This design unifies data augmentation, curriculum learning, and data workflow management under a single differentiable framework, enabling provenance-aware replay and continuous data quality monitoring. Extensive experiments on forecasting and reinforcement learning trading tasks demonstrate that our framework enhances model robustness and improves risk-adjusted returns. The system provides a generalizable approach to adaptive data management and learning-guided workflow automation for financial data.

</details>


### [174] [DecisionLLM: Large Language Models for Long Sequence Decision Exploration](https://arxiv.org/abs/2601.10148)
*Xiaowei Lv,Zhilin Zhang,Yijun Li,Yusen Huo,Siyuan Ju,Xuyan Li,Chunxiang Hong,Tianyu Wang,Yongcai Wang,Peng Sun,Chuan Yu,Jian Xu,Bo Zheng*

Main category: cs.AI

TL;DR: DecisionLLM将大语言模型（LLM）应用于离线决策任务，通过将轨迹数据处理为一种模态并与自然语言任务描述对齐，解决了LLM对连续数值的理解问题，在迷宫和竞价任务中超越了传统决策变换器。


<details>
  <summary>Details</summary>
Motivation: 研究LLM在长序列决策问题中的应用潜力。虽然决策变换器（DT）已经通过自回归序列建模在强化学习中取得了成功，但同样基于Transformer架构的大语言模型（LLM）在更大规模上展现了复杂推理和规划能力，这启发我们探索LLM是否能进一步提升长期顺序决策的性能。然而，LLM缺乏对连续数值（例如表示为文本字符串的数字）的数值大小和顺序的固有理解能力，这构成了一个核心挑战。

Method: 提出DecisionLLM。该方法将轨迹视为一种独特的模态。通过学习将轨迹数据与自然语言任务描述进行对齐，模型能够在一个统一的框架内自回归地预测未来的决策。该研究还为此范式建立了一套缩放法则（scaling laws），表明性能取决于三个关键因素：模型规模、数据量和数据质量。

Result: 在离线实验基准和竞价场景中，DecisionLLM表现优异。具体而言，DecisionLLM-3B在Maze2D umaze-v1上优于传统决策变换器（DT）69.4分，在AuctionNet上优于DT 0.085分。

Conclusion: DecisionLLM扩展了AIGB范式，并为未来在在线竞价等领域的探索指明了有前景的方向。

Abstract: Long-sequence decision-making, which is usually addressed through reinforcement learning (RL), is a critical component for optimizing strategic operations in dynamic environments, such as real-time bidding in computational advertising. The Decision Transformer (DT) introduced a powerful paradigm by framing RL as an autoregressive sequence modeling problem. Concurrently, Large Language Models (LLMs) have demonstrated remarkable success in complex reasoning and planning tasks. This inspires us whether LLMs, which share the same Transformer foundation, but operate at a much larger scale, can unlock new levels of performance in long-horizon sequential decision-making problem. This work investigates the application of LLMs to offline decision making tasks. A fundamental challenge in this domain is the LLMs' inherent inability to interpret continuous values, as they lack a native understanding of numerical magnitude and order when values are represented as text strings. To address this, we propose treating trajectories as a distinct modality. By learning to align trajectory data with natural language task descriptions, our model can autoregressively predict future decisions within a cohesive framework we term DecisionLLM. We establish a set of scaling laws governing this paradigm, demonstrating that performance hinges on three factors: model scale, data volume, and data quality. In offline experimental benchmarks and bidding scenarios, DecisionLLM achieves strong performance. Specifically, DecisionLLM-3B outperforms the traditional Decision Transformer (DT) by 69.4 on Maze2D umaze-v1 and by 0.085 on AuctionNet. It extends the AIGB paradigm and points to promising directions for future exploration in online bidding.

</details>


### [175] [MHub.ai: A Simple, Standardized, and Reproducible Platform for AI Models in Medical Imaging](https://arxiv.org/abs/2601.10154)
*Leonard Nürnberg,Dennis Bontempi,Suraj Pai,Curtis Lisle,Steve Pieper,Ron Kikinis,Sil van de Leemput,Rahul Soni,Gowtham Murugesan,Cosmin Ciausu,Miriam Groeneveld,Felix J. Dorfner,Jue Jiang,Aneesh Rangnekar,Harini Veeraraghavan,Joeran S. Bosma,Keno Bressem,Raymond Mak,Andrey Fedorov,Hugo JWL Aerts*

Main category: cs.AI

TL;DR: 针对医学影像AI模型的多样性和可重复性问题，MHub.ai提出了一个基于容器的开源平台，旨在统一模型访问、提升可重复性，并支持临床转化。


<details>
  <summary>Details</summary>
Motivation: 医学影像AI研究及临床应用的局限在于模型实现和架构的多样性、文档不一致以及可重复性问题。现有工具缺乏统一、易用的标准化平台。

Method: MHub.ai将同行评议的模型打包为标准容器，支持DICOM等多种格式，提供统一接口和结构化元数据，并配备公开参考数据验证模型运行。平台采用模块化框架，允许模型适配和社区贡献。

Result: 平台上已包含多种模态的先进分割、预测和特征提取模型。通过在肺部分割模型上的对比评估，展示了平台的实用性，并公开了分割结果与评估指标，提供了可交互的分析看板。

Conclusion: MHub.ai通过简化模型使用、统一执行命令和输出标准，降低了临床转化的门槛，为医学影像AI的可访问性和可重复性提供了有效解决方案。

Abstract: Artificial intelligence (AI) has the potential to transform medical imaging by automating image analysis and accelerating clinical research. However, research and clinical use are limited by the wide variety of AI implementations and architectures, inconsistent documentation, and reproducibility issues. Here, we introduce MHub.ai, an open-source, container-based platform that standardizes access to AI models with minimal configuration, promoting accessibility and reproducibility in medical imaging. MHub.ai packages models from peer-reviewed publications into standardized containers that support direct processing of DICOM and other formats, provide a unified application interface, and embed structured metadata. Each model is accompanied by publicly available reference data that can be used to confirm model operation. MHub.ai includes an initial set of state-of-the-art segmentation, prediction, and feature extraction models for different modalities. The modular framework enables adaptation of any model and supports community contributions. We demonstrate the utility of the platform in a clinical use case through comparative evaluation of lung segmentation models. To further strengthen transparency and reproducibility, we publicly release the generated segmentations and evaluation metrics and provide interactive dashboards that allow readers to inspect individual cases and reproduce or extend our analysis. By simplifying model use, MHub.ai enables side-by-side benchmarking with identical execution commands and standardized outputs, and lowers the barrier to clinical translation.

</details>


### [176] [MMPG: MoE-based Adaptive Multi-Perspective Graph Fusion for Protein Representation Learning](https://arxiv.org/abs/2601.10157)
*Yusong Wang,Jialun Shen,Zhihao Wu,Yicheng Xu,Shiyin Tan,Mingkun Xu,Changshuo Wang,Zixing Song,Prayag Tiwari*

Main category: cs.AI

TL;DR: 提出了MMPG框架，通过多角度（物理、化学、几何）构建蛋白图并自适应融合，解决传统GNN蛋白表示方法单视角的局限性，提升了蛋白表示的完整性和下游任务的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的GNN蛋白表示学习方法多采用单一视角构建图，仅捕获残基相互作用的部分属性，导致蛋白表示不完整。

Method: 1. 多视角蛋白图构建：物理、化学、几何视角；2. 混合专家（MoE）自适应融合模块，动态路由视角至专家层，学习视角内特征和视角间协同作用；3. MoE从个体表示→成对视角协同→全局共识，捕捉多层次信息。

Result: 定量验证MoE能自动使专家在三个层次（个体表示、视角间协同、全局共识）上专业化；在四个不同的下游蛋白任务中取得了先进的性能。

Conclusion: MMPG通过多视角构造和MoE自适应融合，有效提升了蛋白质表示质量，并在多项下游任务中表现出优越的性能。

Abstract: Graph Neural Networks (GNNs) have been widely adopted for Protein Representation Learning (PRL), as residue interaction networks can be naturally represented as graphs. Current GNN-based PRL methods typically rely on single-perspective graph construction strategies, which capture partial properties of residue interactions, resulting in incomplete protein representations. To address this limitation, we propose MMPG, a framework that constructs protein graphs from multiple perspectives and adaptively fuses them via Mixture of Experts (MoE) for PRL. MMPG constructs graphs from physical, chemical, and geometric perspectives to characterize different properties of residue interactions. To capture both perspective-specific features and their synergies, we develop an MoE module, which dynamically routes perspectives to specialized experts, where experts learn intrinsic features and cross-perspective interactions. We quantitatively verify that MoE automatically specializes experts in modeling distinct levels of interaction from individual representations, to pairwise inter-perspective synergies, and ultimately to a global consensus across all perspectives. Through integrating this multi-level information, MMPG produces superior protein representations and achieves advanced performance on four different downstream protein tasks.

</details>


### [177] [How does downsampling affect needle electromyography signals? A generalisable workflow for understanding downsampling effects on high-frequency time series](https://arxiv.org/abs/2601.10191)
*Mathieu Cherpitel,Janne Luijten,Thomas Bäck,Camiel Verhamme,Martijn Tannemaat,Anna Kononova*

Main category: cs.AI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Automated analysis of needle electromyography (nEMG) signals is emerging as a tool to support the detection of neuromuscular diseases (NMDs), yet the signals' high and heterogeneous sampling rates pose substantial computational challenges for feature-based machine-learning models, particularly for near real-time analysis. Downsampling offers a potential solution, but its impact on diagnostic signal content and classification performance remains insufficiently understood. This study presents a workflow for systematically evaluating information loss caused by downsampling in high-frequency time series. The workflow combines shape-based distortion metrics with classification outcomes from available feature-based machine learning models and feature space analysis to quantify how different downsampling algorithms and factors affect both waveform integrity and predictive performance. We use a three-class NMD classification task to experimentally evaluate the workflow. We demonstrate how the workflow identifies downsampling configurations that preserve diagnostic information while substantially reducing computational load. Analysis of shape-based distortion metrics showed that shape-aware downsampling algorithms outperform standard decimation, as they better preserve peak structure and overall signal morphology. The results provide practical guidance for selecting downsampling configurations that enable near real-time nEMG analysis and highlight a generalisable workflow that can be used to balance data reduction with model performance in other high-frequency time-series applications as well.

</details>


### [178] [GFM4GA: Graph Foundation Model for Group Anomaly Detection](https://arxiv.org/abs/2601.10193)
*Jiujiu Chen,Weijun Zeng,Shaofeng Hu,Sihong Xie,Hui Xiong*

Main category: cs.AI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Group anomaly detection is crucial in many network applications, but faces challenges due to diverse anomaly patterns. Motivated by the success of large language models (LLMs) in natural language processing, graph foundation models (GFMs) is proposed to handle few-shot learning task with fewer labeling efforts. GFMs have been successfully applied to detection of individual anomalies but cannot be generalized to group anomalies, as group anomaly patterns must be detected as a whole and individuals in an abnormal group can look rather normal. Therefore, we propose GFM4GA, a novel graph foundation model for group anomaly detection. The pipeline is pretrained via dual-level contrastive learning based on feature-based estimation and group extraction, to capture potential group anomaly structure and feature inconsistencies. In the downstream tasks, the pipeline is finetuned in parameter-constrained and group-anomaly-proportion weighted few-shot settings, and its adaptive ability to unseen group anomalies expanded via group contexts determined by labeled anomaly neighbors. Experiments show that GFM4GA surpasses group anomaly detectors and GFMs for individual anomalies, achieving average improvements of 2.85% in AUROC and 2.55% in AUPRC.

</details>


### [179] [Topo-RAG: Topology-aware retrieval for hybrid text-table documents](https://arxiv.org/abs/2601.10215)
*Alex Dantart,Marco Kóvacs-Navarro*

Main category: cs.AI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: In enterprise datasets, documents are rarely pure. They are not just text, nor just numbers; they are a complex amalgam of narrative and structure. Current Retrieval-Augmented Generation (RAG) systems have attempted to address this complexity with a blunt tool: linearization. We convert rich, multidimensional tables into simple Markdown-style text strings, hoping that an embedding model will capture the geometry of a spreadsheet in a single vector. But it has already been shown that this is mathematically insufficient.
  This work presents Topo-RAG, a framework that challenges the assumption that "everything is text". We propose a dual architecture that respects the topology of the data: we route fluid narrative through traditional dense retrievers, while tabular structures are processed by a Cell-Aware Late Interaction mechanism, preserving their spatial relationships. Evaluated on SEC-25, a synthetic enterprise corpus that mimics real-world complexity, Topo-RAG demonstrates an 18.4% improvement in nDCG@10 on hybrid queries compared to standard linearization approaches. It's not just about searching better; it's about understanding the shape of information.

</details>


### [180] [TRIM: Hybrid Inference via Targeted Stepwise Routing in Multi-Step Reasoning Tasks](https://arxiv.org/abs/2601.10245)
*Vansh Kapoor,Aman Gupta,Hao Chen,Anurag Beniwal,Jing Huang,Aviral Kumar*

Main category: cs.AI

TL;DR: TRIM在数学推理等多步任务中提出针对性步骤级路由，仅将关键步骤分配给大模型，显著提升推理效率与成本效益。


<details>
  <summary>Details</summary>
Motivation: 多步推理任务存在级联失败风险，现有LLM路由方法将整个查询分配给单一模型，无法区分关键步骤与常规步骤，导致计算资源浪费与效率低下。

Method: 提出TRIM框架：通过过程奖励模型识别错误步骤；基于步骤级不确定性和预算约束进行路由决策；开发从简单阈值策略到考虑长时精度-成本权衡的复杂策略多种路由方法。

Result: 在MATH-500上，简单阈值策略比之前路由方法提升5倍成本效率；高级策略在减少80%昂贵模型token的情况下匹配强模型性能；在AIME等更难基准上实现高达6倍的成本效率提升；所有方法在数学推理任务中展现良好泛化能力。

Conclusion: 步骤级难度是推理任务的基本特征；通过针对性步骤干预可以革命性地改变推理效率，将昂贵调用限制在真正需要更强模型以防止级联错误的关键步骤。

Abstract: Multi-step reasoning tasks like mathematical problem solving are vulnerable to cascading failures, where a single incorrect step leads to complete solution breakdown. Current LLM routing methods assign entire queries to one model, treating all reasoning steps as equal. We propose TRIM (Targeted routing in multi-step reasoning tasks), which routes only critical steps$\unicode{x2013}$those likely to derail the solution$\unicode{x2013}$to larger models while letting smaller models handle routine continuations. Our key insight is that targeted step-level interventions can fundamentally transform inference efficiency by confining expensive calls to precisely those steps where stronger models prevent cascading errors. TRIM operates at the step-level: it uses process reward models to identify erroneous steps and makes routing decisions based on step-level uncertainty and budget constraints. We develop several routing strategies within TRIM, ranging from a simple threshold-based policy to more expressive policies that reason about long-horizon accuracy-cost trade-offs and uncertainty in step-level correctness estimates. On MATH-500, even the simplest thresholding strategy surpasses prior routing methods with 5x higher cost efficiency, while more advanced policies match the strong, expensive model's performance using 80% fewer expensive model tokens. On harder benchmarks such as AIME, TRIM achieves up to 6x higher cost efficiency. All methods generalize effectively across math reasoning tasks, demonstrating that step-level difficulty represents fundamental characteristics of reasoning.

</details>


### [181] [Evidence-Augmented Policy Optimization with Reward Co-Evolution for Long-Context Reasoning](https://arxiv.org/abs/2601.10306)
*Xin Guan,Zijian Li,Shen Huang,Pengjun Xie,Jingren Zhou,Jiuxin Cao*

Main category: cs.AI

TL;DR: EAPO (Evidence-Augmented Policy Optimization)为长上下文推理设计，通过Group-Relative Evidence Reward提供密集过程监督，提升证据质量，改善稀疏奖励问题。


<details>
  <summary>Details</summary>
Motivation: 传统强化学习在长上下文推理中因结果奖励稀疏，无法有效约束无根据的“幸运猜测”，导致证据检索过程缺乏监督。

Method: 提出Evidence-Augmented Reasoning范式，通过Tree-Structured Evidence Sampling验证证据提取是关键瓶颈；EAPO引入Group-Relative Evidence Reward模型提供密集过程监督，并采用Adaptive Reward-Policy Co-Evolution机制迭代优化奖励模型。

Result: 在八个基准测试中，EAPO相比现有最优方法显著提升了长上下文推理性能。

Conclusion: EAPO通过增强证据监督和奖励-策略协同进化机制，有效解决了长上下文推理中强化学习的稀疏奖励问题，提升了推理质量。

Abstract: While Reinforcement Learning (RL) has advanced LLM reasoning, applying it to long-context scenarios is hindered by sparsity of outcome rewards. This limitation fails to penalize ungrounded "lucky guesses," leaving the critical process of needle-in-a-haystack evidence retrieval largely unsupervised. To address this, we propose EAPO (Evidence-Augmented Policy Optimization). We first establish the Evidence-Augmented Reasoning paradigm, validating via Tree-Structured Evidence Sampling that precise evidence extraction is the decisive bottleneck for long-context reasoning. Guided by this insight, EAPO introduces a specialized RL algorithm where a reward model computes a Group-Relative Evidence Reward, providing dense process supervision to explicitly improve evidence quality. To sustain accurate supervision throughout training, we further incorporate an Adaptive Reward-Policy Co-Evolution mechanism. This mechanism iteratively refines the reward model using outcome-consistent rollouts, sharpening its discriminative capability to ensure precise process guidance. Comprehensive evaluations across eight benchmarks demonstrate that EAPO significantly enhances long-context reasoning performance compared to SOTA baselines.

</details>


### [182] [C-GRASP: Clinically-Grounded Reasoning for Affective Signal Processing](https://arxiv.org/abs/2601.10342)
*Cheng Lin Cheng,Ting Chuan Lin,Chai Kai Chang*

Main category: cs.AI

TL;DR: 该研究提出了一种名为C-GRASP的框架，用于改善大型语言模型在心率变异性分析中的临床推理准确性，通过分层评分系统、个体化基线校正和自动化防护机制来减少生理伪影和群体偏见


<details>
  <summary>Details</summary>
Motivation: 心率变异性是自主神经监控的重要无创指标，但大型语言模型在HRV解释中存在生理幻觉问题，包括呼吸性窦性心律不齐污染、非线性指标的数据不稳定性，以及过度依赖群体标准而非个体化基线

Method: 开发C-GRASP框架，将HRV解释分解为八个可追踪的推理步骤，包含Z分数优先级层次结构，优先考虑个体化基线偏移而非规范统计，通过自动化RSA感知防护机制避免频域指标污染

Result: 在DREAMER数据集的414个试验中，C-GRASP与高规模推理模型（如MedGemma3-thinking）结合，在四类情绪分类中达到37.3%的准确率和69.6%的临床推理一致性评分，消融研究确认个体化Delta Z分数模块是关键逻辑锚点

Conclusion: C-GRASP将情感计算从黑盒分类转变为透明、基于证据的临床决策支持系统，为生物医学工程中更安全的AI集成铺平道路

Abstract: Heart rate variability (HRV) is a pivotal noninvasive marker for autonomic monitoring; however, applying Large Language Models (LLMs) to HRV interpretation is hindered by physiological hallucinations. These include respiratory sinus arrhythmia (RSA) contamination, short-data instability in nonlinear metrics, and the neglect of individualized baselines in favor of population norms. We propose C-GRASP (Clinically-Grounded Reasoning for Affective Signal Processing), a guardrailed RAG-enhanced pipeline that decomposes HRV interpretation into eight traceable reasoning steps. Central to C-GRASP is a Z-score Priority Hierarchy that enforces the weighting of individualized baseline shifts over normative statistics. The system effectively mitigates spectral hallucinations through automated RSA-aware guardrails, preventing contamination of frequency-domain indices. Evaluated on 414 trials from the DREAMER dataset, C-GRASP integrated with high-scale reasoning models (e.g., MedGemma3-thinking) achieved superior performance in 4-class emotion classification (37.3% accuracy) and a Clinical Reasoning Consistency (CRC) score of 69.6%. Ablation studies confirm that the individualized Delta Z-score module serves as the critical logical anchor, preventing the "population bias" common in native LLMs. Ultimately, C-GRASP transitions affective computing from black-box classification to transparent, evidence-based clinical decision support, paving the way for safer AI integration in biomedical engineering.

</details>


### [183] [LatentRefusal: Latent-Signal Refusal for Unanswerable Text-to-SQL Queries](https://arxiv.org/abs/2601.10398)
*Xuancheng Ren,Shijing Hu,Zhihui Lu,Jiangqi Huang,Qiang Duan*

Main category: cs.AI

TL;DR: LLM-based text-to-SQL系统中存在不可回答和未充分指定的查询会导致生成错误SQL程序，现有拒绝策略存在缺陷，本文提出LatentRefusal机制，通过中间隐藏激活预测查询可回答性，提高安全性。


<details>
  <summary>Details</summary>
Motivation: LLM-based text-to-SQL系统中，不可回答和未充分指定的用户查询可能生成错误的SQL程序，导致误导性结果或违反安全约束，现有拒绝策略要么依赖脆弱的输出级指令遵循，要么需要复杂的输出不确定性估计，难以安全部署。

Method: 将安全拒绝形式化为可回答性门控问题，提出LatentRefusal机制，利用大语言模型的中间隐藏激活预测查询可回答性；设计Tri-Residual Gated Encoder轻量级探测架构，抑制模式噪声，放大问题与模式不匹配的稀疏局部线索。

Result: 在多种模糊和不可回答场景下的广泛实验表明，LatentRefusal方法有效，在四个基准测试中平均F1提高到88.5%，同时仅增加约2毫秒的探测开销，可作为可附加的高效安全层。

Conclusion: LatentRefusal提供了一种高效、可附加的安全拒绝机制，通过分析LLM中间表征准确预测查询可回答性，为文本到SQL系统的安全部署提供了有效解决方案。

Abstract: In LLM-based text-to-SQL systems, unanswerable and underspecified user queries may generate not only incorrect text but also executable programs that yield misleading results or violate safety constraints, posing a major barrier to safe deployment. Existing refusal strategies for such queries either rely on output-level instruction following, which is brittle due to model hallucinations, or estimate output uncertainty, which adds complexity and overhead. To address this challenge, we formalize safe refusal in text-to-SQL systems as an answerability-gating problem and propose LatentRefusal, a latent-signal refusal mechanism that predicts query answerability from intermediate hidden activations of a large language model. We introduce the Tri-Residual Gated Encoder, a lightweight probing architecture, to suppress schema noise and amplify sparse, localized cues of question-schema mismatch that indicate unanswerability. Extensive empirical evaluations across diverse ambiguous and unanswerable settings, together with ablation studies and interpretability analyses, demonstrate the effectiveness of the proposed approach and show that LatentRefusal provides an attachable and efficient safety layer for text-to-SQL systems. Across four benchmarks, LatentRefusal improves average F1 to 88.5 percent on both backbones while adding approximately 2 milliseconds of probe overhead.

</details>


### [184] [ErrEval: Error-Aware Evaluation for Question Generation through Explicit Diagnostics](https://arxiv.org/abs/2601.10406)
*Weiping Fu,Bifan Wei,Jingyi Hao,Yushun Zhang,Jian Zhang,Jiaxin Wang,Bo Li,Yu He,Lingling Zhang,Jun Liu*

Main category: cs.AI

TL;DR: ErrEval是一个面向自动问句生成（QG）的错误感知评估框架，通过显式错误诊断提升评估质量。


<details>
  <summary>Details</summary>
Motivation: 现有自动问句生成评估方法（包括LLM评估器）多为黑盒式整体评估，缺乏显式错误建模，导致忽略事实幻觉、答案不匹配等关键缺陷，高估了问题质量。

Method: ErrEval将评估重新定义为两阶段过程：1. 轻量级即插即用错误识别器检测和分类结构、语言和内容相关的常见错误；2. 将这些诊断信号作为显式证据，指导LLM评估器做出更细粒度、有根据的判断。

Result: 在三个基准测试上的广泛实验表明，ErrEval能有效提升与人工判断的一致性，并有效缓解对低质量问题的过高估计。

Conclusion: ErrEval通过显式错误诊断改进了问句生成评估，展示了结合诊断信号的有效性，为更准确的QG质量评估提供了新框架。

Abstract: Automatic Question Generation (QG) often produces outputs with critical defects, such as factual hallucinations and answer mismatches. However, existing evaluation methods, including LLM-based evaluators, mainly adopt a black-box and holistic paradigm without explicit error modeling, leading to the neglect of such defects and overestimation of question quality. To address this issue, we propose ErrEval, a flexible and Error-aware Evaluation framework that enhances QG evaluation through explicit error diagnostics. Specifically, ErrEval reformulates evaluation as a two-stage process of error diagnosis followed by informed scoring. At the first stage, a lightweight plug-and-play Error Identifier detects and categorizes common errors across structural, linguistic, and content-related aspects. These diagnostic signals are then incorporated as explicit evidence to guide LLM evaluators toward more fine-grained and grounded judgments. Extensive experiments on three benchmarks demonstrate the effectiveness of ErrEval, showing that incorporating explicit diagnostics improves alignment with human judgments. Further analyses confirm that ErrEval effectively mitigates the overestimation of low-quality questions.

</details>


### [185] [LADFA: A Framework of Using Large Language Models and Retrieval-Augmented Generation for Personal Data Flow Analysis in Privacy Policies](https://arxiv.org/abs/2601.10413)
*Haiyue Yuan,Nikolay Matyunin,Ali Raza,Shujun Li*

Main category: cs.AI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Privacy policies help inform people about organisations' personal data processing practices, covering different aspects such as data collection, data storage, and sharing of personal data with third parties. Privacy policies are often difficult for people to fully comprehend due to the lengthy and complex legal language used and inconsistent practices across different sectors and organisations. To help conduct automated and large-scale analyses of privacy policies, many researchers have studied applications of machine learning and natural language processing techniques, including large language models (LLMs). While a limited number of prior studies utilised LLMs for extracting personal data flows from privacy policies, our approach builds on this line of work by combining LLMs with retrieval-augmented generation (RAG) and a customised knowledge base derived from existing studies. This paper presents the development of LADFA, an end-to-end computational framework, which can process unstructured text in a given privacy policy, extract personal data flows and construct a personal data flow graph, and conduct analysis of the data flow graph to facilitate insight discovery. The framework consists of a pre-processor, an LLM-based processor, and a data flow post-processor. We demonstrated and validated the effectiveness and accuracy of the proposed approach by conducting a case study that involved examining ten selected privacy policies from the automotive industry. Moreover, it is worth noting that LADFA is designed to be flexible and customisable, making it suitable for a range of text-based analysis tasks beyond privacy policy analysis.

</details>


### [186] [LLMdoctor: Token-Level Flow-Guided Preference Optimization for Efficient Test-Time Alignment of Large Language Models](https://arxiv.org/abs/2601.10416)
*Tiesunlong Shen,Rui Mao,Jin Wang,Heming Sun,Jian Zhang,Xuejie Zhang,Erik Cambria*

Main category: cs.AI

TL;DR: 针对传统大语言模型对齐方法计算成本高、灵活性差的问题，LLMdoctor框架提出一种基于分诊范式的高效测试时对齐方法，通过细粒度token级奖励获取与流引导优化在保留基础模型生成多样性的同时实现性能超越


<details>
  <summary>Details</summary>
Motivation: 传统的大语言模型与人类偏好对齐方法（如微调）计算成本高昂且缺乏灵活性，而现有的测试时对齐方法大多依赖扭曲的轨迹级信号或低效采样，导致性能受限且难以保持基础模型的生成多样性

Method: LLMdoctor采用患者-医生分诊范式，通过细粒度token级奖励获取机制从基础模型（患者LLM）的行为变化中提取偏好信号，再通过token级流引导偏好优化（TFPO）训练一个小型专门化医生模型，该系统在所有子轨迹上建立流一致性，实现逐token的精确对齐

Result: 广泛的实验表明，LLMdoctor显著优于现有的测试时对齐方法，甚至超越了DPO等完整微调方法的性能，同时在保留生成多样性方面表现出色

Conclusion: LLMdoctor框架为高效且灵活的大语言模型对齐提供了新的解决方案，通过在token级别进行精确控制，克服了传统测试时对齐方法的局限性，在性能、效率和多样性保护方面取得了平衡

Abstract: Aligning Large Language Models (LLMs) with human preferences is critical, yet traditional fine-tuning methods are computationally expensive and inflexible. While test-time alignment offers a promising alternative, existing approaches often rely on distorted trajectory-level signals or inefficient sampling, fundamentally capping performance and failing to preserve the generative diversity of the base model. This paper introduces LLMdoctor, a novel framework for efficient test-time alignment that operates via a patient-doctor paradigm. It integrates token-level reward acquisition with token-level flow-guided preference optimization (TFPO) to steer a large, frozen patient LLM with a smaller, specialized doctor model. Unlike conventional methods that rely on trajectory-level rewards, LLMdoctor first extracts fine-grained, token-level preference signals from the patient model's behavioral variations. These signals then guide the training of the doctor model via TFPO, which establishes flow consistency across all subtrajectories, enabling precise token-by-token alignment while inherently preserving generation diversity. Extensive experiments demonstrate that LLMdoctor significantly outperforms existing test-time alignment methods and even surpasses the performance of full fine-tuning approaches like DPO.

</details>


### [187] [NSR-Boost: A Neuro-Symbolic Residual Boosting Framework for Industrial Legacy Models](https://arxiv.org/abs/2601.10457)
*Ziming Dai,Dabiao Ma,Jinle Tong,Mengyuan Han,Jian Yang,Haojun Fei*

Main category: cs.AI

TL;DR: NSR-Boost是一个神经符号残差提升框架，针对高并发生产环境，无需重新训练传统GBDT模型，通过修复预测失败的'困难区域'实现非侵入式升级。


<details>
  <summary>Details</summary>
Motivation: 解决工业环境中梯度提升决策树(GBDT)模型升级的高昂重训练成本和系统风险问题，特别是针对高并发生产环境中的遗留模型升级挑战。

Method: 1) 通过残差识别'困难区域'; 2) 使用大语言模型生成符号代码结构，通过贝叶斯优化微调参数来创建可解释专家; 3) 通过轻量级聚合器将专家与遗留模型输出动态集成。

Result: 在六个公共数据集和一个私有数据集上显著优于最先进的基线方法，在Qfin Holdings核心金融风控系统的实际部署中，线上数据表现出色性能提升，有效捕捉传统模型遗漏的长尾风险。

Conclusion: 为工业环境提供了一个安全、低成本的进化范式，能够在不大规模重训练的情况下有效提升模型性能。

Abstract: Although the Gradient Boosted Decision Trees (GBDTs) dominate industrial tabular applications, upgrading legacy models in high-concurrency production environments still faces prohibitive retraining costs and systemic risks. To address this problem, we present NSR-Boost, a neuro-symbolic residual boosting framework designed specifically for industrial scenarios. Its core advantage lies in being "non-intrusive". It treats the legacy model as a frozen model and performs targeted repairs on "hard regions" where predictions fail. The framework comprises three key stages: first, finding hard regions through residuals, then generating interpretable experts by generating symbolic code structures using Large Language Model (LLM) and fine-tuning parameters using Bayesian optimization, and finally dynamically integrating experts with legacy model output through a lightweight aggregator. We report on the successful deployment of NSR-Boost within the core financial risk control system at Qfin Holdings. This framework not only significantly outperforms state-of-the-art (SOTA) baselines across six public datasets and one private dataset, more importantly, shows excellent performance gains on real-world online data. In conclusion, it effectively captures long-tail risks missed by traditional models and offers a safe, low-cost evolutionary paradigm for industry.

</details>


### [188] [ChartComplete: A Taxonomy-based Inclusive Chart Dataset](https://arxiv.org/abs/2601.10462)
*Ahmad Mustapha,Charbel Toumieh,Mariette Awad*

Main category: cs.AI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: With advancements in deep learning (DL) and computer vision techniques, the field of chart understanding is evolving rapidly. In particular, multimodal large language models (MLLMs) are proving to be efficient and accurate in understanding charts. To accurately measure the performance of MLLMs, the research community has developed multiple datasets to serve as benchmarks. By examining these datasets, we found that they are all limited to a small set of chart types. To bridge this gap, we propose the ChartComplete dataset. The dataset is based on a chart taxonomy borrowed from the visualization community, and it covers thirty different chart types. The dataset is a collection of classified chart images and does not include a learning signal. We present the ChartComplete dataset as is to the community to build upon it.

</details>


### [189] [Panning for Gold: Expanding Domain-Specific Knowledge Graphs with General Knowledge](https://arxiv.org/abs/2601.10485)
*Runhao Zhao,Weixin Zeng,Wentao Zhang,Chong Chen,Zhengpin Li,Xiang Zhao,Lei Chen*

Main category: cs.AI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Domain-specific knowledge graphs (DKGs) often lack coverage compared to general knowledge graphs (GKGs). To address this, we introduce Domain-specific Knowledge Graph Fusion (DKGF), a novel task that enriches DKGs by integrating relevant facts from GKGs. DKGF faces two key challenges: high ambiguity in domain relevance and misalignment in knowledge granularity across graphs. We propose ExeFuse, a simple yet effective Fact-as-Program paradigm. It treats each GKG fact as a latent semantic program, maps abstract relations to granularity-aware operators, and verifies domain relevance via program executability on the target DKG. This unified probabilistic framework jointly resolves relevance and granularity issues. We construct two benchmarks, DKGF(W-I) and DKGF(Y-I), with 21 evaluation configurations. Extensive experiments validate the task's importance and our model's effectiveness, providing the first standardized testbed for DKGF.

</details>


### [190] [Diagnosing Generalization Failures in Fine-Tuned LLMs: A Cross-Architectural Study on Phishing Detection](https://arxiv.org/abs/2601.10524)
*Frank Bobe,Gregory D. Vetaw,Chase Pavlick,Darshan Bryner,Matthew Cook,Jose Salas-Vernis*

Main category: cs.AI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: The practice of fine-tuning Large Language Models (LLMs) has achieved state-of-the-art performance on specialized tasks, yet diagnosing why these models become brittle and fail to generalize remains a critical open problem. To address this, we introduce and apply a multi-layered diagnostic framework to a cross-architectural study. We fine-tune Llama 3.1 8B, Gemma 2 9B, and Mistral models on a high-stakes phishing detection task and use SHAP analysis and mechanistic interpretability to uncover the root causes of their generalization failures. Our investigation reveals three critical findings: (1) Generalization is driven by a powerful synergy between architecture and data diversity. The Gemma 2 9B model achieves state-of-the-art performance (>91\% F1), but only when trained on a stylistically diverse ``generalist'' dataset. (2) Generalization is highly architecture-dependent. We diagnose a specific failure mode in Llama 3.1 8B, which performs well on a narrow domain but cannot integrate diverse data, leading to a significant performance drop. (3) Some architectures are inherently more generalizable. The Mistral model proves to be a consistent and resilient performer across multiple training paradigms. By pinpointing the flawed heuristics responsible for these failures, our work provides a concrete methodology for diagnosing and understanding generalization failures, underscoring that reliable AI requires deep validation of the interplay between architecture, data, and training strategy.

</details>


### [191] [A Safety Report on GPT-5.2, Gemini 3 Pro, Qwen3-VL, Doubao 1.8, Grok 4.1 Fast, Nano Banana Pro, and Seedream 4.5](https://arxiv.org/abs/2601.10527)
*Xingjun Ma,Yixu Wang,Hengyuan Xu,Yutao Wu,Yifan Ding,Yunhan Zhao,Zilong Wang,Jiabin Hua,Ming Wen,Jianan Liu,Ranjie Duan,Yifeng Gao,Yingshui Tan,Yunhao Chen,Hui Xue,Xin Wang,Wei Cheng,Jingjing Chen,Zuxuan Wu,Bo Li,Yu-Gang Jiang*

Main category: cs.AI

TL;DR: 对GPT-5.2、Gemini 3 Pro等7个前沿模型在语言、视觉语言和图像生成三种模态下的综合安全评估，揭示安全性表现差异显著且多维度的复杂图景。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型和多模态大语言模型的快速发展在推理、感知和生成能力方面取得了显著进步，但这些进步是否带来了相应的安全性提升尚不明确。当前评估实践分散在单一模态或威胁模型中，缺乏综合性分析。

Method: 采用统一协议对7个前沿模型进行综合评估，包括基准评估、对抗性评估、多语言评估和合规性评估，涵盖语言、视觉语言和图像生成三种设置。

Result: GPT-5.2在所有评估中表现出持续强大且平衡的安全性能。其他模型在基准安全性、对抗对齐、多语言泛化和监管合规性之间存在显著权衡。语言和视觉语言模态在对抗性评估中表现出明显脆弱性。文生图模型在受监管的视觉风险类别中相对更强对齐，但在对抗性或语义模糊提示下仍显脆弱。

Conclusion: 前沿模型的安全性本质上是多维度的，受模态、语言和评估方案共同影响。需要进行标准化安全评估以准确评估现实世界风险，指导负责任模型的开发和部署。

Abstract: The rapid evolution of Large Language Models (LLMs) and Multimodal Large Language Models (MLLMs) has produced substantial gains in reasoning, perception, and generative capability across language and vision. However, whether these advances yield commensurate improvements in safety remains unclear, in part due to fragmented evaluation practices limited to single modalities or threat models. In this report, we present an integrated safety evaluation of 7 frontier models: GPT-5.2, Gemini 3 Pro, Qwen3-VL, Doubao 1.8, Grok 4.1 Fast, Nano Banana Pro, and Seedream 4.5. We evaluate each model across language, vision-language, and image generation settings using a unified protocol that integrates benchmark evaluation, adversarial evaluation, multilingual evaluation, and compliance evaluation. Aggregating our evaluations into safety leaderboards and model safety profiles across multiple evaluation modes reveals a sharply heterogeneous safety landscape. While GPT-5.2 demonstrates consistently strong and balanced safety performance across evaluations, other models exhibit pronounced trade-offs among benchmark safety, adversarial alignment, multilingual generalization, and regulatory compliance. Both language and vision-language modalities show significant vulnerability under adversarial evaluation, with all models degrading substantially despite strong results on standard benchmarks. Text-to-image models achieve relatively stronger alignment in regulated visual risk categories, yet remain brittle under adversarial or semantically ambiguous prompts. Overall, these results show that safety in frontier models is inherently multidimensional--shaped by modality, language, and evaluation scheme, underscoring the need for standardized safety evaluations to accurately assess real-world risk and guide responsible model development and deployment.

</details>


### [192] [Defending Large Language Models Against Jailbreak Attacks via In-Decoding Safety-Awareness Probing](https://arxiv.org/abs/2601.10543)
*Yinzhi Zhao,Ming Wang,Shi Feng,Xiaocui Yang,Daling Wang,Yifei Zhang*

Main category: cs.AI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Large language models (LLMs) have achieved impressive performance across natural language tasks and are increasingly deployed in real-world applications. Despite extensive safety alignment efforts, recent studies show that such alignment is often shallow and remains vulnerable to jailbreak attacks. Existing defense mechanisms, including decoding-based constraints and post-hoc content detectors, struggle against sophisticated jailbreaks, often intervening robust detection or excessively degrading model utility. In this work, we examine the decoding process of LLMs and make a key observation: even when successfully jailbroken, models internally exhibit latent safety-related signals during generation. However, these signals are overridden by the model's drive for fluent continuation, preventing timely self-correction or refusal. Building on this observation, we propose a simple yet effective approach that explicitly surfaces and leverages these latent safety signals for early detection of unsafe content during decoding. Experiments across diverse jailbreak attacks demonstrate that our approach significantly enhances safety, while maintaining low over-refusal rates on benign inputs and preserving response quality. Our results suggest that activating intrinsic safety-awareness during decoding offers a promising and complementary direction for defending against jailbreak attacks. Code is available at: https://github.com/zyz13590/SafeProbing.

</details>


### [193] [Generative AI collective behavior needs an interactionist paradigm](https://arxiv.org/abs/2601.10567)
*Laura Ferrarotti,Gian Maria Campedelli,Roberto Dessì,Andrea Baronchelli,Giovanni Iacca,Kathleen M. Carley,Alex Pentland,Joel Z. Leibo,James Evans,Bruno Lepri*

Main category: cs.AI

TL;DR: 本文主张理解基于大型语言模型（LLM）的智能体集体行为是一个关键的研究领域，对社会的风险与收益有重要影响。LLM的特点（预训练知识、隐含社会先验、上下文学习能力）要求采用互动主义范式来系统研究先验知识与嵌入价值观如何与社会环境相互作用，从而影响多智能体生成式AI系统的涌现现象。


<details>
  <summary>Details</summary>
Motivation: 研究表明，LLM驱动的多智能体系统在复杂社会环境中展现出独特的集体行为，这些行为既可能带来社会效益，也可能产生风险。目前缺少专门的理论和方法来系统理解LLM智能体集体行为的形成机制、影响因素和社会影响。

Method: 提出互动主义研究范式，结合替代性理论基础、方法论和分析工具。重点探讨四个关键方向：1）理论建设，2）方法创新，3）跨学科对话，4）系统部署框架。

Result: 论文建立了LLM集体行为研究的基本框架，明确了LLM智能体的三大特征（预训练知识、社会先验、上下文学习）对集体行为的影响机制，提出了系统的研究方向和方法路径。

Conclusion: LLM集体行为研究需要新的理论范式和方法工具，以理解先验知识与社交环境的互动如何塑造AI系统的涌现现象。跨学科合作、理论创新和方法发展对这一领域至关重要，将对LLM系统的安全部署和社会影响评估产生深远影响。

Abstract: In this article, we argue that understanding the collective behavior of agents based on large language models (LLMs) is an essential area of inquiry, with important implications in terms of risks and benefits, impacting us as a society at many levels. We claim that the distinctive nature of LLMs--namely, their initialization with extensive pre-trained knowledge and implicit social priors, together with their capability of adaptation through in-context learning--motivates the need for an interactionist paradigm consisting of alternative theoretical foundations, methodologies, and analytical tools, in order to systematically examine how prior knowledge and embedded values interact with social context to shape emergent phenomena in multi-agent generative AI systems. We propose and discuss four directions that we consider crucial for the development and deployment of LLM-based collectives, focusing on theory, methods, and trans-disciplinary dialogue.

</details>


### [194] [From Single to Multi-Agent Reasoning: Advancing GeneGPT for Genomics QA](https://arxiv.org/abs/2601.10581)
*Kimia Abedini,Farzad Shami,Gianmaria Silvello*

Main category: cs.AI

TL;DR: GenomAgent：一个用于基因组问答的多智能体框架，通过协调专门化智能体来处理复杂查询，在GeneTuring基准测试中平均比GeneGPT提升12%


<details>
  <summary>Details</summary>
Motivation: 基因组信息理解对于生物医学研究至关重要，但从复杂分布式数据库中提取数据仍然具有挑战性。大型语言模型在基因组问答方面具有潜力，但受限于对领域特定数据库的访问限制。当前最先进的GeneGPT系统虽然通过专用API调用增强了LLM能力，但仍受到API依赖僵化和适应性有限的约束

Method: 开发GenomAgent，这是一个多智能体框架，能够高效协调专门化智能体处理复杂基因组查询。该框架基于对GeneGPT的复刻和改进，采用灵活的架构设计

Result: 在GeneTuring基准测试的9个任务上，GenomAgent平均性能比GeneGPT高出12%。其灵活的架构不仅适用于基因组学，还可扩展到需要专家知识提取的各种科学领域

Conclusion: GenomAgent通过创新的多智能体框架，成功解决了基因组问答中访问领域特定数据库的挑战，显著超越了现有最先进方法，展示出在多个科学领域的广泛应用潜力

Abstract: Comprehending genomic information is essential for biomedical research, yet extracting data from complex distributed databases remains challenging. Large language models (LLMs) offer potential for genomic Question Answering (QA) but face limitations due to restricted access to domain-specific databases. GeneGPT is the current state-of-the-art system that enhances LLMs by utilizing specialized API calls, though it is constrained by rigid API dependencies and limited adaptability. We replicate GeneGPT and propose GenomAgent, a multi-agent framework that efficiently coordinates specialized agents for complex genomics queries. Evaluated on nine tasks from the GeneTuring benchmark, GenomAgent outperforms GeneGPT by 12% on average, and its flexible architecture extends beyond genomics to various scientific domains needing expert knowledge extraction.

</details>


### [195] [Multi-Property Synthesis](https://arxiv.org/abs/2601.10651)
*Christoph Weinhuber,Yannik Schnitzer,Alessandro Abate,David Parker,Giuseppe De Giacomo,Moshe Y. Vardi*

Main category: cs.AI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: We study LTLf synthesis with multiple properties, where satisfying all properties may be impossible. Instead of enumerating subsets of properties, we compute in one fixed-point computation the relation between product-game states and the goal sets that are realizable from them, and we synthesize strategies achieving maximal realizable sets. We develop a fully symbolic algorithm that introduces Boolean goal variables and exploits monotonicity to represent exponentially many goal combinations compactly. Our approach substantially outperforms enumeration-based baselines, with speedups of up to two orders of magnitude.

</details>


### [196] [Are Your Reasoning Models Reasoning or Guessing? A Mechanistic Analysis of Hierarchical Reasoning Models](https://arxiv.org/abs/2601.10679)
*Zirui Ren,Ziming Liu*

Main category: cs.AI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Hierarchical reasoning model (HRM) achieves extraordinary performance on various reasoning tasks, significantly outperforming large language model-based reasoners. To understand the strengths and potential failure modes of HRM, we conduct a mechanistic study on its reasoning patterns and find three surprising facts: (a) Failure of extremely simple puzzles, e.g., HRM can fail on a puzzle with only one unknown cell. We attribute this failure to the violation of the fixed point property, a fundamental assumption of HRM. (b) "Grokking" dynamics in reasoning steps, i.e., the answer is not improved uniformly, but instead there is a critical reasoning step that suddenly makes the answer correct; (c) Existence of multiple fixed points. HRM "guesses" the first fixed point, which could be incorrect, and gets trapped there for a while or forever. All facts imply that HRM appears to be "guessing" instead of "reasoning". Leveraging this "guessing" picture, we propose three strategies to scale HRM's guesses: data augmentation (scaling the quality of guesses), input perturbation (scaling the number of guesses by leveraging inference randomness), and model bootstrapping (scaling the number of guesses by leveraging training randomness). On the practical side, by combining all methods, we develop Augmented HRM, boosting accuracy on Sudoku-Extreme from 54.5% to 96.9%. On the scientific side, our analysis provides new insights into how reasoning models "reason".

</details>


### [197] [Structure and Diversity Aware Context Bubble Construction for Enterprise Retrieval Augmented Systems](https://arxiv.org/abs/2601.10681)
*Amir Khurshid,Abhishek Sehgal*

Main category: cs.AI

TL;DR: 提出了一种基于文档结构约束的上下文构建框架，与传统top-k检索相比，能减少碎片化、冗余内容，改善覆盖率并在有限上下文窗口中提升答案质量和引用准确性。


<details>
  <summary>Details</summary>
Motivation: 传统RAG方法通过top-k检索构造LLM上下文时存在信息图碎片化、过度检索、内容冗余、无法涵盖查询的二级和三级层面等问题。

Method: 设计了一种结构感知的多样性约束上下文泡沫构建框架：识别高相关性锚点跨度，通过平衡查询相关性、边际覆盖率和冗余惩罚的约束选择构建上下文泡沫，利用任务条件化结构先验引导检索，并在严格token预算内组织多粒度跨度（如章节、行）。

Result: 在企业文档上的实验表明，该方法显著减少了冗余上下文，更好地覆盖了次要层面，在有限上下文窗口中获得了更好的答案质量和引用准确性。消融实验显示结构先验和多样性约束选择都是必要的。

Conclusion: 提出的上下文泡沫框架通过结构约束和多样性控制，能够更高效地构建紧凑、信息丰富且可审计的上下文集，优于传统的top-k检索方法。

Abstract: Large language model (LLM) contexts are typically constructed using retrieval-augmented generation (RAG), which involves ranking and selecting the top-k passages. The approach causes fragmentation in information graphs in document structures, over-retrieval, and duplication of content alongside insufficient query context, including 2nd and 3rd order facets. In this paper, a structure-informed and diversity-constrained context bubble construction framework is proposed that assembles coherent, citable bundles of spans under a strict token budget. The method preserves and exploits inherent document structure by organising multi-granular spans (e.g., sections and rows) and using task-conditioned structural priors to guide retrieval. Starting from high-relevance anchor spans, a context bubble is constructed through constrained selection that balances query relevance, marginal coverage, and redundancy penalties. It will explicitly constrain diversity and budget, producing compact and informative context sets, unlike top-k retrieval. Moreover, a full retrieval is emitted that traces the scoring and selection choices of the records, thus providing auditability and deterministic tuning. Experiments on enterprise documents demonstrate the efficiency of context bubble as it significantly reduces redundant context, is better able to cover secondary facets and has a better answer quality and citation faithfulness within a limited context window. Ablation studies demonstrate that both structural priors as well as diversity constraint selection are necessary; removing either component results in a decline in coverage and an increase in redundant or incomplete context.

</details>


### [198] [The Impact of Generative AI on Architectural Conceptual Design: Performance, Creative Self-Efficacy and Cognitive Load](https://arxiv.org/abs/2601.10696)
*Han Jiang,Yao Xiao,Rachel Hurley,Shichao Liu*

Main category: cs.AI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Our study examines how generative AI (GenAI) influences performance, creative self-efficacy, and cognitive load in architectural conceptual design tasks. Thirty-six student participants from Architectural Engineering and other disciplines completed a two-phase architectural design task, first independently and then with external tools (GenAI-assisted condition and control condition using an online repository of existing architectural projects). Design outcomes were evaluated by expert raters, while self-efficacy and cognitive load were self-reported after each phase. Difference-in-differences analyses revealed no overall performance advantage of GenAI across participants; however, subgroup analyses showed that GenAI significantly improved design performance for novice designers. In contrast, general creative self-efficacy declined for students using GenAI. Cognitive load did not differ significantly between conditions, though prompt usage patterns showed that iterative idea generation and visual feedback prompts were linked to greater reductions in cognitive load. These findings suggest that GenAI effectiveness depends on users' prior expertise and interaction strategies through prompting.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [199] [Cell Behavior Video Classification Challenge, a benchmark for computer vision methods in time-lapse microscopy](https://arxiv.org/abs/2601.10250)
*Raffaella Fiamma Cabini,Deborah Barkauskas,Guangyu Chen,Zhi-Qi Cheng,David E Cicchetti,Judith Drazba,Rodrigo Fernandez-Gonzalez,Raymond Hawkins,Yujia Hu,Jyoti Kini,Charles LeWarne,Xufeng Lin,Sai Preethi Nakkina,John W Peterson,Koert Schreurs,Ayushi Singh,Kumaran Bala Kandan Viswanathan,Inge MN Wortel,Sanjian Zhang,Rolf Krause,Santiago Fernandez Gonzalez,Diego Ulisse Pizzagalli*

Main category: eess.IV

TL;DR: 该研究组织细胞行为视频分类挑战赛(CBVCC)，比较了35种基于跟踪特征、端到端深度学习或特征融合的方法，旨在推动细胞动态研究的计算机视觉技术发展。


<details>
  <summary>Details</summary>
Motivation: 显微镜视频中对复杂细胞行为的分类对于理解和量化生物过程的动态变化至关重要，但这是计算机视觉的前沿领域，需要能够有效建模无刚性边界物体的形状和运动、从整个图像序列提取层次化时空特征并考虑视野中多个对象的方法。

Method: 组织细胞行为视频分类挑战赛(CBVCC)，对35种方法进行基准测试，包括：1)基于跟踪提取特征的分类；2)端到端深度学习架构直接从整个视频序列学习时空特征而无需显式细胞跟踪；3)跟踪特征与图像特征的集成方法。

Result: 研究讨论了参赛者取得的结果，比较了每种方法的潜力和局限性，为促进研究细胞动态的计算机视觉方法发展提供了基础。

Conclusion: 该挑战赛为解决细胞行为视频分类问题提供了系统性的方法评估和比较，为未来开发更有效的计算机视觉方法用于细胞动态研究奠定了重要基础。

Abstract: The classification of microscopy videos capturing complex cellular behaviors is crucial for understanding and quantifying the dynamics of biological processes over time. However, it remains a frontier in computer vision, requiring approaches that effectively model the shape and motion of objects without rigid boundaries, extract hierarchical spatiotemporal features from entire image sequences rather than static frames, and account for multiple objects within the field of view.
  To this end, we organized the Cell Behavior Video Classification Challenge (CBVCC), benchmarking 35 methods based on three approaches: classification of tracking-derived features, end-to-end deep learning architectures to directly learn spatiotemporal features from the entire video sequence without explicit cell tracking, or ensembling tracking-derived with image-derived features.
  We discuss the results achieved by the participants and compare the potential and limitations of each approach, serving as a basis to foster the development of computer vision methods for studying cellular dynamics.

</details>


### [200] [An effective interactive brain cytoarchitectonic parcellation framework using pretrained foundation model](https://arxiv.org/abs/2601.10412)
*Shiqi Zhang,Fang Xu,Pengcheng Zhou*

Main category: eess.IV

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Cytoarchitectonic mapping provides anatomically grounded parcellations of brain structure and forms a foundation for integrative, multi-modal neuroscience analyses. These parcellations are defined based on the shape, density, and spatial arrangement of neuronal cell bodies observed in histological imaging. Recent works have demonstrated the potential of using deep learning models toward fully automatic segmentation of cytoarchitectonic areas in large-scale datasets, but performance is mainly constrained by the scarcity of training labels and the variability of staining and imaging conditions. To address these challenges, we propose an interactive cytoarchitectonic parcellation framework that leverages the strong transferability of the DINOv3 vision transformer. Our framework combines (i) multi-layer DINOv3 feature fusion, (ii) a lightweight segmentation decoder, and (iii) real-time user-guided training from sparse scribbles. This design enables rapid human-in-the-loop refinement while maintaining high segmentation accuracy. Compared with training an nnU-Net from scratch, transfer learning with DINOv3 yields markedly improved performance. We also show that features extracted by DINOv3 exhibit clear anatomical correspondence and demonstrate the method's practical utility for brain region segmentation using sparse labels. These results highlight the potential of foundation-model-driven interactive segmentation for scalable and efficient cytoarchitectonic mapping.

</details>
