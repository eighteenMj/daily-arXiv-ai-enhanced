{"id": "2510.24190", "categories": ["cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2510.24190", "abs": "https://arxiv.org/abs/2510.24190", "authors": ["Hong Niu", "Jiancheng An", "Chau Yuen"], "title": "Flexible Intelligent Layered Metasurfaces for Downlink Multi-user MISO Communications", "comment": "13 pages", "summary": "Stacked intelligent metasurfaces (SIMs) have recently gained attention as a\nparadigm for wave-domain signal processing with reduced reliance on costly\nradio-frequency (RF) chains. However, conventional SIMs rely on uniform\ninter-layer spacing and require deep stacking to ensure processing capability,\nresulting in severe power attenuation in practice. To address this issue, we\npropose a flexible intelligent layered metasurface (FILM) architecture\nconsisting of two shape-controllable flexible metasurface layers. By replacing\nrigid metasurfaces with flexible ones in both layers, the transmission\ncoefficient matrix can be dynamically adjusted, significantly decreasing the\nnumber of required layers while maintaining signal processing performance.\nFirstly, we develop a two-layer FILM-assisted multi-user multiple-input\nsingle-output (MU-MISO) system, wherein we formulate a channel fitting problem\naimed at reducing the difference between the FILM-induced and target channels.\nThen, we solve this non-convex problem by employing an alternating optimization\n(AO) method, featuring closed-form phase shift updates and a gradient\ndescent-based shape optimization. Furthermore, we analyze the upper bound on\nsum-rate and the complexity of computation to provide insights into design\ntrade-offs. Finally, simulation results demonstrated that the proposed\ntransmissive FILM architecture achieves over 200\\% improvement in sum-rate and\nmore than 7 dB bit-error rate (BER) gain compared to the conventional\nseven-layer SIMs.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u67d4\u6027\u667a\u80fd\u5c42\u72b6\u8d85\u8868\u9762\uff08FILM\uff09\u67b6\u6784\uff0c\u901a\u8fc7\u91c7\u7528\u4e24\u4e2a\u5f62\u72b6\u53ef\u63a7\u5236\u7684\u67d4\u6027\u8d85\u8868\u9762\u5c42\u6765\u89e3\u51b3\u4f20\u7edf\u667a\u80fd\u8d85\u8868\u9762\uff08SIM\uff09\u6240\u9700\u5c42\u6570\u591a\u4e14\u4f20\u8f93\u7cfb\u6570\u77e9\u9635\u96be\u4ee5\u52a8\u6001\u8c03\u6574\u7684\u95ee\u9898\u3002FILM\u80fd\u51cf\u5c11\u6240\u9700\u7684\u5c42\u6570\uff0c\u540c\u65f6\u4fdd\u6301\u4fe1\u53f7\u5904\u7406\u6027\u80fd\u3002\u5728\u4e24\u5c42FILM\u8f85\u52a9\u7684\u591a\u7528\u6237\u591a\u8f93\u5165\u5355\u8f93\u51fa\uff08MU-MISO\uff09\u7cfb\u7edf\u4e2d\uff0c\u901a\u8fc7\u4f7f\u7528\u4ea4\u66ff\u4f18\u5316\uff08AO\uff09\u65b9\u6cd5\u89e3\u51b3\u4e86\u4fe1\u9053\u62df\u5408\u95ee\u9898\uff0c\u5b9e\u9a8c\u8bc1\u660eFILM\u67b6\u6784\u80fd\u591f\u663e\u8457\u63d0\u5347\u548c\u6539\u5584\u6027\u80fd\u6307\u6807\u3002\n", "motivation": "\u4f20\u7edf\u667a\u80fd\u8d85\u8868\u9762(SIM)\u9700\u8981\u8f83\u591a\u7684\u5c42\u6570\u6765\u4fdd\u8bc1\u4fe1\u53f7\u5904\u7406\u80fd\u529b\uff0c\u5bfc\u81f4\u5b9e\u8df5\u4e2d\u7684\u529f\u7387\u8870\u51cf\u4e25\u91cd\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u7814\u7a76\u56e2\u961f\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u67d4\u6027\u667a\u80fd\u5c42\u72b6\u8d85\u8868\u9762\uff08FILM\uff09\u67b6\u6784\uff0c\u65e8\u5728\u51cf\u5c11\u6240\u9700\u7684\u5c42\u6570\uff0c\u540c\u65f6\u4fdd\u6301\u4fe1\u53f7\u5904\u7406\u6027\u80fd\u3002\n", "method": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u7531\u4e24\u4e2a\u5f62\u72b6\u53ef\u63a7\u5236\u7684\u67d4\u6027\u8d85\u8868\u9762\u5c42\u7ec4\u6210\u7684FILM\u8f85\u52a9\u7684\u591a\u7528\u6237\u591a\u8f93\u5165\u5355\u8f93\u51fa\uff08MU-MISO\uff09\u7cfb\u7edf\uff0c\u5e76\u901a\u8fc7\u6c42\u89e3\u975e\u51f8\u95ee\u9898\u6765\u51cf\u5c11\u7531FILM\u5f15\u8d77\u7684\u4fe1\u9053\u4e0e\u76ee\u6807\u4fe1\u9053\u4e4b\u95f4\u7684\u5dee\u5f02\uff0c\u4f7f\u7528\u4e86\u4ea4\u66ff\u4f18\u5316\uff08AO\uff09\u65b9\u6cd5\u8fdb\u884c\u89e3\u51b3\u3002\n", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cFILM\u67b6\u6784\u5728\u603b\u901f\u7387\u4e0a\u6709\u4e86\u8d85\u8fc7200%\u7684\u63d0\u9ad8\uff0c\u5e76\u5728\u8bef\u7801\u7387\uff08BER\uff09\u4e0a\u76f8\u5bf9\u4e8e\u4f20\u7edf\u7684\u4e03\u5c42SIM\u6709\u8d85\u8fc77 dB\u7684\u589e\u5f3a\u3002\n", "conclusion": "\u7814\u7a76\u5c55\u793a\u4e86\u4e00\u79cd\u65b0\u7684FILM\u67b6\u6784\uff0c\u6210\u529f\u964d\u4f4e\u4e86\u667a\u80fd\u8d85\u8868\u9762\u6240\u9700\u7684\u5c42\u6570\uff0c\u5e76\u901a\u8fc7\u89e3\u51b3\u975e\u51f8\u4fe1\u9053\u62df\u5408\u95ee\u9898\u548c\u4f7f\u7528\u8d1d\u5c14\u6cd5\u65af\u7279\u4f18\u5316\u65b9\u6cd5\u663e\u8457\u6539\u5584\u4e86\u4fe1\u53f7\u5904\u7406\u6027\u80fd\u3002\n"}}
{"id": "2510.24215", "categories": ["cs.IT", "cs.LG", "eess.SP", "math.IT"], "pdf": "https://arxiv.org/pdf/2510.24215", "abs": "https://arxiv.org/abs/2510.24215", "authors": ["Vishal Halder", "Alexandre Reiffers-Masson", "Abdeldjalil A\u00efssa-El-Bey", "Gugan Thoppe"], "title": "What Can Be Recovered Under Sparse Adversarial Corruption? Assumption-Free Theory for Linear Measurements", "comment": null, "summary": "Let \\(\\bm{A} \\in \\mathbb{R}^{m \\times n}\\) be an arbitrary, known matrix and\n\\(\\bm{e}\\) a \\(q\\)-sparse adversarial vector. Given \\(\\bm{y} = \\bm{A} x^* +\n\\bm{e}\\) and \\(q\\), we seek the smallest set containing \\(x^*\\)-hence the one\nconveying maximal information about \\(x^*\\)-that is uniformly recoverable from\n\\(\\bm{y}\\) without knowing \\(\\bm{e}\\). While exact recovery of \\(x^*\\) via\nstrong (and often impractical) structural assumptions on \\(\\bm{A}\\) or \\(x^*\\)\n(for example, restricted isometry, sparsity) is well studied, recoverability\nfor arbitrary \\(\\bm{A}\\) and \\(x^*\\) remains open. Our main result shows that\nthe best that one can hope to recover is \\(x^* + \\ker(\\bm{U})\\), where\n\\(\\bm{U}\\) is the unique projection matrix onto the intersection of rowspaces\nof all possible submatrices of \\(\\bm{A}\\) obtained by deleting \\(2q\\) rows.\nMoreover, we prove that every \\(x\\) that minimizes the \\(\\ell\\_0\\)-norm of\n\\(\\bm{y} - \\bm{A} x\\) lies in \\(x^* + \\ker(\\bm{U})\\), which then gives a\nconstructive approach to recover this set.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63a2\u8ba8\u4e86\u5728\u5b58\u5728\u672a\u77e5\u7a00\u758f\u6270\u52a8\u7684\u60c5\u51b5\u4e0b\uff0c\u4ece\u5df2\u77e5\u77e9\u9635A\u53ca\u5176\u7ebf\u6027\u53d8\u6362\u7684\u7ed3\u679cy\u4e2d\u6062\u590d\u539f\u5411\u91cfx*\u7684\u95ee\u9898\u3002\u4e3b\u8981\u7ed3\u679c\u8868\u660e\uff0c\u6700\u4f18\u60c5\u51b5\u4e0b\u53ef\u4ee5\u6062\u590d\u7684\u662fx*\u52a0\u4e0a\u67d0\u4e2a\u7279\u5b9a\u7684\u96f6\u7a7a\u95f4\u5411\u91cf\u3002\u5e76\u63d0\u4f9b\u4e86\u4e00\u79cd\u901a\u8fc7\u6700\u5c0f\u5316l0\u8303\u6570\u7684\u6784\u9020\u6027\u65b9\u6cd5\u6765\u6062\u590d\u8be5\u96c6\u5408\u7684\u9014\u5f84\u3002", "motivation": "\u7814\u7a76\u5728\u672a\u77e5\u7a00\u758f\u6270\u52a8\u5b58\u5728\u7684\u80cc\u666f\u4e0b\uff0c\u5982\u4f55\u4ece\u89c2\u5bdf\u503c\bm{y}\u4e2d\u83b7\u5f97\u5c3d\u53ef\u80fd\u591a\u7684\u5173\u4e8e\u539f\u5411\u91cf\bm{x*}\u7684\u4fe1\u606f", "method": "\u901a\u8fc7\u5b9a\u4e49\u4e00\u4e2a\u7279\u6b8a\u7684\u6295\u5f71\u77e9\u9635\bm{U}\uff0c\u8be5\u77e9\u9635\u662f\u5728\u4efb\u610f\bm{A}\u548c\bm{x*}\u7684\u60c5\u51b5\u4e0b\u552f\u4e00\u8868\u793a\u4e86\u5220\u96642q\u884c\u540e\u7684\u6240\u6709\u5b50\u77e9\u9635\u884c\u7a7a\u95f4\u7684\u4ea4\u96c6\u7684\u6295\u5f71\u77e9\u9635\u3002\u8fdb\u800c\u8bc1\u660e\u6bcf\u4e2a\u6700\u5c0f\u5316\bm{y} - \bm{A} x\u7684\bm{x}\u7684\bm{l0}\u8303\u6570\u7684\u89e3\u90fd\u5728x*+\bm{ker(U)}\u4e2d\uff0c\u63d0\u4f9b\u4e86\u4e00\u79cd\u6784\u9020\u6027\u7684\u6062\u590d\u65b9\u6cd5\u3002", "result": "\u8868\u660e\u5728\u672a\u77e5\u7a00\u758f\u6270\u52a8\u7684\u60c5\u51b5\u4e0b\uff0c\u6700\u4f73\u6062\u590d\u6548\u679c\u662f\u80fd\u6062\u590d\u5230x*\u52a0\u4e0a\u67d0\u4e2a\u7279\u5b9a\u96f6\u7a7a\u95f4\u5411\u91cf\u7684\u96c6\u5408\u3002\u627e\u5230\u8fd9\u4e2a\u96c6\u5408\u7684\u4e00\u4e2a\u7b97\u6cd5\u662f\u901a\u8fc7\u6700\u5c0f\u5316\bm{l0}\u8303\u6570\u5b9e\u73b0\u7684\u3002", "conclusion": "\u5728\u7ed9\u5b9a\bm{y}\u548c\u7a00\u758f\u5ea6\bm{q}\u800c\u672a\u77e5\bm{e}\u7684\u60c5\u51b5\u4e0b\uff0c\u6700\u4f18\u7684\u89e3\u5f62\u5f0f\u662fx*\u52a0\u4e0a\u901a\u8fc7\u6700\u5c0f\u5316l0\u8303\u6570\u6240\u786e\u5b9a\u7684\u6838\u7a7a\u95f4\u3002\n\u8fd9\u4e00\u53d1\u73b0\u4e3a\u91cd\u5efa\u672a\u77e5\u5411\u91cf\u7684\u53ef\u80fd\u8303\u56f4\u63d0\u4f9b\u4e86\u4e00\u4e2a\u754c\u9650\uff0c\u4e14\u7ed9\u51fa\u4e86\u8be5\u8303\u56f4\u7684\u6784\u9020\u65b9\u6cd5\u3002"}}
{"id": "2510.24480", "categories": ["cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2510.24480", "abs": "https://arxiv.org/abs/2510.24480", "authors": ["Qing Xue", "Yun Lan", "Jiajia Guo", "Qianbin Chen", "Shaodan Ma"], "title": "Joint Active and Passive Beamforming with Sensing-Assisted Discrete Phase Shifts for Dual-RIS ISAC Systems", "comment": null, "summary": "Targeting the requirements of 6G, this paper investigates a semi-passive\ndual-reconfigurable intelligent surface (RIS)-assisted integrated sensing and\ncommunication (ISAC) system, tackling the max-min user\nsignal-to-interference-plus-noise ratio (SINR) problem via joint active and\npassive beamforming to enhance system performance and ensure user fairness.\nAddressing this challenge, we first utilize dual RISs for user angle estimation\nto simplify the solution process of the formulated problem, an efficient\nalternating optimization algorithm is then developed. Specifically,\nsemi-definite relaxation and the bisection method are employed to solve the\ntransmit beamforming optimization subproblem. For the RIS discrete phase\nshifts, a sensing-assisted approach is adopted to constrain the optimization\nsearch space, with two distinct low-complexity search strategies introduced for\ndifferent RIS sizes. Numerical simulation results demonstrate that the proposed\nalgorithm achieves performance close to the ideal continuous phase shift\nbenchmark, outperforms conventional discrete phase shift optimization\nalgorithms, and exhibits a significant improvement over single-RIS systems.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u534a\u88ab\u52a8\u53cc\u53ef\u91cd\u6784\u667a\u80fd\u8868\u9762\uff08RIS\uff09\u8f85\u52a9\u7684\u96c6\u6210\u4f20\u611f\u548c\u901a\u4fe1\uff08ISAC\uff09\u7cfb\u7edf\u6765\u89e3\u51b36G\u9700\u6c42\u4e0b\u7684\u6700\u5927\u6700\u5c0f\u7528\u6237\u4fe1\u53f7\u4e0e\u5e72\u6270\u52a0\u566a\u58f0\u6bd4\uff08SINR\uff09\u95ee\u9898\uff0c\u901a\u8fc7\u8054\u5408\u6709\u6e90\u548c\u65e0\u6e90\u6ce2\u675f\u6210\u5f62\u63d0\u9ad8\u7cfb\u7edf\u6027\u80fd\u5e76\u786e\u4fdd\u7528\u6237\u516c\u5e73\u6027\u3002\u63d0\u51fa\u4e86\u4e00\u79cd\u4ea4\u66ff\u4f18\u5316\u7b97\u6cd5\uff0c\u5e76\u901a\u8fc7\u4eff\u771f\u9a8c\u8bc1\u4e86\u5176\u6027\u80fd\u4f18\u8d8a\u6027\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b36G\u7cfb\u7edf\u4e2d\u7684\u6700\u5927\u6700\u5c0f\u7528\u6237SINR\u95ee\u9898\uff0c\u63d0\u9ad8\u7cfb\u7edf\u6027\u80fd\u5e76\u4fdd\u8bc1\u7528\u6237\u516c\u5e73\u6027\uff0c\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u534a\u88ab\u52a8\u53cc\u53ef\u91cd\u6784\u667a\u80fd\u8868\u9762\u8f85\u52a9\u7684\u96c6\u6210\u4f20\u611f\u548c\u901a\u4fe1\u7cfb\u7edf\u65b9\u6848\u3002\u901a\u8fc7\u8054\u5408\u6709\u6e90\u548c\u65e0\u6e90\u6ce2\u675f\u6210\u5f62\u6280\u672f\uff0c\u63d0\u9ad8\u7cfb\u7edf\u6574\u4f53\u6548\u80fd\u3002\u540c\u65f6\uff0c\u5229\u7528\u53ccRIS\u7b80\u5316\u4e86\u89e3\u51b3\u8be5\u95ee\u9898\u7684\u8fc7\u7a0b\uff0c\u5e76\u901a\u8fc7\u6709\u6548\u7684\u4ea4\u66ff\u4f18\u5316\u7b97\u6cd5\u89e3\u51b3\u76f8\u5173\u4f18\u5316\u95ee\u9898\u3002", "method": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u5957\u8054\u5408\u5229\u7528\u53ccRIS\u8fdb\u884c\u7528\u6237\u89d2\u5ea6\u4f30\u8ba1\u548c\u4f18\u5316\u6ce2\u675f\u6210\u5f62\u7684\u7b56\u7565\u3002\u7279\u522b\u5730\uff0c\u91c7\u7528\u4e86\u534a\u5b9a\u4e49\u677e\u5f1b\u548c\u4e8c\u5206\u6cd5\u6765\u89e3\u51b3\u53d1\u5c04\u6ce2\u675f\u6210\u5f62\u4f18\u5316\u6b21\u95ee\u9898\u3002\u5bf9\u4e8eRIS\u7684\u79bb\u6563\u76f8\u79fb\uff0c\u5e94\u7528\u4e86\u57fa\u4e8e\u4f20\u611f\u7684\u6280\u672f\u6765\u9650\u5236\u4f18\u5316\u641c\u7d22\u7a7a\u95f4\uff0c\u5e76\u5bf9\u4e8e\u4e0d\u540c\u5927\u5c0f\u7684RIS\u63d0\u51fa\u4e86\u4e24\u79cd\u4e0d\u540c\u7684\u4f4e\u590d\u6742\u5ea6\u641c\u7d22\u7b56\u7565\u3002", "result": "\u901a\u8fc7\u6570\u503c\u6a21\u62df\uff0c\u8bba\u6587\u5c55\u793a\u4e86\u6240\u63d0\u51fa\u7b97\u6cd5\u5728\u6027\u80fd\u4e0a\u63a5\u8fd1\u7406\u60f3\u7684\u8fde\u7eed\u76f8\u79fb\u57fa\u51c6\uff0c\u4f18\u4e8e\u4f20\u7edf\u79bb\u6563\u76f8\u79fb\u4f18\u5316\u7b97\u6cd5\uff0c\u5e76\u4e14\u5728\u5355RIS\u7cfb\u7edf\u4e0a\u8868\u73b0\u51fa\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u6587\u4e2d\u63d0\u51fa\u7684\u7b97\u6cd5\u80fd\u591f\u6709\u6548\u89e3\u51b3\u534a\u88ab\u52a8\u53ccRIS\u8f85\u52a9ISAC\u7cfb\u7edf\u4e2d\u7684\u6700\u5927\u6700\u5c0f\u7528\u6237SINR\u95ee\u9898\uff0c\u5176\u6027\u80fd\u4f18\u8d8a\uff0c\u5e76\u80fd\u63d0\u5347\u6574\u4f53\u7cfb\u7edf\u6548\u7387\uff0c\u4e3a\u8bbe\u8ba1\u9ad8\u6548\u3001\u516c\u5e73\u76846G\u901a\u4fe1\u7cfb\u7edf\u63d0\u4f9b\u4e86\u65b0\u7684\u601d\u8def\u548c\u6280\u672f\u652f\u6301\u3002"}}
{"id": "2510.23775", "categories": ["cs.CV", "cs.AI", "eess.IV"], "pdf": "https://arxiv.org/pdf/2510.23775", "abs": "https://arxiv.org/abs/2510.23775", "authors": ["Aryan Mathur", "Asaduddin Ahmed", "Pushti Amit Vasoya", "Simeon Kandan Sonar", "Yasir Z", "Madesh Kuppusamy"], "title": "Explainable Detection of AI-Generated Images with Artifact Localization Using Faster-Than-Lies and Vision-Language Models for Edge Devices", "comment": null, "summary": "The increasing realism of AI-generated imagery poses challenges for verifying\nvisual authenticity. We present an explainable image authenticity detection\nsystem that combines a lightweight convolutional classifier\n(\"Faster-Than-Lies\") with a Vision-Language Model (Qwen2-VL-7B) to classify,\nlocalize, and explain artifacts in 32x32 images. Our model achieves 96.5%\naccuracy on the extended CiFAKE dataset augmented with adversarial\nperturbations and maintains an inference time of 175ms on 8-core CPUs, enabling\ndeployment on local or edge devices. Using autoencoder-based reconstruction\nerror maps, we generate artifact localization heatmaps, which enhance\ninterpretability for both humans and the VLM. We further categorize 70 visual\nartifact types into eight semantic groups and demonstrate explainable text\ngeneration for each detected anomaly. This work highlights the feasibility of\ncombining visual and linguistic reasoning for interpretable authenticity\ndetection in low-resolution imagery and outlines potential cross-domain\napplications in forensics, industrial inspection, and social media moderation.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u53ef\u89e3\u91ca\u7684\u56fe\u50cf\u771f\u5b9e\u6027\u68c0\u6d4b\u7cfb\u7edf\uff0c\u7ed3\u5408\u4e86\u8f7b\u91cf\u7ea7\u5377\u79ef\u5206\u7c7b\u5668\u548c\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\uff0c\u53ef\u4ee5\u5206\u7c7b\u3001\u5b9a\u4f4d\u5e76\u89e3\u91ca32x32\u56fe\u50cf\u4e2d\u7684\u4f2a\u5f71\uff0c\u51c6\u786e\u7387\u8fbe\u523096.5%\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u8f83\u77ed\u7684\u63a8\u7406\u65f6\u95f4\uff0c\u63d0\u9ad8\u4e86\u89e3\u91ca\u6027\u548c\u5b9e\u7528\u6027\uff0c\u9002\u7528\u4e8e\u591a\u4e2a\u9886\u57df\u5982\u6cd5\u533b\u3001\u5de5\u4e1a\u68c0\u67e5\u548c\u793e\u4f1a\u5a92\u4f53\u76d1\u63a7\u7b49\u3002", "motivation": "\u9274\u4e8eAI\u751f\u6210\u56fe\u50cf\u7684\u903c\u771f\u5ea6\u65e5\u76ca\u63d0\u9ad8\uff0c\u4f7f\u5f97\u9a8c\u8bc1\u89c6\u89c9\u771f\u5b9e\u6027\u53d8\u5f97\u66f4\u5177\u6311\u6218\u6027\uff0c\u672c\u8bba\u6587\u65e8\u5728\u5f00\u53d1\u4e00\u79cd\u80fd\u591f\u5728\u4f4e\u5206\u8fa8\u7387\u56fe\u50cf\u4e2d\u8fdb\u884c\u53ef\u7406\u89e3\u7684\u771f\u5b9e\u6027\u68c0\u6d4b\u7684\u65b9\u6cd5\u3002\u901a\u8fc7\u7ed3\u5408\u89c6\u89c9\u548c\u8bed\u8a00\u9886\u57df\u7684\u6a21\u578b\uff0c\u63d0\u9ad8\u4e86\u68c0\u6d4b\u8fc7\u7a0b\u7684\u53ef\u89e3\u91ca\u6027\uff0c\u4f7f\u5176\u4e0d\u4ec5\u6709\u6548\uff0c\u800c\u4e14\u5177\u5907\u6df1\u523b\u7684\u89e3\u91ca\u80fd\u529b\u3002\u8fd9\u79cd\u65b9\u6cd5\u4e0d\u4ec5\u53ef\u4ee5\u63d0\u5347\u68c0\u6d4b\u7cfb\u7edf\u7684\u7cbe\u5ea6\uff0c\u8fd8\u53ef\u4ee5\u589e\u5f3a\u51b3\u7b56\u8005\u7684\u4fe1\u4efb\uff0c\u7279\u522b\u662f\u5728\u90e8\u7f72\u5230\u6cd5\u533b\u5b66\u3001\u5de5\u4e1a\u68c0\u67e5\u548c\u793e\u4f1a\u5a92\u4f53\u76d1\u63a7\u7b49\u573a\u666f\u4e0b\u3002", "method": "\u8be5\u8bba\u6587\u7684\u65b9\u6cd5\u5305\u62ec\u6784\u5efa\u4e00\u4e2a\u57fa\u4e8e\u8f7b\u91cf\u7ea7\u5377\u79ef\u5206\u7c7b\u5668\u548c\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b(Qwen2-VL-7B)\u7684\u7cfb\u7edf\uff0c\u8be5\u7cfb\u7edf\u80fd\u591f\u5bf932x32\u50cf\u7d20\u5927\u5c0f\u7684\u56fe\u50cf\u8fdb\u884c\u5206\u7c7b\u5e76\u5b9a\u4f4d\u5176\u4e2d\u7684\u4f2a\u5f71\u3002\u4f7f\u7528\u81ea\u7f16\u7801\u5668\u751f\u6210\u91cd\u6784\u9519\u8bef\u5730\u56fe\uff0c\u4ee5\u589e\u5f3a\u56fe\u50cf\u4e2d\u4f2a\u5f71\u4f4d\u7f6e\u7684\u53ef\u89c6\u5316\uff1b\u540c\u65f6\uff0c\u57fa\u4e8e\u8fd9\u4e9b\u5b9a\u4f4d\uff0c\u751f\u6210\u53ef\u89e3\u91ca\u7684\u6587\u672c\u63cf\u8ff0\u4ee5\u8bf4\u660e\u53d1\u73b0\u7684\u5f02\u5e38\u60c5\u51b5\u3002\u8be5\u7cfb\u7edf\u4e0d\u4ec5\u80fd\u591f\u4fdd\u8bc1\u8f83\u5feb\u7684\u63a8\u7406\u901f\u5ea6\uff0c\u4ee5\u4fbf\u90e8\u7f72\u5728\u672c\u5730\u6216\u8005\u8fb9\u7f18\u8bbe\u5907\u4e0a\uff0c\u8fd8\u80fd\u63d0\u4f9b\u5f3a\u70c8\u7684\u89e3\u91ca\u80fd\u529b\uff0c\u4f7f\u7cfb\u7edf\u7684\u7ed3\u679c\u6613\u4e8e\u88ab\u4eba\u7c7b\u4ee5\u53ca\u540e\u7eed\u7684\u51b3\u7b56\u6b65\u9aa4\u6240\u7406\u89e3\u3002", "result": "\u8be5\u7cfb\u7edf\u5728\u52a0\u5165\u4e86\u5bf9\u6297\u6027\u6270\u52a8\u7684\u6269\u5c55CiFAKE\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e8696.5%\u7684\u5206\u7c7b\u51c6\u786e\u7387\uff0c\u5e76\u4e14\u57288\u6838CPU\u4e0a\u5b9e\u73b0\u4e86175\u6beb\u79d2\u7684\u63a8\u7406\u65f6\u95f4\uff0c\u5177\u5907\u4e86\u5728\u4f4e\u529f\u8017\u73af\u5883\u5982\u8fb9\u7f18\u8bbe\u5907\u4e0a\u7684\u90e8\u7f72\u80fd\u529b\u3002\u7cfb\u7edf\u901a\u8fc7\u751f\u6210\u81ea\u7f16\u7801\u5668\u7684\u8bef\u5dee\u56fe\u6765\u589e\u5f3a\u4f2a\u5f71\u68c0\u6d4b\u7684\u5b9a\u4f4d\u80fd\u529b\uff0c\u5e76\u4e14\u80fd\u591f\u6e05\u6670\u5730\u8bf4\u660e\u6bcf\u4e2a\u5f02\u5e38\u88ab\u68c0\u6d4b\u7684\u539f\u56e0\uff0c\u4f7f\u5f97\u7cfb\u7edf\u4e0d\u4ec5\u4ec5\u662f\u4e00\u4e2a\u9ad8\u6548\u7684\u5de5\u5177\uff0c\u540c\u65f6\u4e5f\u6210\u4e3a\u4e00\u4e2a\u53ef\u89e3\u91ca\u548c\u5bcc\u6709\u89c1\u89e3\u7684\u5e73\u53f0\u3002", "conclusion": "\u603b\u4e4b\uff0c\u8fd9\u9879\u5de5\u4f5c\u4e0d\u4ec5\u8bc1\u660e\u4e86\u5728\u4f4e\u5206\u8fa8\u7387\u56fe\u50cf\u4e2d\u5b9e\u73b0\u53ef\u89e3\u91ca\u6027\u7684\u771f\u5b9e\u6027\u68c0\u6d4b\u662f\u53ef\u884c\u7684\uff0c\u8fd8\u5c55\u793a\u4e86\u8fd9\u79cd\u65b9\u6cd5\u5982\u4f55\u901a\u8fc7\u7ed3\u5408\u89c6\u89c9\u548c\u8bed\u8a00\u4fe1\u606f\u6765\u589e\u5f3a\u51b3\u7b56\u7684\u53ef\u4fe1\u5ea6\u3002\u8fd9\u79cd\u65b9\u6cd5\u5177\u6709\u5e7f\u6cdb\u7684\u5e94\u7528\u6f5c\u529b\uff0c\u7279\u522b\u662f\u5728\u90a3\u4e9b\u9700\u8981\u5feb\u901f\u51c6\u786e\u4fe1\u606f\u7684\u9886\u57df\uff0c\u5982\u6cd5\u533b\u8c03\u67e5\u3001\u5de5\u4e1a\u8d28\u91cf\u548c\u5b89\u5168\u68c0\u67e5\uff0c\u4ee5\u53ca\u793e\u4ea4\u5a92\u4f53\u5185\u5bb9\u7684\u76d1\u63a7\u3002\u672a\u6765\u7684\u7814\u7a76\u5c06\u8fdb\u4e00\u6b65\u63a2\u7d22\u5982\u4f55\u4f18\u5316\u6a21\u578b\u7684\u6027\u80fd\uff0c\u4ee5\u53ca\u5982\u4f55\u62d3\u5c55\u81f3\u66f4\u9ad8\u5206\u8fa8\u7387\u7684\u56fe\u50cf\u4e0a\u7684\u68c0\u6d4b\u4efb\u52a1\u3002"}}
{"id": "2510.23617", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.23617", "abs": "https://arxiv.org/abs/2510.23617", "authors": ["Phuong Q. Dao", "Mark Roantree", "Vuong M. Ngo"], "title": "An Enhanced Dual Transformer Contrastive Network for Multimodal Sentiment Analysis", "comment": "The paper has been accepted for presentation at the MEDES 2025\n  conference", "summary": "Multimodal Sentiment Analysis (MSA) seeks to understand human emotions by\njointly analyzing data from multiple modalities typically text and images\noffering a richer and more accurate interpretation than unimodal approaches. In\nthis paper, we first propose BERT-ViT-EF, a novel model that combines powerful\nTransformer-based encoders BERT for textual input and ViT for visual input\nthrough an early fusion strategy. This approach facilitates deeper cross-modal\ninteractions and more effective joint representation learning. To further\nenhance the model's capability, we propose an extension called the Dual\nTransformer Contrastive Network (DTCN), which builds upon BERT-ViT-EF. DTCN\nincorporates an additional Transformer encoder layer after BERT to refine\ntextual context (before fusion) and employs contrastive learning to align text\nand image representations, fostering robust multimodal feature learning.\nEmpirical results on two widely used MSA benchmarks MVSA-Single and TumEmo\ndemonstrate the effectiveness of our approach. DTCN achieves best accuracy\n(78.4%) and F1-score (78.3%) on TumEmo, and delivers competitive performance on\nMVSA-Single, with 76.6% accuracy and 75.9% F1-score. These improvements\nhighlight the benefits of early fusion and deeper contextual modeling in\nTransformer-based multimodal sentiment analysis.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6a21\u578bBERT-ViT-EF\uff0c\u7ed3\u5408BERT\u548cViT\u901a\u8fc7\u65e9\u671f\u878d\u5408\u7b56\u7565\u5171\u540c\u5904\u7406\u6587\u672c\u548c\u56fe\u50cf\uff0c\u5e76\u8fdb\u4e00\u6b65\u63d0\u51fa\u4e86DTCN\uff0c\u8be5\u6a21\u578b\u901a\u8fc7\u5bf9\u6bd4\u5b66\u4e60\u589e\u5f3a\u8de8\u6a21\u6001\u878d\u5408\u6548\u679c\u3002\u5728\u4e24\u4e2a\u516c\u5171\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5c55\u793a\u4e86\u65e9\u671f\u878d\u5408\u548c\u6df1\u5ea6\u4e0a\u4e0b\u6587\u5efa\u6a21\u7684\u4f18\u52bf\u3002", "motivation": "\u4f20\u7edf\u7684\u4e00\u6a21\u6001\u5206\u6790\u65b9\u6cd5\u65e0\u6cd5\u7cbe\u786e\u6355\u6349\u4eba\u7c7b\u60c5\u7eea\uff0c\u6587\u7ae0\u63d0\u51fa\u7ed3\u5408\u6587\u672c\u548c\u89c6\u89c9\u8f93\u5165\u7684\u65b9\u6cd5\u6765\u63d0\u9ad8\u60c5\u611f\u5206\u6790\u7684\u51c6\u786e\u6027\u53ca\u6709\u6548\u6027\u3002", "method": "\u63d0\u51fa\u4e86BERT-ViT-EF\u6a21\u578b\uff0c\u7ed3\u5408BERT\u548cViT\uff0c\u4ee5\u53caDTCN\uff0c\u901a\u8fc7\u65e9\u671f\u878d\u5408\u548c\u5bf9\u6bd4\u5b66\u4e60\u6765\u52a0\u5f3a\u6a21\u6001\u4ea4\u4e92\u6548\u679c\u3002", "result": "\u5728MVSA-Single\u548cTumEmo\u6570\u636e\u96c6\u4e0a\uff0cDTCN\u53d6\u5f97\u4e86\u6700\u4f73\u7684\u51c6\u786e\u7387\u548cF1\u5206\u503c\uff0c\u663e\u793a\u4e86\u672c\u6587\u5de5\u4f5c\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u7814\u7a76\u8bc1\u660e\u65e9\u671f\u878d\u5408\u548c\u6df1\u5c42\u6b21\u4e0a\u4e0b\u6587\u5efa\u6a21\u5bf9\u4e8e\u6539\u5584Transformer\u7684\u591a\u6a21\u6001\u60c5\u611f\u5206\u6790\u6548\u679c\u5177\u6709\u663e\u8457\u7684\u4f18\u52bf\u3002"}}
{"id": "2510.24334", "categories": ["eess.IV"], "pdf": "https://arxiv.org/pdf/2510.24334", "abs": "https://arxiv.org/abs/2510.24334", "authors": ["Suvrojit Mitra", "G B Kevin Arjun", "Sanjay Ghosh"], "title": "High-Quality and Large-Scale Image Downscaling for Modern Display Devices", "comment": "10 pages, 3 tables, and 6 figures", "summary": "In modern display technology and visualization tools, downscaling images is\none of the most important activities. This procedure aims to maintain both\nvisual authenticity and structural integrity while reducing the dimensions of\nan image at a large scale to fit the dimension of the display devices. In this\nstudy, we proposed a new technique for downscaling images that uses\nco-occurrence learning to maintain structural and perceptual information while\nreducing resolution. The technique uses the input image to create a data-driven\nco-occurrence profile that captures the frequency of intensity correlations in\nnearby neighborhoods. A refined filtering process is guided by this profile,\nwhich acts as a content-adaptive range kernel. The contribution of each input\npixel is based on how closely it resembles pair-wise intensity values with it's\nneighbors. We validate our proposed technique on four datasets: DIV2K, BSD100,\nUrban100, and RealSR to show its effective downscaling capacity. Our technique\ncould obtain up to 39.22 dB PSNR on the DIV2K dataset and PIQE up to 26.35 on\nthe same dataset when downscaling by 8x and 16x, respectively. Numerous\nexperimental findings attest to the ability of the suggested picture\ndownscaling method to outperform more contemporary approaches in terms of both\nvisual quality and performance measures. Unlike most existing methods, which\ndid not focus on the large-scale image resizing scenario, we achieve\nhigh-quality downscaled images without texture loss or edge blurring. Our\nmethod, LSID (large scale image downscaling), successfully preserves\nhigh-frequency structures like edges, textures, and repeating patterns by\nfocusing on statistically consistent pixels while reducing aliasing and\nblurring artifacts that are typical of traditional downscaling techniques.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u56fe\u50cf\u7f29\u5c0f\u6280\u672f\uff0c\u8be5\u6280\u672f\u901a\u8fc7\u5171\u73b0\u5b66\u4e60\u6765\u7ef4\u6301\u7ed3\u6784\u548c\u611f\u77e5\u4fe1\u606f\u4ee5\u51cf\u5c11\u5206\u8fa8\u7387\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u89c6\u89c9\u8d28\u91cf\u548c\u6027\u80fd\u5ea6\u91cf\u65b9\u9762\u4f18\u4e8e\u73b0\u4ee3\u6280\u672f\u3002\u8fd9\u79cd\u65b9\u6cd5\u80fd\u591f\u5728\u5927\u89c4\u6a21\u56fe\u50cf\u589e\u5927\u540e\u518d\u7f29\u5c0f\u7684\u8fc7\u7a0b\u4e2d\u4fdd\u6301\u7eb9\u7406\u548c\u8fb9\u7f18\u6e05\u6670\uff0c\u907f\u514d\u4f20\u7edf\u7f29\u5c0f\u6280\u672f\u4e2d\u7684\u952f\u9f7f\u548c\u6a21\u7cca\u73b0\u8c61\u3002\u8be5\u6280\u672f\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u5f97\u5230\u9a8c\u8bc1\uff0c\u83b7\u5f97\u4e86\u8f83\u9ad8\u7684PSNR\u548cPIQE\u8bc4\u5206\u3002", "motivation": "\u7f29\u5c0f\u56fe\u50cf\u65f6\u901a\u5e38\u4f1a\u5931\u53bb\u7ed3\u6784\u548c\u89c6\u89c9\u771f\u5b9e\u6027\uff0c\u7279\u522b\u662f\u5bf9\u4e8e\u5927\u89c4\u6a21\u56fe\u50cf\u3002\u8fd9\u9879\u6280\u672f\u65e8\u5728\u89e3\u51b3\u5728\u7f29\u5c0f\u56fe\u50cf\u65f6\u4fdd\u6301\u56fe\u50cf\u7ed3\u6784\u548c\u611f\u77e5\u4fe1\u606f\u7684\u95ee\u9898\uff0c\u7279\u522b\u662f\u5728\u4fdd\u7559\u8fb9\u7f18\u548c\u7eb9\u7406\u7b49\u65b9\u9762\u3002\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u56fe\u50cf\u7f29\u5c0f\u6280\u672f\u65e8\u5728\u89e3\u51b3\u4f20\u7edf\u7f29\u5c0f\u6280\u672f\u4e2d\u7684\u95ee\u9898\uff0c\u540c\u65f6\u907f\u514d\u4e86\u952f\u9f7f\u548c\u6a21\u7cca\u7684\u4ea7\u751f\u3002", "method": "\u4f7f\u7528\u8f93\u5165\u56fe\u50cf\u521b\u5efa\u4e00\u4e2a\u6570\u636e\u9a71\u52a8\u7684\u5171\u73b0\u914d\u7f6e\u6587\u4ef6\uff0c\u8be5\u914d\u7f6e\u6587\u4ef6\u6355\u6349\u4e86\u90bb\u57df\u5185\u5f3a\u5ea6\u76f8\u5173\u6027\u7684\u9891\u7387\u3002\u7136\u540e\u901a\u8fc7\u4e00\u79cd\u7531\u8be5\u914d\u7f6e\u6587\u4ef6\u5f15\u5bfc\u7684\u7ec6\u5316\u8fc7\u6ee4\u8fc7\u7a0b\uff0c\u8be5\u8fc7\u7a0b\u7528\u4f5c\u5185\u5bb9\u81ea\u9002\u5e94\u8303\u56f4\u6838\u3002\u8f93\u5165\u50cf\u7d20\u7684\u8d21\u732e\u57fa\u4e8e\u5176\u4e0e\u90bb\u8fd1\u50cf\u7d20\u5728\u5f3a\u5ea6\u503c\u4e0a\u7684\u76f8\u4f3c\u7a0b\u5ea6\u3002\u57fa\u4e8e\u6b64\u65b9\u6cd5\uff0c\u5728\u56db\u4e2a\u6570\u636e\u96c6\uff08DIV2K\uff0cBSD100\uff0cUrban100\u548cRealSR\uff09\u4e0a\u8fdb\u884c\u4e86\u9a8c\u8bc1\uff0c\u5e76\u53d6\u5f97\u4e86\u4f18\u79c0\u7684\u7f29\u5c0f\u6548\u679c\u3002\u8fd9\u79cd\u65b9\u6cd5\u79f0\u4e3aLSID\uff08\u5927\u89c4\u6a21\u56fe\u50cf\u7f29\u5c0f\uff09\u3002", "result": "\u65b9\u6cd5\u5728Downscaling Capacity of Large-Scale Image\u65b9\u9762\u4f18\u4e8e\u5176\u4ed6\u73b0\u4ee3\u65b9\u6cd5\uff0c\u65e0\u8bba\u662f\u5728\u89c6\u89c9\u8d28\u91cf\u8fd8\u662f\u6027\u80fd\u5ea6\u91cf\u4e0a\u3002\u5728\u7f29\u5c0f8x \u548c16x\u7684\u60c5\u51b5\u4e0b\uff0c\u5728DIV2K\u6570\u636e\u96c6\u4e0a\u5206\u522b\u83b7\u5f97\u4e86\u9ad8\u8fbe39.22 dB\u7684PSNR\u548c\u9ad8\u8fbe26.35\u7684PIQE\u8bc4\u5206\u3002\u5b9e\u9a8c\u7ed3\u679c\u8bc1\u660e\u4e86\u8be5\u65b9\u6cd5\u5728\u89c6\u89c9\u8d28\u91cf\u548c\u6027\u80fd\u5ea6\u91cf\u4e2d\u7684\u5353\u8d8a\u8868\u73b0\uff0c\u7279\u522b\u5728\u4fdd\u6301\u56fe\u50cf\u8be6\u7ec6\u7ed3\u6784\u65b9\u9762\u6709\u5f88\u5927\u4f18\u52bf\u3002", "conclusion": "LSID\uff08\u5927\u89c4\u6a21\u56fe\u50cf\u7f29\u5c0f\uff09\u65b9\u6cd5\u80fd\u591f\u4fdd\u6301\u5927\u89c4\u6a21\u56fe\u50cf\u5728\u7f29\u5c0f\u540e\u4fdd\u7559\u9ad8\u9891\u7387\u7ed3\u6784\u5982\u8fb9\u7f18\u3001\u7eb9\u7406\u548c\u91cd\u590d\u6a21\u5f0f\uff0c\u907f\u514d\u4e86\u4f20\u7edf\u7f29\u5c0f\u6280\u672f\u4e2d\u7684\u6a21\u7cca\u548c\u5931\u771f\u73b0\u8c61\u3002"}}
{"id": "2510.24242", "categories": ["cs.NI", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.24242", "abs": "https://arxiv.org/abs/2510.24242", "authors": ["Zihan Li", "Jiahao Yang", "Yuxin Zhang", "Zhe Chen", "Yue Gao"], "title": "Enabling Near-realtime Remote Sensing via Satellite-Ground Collaboration of Large Vision-Language Models", "comment": "15 pages, 11 figures", "summary": "Large vision-language models (LVLMs) have recently demonstrated great\npotential in remote sensing (RS) tasks (e.g., disaster monitoring) conducted by\nlow Earth orbit (LEO) satellites. However, their deployment in real-world LEO\nsatellite systems remains largely unexplored, hindered by limited onboard\ncomputing resources and brief satellite-ground contacts. We propose Grace, a\nsatellite-ground collaborative system designed for near-realtime LVLM inference\nin RS tasks. Accordingly, we deploy compact LVLM on satellites for realtime\ninference, but larger ones on ground stations (GSs) to guarantee end-to-end\nperformance. Grace is comprised of two main phases that are asynchronous\nsatellite-GS Retrieval-Augmented Generation (RAG), and a task dispatch\nalgorithm. Firstly, we still the knowledge archive of GS RAG to satellite\narchive with tailored adaptive update algorithm during limited satellite-ground\ndata exchange period. Secondly, propose a confidence-based test algorithm that\neither processes the task onboard the satellite or offloads it to the GS.\nExtensive experiments based on real-world satellite orbital data show that\nGrace reduces the average latency by 76-95% compared to state-of-the-art\nmethods, without compromising inference accuracy.", "AI": {"tldr": "Grace \u662f\u4e00\u4e2a\u536b\u661f-\u5730\u9762\u534f\u4f5c\u7cfb\u7edf\uff0c\u7528\u4e8e\u5728\u9065\u611f\u4efb\u52a1\u4e2d\u8fdb\u884c\u5b9e\u65f6 LVLM \u63a8\u7406\u3002\u5b83\u901a\u8fc7\u90e8\u7f72\u7d27\u51d1\u7684 LVLM \u5230\u536b\u661f\u4e0a\uff0c\u5e76\u5728\u5730\u9762\u7ad9\u4e0a\u4fdd\u7559\u66f4\u5927\u7684\u6a21\u578b\uff0c\u4ece\u800c\u4fdd\u8bc1\u6574\u4f53\u6027\u80fd\u3002Grace \u901a\u8fc7\u536b\u661f-\u5730\u9762\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u548c\u4efb\u52a1\u8c03\u5ea6\u7b97\u6cd5\u5b9e\u73b0\u8fd1\u5b9e\u65f6\u7684 LVLM \u63a8\u7406\uff0c\u6bd4\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u51cf\u5c1176-95%\u7684\u5ef6\u8fdf\uff0c\u800c\u4e0d\u4f1a\u727a\u7272\u63a8\u7406\u7cbe\u5ea6\u3002", "motivation": "\u5927\u578b\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u5728\u4f4e\u5730\u7403\u8f68\u9053\u536b\u661f\u7684\u9065\u611f\u4efb\u52a1\u4e2d\u663e\u793a\u51fa\u5de8\u5927\u6f5c\u529b\uff0c\u4f46\u53d7\u5230\u536b\u661f\u8ba1\u7b97\u8d44\u6e90\u6709\u9650\u548c\u536b\u661f\u4e0e\u5730\u9762\u95f4\u901a\u4fe1\u77ed\u6682\u7684\u9650\u5236\uff0c\u8fd9\u4e9b\u6a21\u578b\u5728\u5b9e\u9645\u90e8\u7f72\u4e2d\u7684\u5e94\u7528\u8fd8\u5904\u4e8e\u521d\u6b65\u63a2\u7d22\u9636\u6bb5\u3002\u4e3a\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u63d0\u51fa Grace \u7cfb\u7edf\u3002", "method": "Grace \u7cfb\u7edf\u5305\u542b\u4e24\u4e2a\u4e3b\u8981\u9636\u6bb5\uff1a\u536b\u661f-\u5730\u9762\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u548c\u4efb\u52a1\u8c03\u5ea6\u7b97\u6cd5\u3002\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u9636\u6bb5\u4e2d\uff0c\u4f7f\u7528\u5b9a\u5236\u7684\u81ea\u9002\u5e94\u66f4\u65b0\u7b97\u6cd5\u5c06\u5730\u9762\u7ad9\u4e0a\u7684\u77e5\u8bc6\u5e93\u4f20\u8f93\u5230\u536b\u661f\u4e0a\u3002\u4efb\u52a1\u8c03\u5ea6\u7b97\u6cd5\u901a\u8fc7\u4e00\u4e2a\u57fa\u4e8e\u7f6e\u4fe1\u5ea6\u7684\u6d4b\u8bd5\u7b97\u6cd5\u51b3\u5b9a\u4efb\u52a1\u662f\u5728\u536b\u661f\u4e0a\u5904\u7406\u8fd8\u662f\u4e0b\u4f20\u5230\u5730\u9762\u7ad9\u5904\u7406\u3002", "result": "\u57fa\u4e8e\u5b9e\u9645\u536b\u661f\u8f68\u9053\u6570\u636e\u7684\u5b9e\u9a8c\u8868\u660e\uff0cGrace \u7cfb\u7edf\u76f8\u8f83\u4e8e\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u5e73\u5747\u5ef6\u8fdf\u964d\u4f4e\u4e86 76-95%\uff0c\u4e14\u4e0d\u4f1a\u5f71\u54cd\u63a8\u7406\u51c6\u786e\u6027\u3002", "conclusion": "Grace \u7cfb\u7edf\u5728\u4fdd\u6301\u8f83\u9ad8\u63a8\u7406\u7cbe\u5ea6\u7684\u60c5\u51b5\u4e0b\uff0c\u80fd\u591f\u5927\u5e45\u5ea6\u964d\u4f4e\u9065\u611f\u4efb\u52a1\u4e2d\u7684\u5ef6\u8fdf\uff0c\u5c55\u793a\u4e86\u536b\u661f-\u5730\u9762\u534f\u4f5c\u7cfb\u7edf\u5728\u8fd1\u5b9e\u65f6\u5927\u578b\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u4e0a\u7684\u5de8\u5927\u6f5c\u529b\u3002"}}
{"id": "2510.23785", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.23785", "abs": "https://arxiv.org/abs/2510.23785", "authors": ["Md Tanvir Hossain", "Akif Islam", "Mohd Ruhul Ameen"], "title": "CountFormer: A Transformer Framework for Learning Visual Repetition and Structure in Class-Agnostic Object Counting", "comment": "6 pages, 2 tables, 6 figures. Submitted to IEEE 5th International\n  Conference on Electrical, Computer and Telecommunication Engineering (ICECTE\n  2025)", "summary": "Humans can effortlessly count diverse objects by perceiving visual repetition\nand structural relationships rather than relying on class identity. However,\nmost existing counting models fail to replicate this ability; they often\nmiscount when objects exhibit complex shapes, internal symmetry, or overlapping\ncomponents. In this work, we introduce CountFormer, a transformer-based\nframework that learns to recognize repetition and structural coherence for\nclass-agnostic object counting. Built upon the CounTR architecture, our model\nreplaces its visual encoder with the self-supervised foundation model DINOv2,\nwhich produces richer and spatially consistent feature representations. We\nfurther incorporate positional embedding fusion to preserve geometric\nrelationships before decoding these features into density maps through a\nlightweight convolutional decoder. Evaluated on the FSC-147 dataset, our model\nachieves performance comparable to current state-of-the-art methods while\ndemonstrating superior accuracy on structurally intricate or densely packed\nscenes. Our findings indicate that integrating foundation models such as DINOv2\nenables counting systems to approach human-like structural perception,\nadvancing toward a truly general and exemplar-free counting paradigm.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86CountFormer\uff0c\u4e00\u79cd\u57fa\u4e8eTransformer\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u65e0\u7c7b\u522b\u8eab\u4efd\u8bc6\u522b\u7684\u5bf9\u8c61\u8ba1\u6570\uff0c\u901a\u8fc7\u81ea\u76d1\u7763\u57fa\u7840\u6a21\u578bDINOv2\u548c\u4f4d\u7f6e\u5d4c\u5165\u878d\u5408\u63d0\u9ad8\u4e86\u590d\u6742\u573a\u666f\u4e2d\u7684\u8ba1\u6570\u51c6\u786e\u6027\uff0c\u8fbe\u5230\u4e86\u5f53\u524d\u6700\u5148\u8fdb\u7684\u6c34\u5e73\uff0c\u540c\u65f6\u63a8\u52a8\u4e86\u5411\u901a\u7528\u548c\u65e0\u6837\u4f8b\u7684\u8ba1\u6570\u8303\u4f8b\u7684\u524d\u8fdb\u3002", "motivation": "\u73b0\u6709\u7684\u8ba1\u6570\u6a21\u578b\u5e38\u5e38\u5728\u5904\u7406\u5177\u6709\u590d\u6742\u5f62\u72b6\u3001\u5185\u90e8\u5bf9\u79f0\u6027\u6216\u91cd\u53e0\u7ec4\u4ef6\u7684\u5bf9\u8c61\u65f6\u51fa\u9519\u3002\u56e0\u6b64\uff0c\u672c\u6587\u5e0c\u671b\u901a\u8fc7\u5f15\u5165CountFormer\u6765\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u63d0\u5347\u6a21\u578b\u5728\u590d\u6742\u573a\u666f\u4e2d\u8ba1\u6570\u7684\u51c6\u786e\u6027\u3002", "method": "CountFormer\u5728CounTR\u67b6\u6784\u7684\u57fa\u7840\u4e0a\u66ff\u6362\u89c6\u89c9\u7f16\u7801\u5668\u4e3a\u81ea\u76d1\u7763\u57fa\u7840\u6a21\u578bDINOv2\uff0c\u5c06\u4f4d\u7f6e\u5d4c\u5165\u878d\u5408\u5230\u6a21\u578b\u4e2d\u4fdd\u5b58\u51e0\u4f55\u5173\u7cfb\uff0c\u5e76\u901a\u8fc7\u8f7b\u91cf\u7ea7\u5377\u79ef\u89e3\u7801\u5668\u5c06\u8fd9\u4e9b\u7279\u5f81\u89e3\u7801\u4e3a\u5bc6\u5ea6\u56fe\u3002", "result": "\u5728FSC-147\u6570\u636e\u96c6\u4e0a\u7684\u8bc4\u4f30\u8868\u660e\uff0c\u8be5\u6a21\u578b\u8fbe\u5230\u4e86\u5f53\u524d\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u7279\u522b\u662f\u5728\u7ed3\u6784\u590d\u6742\u6216\u5bc6\u96c6\u7684\u573a\u666f\u4e2d\u8868\u73b0\u51fa\u66f4\u9ad8\u7684\u51c6\u786e\u6027\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u5c06\u57fa\u7840\u6a21\u578b\u5982DINOv2\u96c6\u6210\u5230\u8ba1\u6570\u7cfb\u7edf\u4e2d\uff0c\u53ef\u4ee5\u4f7f\u7cfb\u7edf\u63a5\u8fd1\u4e8e\u4eba\u7c7b\u7684\u7ed3\u6784\u611f\u77e5\u80fd\u529b\uff0c\u63a8\u52a8\u4e86\u5411\u901a\u7528\u548c\u65e0\u6837\u4f8b\u8ba1\u6570\u8303\u4f8b\u7684\u524d\u8fdb\u3002"}}
{"id": "2510.23734", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.23734", "abs": "https://arxiv.org/abs/2510.23734", "authors": ["Eamon Duede"], "title": "AI and the Decentering of Disciplinary Creativity", "comment": null, "summary": "This paper examines the role of artificial intelligence in scientific\nproblem-solving, with a focus on its implications for disciplinary creativity.\nDrawing on recent work in the philosophy of creativity, I distinguish between\ncreative approaches and creative products, and introduce the concept of\ndisciplinary creativity -the creative application of discipline-specific\nexpertise to a valued problem within that field. Through two cases in\nmathematics, I show that while computation can extend disciplinary creativity,\ncertain approaches involving AI can serve to displace it. This displacement has\nthe potential to alter (and, perhaps, diminish) the value of scientific\npursuit.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63a2\u8ba8\u4e86\u4eba\u5de5\u667a\u80fd\u5728\u79d1\u5b66\u95ee\u9898\u89e3\u51b3\u4e2d\u7684\u4f5c\u7528\uff0c\u7279\u522b\u662f\u5b83\u5bf9\u5b66\u79d1\u521b\u9020\u529b\u7684\u5f71\u54cd\u3002\u901a\u8fc7\u4e24\u4e2a\u6570\u5b66\u6848\u4f8b\uff0c\u4f5c\u8005\u5c55\u793a\u4e86\u8ba1\u7b97\u53ef\u4ee5\u6269\u5c55\u5b66\u79d1\u521b\u9020\u529b\uff0c\u4f46\u67d0\u4e9b\u6d89\u53caAI\u7684\u65b9\u6cd5\u53ef\u80fd\u4f1a\u66ff\u4ee3\u4eba\u7c7b\u7684\u521b\u9020\u529b\uff0c\u8fd9\u53ef\u80fd\u6539\u53d8\uff08\u4e5f\u8bb8\u51cf\u5c11\uff09\u79d1\u5b66\u8ffd\u6c42\u7684\u4ef7\u503c\u3002", "motivation": "\u8bba\u6587\u65e8\u5728\u63a2\u8ba8\u4eba\u5de5\u667a\u80fd\u5728\u79d1\u5b66\u95ee\u9898\u89e3\u51b3\u4e2d\u7684\u89d2\u8272\u53ca\u5176\u5bf9\u5b66\u79d1\u521b\u9020\u529b\u7684\u5f71\u54cd\uff0c\u4ece\u800c\u66f4\u597d\u5730\u7406\u89e3\u4eba\u5de5\u667a\u80fd\u5bf9\u79d1\u5b66\u521b\u65b0\u548c\u4ef7\u503c\u7684\u5f71\u54cd\u3002", "method": "\u901a\u8fc7\u4e24\u4e2a\u6570\u5b66\u6848\u4f8b\uff0c\u4f5c\u8005\u5206\u6790\u4e86\u8ba1\u7b97\u5728\u6269\u5c55\u5b66\u79d1\u521b\u9020\u529b\u65b9\u9762\u7684\u6f5c\u529b\uff0c\u540c\u65f6\u63a2\u8ba8\u4e86\u67d0\u4e9b\u6d89\u53caAI\u7684\u65b9\u6cd5\u5728\u66ff\u4ee3\u4eba\u7c7b\u521b\u9020\u529b\u65b9\u9762\u7684\u53ef\u80fd\u6027\u3002\u8fd9\u79cd\u65b9\u6cd5\u7ed3\u5408\u4e86\u7406\u8bba\u5206\u6790\u548c\u6848\u4f8b\u7814\u7a76\uff0c\u65e8\u5728\u63ed\u793aAI\u5728\u79d1\u5b66\u521b\u65b0\u4e2d\u7684\u4f5c\u7528\u53ca\u5176\u6f5c\u5728\u5f71\u54cd\u3002", "result": "\u7814\u7a76\u8868\u660e\uff0c\u867d\u7136\u8ba1\u7b97\u80fd\u591f\u6269\u5c55\u5b66\u79d1\u521b\u9020\u529b\uff0c\u4f46\u67d0\u4e9b\u6d89\u53caAI\u7684\u65b9\u6cd5\u4e5f\u53ef\u80fd\u5bfc\u81f4\u4eba\u7c7b\u521b\u9020\u529b\u7684\u51cf\u5c11\uff0c\u8fd9\u53ef\u80fd\u4f1a\u5f71\u54cd\u79d1\u5b66\u8ffd\u6c42\u7684\u4ef7\u503c\u3002", "conclusion": "\u603b\u4e4b\uff0c\u4eba\u5de5\u667a\u80fd\u53ef\u4ee5\u589e\u5f3a\u5b66\u79d1\u521b\u9020\u529b\uff0c\u4f46\u5176\u66ff\u4ee3\u4eba\u7c7b\u521b\u9020\u529b\u7684\u6f5c\u529b\u4e5f\u63d0\u9192\u6211\u4eec\uff0c\u9700\u8981\u8c28\u614e\u8bc4\u4f30\u5176\u5bf9\u79d1\u5b66\u521b\u65b0\u7684\u5f71\u54cd\uff0c\u4ee5\u786e\u4fdd\u5176\u53d1\u5c55\u80fd\u591f\u4fc3\u8fdb\u800c\u4e0d\u662f\u963b\u788d\u79d1\u5b66\u8fdb\u6b65\u3002"}}
{"id": "2510.23621", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.23621", "abs": "https://arxiv.org/abs/2510.23621", "authors": ["Alexandre Benoit"], "title": "Speeding Up MACE: Low-Precision Tricks for Equivarient Force Fields", "comment": "78 pages, 21 figures", "summary": "Machine-learning force fields can deliver accurate molecular dynamics (MD) at\nhigh computational cost. For SO(3)-equivariant models such as MACE, there is\nlittle systematic evidence on whether reduced-precision arithmetic and\nGPU-optimized kernels can cut this cost without harming physical fidelity. This\nthesis aims to make MACE cheaper and faster while preserving accuracy by\nidentifying computational bottlenecks and evaluating low-precision execution\npolicies. We profile MACE end-to-end and per block, compare the e3nn and NVIDIA\ncuEquivariance backends, and assess FP64/FP32/BF16/FP16 settings (with FP32\naccumulation) for inference, short NVT and long NPT water simulations, and toy\ntraining runs under reproducible, steady-state timing. cuEquivariance reduces\ninference latency by about $3\\times$. Casting only linear layers to BF16/FP16\nwithin an FP32 model yields roughly 4x additional speedups, while energies and\nthermodynamic observables in NVT/NPT MD remain within run-to-run variability.\nHalf-precision weights during training degrade force RMSE. Mixing e3nn and cuEq\nmodules without explicit adapters causes representation mismatches. Fused\nequivariant kernels and mixed-precision inference can substantially accelerate\nstate-of-the-art force fields with negligible impact on downstream MD. A\npractical policy is to use cuEquivariance with FP32 by default and enable\nBF16/FP16 for linear layers (keeping FP32 accumulations) for maximum\nthroughput, while training remains in FP32. Further gains are expected on\nAmpere/Hopper GPUs (TF32/BF16) and from kernel-level FP16/BF16 paths and\npipeline fusion.", "AI": {"tldr": "\u672c\u6587\u901a\u8fc7\u5206\u6790MACE\u6a21\u578b\u7684\u8ba1\u7b97\u74f6\u9888\uff0c\u7814\u7a76\u4e86\u4f4e\u7cbe\u5ea6\u6267\u884c\u7b56\u7565\u5728\u4e0d\u635f\u5bb3\u7269\u7406\u51c6\u786e\u6027\u7684\u524d\u63d0\u4e0b\uff0c\u5982\u4f55\u4f7fMACE\u6a21\u578b\u66f4\u52a0\u7ecf\u6d4e\u548c\u5feb\u901f\u3002\u7814\u7a76\u53d1\u73b0\uff0c\u4f7f\u7528cuEquivariance\u540e\u7aef\u548c\u6df7\u5408\u7cbe\u5ea6\u63a8\u7406\u53ef\u4ee5\u663e\u8457\u52a0\u901f\u6700\u5148\u8fdb\u7684\u529b\u573a\uff0c\u5bf9\u4e0b\u6e38\u7684MD\u5f71\u54cd\u51e0\u4e4e\u53ef\u4ee5\u5ffd\u7565\u4e0d\u8ba1\u3002\u63a8\u8350\u7b56\u7565\u662f\u5728\u9ed8\u8ba4\u60c5\u51b5\u4e0b\u4f7f\u7528cuEquivariance\u548cFP32\uff0c\u5e76\u4e3a\u7ebf\u6027\u5c42\u542f\u7528BF16/FP16\uff08\u4fdd\u6301FP32\u7d2f\u52a0\uff09\uff0c\u4ee5\u8fbe\u5230\u6700\u5927\u541e\u5410\u91cf\uff0c\u800c\u8bad\u7ec3\u4ecd\u4fdd\u6301\u5728FP32\u7cbe\u5ea6\u4e0b\u8fdb\u884c\u3002\u5728Ampere/Hopper GPU\u548c\u4ece\u5185\u6838\u7ea7FP16/BF16\u8def\u5f84\u548c\u6d41\u6c34\u7ebf\u878d\u5408\u4e2d\uff0c\u6709\u671b\u83b7\u5f97\u8fdb\u4e00\u6b65\u7684\u6027\u80fd\u63d0\u5347\u3002", "motivation": "\u8fd9\u7bc7\u8bba\u6587\u7684\u4e3b\u8981\u52a8\u673a\u662f\u901a\u8fc7\u5bfb\u627e\u548c\u89e3\u51b3\u8ba1\u7b97\u74f6\u9888\uff0c\u5e76\u8bc4\u4f30\u4f4e\u7cbe\u5ea6\u6267\u884c\u7b56\u7565\uff0c\u4f7f\u5f97SO(3)-\u7b49\u53d8\u6a21\u578b\uff08\u5982MACE\uff09\u53ef\u4ee5\u5728\u4e0d\u635f\u5bb3\u7269\u7406\u51c6\u786e\u6027\u7684\u524d\u63d0\u4e0b\uff0c\u4ee5\u66f4\u4f4e\u7684\u6210\u672c\u548c\u66f4\u9ad8\u7684\u901f\u5ea6\u8fd0\u884c\u3002", "method": "\u672c\u6587\u91c7\u7528\u4e86\u6027\u80fd\u5206\u6790\u65b9\u6cd5\uff0c\u9488\u5bf9MACE\u6a21\u578b\u8fdb\u884c\u4e86\u7aef\u5230\u7aef\u548c\u6309\u6a21\u5757\u7684\u6027\u80fd\u5206\u6790\uff0c\u6bd4\u8f83\u4e86e3nn\u548cNVIDIA\u7684cuEquivariance\u540e\u7aef\uff0c\u5e76\u8bc4\u4f30\u4e86FP64/FP32/BF16/FP16\u8bbe\u7f6e\uff08FP32\u7d2f\u52a0\uff09\u5728\u63a8\u7406\u3001\u77edNVT\u548c\u957fNPT\u6c34\u6a21\u62df\u4ee5\u53ca\u73a9\u5177\u8bad\u7ec3\u8fd0\u884c\u4e2d\u7684\u6548\u679c\u3002", "result": "cuEquivariance\u540e\u7aef\u51cf\u5c11\u4e86\u5927\u7ea63\u500d\u7684\u63a8\u7406\u5ef6\u8fdf\u3002\u5c06\u7ebf\u6027\u5c42\u8f6c\u6362\u4e3aBF16/FP16\u7684\u4f4e\u7cbe\u5ea6\u6a21\u578b\u5728\u5927\u7ea64\u500d\u7684\u901f\u5ea6\u4e0a\u83b7\u5f97\u989d\u5916\u7684\u52a0\u901f\uff0c\u540c\u65f6\u5728NVT/NPT MD\u4e2d\u7684\u80fd\u91cf\u548c\u70ed\u529b\u5b66\u89c2\u6d4b\u503c\u5728\u8fd0\u884c\u95f4\u7684\u53d8\u5316\u8303\u56f4\u5185\u3002\u6df7\u5408\u7cbe\u5ea6\u8bad\u7ec3\u4f1a\u589e\u52a0\u529b\u7684\u5747\u65b9\u6839\u8bef\u5dee\u3002\u6df7\u5408e3nn\u548ccuEq\u6a21\u5757\u5c06\u4f1a\u5bfc\u81f4\u8868\u793a\u8bef\u5dee\u3002\u878d\u5408\u7b49\u53d8\u6838\u51fd\u6570\u548c\u6df7\u5408\u7cbe\u5ea6\u63a8\u7406\u53ef\u4ee5\u663e\u8457\u52a0\u901f\u6700\u5148\u8fdb\u7684\u529b\u573a\uff0c\u5e76\u5bf9\u4e0b\u6e38MD\u7684\u5f71\u54cd\u51e0\u4e4e\u53ef\u4ee5\u5ffd\u7565\u4e0d\u8ba1\u3002", "conclusion": "\u53d1\u73b0\u4f7f\u7528cuEquivariance\u540e\u7aef\u548c\u6df7\u5408\u7cbe\u5ea6\u63a8\u7406\u53ef\u4ee5\u663e\u8457\u52a0\u5feb\u6700\u5148\u8fdb\u7684\u529b\u573a\uff0c\u800c\u5bf9\u7269\u7406\u51c6\u786e\u6027\u7684\u8d1f\u9762\u5f71\u54cd\u51e0\u4e4e\u53ef\u4ee5\u5ffd\u7565\u4e0d\u8ba1\u3002\u63a8\u8350\u4f7f\u7528cuEquivariance\u540e\u7aef\uff0c\u5c06\u7ebf\u6027\u5c42\u8bbe\u7f6e\u4e3aBF16/FP16\uff0c\u4fdd\u6301FP32\u7d2f\u52a0\u4ee5\u5b9e\u73b0\u6700\u5927\u541e\u5410\u91cf\u3002\u5728\u8bad\u7ec3\u9636\u6bb5\u4ecd\u5efa\u8bae\u4f7f\u7528FP32\u3002\n\n\u4ee5\u4e0a\u7b56\u7565\u9002\u7528\u4e8e\u6700\u65b0\u7248\u672c\u7684GPU\uff08\u5982TF32/BF16\uff09\u4ee5\u53ca\u4ece\u5185\u6838\u7ea7FP16/BF16\u8def\u5f84\u548c\u6d41\u6c34\u7ebf\u878d\u5408\u4e2d\uff0c\u6709\u671b\u8fdb\u4e00\u6b65\u63d0\u9ad8\u6027\u80fd\u3002"}}
{"id": "2510.24687", "categories": ["eess.IV", "cs.AI", "cs.NA", "math.AP", "math.NA", "math.OC"], "pdf": "https://arxiv.org/pdf/2510.24687", "abs": "https://arxiv.org/abs/2510.24687", "authors": ["Andreas Hauptmann", "Leonid Kunyansky", "Jenni Poimala"], "title": "Fast algorithms enabling optimization and deep learning for photoacoustic tomography in a circular detection geometry", "comment": null, "summary": "The inverse source problem arising in photoacoustic tomography and in several\nother coupled-physics modalities is frequently solved by iterative algorithms.\nSuch algorithms are based on the minimization of a certain cost functional. In\naddition, novel deep learning techniques are currently being investigated to\nfurther improve such optimization approaches. All such methods require multiple\napplications of the operator defining the forward problem, and of its adjoint.\nIn this paper, we present new asymptotically fast algorithms for numerical\nevaluation of the forward and adjoint operators, applicable in the circular\nacquisition geometry. For an $(n \\times n)$ image, our algorithms compute these\noperators in $\\mathcal{O}(n^2 \\log n)$ floating point operations. We\ndemonstrate the performance of our algorithms in numerical simulations, where\nthey are used as an integral part of several iterative image reconstruction\ntechniques: classic variational methods, such as non-negative least squares and\ntotal variation regularized least squares, as well as deep learning methods,\nsuch as learned primal dual. A Python implementation of our algorithms and\ncomputational examples is available to the general public.", "AI": {"tldr": "\u672c\u6587\u5f00\u53d1\u4e86\u5e94\u7528\u4e8e\u5706\u73af\u91c7\u96c6\u51e0\u4f55\u7684\u65b0\u7b97\u6cd5\uff0c\u80fd\u5feb\u901f\u8ba1\u7b97\u6b63\u5411\u548c\u9006\u5411\u64cd\u4f5c\u7b26\uff0c\u590d\u6742\u5ea6\u4e3a\\(\\mathcal{O}(n^2 \\log n)\\)\u3002\u8fd9\u4e9b\u7b97\u6cd5\u7528\u4e8e\u8fed\u4ee3\u56fe\u50cf\u91cd\u5efa\u65b9\u6cd5\uff0c\u5305\u62ec\u7ecf\u5178\u7684\u53d8\u5206\u65b9\u6cd5\u548c\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\uff0c\u5e76\u4e14\u5f00\u6e90\u3002", "motivation": "\u5149\u58f0\u5c42\u6790\u6210\u50cf\u53ca\u5176\u4ed6\u8026\u5408\u673a\u5236\u6a21\u5f0f\u4e2d\u7684\u53cd\u95ee\u9898\u7ecf\u5e38\u901a\u8fc7\u8fed\u4ee3\u7b97\u6cd5\u6765\u89e3\u51b3\u3002\u8fd9\u4e9b\u7b97\u6cd5\u57fa\u4e8e\u6210\u672c\u51fd\u6570\u6700\u5c0f\u5316\u3002\u4e3a\u4e86\u8fdb\u4e00\u6b65\u6539\u8fdb\u6b64\u7c7b\u4f18\u5316\u65b9\u6cd5\uff0c\u6b63\u5728\u63a2\u7d22\u65b0\u7684\u6df1\u5ea6\u5b66\u4e60\u6280\u672f\u3002\u7136\u800c\uff0c\u6240\u6709\u8fd9\u4e9b\u65b9\u6cd5\u90fd\u9700\u8981\u8fdb\u884c\u591a\u6b21\u6b63\u5411\u95ee\u9898\u64cd\u4f5c\u7b26\u53ca\u5176\u4f34\u968f\u64cd\u4f5c\u7b26\u7684\u8ba1\u7b97\u3002\u56e0\u6b64\uff0c\u8bba\u6587\u610f\u56fe\u63d0\u51fa\u5feb\u901f\u7b97\u6cd5\u6765\u5b9e\u73b0\u8fd9\u4e00\u64cd\u4f5c\u7b26\u7684\u6570\u503c\u8bc4\u4f30\uff0c\u63d0\u9ad8\u6548\u7387\uff0c\u51cf\u5c11\u8ba1\u7b97\u91cf\u3002", "method": "\u5f00\u53d1\u4e24\u79cd\u65b0\u7684\u6e10\u8fdb\u5f0f\u5feb\u901f\u7b97\u6cd5\uff0c\u7528\u4e8e\u6570\u503c\u8bc4\u4f30\u6b63\u5411\u548c\u4f34\u968f\u64cd\u4f5c\u7b26\uff0c\u5728\u5706\u73af\u51e0\u4f55\u6a21\u5f0f\u4e0b\u4f7f\u7528\u3002\u8fd9\u4e9b\u7b97\u6cd5\u53ef\u4ee5\u5728\\((n \\times n)\\)\u56fe\u50cf\u4e2d\u8ba1\u7b97\u8fd9\u4e9b\u64cd\u4f5c\u7b26\uff0c\u6d6e\u70b9\u8fd0\u7b97\u6b21\u6570\u4e3a\\(\\mathcal{O}(n^2 \\log n)\\)\u3002\u5e76\u4f7f\u7528\u591a\u79cd\u8fed\u4ee3\u56fe\u50cf\u91cd\u5efa\u6280\u672f\uff0c\u8fdb\u884c\u5b9e\u9645\u6f14\u793a\u3002\u5305\u62ec\u7ecf\u5178\u53d8\u5206\u65b9\u6cd5\uff0c\u5982\u975e\u8d1f\u6700\u5c0f\u4e8c\u4e58\u548c\u5168\u53d8\u5206\u6b63\u5219\u5316\u6700\u5c0f\u4e8c\u4e58\uff0c\u4ee5\u53ca\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u65b9\u6cd5\uff0c\u5982\u5b66\u4e60\u7684\u539f\u59cb\u53cc\u91cd\u65b9\u6cd5\u3002\u5e76\u5c06\u8fd9\u4e9b\u7b97\u6cd5\u53ca\u5176\u8ba1\u7b97\u793a\u4f8b\u4ee5Python\u5b9e\u73b0\u5e76\u516c\u5f00\u53d1\u5e03\u3002", "result": "\u5728\u6570\u503c\u4eff\u771f\u5b9e\u9a8c\u4e2d\uff0c\u6211\u4eec\u7684\u7b97\u6cd5\u4ee5\u5b9e\u73b0\u51e0\u7c7b\u8fed\u4ee3\u56fe\u50cf\u91cd\u5efa\u65b9\u6cd5\uff0c\u8868\u73b0\u51fa\u4e86\u51fa\u8272\u7684\u6027\u80fd\u3002\u8fd9\u4e9b\u7b97\u6cd5\u5df2\u5b9e\u73b0\u4e8ePython\u4ee3\u7801\uff0c\u5e76\u63d0\u4f9b\u4e86\u7ed9\u516c\u4f17\u4f7f\u7528\u3002", "conclusion": "\u63d0\u51fa\u4e86\u65b0\u7b97\u6cd5\uff0c\u63d0\u4f9b\u4e86\u5feb\u901f\u8ba1\u7b97\u6b63\u5411\u548c\u4f34\u968f\u64cd\u4f5c\u7b26\u7684\u6709\u6548\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u56fe\u50cf\u91cd\u5efa\u6280\u672f\uff0c\u5e76\u5f00\u6e90\u5171\u4eab\u3002"}}
{"id": "2510.24595", "categories": ["cs.NI"], "pdf": "https://arxiv.org/pdf/2510.24595", "abs": "https://arxiv.org/abs/2510.24595", "authors": ["Azadeh Pourkabirian", "Kai Li", "Photios A. Stavrou", "Wei Ni"], "title": "A New Hybrid Precoding Approach for Multi-user Massive MIMO over Fading Channels", "comment": null, "summary": "Hybrid precoding is an indispensable technique to harness the full potential\nof a multi-user massive multiple-input, multiple-output (MU-MMIMO) system. In\nthis paper, we propose a new hybrid precoding approach that combines digital\nand analog precoding to optimize data transmission over multiple antennas. This\napproach steers signals in specific directions, leading to maximizing sum-rate\nand suppressing side-lobe interference. When dealing with complex signals,\nchanges in phase are naturally associated with changes in angle, and these\nvariations are inherently correlated. The correlation between the angle and\nphase is essential for accurately determining the channel characteristics. An\nimportant aspect of this approach is that we model the angle and phase as\ncorrelated variables following a bivariate Gaussian distribution, and for the\nfirst time, we define a joint angle and phase entropy to measure the\nuncertainty of angle and phase variations in wireless channels. This entropy is\ncrucial to adapt the proposed precoding method with variations. Simulation\nresult validate the accuracy of our analytical findings, demonstrating 18.31%\nincrease in sum-rate and an 11.47% improvement in robustness compared to other\nstate-of-the-art methods.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u6570\u5b57\u548c\u6a21\u62df\u9884\u7f16\u7801\u7684\u65b0\u578b\u6742\u5408\u9884\u7f16\u7801\u65b9\u6cd5\uff0c\u7528\u4e8e\u4f18\u5316\u591a\u7528\u6237\u5927\u89c4\u6a21\u591a\u8f93\u5165\u591a\u8f93\u51fa\u7cfb\u7edf\u4e2d\u7684\u6570\u636e\u4f20\u8f93\uff0c\u5b9e\u73b0\u4e86\u6700\u5927\u548c\u7387\u5e76\u6291\u5236\u526f\u74e3\u5e72\u6270\u3002\u901a\u8fc7\u5c06\u89d2\u5ea6\u548c\u76f8\u4f4d\u89c6\u4e3a\u76f8\u5173\u7684\u53cc\u53d8\u91cf\u9ad8\u65af\u5206\u5e03\u5efa\u6a21\uff0c\u672c\u6587\u9996\u6b21\u5b9a\u4e49\u8054\u5408\u89d2\u5ea6\u548c\u76f8\u4f4d\u71b5\u6765\u5ea6\u91cf\u65e0\u7ebf\u4fe1\u9053\u4e2d\u7684\u89d2\u5ea6\u548c\u76f8\u4f4d\u53d8\u5316\u7684\u4e0d\u786e\u5b9a\u6027\uff0c\u4eff\u771f\u7ed3\u679c\u9a8c\u8bc1\u4e86\u8fd9\u4e00\u65b9\u6cd5\u7684\u51c6\u786e\u6027\u548c\u6709\u6548\u6027\uff0c\u76f8\u8f83\u4e8e\u5176\u4ed6\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\uff0c\u548c\u7387\u63d0\u9ad8\u4e8618.31%\uff0c\u9c81\u68d2\u6027\u63d0\u9ad8\u4e8611.47%\u3002 ", "motivation": "\u4e3a\u4e86\u6700\u5927\u5316\u591a\u7528\u6237\u5927\u89c4\u6a21\u591a\u8f93\u5165\u591a\u8f93\u51fa\u7cfb\u7edf\u7684\u6f5c\u529b\uff0c\u5e76\u63d0\u9ad8\u6570\u636e\u4f20\u8f93\u7684\u6548\u7387\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u65b9\u6cd5\u6765\u4f18\u5316\u4fe1\u53f7\u7684\u65b9\u5411\uff0c\u6700\u7ec8\u63d0\u9ad8\u4f20\u8f93\u6027\u80fd\u3002\u901a\u8fc7\u6539\u8fdb\u9884\u7f16\u7801\u65b9\u5f0f\uff0c\u80fd\u591f\u66f4\u597d\u5730\u7ba1\u7406\u4fe1\u53f7\u5e72\u6270\uff0c\u540c\u65f6\u4fdd\u8bc1\u4fe1\u53f7\u8d28\u91cf\u3002\u540c\u65f6\uff0c\u5b9a\u4e49\u4e00\u4e2a\u65b0\u7684\u53c2\u6570\u6765\u5ea6\u91cf\u65e0\u7ebf\u4fe1\u9053\u4e2d\u7684\u4e0d\u786e\u5b9a\u6027\uff0c\u5bf9\u4e8e\u7cfb\u7edf\u52a8\u6001\u9002\u5e94\u4fe1\u9053\u53d8\u5316\u81f3\u5173\u91cd\u8981\u3002 ", "method": "\u672c\u6587\u7ed3\u5408\u4e86\u6570\u5b57\u548c\u6a21\u62df\u9884\u7f16\u7801\u6280\u672f\uff0c\u901a\u8fc7\u4e00\u79cd\u65b0\u7684\u65b9\u6cd5\u6765\u4f18\u5316\u4fe1\u53f7\u7684\u65b9\u5411\uff0c\u5373\u6700\u5927\u5316\u548c\u7387\u5e76\u51cf\u5c11\u526f\u74e3\u5e72\u6270\u3002\u89d2\u5ea6\u548c\u76f8\u4f4d\u7684\u53d8\u5316\u662f\u76f8\u5173\u8054\u7684\uff0c\u6211\u4eec\u5229\u7528\u8fd9\u4e00\u7279\u70b9\uff0c\u5728\u6a21\u578b\u4e2d\u8003\u8651\u4e86\u5b83\u4eec\u7684\u8054\u7cfb\uff0c\u5e76\u9996\u6b21\u4f7f\u7528\u8054\u5408\u89d2\u5ea6\u548c\u76f8\u4f4d\u71b5\u6765\u91cf\u5316\u65e0\u7ebf\u4fe1\u9053\u7684\u4e0d\u786e\u5b9a\u6027\u3002 ", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u5bf9\u6bd4\u73b0\u6709\u65b9\u6cd5\uff0c\u6211\u4eec\u63d0\u51fa\u7684\u65b9\u6cd5\u53ef\u4ee5\u63d0\u9ad818.31%\u7684\u548c\u7387\uff0c\u5e76\u4e14\u9c81\u68d2\u6027\u4e5f\u63d0\u9ad8\u4e8611.47%\uff0c\u8fd9\u4e00\u7ed3\u679c\u8bc1\u660e\u4e86\u6211\u4eec\u63d0\u51fa\u7684\u65b9\u6cd5\u7684\u4f18\u52bf\u53ca\u5176\u51c6\u786e\u6027\u3002 ", "conclusion": "\u65b0\u65b9\u6cd5\u901a\u8fc7\u7ed3\u5408\u4e0d\u540c\u7c7b\u578b\u7684\u9884\u7f16\u7801\u6280\u672f\uff0c\u4e0d\u4ec5\u63d0\u9ad8\u4e86\u7cfb\u7edf\u7684\u6027\u80fd\uff0c\u8fd8\u589e\u5f3a\u4e86\u7cfb\u7edf\u7684\u9c81\u68d2\u6027\u3002\u6b64\u5916\uff0c\u901a\u8fc7\u5bf9\u89d2\u5ea6\u548c\u76f8\u4f4d\u53d8\u5316\u7684\u5efa\u6a21\u548c\u91cf\u5316\uff0c\u53ef\u4ee5\u66f4\u597d\u5730\u7406\u89e3\u548c\u9002\u5e94\u590d\u6742\u7684\u65e0\u7ebf\u73af\u5883\u3002 "}}
{"id": "2510.23819", "categories": ["eess.SY", "cs.SY", "eess.SP"], "pdf": "https://arxiv.org/pdf/2510.23819", "abs": "https://arxiv.org/abs/2510.23819", "authors": ["Avishka Herath", "Malith Jayalath", "Kumudu Kaushalya", "Sanjana Kapukotuwa", "Chathuni Wijegunawardena", "Pahan Mendis", "Kithmin Wickremasinghe", "Duminda Samarasinghe", "Wageesha N. Manamperi", "Chamira U. S. Edussooriya"], "title": "A Simultaneous ECG-PCG Acquisition System with Real-Time Burst-Adaptive Noise Cancellation", "comment": "Paper submitted to IEEE International Symposium on Circuits and\n  Systems (ISCAS) 2026", "summary": "Cardiac auscultation is an essential clinical skill, requiring excellent\nhearing to distinguish subtle differences in timing and pitch of heart sounds.\nHowever, diagnosing solely from these sounds is often challenging due to\ninterference from surrounding noise, and the information may be limited.\nExisting solutions that adaptively cancel external noise are either not\nreal-time or are computationally intensive, making them unsuitable for\nimplementation in a portable system. This work proposes an end-to-end system\nwith a real-time adaptive noise cancellation pipeline integrated into a device\nthat simultaneously acquires electrocardiogram (ECG) and phonocardiogram (PCG)\nsignals. The performance of the system is validated using real-world hospital\nnoise datasets and recordings captured with the dual-modality device. For PCG\nand ECG signals recorded from the device in noisy hospital settings, the\nproposed algorithms achieved signal-to-noise ratio improvements of 37.01 dB and\n30.32 dB, respectively. These results demonstrate the systems effectiveness in\nenabling reliable and accessible cardiac screening, including noisy hospital\nenvironments typical of resource-constrained settings.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u96c6\u6210\u4e86\u5b9e\u65f6\u81ea\u9002\u5e94\u566a\u58f0\u6d88\u9664\u7ba1\u9053\u7684\u7aef\u5230\u7aef\u7cfb\u7edf\uff0c\u8be5\u7cfb\u7edf\u540c\u65f6\u83b7\u53d6\u5fc3\u7535\u56fe\uff08ECG\uff09\u548c\u5fc3\u97f3\u56fe\uff08PCG\uff09\u4fe1\u53f7\u3002\u5728\u5608\u6742\u7684\u533b\u9662\u73af\u5883\u4e2d\uff0c\u8be5\u7cfb\u7edf\u5bf9PCG\u548cECG\u4fe1\u53f7\u7684\u4fe1\u566a\u6bd4\u5206\u522b\u63d0\u9ad8\u4e8637.01 dB\u548c30.32 dB\uff0c\u4ece\u800c\u5728\u8d44\u6e90\u6709\u9650\u7684\u73af\u5883\u4e2d\u5b9e\u73b0\u4e86\u53ef\u9760\u548c\u53ef\u8bbf\u95ee\u7684\u5fc3\u810f\u7b5b\u67e5\u65b9\u6cd5\u3002", "motivation": "\u4f20\u7edf\u7684\u5fc3\u97f3\u542c\u8bca\u6280\u672f\u56e0\u5468\u56f4\u566a\u58f0\u5e72\u6270\u53ca\u4fe1\u606f\u6709\u9650\uff0c\u8bca\u65ad\u96be\u5ea6\u5927\u3002\u73b0\u6709\u7684\u566a\u58f0\u6d88\u9664\u6280\u672f\u4e0d\u5177\u5907\u5b9e\u65f6\u6027\u6216\u8ba1\u7b97\u590d\u6742\u5ea6\u8fc7\u9ad8\uff0c\u4e0d\u9002\u5408\u4fbf\u643a\u8bbe\u5907\u5e94\u7528\u3002\u56e0\u6b64\uff0c\u63d0\u51fa\u4e00\u79cd\u9002\u7528\u4e8e\u4fbf\u643a\u8bbe\u5907\u7684\u5b9e\u65f6\u566a\u58f0\u6d88\u9664\u7cfb\u7edf\uff0c\u6539\u5584\u5fc3\u810f\u7b5b\u67e5\u7684\u6548\u679c\u3002", "method": "\u7814\u53d1\u4e86\u4e00\u4e2a\u5305\u542b\u5b9e\u65f6\u81ea\u9002\u5e94\u566a\u58f0\u6d88\u9664\u7ba1\u9053\u7684\u7aef\u5230\u7aef\u7cfb\u7edf\uff0c\u8be5\u7cfb\u7edf\u80fd\u591f\u540c\u65f6\u91c7\u96c6\u5fc3\u7535\u56fe\uff08ECG\uff09\u548c\u5fc3\u97f3\u56fe\uff08PCG\uff09\u4fe1\u53f7\u3002\u5b9e\u9a8c\u4e2d\u4f7f\u7528\u4e86\u771f\u5b9e\u7684\u533b\u9662\u566a\u58f0\u6570\u636e\u96c6\u548c\u53cc\u6a21\u6001\u8bbe\u5907\u6355\u6349\u7684\u8bb0\u5f55\u3002", "result": "\u5728\u533b\u9662\u5608\u6742\u73af\u5883\u4e2d\u5bf9\u8be5\u7cfb\u7edf\u8fdb\u884c\u9a8c\u8bc1\uff0c\u7ed3\u679c\u663e\u793a\uff0c\u5bf9\u4e8e\u5b9e\u9a8c\u8bbe\u5907\u91c7\u96c6\u7684PCG\u548cECG\u4fe1\u53f7\uff0c\u8be5\u7b97\u6cd5\u5206\u522b\u5b9e\u73b0\u4e8637.01 dB\u548c30.32 dB\u7684\u4fe1\u566a\u6bd4\u63d0\u9ad8\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u7cfb\u7edf\u5728\u8d44\u6e90\u6709\u9650\u7684\u73af\u5883\u4e2d\u4e3a\u53ef\u9760\u4e14\u53ef\u8bbf\u95ee\u7684\u5fc3\u810f\u68c0\u67e5\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u652f\u6301\u3002", "conclusion": "\u7814\u7a76\u63d0\u51fa\u4e00\u79cd\u96c6\u6210\u5b9e\u65f6\u81ea\u9002\u5e94\u566a\u58f0\u6d88\u9664\u7ba1\u9053\u7684\u4fbf\u643a\u5f0f\u53cc\u6a21\u6001\u5fc3\u97f3\u91c7\u96c6\u53ca\u5206\u6790\u7cfb\u7edf\uff0c\u5728\u8d44\u6e90\u6709\u9650\u7684\u73af\u5883\u4e2d\u663e\u8457\u63d0\u9ad8\u4e86\u5fc3\u97f3\u542c\u8bca\u7684\u51c6\u786e\u6027\u548c\u53ef\u9760\u6027\uff0c\u4e3a\u533b\u9662\u7b49\u73af\u5883\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u5fc3\u810f\u7b5b\u67e5\u624b\u6bb5\u3002"}}
{"id": "2510.23992", "categories": ["cs.LG", "cs.IT", "math.IT", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.23992", "abs": "https://arxiv.org/abs/2510.23992", "authors": ["Yuxiao Wen", "Yanjun Han", "Zhengyuan Zhou"], "title": "Optimal Arm Elimination Algorithms for Combinatorial Bandits", "comment": null, "summary": "Combinatorial bandits extend the classical bandit framework to settings where\nthe learner selects multiple arms in each round, motivated by applications such\nas online recommendation and assortment optimization. While extensions of upper\nconfidence bound (UCB) algorithms arise naturally in this context, adapting arm\nelimination methods has proved more challenging. We introduce a novel\nelimination scheme that partitions arms into three categories (confirmed,\nactive, and eliminated), and incorporates explicit exploration to update these\nsets. We demonstrate the efficacy of our algorithm in two settings: the\ncombinatorial multi-armed bandit with general graph feedback, and the\ncombinatorial linear contextual bandit. In both cases, our approach achieves\nnear-optimal regret, whereas UCB-based methods can provably fail due to\ninsufficient explicit exploration. Matching lower bounds are also provided.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u65b0\u7684\u6d88\u51cf\u65b9\u6848\uff0c\u8be5\u65b9\u6848\u5c06\u81c2\u5206\u4e3a\u4e09\u4e2a\u7c7b\u522b\uff08\u80af\u5b9a\uff0c\u6d3b\u8dc3\u548c\u6d88\u9664\uff09\uff0c\u5305\u542b\u660e\u786e\u7684\u63a2\u7d22\u4ee5\u66f4\u65b0\u8fd9\u4e9b\u96c6\u5408\u3002\u6211\u4eec\u7684\u7b97\u6cd5\u5728\u7ec4\u5408\u591a\u81c2\u8001\u864e\u673a\u548c\u7ec4\u5408\u7ebf\u6027\u4e0a\u4e0b\u6587\u8001\u864e\u673a\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u800cUCB\u65b9\u6cd5\u7531\u4e8e\u63a2\u7d22\u4e0d\u8db3\u53ef\u80fd\u8868\u73b0\u4e0d\u4f73\u3002\u8bba\u6587\u8fd8\u63d0\u4f9b\u4e86\u5339\u914d\u7684\u4e0b\u9650\u3002 ", "motivation": "\u5728\u50cf\u5728\u7ebf\u63a8\u8350\u548c\u5546\u54c1\u9009\u62e9\u4f18\u5316\u8fd9\u6837\u7684\u73af\u5883\u4e2d\uff0c\u4f20\u7edf\u7684\u591a\u81c2\u8001\u864e\u673a\u6846\u67b6\u9700\u8981\u6269\u5c55\u5230\u5141\u8bb8\u5728\u6bcf\u4e00\u8f6e\u9009\u62e9\u591a\u4e2a\u81c2\u7684\u573a\u666f\u4e2d\uff0c\u800c\u672c\u6587\u7814\u7a76\u5982\u4f55\u5c06\u81c2\u6d88\u51cf\u65b9\u6cd5\u5f15\u5165\u8be5\u6846\u67b6\u4e2d\u3002 ", "method": "\u672c\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6d88\u51cf\u65b9\u6848\uff0c\u5c06\u81c2\u5206\u4e3a\u4e09\u4e2a\u7c7b\u522b\uff08\u80af\u5b9a\uff0c\u6d3b\u8dc3\u548c\u6d88\u9664\uff09\u5e76\u8fdb\u884c\u660e\u786e\u7684\u63a2\u7d22\u66f4\u65b0\u8fd9\u4e9b\u96c6\u5408\u3002\u5206\u522b\u5728\u7ec4\u5408\u591a\u81c2\u8001\u864e\u673a\u548c\u7ec4\u5408\u7ebf\u6027\u4e0a\u4e0b\u6587\u8001\u864e\u673a\u73af\u5883\u4e2d\u6d4b\u8bd5\u8be5\u7b97\u6cd5\u3002 ", "result": "\u7b97\u6cd5\u5728\u7ec4\u5408\u591a\u81c2\u8001\u864e\u673a\u548c\u7ec4\u5408\u7ebf\u6027\u4e0a\u4e0b\u6587\u8001\u864e\u673a\u73af\u5883\u4e2d\u90fd\u80fd\u8fbe\u5230\u8fd1\u4f3c\u6700\u4f18\u7684\u9057\u61be\u503c\uff0c\u800c\u5728\u672a\u8fdb\u884c\u5145\u5206\u63a2\u7d22\u60c5\u51b5\u4e0b\u57fa\u4e8eUCB\u7b97\u6cd5\u7684\u65b9\u6cd5\u8868\u73b0\u53ef\u80fd\u4e0d\u4f73\u3002 ", "conclusion": "\u6211\u4eec\u7684\u65b9\u6cd5\u901a\u8fc7\u96c6\u6210\u81c2\u6d88\u51cf\u65b9\u6cd5\u548c\u660e\u786e\u63a2\u7d22\u89e3\u51b3\u4e86\u591a\u81c2\u8001\u864e\u673a\u6709\u5173\u95ee\u9898\uff0c\u8868\u73b0\u4e0a\u8d85\u8fc7\u4e86UCB\u65b9\u6cd5\u5e76\u4e14\u8fd8\u63d0\u4f9b\u4e86\u76f8\u5339\u914d\u7684\u9057\u61be\u4e0b\u9650\u8bc1\u660e\u4e86\u5176\u6709\u6548\u6027\u3002"}}
{"id": "2510.23798", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.23798", "abs": "https://arxiv.org/abs/2510.23798", "authors": ["Gauthier Grimmer", "Romain Wenger", "Cl\u00e9ment Flint", "Germain Forestier", "Gilles Rixhon", "Valentin Chardon"], "title": "A geometric and deep learning reproducible pipeline for monitoring floating anthropogenic debris in urban rivers using in situ cameras", "comment": null, "summary": "The proliferation of floating anthropogenic debris in rivers has emerged as a\npressing environmental concern, exerting a detrimental influence on\nbiodiversity, water quality, and human activities such as navigation and\nrecreation. The present study proposes a novel methodological framework for the\nmonitoring the aforementioned waste, utilising fixed, in-situ cameras. This\nstudy provides two key contributions: (i) the continuous quantification and\nmonitoring of floating debris using deep learning and (ii) the identification\nof the most suitable deep learning model in terms of accuracy and inference\nspeed under complex environmental conditions. These models are tested in a\nrange of environmental conditions and learning configurations, including\nexperiments on biases related to data leakage. Furthermore, a geometric model\nis implemented to estimate the actual size of detected objects from a 2D image.\nThis model takes advantage of both intrinsic and extrinsic characteristics of\nthe camera. The findings of this study underscore the significance of the\ndataset constitution protocol, particularly with respect to the integration of\nnegative images and the consideration of temporal leakage. In conclusion, the\nfeasibility of metric object estimation using projective geometry coupled with\nregression corrections is demonstrated. This approach paves the way for the\ndevelopment of robust, low-cost, automated monitoring systems for urban aquatic\nenvironments.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u65b9\u6cd5\u6846\u67b6\uff0c\u5229\u7528\u56fa\u5b9a\u6444\u50cf\u673a\u8fde\u7eed\u91cf\u5316\u548c\u76d1\u6d4b\u6cb3\u6d41\u4e2d\u7684\u6f02\u6d6e\u5783\u573e\uff0c\u5e76\u901a\u8fc7\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u8bc6\u522b\u6700\u4f73\u6a21\u578b\uff0c\u540c\u65f6\u5b9e\u65bd\u51e0\u4f55\u6a21\u578b\u4ee5\u4f30\u8ba12D\u56fe\u50cf\u4e2d\u68c0\u6d4b\u5230\u5bf9\u8c61\u7684\u5b9e\u9645\u5927\u5c0f\u3002\u7814\u7a76\u7a81\u51fa\u4e86\u6570\u636e\u96c6\u6784\u5efa\u534f\u8bae\u7684\u91cd\u8981\u6027\uff0c\u5c55\u793a\u4e86\u4f7f\u7528\u6295\u5f71\u51e0\u4f55\u548c\u56de\u5f52\u6821\u6b63\u8fdb\u884c\u5ea6\u91cf\u5bf9\u8c61\u4f30\u7b97\u7684\u53ef\u884c\u6027\u3002", "motivation": "\u8be5\u65b9\u6cd5\u65e8\u5728\u89e3\u51b3\u6cb3\u6d41\u4e2d\u6f02\u6d6e\u7684\u4eba\u7c7b\u5e9f\u5f03\u7269\u7684\u73af\u5883\u95ee\u9898\uff0c\u5bf9\u751f\u7269\u591a\u6837\u6027\u3001\u6c34\u8d28\u548c\u4eba\u7c7b\u6d3b\u52a8\u9020\u6210\u5f71\u54cd\u3002\u91c7\u7528\u56fa\u5b9a\u6444\u50cf\u6280\u672f\u548c\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u6765\u76d1\u6d4b\u548c\u91cf\u5316\u8fd9\u79cd\u5e9f\u5f03\u7269\u3002", "method": "\u7814\u7a76\u4f7f\u7528\u4e86\u56fa\u5b9a\u5b89\u88c5\u7684\u6444\u50cf\u673a\uff0c\u901a\u8fc7\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u8fde\u7eed\u91cf\u5316\u548c\u76d1\u6d4b\u6f02\u6d6e\u5783\u573e\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u8bc6\u522b\u51fa\u4e86\u6700\u5408\u9002\u7684\u6a21\u578b\u3002\u540c\u65f6\uff0c\u5b9e\u65bd\u4e86\u5229\u7528\u6444\u50cf\u673a\u7684\u56fa\u6709\u548c\u5916\u5728\u7279\u6027\u4f30\u8ba1\u7269\u4f53\u5b9e\u9645\u5927\u5c0f\u7684\u51e0\u4f55\u6a21\u578b\u3002", "result": "\u7814\u7a76\u8868\u660e\uff0c\u5229\u7528\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u53ef\u4ee5\u5b9e\u73b0\u6f02\u6d6e\u5783\u573e\u7684\u6709\u6548\u76d1\u6d4b\u548c\u91cf\u5316\uff0c\u800c\u51e0\u4f55\u6a21\u578b\u80fd\u591f\u51c6\u786e\u4f30\u8ba1\u5c01\u95ed\u56fe\u50cf\u4e2d\u7269\u4f53\u7684\u5b9e\u9645\u5927\u5c0f\u3002\u540c\u65f6\uff0c\u5f3a\u8c03\u4e86\u6570\u636e\u96c6\u6784\u6210\u7684\u91cd\u8981\u6027\uff0c\u7279\u522b\u662f\u5f15\u5165\u8d1f\u6837\u672c\u548c\u8003\u8651\u65f6\u95f4\u6cc4\u9732\u7684\u5f71\u54cd\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u8bc1\u660e\u4e86\u5229\u7528\u6295\u5f71\u51e0\u4f55\u548c\u56de\u5f52\u4fee\u6b63\u8fdb\u884c\u5ea6\u91cf\u7269\u4f53\u4f30\u7b97\u7684\u53ef\u884c\u6027\uff0c\u4e3a\u5f00\u53d1\u57ce\u5e02\u6c34\u73af\u5883\u4e2d\u4f4e\u6210\u672c\u3001\u81ea\u52a8\u5316\u76d1\u6d4b\u7cfb\u7edf\u94fa\u5e73\u4e86\u9053\u8def\u3002"}}
{"id": "2510.23744", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.23744", "abs": "https://arxiv.org/abs/2510.23744", "authors": ["Eline M. Bovy", "Caleb Probine", "Marnix Suilen", "Ufuk Topcu", "Nils Jansen"], "title": "Multi-Environment POMDPs: Discrete Model Uncertainty Under Partial Observability", "comment": "Accepted at NeurIPS 2025", "summary": "Multi-environment POMDPs (ME-POMDPs) extend standard POMDPs with discrete\nmodel uncertainty. ME-POMDPs represent a finite set of POMDPs that share the\nsame state, action, and observation spaces, but may arbitrarily vary in their\ntransition, observation, and reward models. Such models arise, for instance,\nwhen multiple domain experts disagree on how to model a problem. The goal is to\nfind a single policy that is robust against any choice of POMDP within the set,\ni.e., a policy that maximizes the worst-case reward across all POMDPs. We\ngeneralize and expand on existing work in the following way. First, we show\nthat ME-POMDPs can be generalized to POMDPs with sets of initial beliefs, which\nwe call adversarial-belief POMDPs (AB-POMDPs). Second, we show that any\narbitrary ME-POMDP can be reduced to a ME-POMDP that only varies in its\ntransition and reward functions or only in its observation and reward\nfunctions, while preserving (optimal) policies. We then devise exact and\napproximate (point-based) algorithms to compute robust policies for AB-POMDPs,\nand thus ME-POMDPs. We demonstrate that we can compute policies for standard\nPOMDP benchmarks extended to the multi-environment setting.", "AI": {"tldr": "\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u5904\u7406\u591a\u73af\u5883\u90e8\u5206\u53ef\u89c2\u5bdf\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b(ME-POMDP)\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u5f15\u5165\u5bf9\u624b\u4fe1\u5ff5POMDP(AB-POMDP)\u7684\u6982\u5ff5\uff0c\u6269\u5c55\u4e86\u73b0\u6709\u7684\u7814\u7a76\uff0c\u5e76\u8bbe\u8ba1\u4e86\u7cbe\u786e\u548c\u8fd1\u4f3c\u7b97\u6cd5\u6765\u8ba1\u7b97\u5bf9\u6240\u6709POMDP\u73af\u5883\u90fd\u7a33\u5065\u7684\u7b56\u7565\u3002", "motivation": "\u8be5\u7814\u7a76\u89e3\u51b3\u7684\u662f\u5728\u5b58\u5728\u591a\u4e2a\u53ef\u80fd\u73af\u5883\u6a21\u578b\uff08\u6bd4\u5982\u7531\u591a\u4e2a\u9886\u57df\u4e13\u5bb6\u63d0\u51fa\u7684\u4e0d\u540c\u6a21\u578b\uff09\u7684\u60c5\u51b5\u4e0b\uff0c\u5982\u4f55\u627e\u5230\u4e00\u79cd\u5728\u6240\u6709\u53ef\u80fd\u7684POMDP\u6a21\u578b\u4e0b\u90fd\u80fd\u8868\u73b0\u826f\u597d\u7684\u7b56\u7565\u7684\u95ee\u9898\u3002\u8fd9\u79cd\u65b9\u6cd5\u4f7f\u5f97\u653f\u7b56\u66f4\u52a0\u7a33\u5065\uff0c\u80fd\u591f\u5904\u7406\u4e0d\u786e\u5b9a\u6027\u548c\u6a21\u578b\u5dee\u5f02\u7684\u95ee\u9898\u3002", "method": "\u65b9\u6cd5\u4e0a\uff0c\u7814\u7a76\u5c06ME-POMDP\u6269\u5c55\u4e3a\u5177\u6709\u521d\u59cb\u4fe1\u5ff5\u96c6\u5408\u7684POMDP\uff0c\u79f0\u4e3a\u5bf9\u624b\u4fe1\u5ff5POMDP\u3002\u8fdb\u4e00\u6b65\u5c55\u793a\u4e86\u4efb\u610fME-POMDP\u53ef\u4ee5\u88ab\u7b80\u5316\u4e3a\u4ec5\u5728\u8fc7\u6e21\u51fd\u6570\u6216\u89c2\u6d4b\u51fd\u6570\u4e0a\u4e0d\u540c\u7684ME-POMDP\uff0c\u540c\u65f6\u4fdd\u6301\u6700\u4f18\u7b56\u7565\u3002\u6b64\u5916\uff0c\u8fd8\u63d0\u51fa\u4e86\u8ba1\u7b97AB-POMDP\u548cME-POMDP\u7a33\u5065\u7b56\u7565\u7684\u7cbe\u786e\u548c\u8fd1\u4f3c\uff08\u57fa\u4e8e\u70b9\uff09\u7b97\u6cd5\u3002", "result": "\u7814\u7a76\u5f00\u53d1\u4e86\u8ba1\u7b97AB-POMDP\u548cME-POMDP\u7a33\u5065\u7b56\u7565\u7684\u7b97\u6cd5\uff0c\u5e76\u5c55\u793a\u4e86\u8fd9\u4e9b\u7b97\u6cd5\u53ef\u4ee5\u5728\u6807\u51c6POMDP\u57fa\u51c6\u6d4b\u8bd5\u7684\u591a\u73af\u5883\u8bbe\u7f6e\u4e2d\u8ba1\u7b97\u7b56\u7565\u7684\u80fd\u529b\u3002", "conclusion": "\u8be5\u7814\u7a76\u901a\u8fc7\u5f15\u5165\u5bf9\u624b\u4fe1\u5ff5POMDP\u548c\u5f00\u53d1\u7a33\u5065\u7b56\u7565\u8ba1\u7b97\u7684\u7b97\u6cd5\uff0c\u4e3a\u5904\u7406\u4e0d\u540c\u73af\u5883\u6a21\u578b\u4e2dPOMDP\u95ee\u9898\u63d0\u4f9b\u4e86\u4e00\u79cd\u65b0\u7684\u89c6\u89d2\u548c\u6709\u6548\u65b9\u6cd5\u3002"}}
{"id": "2510.23622", "categories": ["cs.LG", "cs.CR"], "pdf": "https://arxiv.org/pdf/2510.23622", "abs": "https://arxiv.org/abs/2510.23622", "authors": ["Alyssa Gerhart", "Balaji Iyangar"], "title": "Adversarially-Aware Architecture Design for Robust Medical AI Systems", "comment": null, "summary": "Adversarial attacks pose a severe risk to AI systems used in healthcare,\ncapable of misleading models into dangerous misclassifications that can delay\ntreatments or cause misdiagnoses. These attacks, often imperceptible to human\nperception, threaten patient safety, particularly in underserved populations.\nOur study explores these vulnerabilities through empirical experimentation on a\ndermatological dataset, where adversarial methods significantly reduce\nclassification accuracy. Through detailed threat modeling, experimental\nbenchmarking, and model evaluation, we demonstrate both the severity of the\nthreat and the partial success of defenses like adversarial training and\ndistillation. Our results show that while defenses reduce attack success rates,\nthey must be balanced against model performance on clean data. We conclude with\na call for integrated technical, ethical, and policy-based approaches to build\nmore resilient, equitable AI in healthcare.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u533b\u7597AI\u7cfb\u7edf\u4e2d\u5bf9\u6297\u653b\u51fb\u7684\u98ce\u9669\uff0c\u901a\u8fc7\u5728\u76ae\u80a4\u79d1\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\uff0c\u5c55\u793a\u4e86\u8fd9\u4e9b\u653b\u51fb\u5bf9\u6a21\u578b\u5206\u7c7b\u51c6\u786e\u7387\u7684\u4e25\u91cd\u5f71\u54cd\u3002\u5c3d\u7ba1\u5bf9\u6297\u8bad\u7ec3\u7b49\u9632\u5fa1\u65b9\u6cd5\u5728\u4e00\u5b9a\u7a0b\u5ea6\u4e0a\u6709\u6548\uff0c\u4f46\u9700\u8981\u5728\u653b\u51fb\u6210\u529f\u7387\u548c\u6a21\u578b\u5728\u5e72\u51c0\u6570\u636e\u4e0a\u7684\u6027\u80fd\u4e4b\u95f4\u8fdb\u884c\u6743\u8861\u3002\u7814\u7a76\u5f3a\u8c03\u4e86\u6280\u672f\u3001\u4f26\u7406\u548c\u653f\u7b56\u76f8\u7ed3\u5408\u7684\u91cd\u8981\u6027\uff0c\u4ee5\u6784\u5efa\u66f4\u5b89\u5168\u3001\u66f4\u5177\u5305\u5bb9\u6027\u7684\u533b\u7597AI\u7cfb\u7edf\u3002", "motivation": "\u6587\u7ae0\u65e8\u5728\u8bc4\u4f30\u533b\u7597AI\u7cfb\u7edf\u5728\u5bf9\u6297\u653b\u51fb\u4e0b\u7684\u8106\u5f31\u6027\uff0c\u5e76\u63a2\u7d22\u6709\u6548\u7684\u9632\u5fa1\u7b56\u7565\uff0c\u4ee5\u63d0\u9ad8AI\u7cfb\u7edf\u7684\u5b89\u5168\u6027\u548c\u516c\u5e73\u6027\u3002\u901a\u8fc7\u8ba8\u8bba\u5bf9\u6297\u653b\u51fb\u5bf9\u60a3\u8005\u5b89\u5168\u7684\u5a01\u80c1\uff0c\u7279\u522b\u5bf9\u4e8e\u5f31\u52bf\u7fa4\u4f53\u7684\u5f71\u54cd\uff0c\u6587\u7ae0\u5f3a\u8c03\u4e86\u8fd9\u4e00\u7814\u7a76\u7684\u91cd\u8981\u6027\u548c\u7d27\u8feb\u6027\u3002", "method": "\u7814\u7a76\u901a\u8fc7\u5bf9\u6297\u6027\u5b9e\u9a8c\u548c\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5bf9\u76ae\u80a4\u79d1\u6570\u636e\u96c6\u4e0a\u7684\u6a21\u578b\u8fdb\u884c\u4e86\u8bc4\u4f30\uff0c\u63a2\u8ba8\u4e86\u5bf9\u6297\u8bad\u7ec3\u548c\u84b8\u998f\u7b49\u9632\u5fa1\u65b9\u6cd5\u7684\u6548\u679c\u3002\u540c\u65f6\u8fdb\u884c\u4e86\u8be6\u7ec6\u7684\u5b89\u5168\u6027\u5efa\u6a21\uff0c\u4ee5\u7cfb\u7edf\u5730\u5206\u6790\u5a01\u80c1\u7684\u4e25\u91cd\u6027\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u5bf9\u6297\u9632\u5fa1\u65b9\u6cd5\u53ef\u4ee5\u964d\u4f4e\u653b\u51fb\u6210\u529f\u7387\uff0c\u4f46\u53ef\u80fd\u4f1a\u5f71\u54cd\u6a21\u578b\u5728\u6b63\u5e38\u6570\u636e\u4e0a\u7684\u6027\u80fd\u3002\u8fd9\u8868\u660e\u9700\u8981\u4e00\u79cd\u7efc\u5408\u7684\u65b9\u6cd5\u6765\u89e3\u51b3\u95ee\u9898\uff0c\u800c\u4e0d\u4ec5\u4ec5\u662f\u4f9d\u9760\u5355\u4e00\u7684\u9632\u5fa1\u7b56\u7565\u3002", "conclusion": "\u6587\u7ae0\u603b\u7ed3\u4e86\u7814\u7a76\u53d1\u73b0\uff0c\u5e76\u5f3a\u8c03\u4e86\u5728\u533b\u7597\u9886\u57df\u5efa\u8bbe\u66f4\u5b89\u5168\u3001\u66f4\u5177\u5305\u5bb9\u6027\u7684AI\u7cfb\u7edf\u7684\u91cd\u8981\u6027\u3002\u76f8\u5bf9\u4e8e\u5355\u4e00\u7684\u6280\u672f\u9632\u5fa1\uff0c\u6587\u7ae0\u547c\u5401\u91c7\u7528\u5305\u62ec\u4f26\u7406\u548c\u653f\u7b56\u5728\u5185\u7684\u7efc\u5408\u65b9\u6cd5\u3002"}}
{"id": "2510.24705", "categories": ["eess.IV"], "pdf": "https://arxiv.org/pdf/2510.24705", "abs": "https://arxiv.org/abs/2510.24705", "authors": ["Ignacio Contreras-Z\u00fa\u00f1iga", "Mathias Lambert", "Benjam\u00edn Palacios", "Cristian Tejos", "Carlos Milovic"], "title": "Dipole-lets: a new multiscale decomposition for MR phase and quantitative susceptibility mapping", "comment": "This preprint is a work in progress and is not the final manuscript\n  for submission", "summary": "Identifying and suppressing streaking artifacts is one of the most\nchallenging problems in quantitative susceptibility mapping. The measured phase\nfrom tissue magnetization is assumed to be the convolution by the magnetic\ndipole kernel; direct inversion or standard regularization methods tend to\ncreate streaking artifacts in the estimated susceptibility. This is caused by\nextreme noise and by the presence of non-dipolar phase contributions, which are\namplified by the dipole kernel following the streaking pattern. In this work,\nwe introduce a multiscale transform, called Dipole-lets, as an optimal\ndecomposition method for identifying dipole incompatibilities in measured field\ndata by extracting features of different characteristic size and orientation\nwith respect to the dipole kernel's zero-valued double-cone surface (the magic\ncone). We provide experiments that showcase that non-dipolar content can be\nextracted by Dipole-lets from phase data through artifact localization. We also\npresent implementations of Dipole-lets as a optimization functional\nregularizator, through simple Tikhonov and infinity norm.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aDipole-lets\u7684\u591a\u5c3a\u5ea6\u53d8\u6362\u65b9\u6cd5\uff0c\u7528\u4e8e\u8bc6\u522b\u548c\u6291\u5236\u5b9a\u91cf\u78c1\u654f\u611f\u6210\u50cf\u4e2d\u7684\u6761\u7eb9\u4f2a\u5f71\u3002Dipole-lets\u901a\u8fc7\u63d0\u53d6\u4e0e\u78c1\u5076\u6781\u5b50\u5185\u6838\u96f6\u503c\u53cc\u5706\u9525\u8868\u9762\u6709\u5173\u7684\u7279\u5f81\u5c3a\u5bf8\u548c\u65b9\u5411\uff0c\u89e3\u51b3\u4e86\u7531\u4e8e\u6781\u7aef\u566a\u58f0\u548c\u975e\u5076\u6781\u76f8\u4f4d\u8d21\u732e\u5bfc\u81f4\u7684\u4f2a\u5f71\u95ee\u9898\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7Tikhonov\u548c\u65e0\u7a77\u8303\u6570\u5b9e\u73b0\u4f18\u5316\u529f\u80fd\u6b63\u5219\u5316\u3002", "motivation": "\u5b9a\u91cf\u78c1\u654f\u611f\u5236\u56fe\u4e2d\u7684\u4e00\u4e2a\u4e3b\u8981\u6311\u6218\u662f\u8bc6\u522b\u548c\u6291\u5236\u6761\u7eb9\u4f2a\u5f71\uff0c\u8fd9\u662f\u7531\u6d4b\u91cf\u4e2d\u7684\u6781\u7aef\u566a\u58f0\u548c\u975e\u5076\u6781\u76f8\u4f4d\u8d21\u732e\u9020\u6210\u7684\u3002\u8fd9\u4e9b\u95ee\u9898\u5bfc\u81f4\u4e86\u7531\u5076\u6781\u5b50\u6838\u51fd\u6570\u653e\u5927\u540e\u4ea7\u751f\u7684\u4f2a\u5f71\u3002\u4e3a\u4e86\u5e94\u5bf9\u8fd9\u4e00\u6311\u6218\uff0c\u8be5\u8bba\u6587\u5f15\u5165\u4e86Dipole-lets\u65b9\u6cd5\u3002", "method": "\u901a\u8fc7\u5f15\u5165Dipole-lets\u591a\u5c3a\u5ea6\u53d8\u6362\u5bf9\u6d4b\u91cf\u573a\u6570\u636e\u4e2d\u7684\u5076\u6781\u4e0d\u7b26\u5408\u9879\u8fdb\u884c\u6700\u4f18\u5206\u89e3\u3002Dipole-lets\u4ece\u76f8\u4f4d\u6570\u636e\u4e2d\u63d0\u53d6\u7279\u5f81\uff0c\u8fd9\u4e9b\u7279\u5f81\u5177\u6709\u4e0d\u540c\u7684\u5c3a\u5ea6\u548c\u65b9\u5411\uff0c\u76f8\u5bf9\u4e8e\u5076\u6781\u6838\u7684\u53cc\u5706\u9525\u9762\u3002\u5177\u4f53\u5730\uff0c\u8be5\u65b9\u6cd5\u901a\u8fc7Tikhonov\u548c\u65e0\u7a77\u8303\u6570\u5b9e\u73b0\u4f18\u5316\u529f\u80fd\u6b63\u5219\u5316\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cDipole-lets\u53ef\u4ee5\u4ece\u76f8\u4f4d\u6570\u636e\u4e2d\u901a\u8fc7\u4f2a\u5f71\u5b9a\u4f4d\u63d0\u53d6\u975e\u5076\u6781\u9879\u3002\u4f7f\u7528Dipole-lets\u4f5c\u4e3a\u4f18\u5316\u529f\u80fd\u6b63\u5219\u5316\uff0c\u6210\u529f\u5730\u51cf\u5c11\u4e86\u6761\u7eb9\u4f2a\u5f71\u3002", "conclusion": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6709\u6548\u7684\u65b9\u6cd5\u6765\u89e3\u51b3\u5b9a\u91cf\u78c1\u654f\u611f\u5236\u56fe\u4e2d\u7684\u6761\u7eb9\u4f2a\u5f71\u95ee\u9898\u3002\u901a\u8fc7\u4f7f\u7528Dipole-lets\uff0c\u7814\u7a76\u8005\u53ef\u4ee5\u66f4\u51c6\u786e\u5730\u8bc6\u522b\u548c\u51cf\u5c11\u7531\u6781\u7aef\u566a\u58f0\u548c\u975e\u5076\u6781\u76f8\u4f4d\u8d21\u732e\u5f15\u8d77\u7684\u4f2a\u5f71\u3002"}}
{"id": "2510.24611", "categories": ["cs.NI"], "pdf": "https://arxiv.org/pdf/2510.24611", "abs": "https://arxiv.org/abs/2510.24611", "authors": ["Azadeh Pourkabirian", "Amir Masoud Rahmani", "Kai Li", "Wei Ni"], "title": "Strategic Task Offloading for Delay-Sensitive IoT Applications: A Game-Theory-Based Demand-Supply Mechanism with Participation Incentives", "comment": null, "summary": "Delay-sensitive Internet of Things (IoT) applications have drawn significant\nattention. Running many of these applications on IoT devices is challenging due\nto the limited processing resources of these devices and the need for real-time\nresponses. Task offloading can minimize latency by transferring computationally\nintensive tasks from IoT devices to resource-rich edge servers, ensuring delay\nand performance guarantees. In this paper, we develop a task-offloading\napproach for delay-sensitive IoT applications in edge computing environments.\nUnlike existing schemes, we model the task offloading problem as an economic\ndemand and supply model to achieve market balance. The proposed model avoids\nunder- and over-supply, ensuring the computational resources at edge servers\n(supply) are allocated in a manner that best meets the processing and\ncomputational needs of user devices (demand). Given the multi-agent nature of\ntask offloading involving users and service providers with different\npreferences and objectives, we design a game-theoretic framework using a\nVickrey-Clarke-Groves (VCG) auction. This framework analyzes agent interactions\nand decision-making processes. Additionally, we develop an incentive mechanism\nto encourage both parties to participate in the auction. The mechanism\nmaximizes user task offloading to edge servers and motivates edge servers to\nshare their computational resources, achieving profitability for both IoT users\nand edge servers. Simulations demonstrate our method maximizes social welfare,\nensures truthfulness, maintains market balance, and provides latency guarantees\nfor delay-sensitive IoT applications.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7ecf\u6d4e\u4f9b\u9700\u6a21\u578b\u7684\u4efb\u52a1\u5378\u8f7d\u65b9\u6cd5\uff0c\u7528\u4e8e\u5ef6\u8fdf\u654f\u611f\u7684\u7269\u8054\u7f51\u5e94\u7528\u3002\u8fd9\u79cd\u65b9\u6cd5\u4f7f\u7528VCG\u62cd\u5356\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u535a\u5f08\u8bba\u6846\u67b6\uff0c\u4ee5\u5b9e\u73b0\u5e02\u573a\u5e73\u8861\u5e76\u6700\u5927\u5316\u793e\u4f1a\u798f\u5229\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u63d0\u9ad8\u4e86\u793e\u4f1a\u798f\u5229\u3001\u4fdd\u8bc1\u4e86\u771f\u76f8\u6027\u5e76\u63d0\u4f9b\u4e86\u5ef6\u8fdf\u4fdd\u969c\u3002", "motivation": "\u8bb8\u591a\u5ef6\u8fdf\u654f\u611f\u7684\u5e94\u7528\u7a0b\u5e8f\u5728\u8fd0\u884c\u65f6\u9047\u5230\u4e86\u6311\u6218\uff0c\u56e0\u4e3a\u7269\u8054\u7f51\u8bbe\u5907\u7684\u5904\u7406\u8d44\u6e90\u6709\u9650\uff0c\u5e76\u4e14\u9700\u8981\u5b9e\u65f6\u54cd\u5e94\u3002\u4efb\u52a1\u5378\u8f7d\u4f5c\u4e3a\u4e00\u79cd\u89e3\u51b3\u65b9\u6848\uff0c\u901a\u8fc7\u5c06\u8ba1\u7b97\u5bc6\u96c6\u578b\u4efb\u52a1\u4ece\u7269\u8054\u7f51\u8bbe\u5907\u8f6c\u79fb\u5230\u8d44\u6e90\u4e30\u5bcc\u7684\u8fb9\u7f18\u670d\u52a1\u5668\u6765\u51cf\u5c11\u5ef6\u8fdf\uff0c\u786e\u4fdd\u5ef6\u8fdf\u548c\u6027\u80fd\u4fdd\u8bc1\u3002", "method": "\u672c\u6587\u5efa\u7acb\u4e86\u4e00\u4e2a\u7c7b\u4f3c\u4e8e\u7ecf\u6d4e\u4f9b\u9700\u6a21\u578b\u7684\u4efb\u52a1\u5378\u8f7d\u65b9\u6cd5\uff0c\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u57fa\u4e8eVCG\u62cd\u5356\u7684\u535a\u5f08\u8bba\u6846\u67b6\uff0c\u9f13\u52b1\u7269\u8054\u7f51\u7528\u6237\u548c\u8fb9\u7f18\u670d\u52a1\u5668\u5171\u4eab\u8d44\u6e90\uff0c\u4ece\u800c\u5b9e\u73b0\u5e02\u573a\u5e73\u8861\u548c\u6700\u5927\u5316\u793e\u4f1a\u798f\u5229\u3002", "result": "\u8bd5\u9a8c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u6781\u5927\u63d0\u9ad8\u4e86\u793e\u4f1a\u798f\u5229\uff0c\u786e\u4fdd\u4e86\u4ea4\u6613\u7684\u771f\u76f8\u6027\uff0c\u5e76\u4e3a\u5ef6\u8fdf\u654f\u611f\u7684\u7269\u8054\u7f51\u5e94\u7528\u7a0b\u5e8f\u63d0\u4f9b\u4e86\u4fdd\u8bc1\u7684\u5ef6\u8fdf\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u7684\u4efb\u52a1\u5378\u8f7d\u65b9\u6cd5\uff0c\u901a\u8fc7\u57fa\u4e8eVCG\u62cd\u5356\u7684\u535a\u5f08\u8bba\u6846\u67b6\u548c\u6fc0\u52b1\u673a\u5236\uff0c\u4ee5\u5b9e\u73b0\u6700\u5927\u7684\u793e\u4f1a\u798f\u5229\uff0c\u4fdd\u8bc1\u4e86\u5e02\u573a\u5e73\u8861\uff0c\u4e3a\u5ef6\u8fdf\u654f\u611f\u7684\u5e94\u7528\u7a0b\u5e8f\u63d0\u4f9b\u4e86\u5ef6\u8fdf\u4fdd\u969c\u3002"}}
{"id": "2510.23820", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2510.23820", "abs": "https://arxiv.org/abs/2510.23820", "authors": ["Shahab Jahanbazi", "Mateen Ashraf", "Onel L. A. L\u00f3pez"], "title": "MDP-based Energy-aware Task Scheduling for Battery-less IoT", "comment": "13 pages, 11 figures", "summary": "Realizing high long-term task completion rates represents a fundamental\nchallenge in battery-less Internet of Things (IoT) devices powered by ambient\nenergy harvesting. This difficulty is primarily due to the stochastic and\ntime-varying characteristics of the available energy, which significantly\ncomplicate the design of optimal task scheduling policies. In this paper, we\nconsider a battery-less IoT device that must periodically report sensing\nmeasurements to a monitoring center. We adopt the Markov decision process (MDP)\nframework to handle energy variability while aiming to maximize the long-term\ntask completion rate. For this, we first identify its components and then\ndefine two appropriate reward functions. We demonstrate the inherent properties\nassociated with the MDP formulation and the related optimal policy.\nSubsequently, we solve the resulting optimization problem, leading to the\noptimal stationary threshold-based (OSTB) scheduling. Simulation results\ndemonstrate that OSTB outperforms the well-known ``as late as possible'' (ALAP)\nscheduling strategy. For instance, an $8.6\\%$ increase in the task completion\nrate, along with a $65\\%$ reduction in power failures and a $86.29\\%$ decrease\nin execution delays during task execution are registered assuming a $4.7$ mF\ncapacitor.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\uff08MDP\uff09\u7684\u6700\u4f18\u9608\u503c\u8c03\u5ea6\u7b56\u7565\uff0c\u7528\u4e8e\u63d0\u9ad8\u65e0\u7535\u6c60\u7269\u8054\u7f51\u8bbe\u5907\u7684\u957f\u671f\u4efb\u52a1\u5b8c\u6210\u7387\u3002\u672c\u6587\u91c7\u7528MDP\u6846\u67b6\uff0c\u5904\u7406\u80fd\u91cf\u53d8\u5316\uff0c\u4f7f\u957f\u671f\u4efb\u52a1\u5b8c\u6210\u7387\u6700\u5927\u5316\u3002\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u63d0\u51fa\u7684\u6700\u4f18\u9759\u6001\u9608\u503c\u57fa\uff08OSTB\uff09\u8c03\u5ea6\u7b56\u7565\u4f18\u4e8e\u7ecf\u5178\u7684\u201c\u5c3d\u53ef\u80fd\u665a\u201d\u7b56\u7565\uff0c\u4efb\u52a1\u5b8c\u6210\u7387\u63d0\u9ad8\u4e868.6%\uff0c\u529f\u7387\u6545\u969c\u51cf\u5c11\u4e8665%\uff0c\u6267\u884c\u5ef6\u8fdf\u51cf\u5c11\u4e8686.29%\u3002", "motivation": "\u7531\u4e8e\u53ef\u7528\u80fd\u91cf\u7684\u968f\u673a\u548c\u65f6\u95f4\u53d8\u5316\u7279\u6027\uff0c\u8bbe\u8ba1\u6700\u4f73\u4efb\u52a1\u8c03\u5ea6\u653f\u7b56\u53d8\u5f97\u590d\u6742\uff0c\u63d0\u9ad8\u65e0\u7535\u6c60\u7269\u8054\u7f51\u8bbe\u5907\u7684\u957f\u671f\u4efb\u52a1\u5b8c\u6210\u7387\u662f\u9700\u8981\u89e3\u51b3\u7684\u57fa\u672c\u6311\u6218", "method": "\u672c\u6587\u91c7\u7528MDP\u6846\u67b6\u5904\u7406\u80fd\u91cf\u53d8\u5316\uff0c\u540c\u65f6\u5c06\u957f\u671f\u4efb\u52a1\u5b8c\u6210\u7387\u6700\u5927\u5316\u3002\u5b9a\u4e49\u4e86\u76f8\u5173\u7684\u6536\u76ca\u51fd\u6570\uff0c\u6df1\u5165\u7814\u7a76\u4e86MDP\u5f62\u5f0f\u5316\u53ca\u5176\u76f8\u5173\u6700\u4f18\u7b56\u7565\u3002\u63d0\u51fa\u4e86\u6700\u4f18\u9759\u6001\u9608\u503c\u57fa\uff08OSTB\uff09\u8c03\u5ea6\u7b56\u7565\uff0c\u4ee5\u89e3\u51b3\u4f18\u5316\u95ee\u9898\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u63d0\u51fa\u7684OSTB\u7b56\u7565\u4f18\u4e8e\u7ecf\u5178\u7684ALAP\u7b56\u7565\uff0c\u4efb\u52a1\u5b8c\u6210\u7387\u63d0\u9ad8\u4e868.6%\u3002\u529f\u7387\u6545\u969c\u51cf\u5c11\u4e8665%\uff0c\u6267\u884c\u5ef6\u8fdf\u51cf\u5c11\u4e8686.29%\u3002\u8fd9\u4e9b\u7ed3\u679c\u793a\u8303\u4e86\u5176\u57284.7 mF\u7535\u5bb9\u5668\u6761\u4ef6\u4e0b\u7684\u6709\u6548\u6027", "conclusion": "\u7efc\u4e0a\u6240\u8ff0\uff0c\u63d0\u51fa\u7684OSTB\u8c03\u5ea6\u7b56\u7565\u6210\u529f\u89e3\u51b3\u4e86\u65e0\u7535\u6c60\u7269\u8054\u7f51\u8bbe\u5907\u7684\u957f\u671f\u4efb\u52a1\u5b8c\u6210\u7387\u6311\u6218\uff0c\u901a\u8fc7\u5728\u80fd\u91cf\u53d8\u5316\u6761\u4ef6\u4e0b\u53d1\u73b0\u6700\u4f18\u8c03\u5ea6\u65b9\u6848\uff0c\u663e\u8457\u63d0\u9ad8\u4efb\u52a1\u5b8c\u6210\u7387\uff0c\u51cf\u5c11\u529f\u7387\u6545\u969c\u548c\u5de5\u4f5c\u5ef6\u8fdf"}}
{"id": "2510.24088", "categories": ["cs.LG", "cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2510.24088", "abs": "https://arxiv.org/abs/2510.24088", "authors": ["Moongyu Jeon", "Sangwoo Shin", "Dongjae Jeon", "Albert No"], "title": "Information-Theoretic Discrete Diffusion", "comment": "Accepted at NeurIPS 2025", "summary": "We present an information-theoretic framework for discrete diffusion models\nthat yields principled estimators of log-likelihood using score-matching\nlosses. Inspired by the I-MMSE identity for the Gaussian setup, we derive\nanalogous results for the discrete setting. Specifically, we introduce the\nInformation-Minimum Denoising Score Entropy (I-MDSE) relation, which links\nmutual information between data and its diffused version to the minimum\ndenoising score entropy (DSE) loss. We extend this theory to masked diffusion\nand establish the Information-Minimum Denoising Cross-Entropy (I-MDCE)\nrelation, connecting cross-entropy losses to mutual information in discrete\nmasked processes. These results provide a time-integral decomposition of the\nlog-likelihood of the data in terms of optimal score-based losses, showing that\ncommonly used losses such as DSE and DCE are not merely variational bounds but\ntight and principled estimators of log-likelihood. The I-MDCE decomposition\nfurther enables practical extensions, including time-free formula, conditional\nlikelihood estimation in prompt-response tasks, and coupled Monte Carlo\nestimation of likelihood ratios. Experiments on synthetic and real-world data\nconfirm the accuracy, variance stability, and utility of our estimators. The\ncode is publicly available at https://github.com/Dongjae0324/infodis.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u79bb\u6563\u6269\u6563\u6a21\u578b\u7684\u4fe1\u606f\u7406\u8bba\u6846\u67b6\uff0c\u901a\u8fc7\u5206\u6570\u5339\u914d\u635f\u5931\u751f\u6210\u4e86log\u4f3c\u7136\u7684\u539f\u7406\u4f30\u6d4b\u5668\u3002\u63d0\u51fa\u4e86\u79bb\u6563\u8bbe\u7f6e\u4e0b\u7684I-MDSE\u5173\u7cfb\uff0c\u5c06\u6570\u636e\u548c\u5176\u6269\u6563\u7248\u672c\u4e4b\u95f4\u7684\u4e92\u4fe1\u606f\u4e0e\u6700\u5c0f\u5206\u6570\u71b5\u635f\u5931\u8fde\u63a5\u8d77\u6765\uff0c\u5e76\u6269\u5c55\u5230\u63a9\u7801\u6269\u6563\u60c5\u51b5\uff0c\u5efa\u7acb\u4e86I-MDCE\u5173\u7cfb\u3002\u5b9e\u9a8c\u7ed3\u679c\u786e\u5b9a\u4e86\u8fd9\u4e9b\u4f30\u6d4b\u5668\u7684\u51c6\u786e\u6027\u3001\u65b9\u5dee\u7a33\u5b9a\u6027\u548c\u5b9e\u7528\u6027\uff0c\u6539\u5584\u4e86\u5e38\u89c1\u7684\u5206\u6570\u71b5\u548c\u4ea4\u53c9\u71b5\u635f\u5931\u5728\u6a21\u578b\u4f30\u6d4b\u4e2d\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u672c\u6587\u52a8\u673a\u5728\u4e8e\u63d0\u4f9b\u4e00\u4e2a\u57fa\u4e8e\u4fe1\u606f\u7406\u8bba\u7684\u79bb\u6563\u6269\u6563\u6a21\u578b\u6846\u67b6\uff0c\u901a\u8fc7\u5206\u6570\u5339\u914d\u635f\u5931\u65b9\u6cd5\u6765\u4f30\u8ba1\u6a21\u578b\u7684log\u4f3c\u7136\u3002\u4e3a\u4e86\u6311\u6218\u4f20\u7edf\u7684\u4ec5\u4f5c\u4e3a\u53d8\u5206\u754c\u7684\u5206\u6570\u71b5\u548c\u4ea4\u53c9\u71b5\u635f\u5931\uff0c\u672c\u6587\u7684\u76ee\u6807\u662f\u901a\u8fc7\u65b0\u7684\u7406\u8bba\u5173\u7cfb\u63d0\u4f9b\u66f4\u5b9e\u7528\u7684log\u4f3c\u7136\u4f30\u6d4b\u5668\u3002", "method": "\u7814\u7a76\u63d0\u51fa\u7684\u65b9\u6cd5\u4e3b\u8981\u5305\u62ec\u4e86\u901a\u8fc7\u65b0\u7684\u4fe1\u606f-\u6700\u5c0f\u5206\u6570\u71b5\u635f\u5931\u7406\u8bba\u5173\u7cfb\u548c\u4fe1\u606f-\u6700\u5c0f\u4ea4\u53c9\u71b5\u635f\u5931\u7406\u8bba\u5173\u7cfb\u6765\u4f30\u8ba1log\u4f3c\u7136\u3002\u5728\u79bb\u6563\u6570\u636e\u7684\u63a9\u7801\u6269\u6563\u8fc7\u7a0b\u4e2d\uff0c\u8fdb\u4e00\u6b65\u63a8\u5bfc\u51fa\u8fd9\u4e9b\u5173\u7cfb\uff0c\u5e76\u6784\u5efa\u4e86\u4e00\u4e2a\u65f6\u95f4\u81ea\u7531\u516c\u5f0f\u548c\u6761\u4ef6\u4f3c\u7136\u7684\u4f30\u8ba1\u65b9\u6cd5\u3002\u6b64\u5916\uff0c\u8fd8\u5f15\u5165\u4e86\u8499\u5730\u5361\u7f57\u65b9\u6cd5\u6765\u4f30\u8ba1\u4f3c\u7136\u6bd4\u3002", "result": "\u901a\u8fc7\u5728\u5408\u6210\u6570\u636e\u548c\u771f\u5b9e\u4e16\u754c\u6570\u636e\u4e0a\u7684\u5b9e\u9a8c\uff0c\u5b9e\u9a8c\u7ed3\u679c\u8bc1\u660e\u4e86\u65b0\u63d0\u51fa\u7684\u4f30\u6d4b\u5668\u7684\u51c6\u786e\u6027\u548c\u65b9\u5dee\u7a33\u5b9a\u6027\uff0c\u5e76\u4e14\u4e5f\u5f3a\u8c03\u4e86\u5176\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u5b9e\u7528\u6027\u3002\u6bd4\u5982\uff0c\u901a\u8fc7I-MDCE\u5206\u89e3\uff0c\u6211\u4eec\u53ef\u4ee5\u5f97\u5230\u65f6\u95f4\u81ea\u7531\u7684\u4f3c\u7136\u4f30\u8ba1\uff0c\u8fd9\u5728\u5b9e\u9645\u4efb\u52a1\u4e2d\u5f88\u6709\u7528\u5904\u3002", "conclusion": "\u672c\u6587\u901a\u8fc7\u63d0\u51fa\u4e00\u79cd\u65b0\u7684\u4fe1\u606f\u7406\u8bba\u6846\u67b6\uff0c\u8d85\u8d8a\u4e86\u4f20\u7edf\u7684\u5206\u6570\u71b5\u548c\u4ea4\u53c9\u71b5\u635f\u5931\u4f5c\u4e3a\u53d8\u5206\u754c\u7684\u5b9a\u4f4d\uff0c\u5c55\u793a\u4e86\u5b83\u4eec\u4f5c\u4e3alog\u4f3c\u7136\u7684\u51c6\u786e\u4f30\u6d4b\u5668\u7684\u5b9e\u7528\u6027\u3002\u5c06\u6765\u53ef\u4ee5\u5728\u66f4\u591a\u5e94\u7528\u4e2d\u63a2\u7d22\u8be5\u6846\u67b6\u7684\u4ef7\u503c\u548c\u6f5c\u529b\uff0c\u63d0\u9ad8\u5b66\u754c\u5bf9\u4fe1\u606f\u7406\u8bba\u5728\u79bb\u6563\u6a21\u578b\u548c\u6269\u6563\u6a21\u578b\u5e94\u7528\u4e2d\u7684\u7406\u89e3\u3002"}}
{"id": "2510.23816", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.23816", "abs": "https://arxiv.org/abs/2510.23816", "authors": ["Forouzan Fallah", "Wenwen Li", "Chia-Yu Hsu", "Hyunho Lee", "Yezhou Yang"], "title": "RareFlow: Physics-Aware Flow-Matching for Cross-Sensor Super-Resolution of Rare-Earth Features", "comment": null, "summary": "Super-resolution (SR) for remote sensing imagery often fails under\nout-of-distribution (OOD) conditions, such as rare geomorphic features captured\nby diverse sensors, producing visually plausible but physically inaccurate\nresults. We present RareFlow, a physics-aware SR framework designed for OOD\nrobustness. RareFlow's core is a dual-conditioning architecture. A Gated\nControlNet preserves fine-grained geometric fidelity from the low-resolution\ninput, while textual prompts provide semantic guidance for synthesizing complex\nfeatures. To ensure physically sound outputs, we introduce a multifaceted loss\nfunction that enforces both spectral and radiometric consistency with sensor\nproperties. Furthermore, the framework quantifies its own predictive\nuncertainty by employing a stochastic forward pass approach; the resulting\noutput variance directly identifies unfamiliar inputs, mitigating feature\nhallucination. We validate RareFlow on a new, curated benchmark of multi-sensor\nsatellite imagery. In blind evaluations, geophysical experts rated our model's\noutputs as approaching the fidelity of ground truth imagery, significantly\noutperforming state-of-the-art baselines. This qualitative superiority is\ncorroborated by quantitative gains in perceptual metrics, including a nearly\n40\\% reduction in FID. RareFlow provides a robust framework for high-fidelity\nsynthesis in data-scarce scientific domains and offers a new paradigm for\ncontrolled generation under severe domain shift.", "AI": {"tldr": "RareFlow \u662f\u4e00\u79cd\u7528\u4e8e\u9065\u611f\u56fe\u50cf\u8d85\u5206\u8fa8\u7387\u7684\u7269\u7406\u611f\u77e5\u6846\u67b6\uff0c\u65e8\u5728\u63d0\u9ad8\u7f55\u89c1\u5730\u5f62\u7279\u5f81\u4e0b\u7684\u7a33\u5065\u6027\u3002\u901a\u8fc7\u53cc\u6761\u4ef6\u67b6\u6784\u548c\u591a\u9762\u635f\u5931\u51fd\u6570\uff0c\u786e\u4fdd\u751f\u6210\u56fe\u50cf\u5728\u5149\u8c31\u548c\u8f90\u5c04\u5ea6\u4e0a\u7684\u7269\u7406\u4e00\u81f4\u6027\uff0c\u540c\u65f6\u901a\u8fc7\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u673a\u5236\u51cf\u5c11\u8bef\u9020\u7279\u5f81\uff0c\u53d6\u5f97\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u7684\u6027\u80fd\u6539\u5584\u3002", "motivation": "\u5f53\u524d\u8d85\u5206\u8fa8\u7387\u6280\u672f\u5728\u5904\u7406\u7f55\u89c1\u5730\u5f62\u7279\u5f81\u65f6\u6548\u679c\u4e0d\u4f73\uff0c\u89c6\u89c9\u4e0a\u770b\u4f3c\u5408\u7406\u4f46\u7269\u7406\u4e0a\u5e76\u4e0d\u51c6\u786e\u3002RareFlow \u65e8\u5728\u89e3\u51b3\u8fd9\u79cd\u7531\u4f20\u611f\u5668\u79cd\u7c7b\u591a\u6837\u5bfc\u81f4\u7684 out-of-distribution \u6570\u636e\u4e0b\u8d85\u5206\u8fa8\u7387\u4efb\u52a1\u7684\u6311\u6218\uff0c\u63d0\u9ad8\u751f\u6210\u7ed3\u679c\u7684\u8d28\u91cf\u548c\u51c6\u786e\u6027\u3002", "method": "RareFlow \u91c7\u7528\u4e86\u53cc\u6761\u4ef6\u67b6\u6784\uff1a\u95e8\u63a7 ControlNet \u7528\u4e8e\u4ece\u4f4e\u5206\u8fa8\u7387\u8f93\u5165\u4e2d\u4fdd\u7559\u7ec6\u7c92\u5ea6\u51e0\u4f55\u4fdd\u771f\u5ea6\uff0c\u800c\u6587\u672c\u63d0\u793a\u7528\u4e8e\u5408\u6210\u590d\u6742\u7279\u5f81\uff1b\u591a\u9762\u635f\u5931\u51fd\u6570\u786e\u4fdd\u8f93\u51fa\u5728\u5149\u8c31\u548c\u8f90\u5c04\u5ea6\u4e0a\u7684\u7269\u7406\u4e00\u81f4\u6027\uff1b\u901a\u8fc7\u968f\u673a\u524d\u5411\u4f20\u64ad\u91cf\u5316\u9884\u6d4b\u4e0d\u786e\u5b9a\u6027\u3002", "result": "\u5728\u65b0\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cRareFlow \u7684\u8f93\u51fa\u88ab\u5730\u7406\u7269\u7406\u4e13\u5bb6\u8bc4\u4e3a\u63a5\u8fd1\u771f\u5b9e\u56fe\u50cf\u7684\u4fdd\u771f\u5ea6\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6700\u4f73\u65b9\u6cd5\uff0c\u4f53\u73b0\u51fa\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u7684\u6027\u80fd\uff0c\u7279\u522b\u662f\u5728\u611f\u77e5\u5ea6\u91cf\u65b9\u9762\u7684 FID \u51e0\u4e4e\u964d\u4f4e\u4e86 40%\u3002", "conclusion": "RareFlow \u4e3a\u6570\u636e\u7a00\u7f3a\u7684\u79d1\u5b66\u9886\u57df\u63d0\u4f9b\u4e86\u9ad8\u4fdd\u771f\u5408\u6210\u7684\u7a33\u5065\u6846\u67b6\uff0c\u4e3a\u5728\u4e25\u82db\u9886\u57df\u504f\u79fb\u4e0b\u63a7\u5236\u751f\u6210\u63d0\u4f9b\u4e86\u4e00\u79cd\u65b0\u8303\u5f0f\u3002"}}
{"id": "2510.23746", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.23746", "abs": "https://arxiv.org/abs/2510.23746", "authors": ["Laura Mismetti", "Marvin Alberts", "Andreas Krause", "Mara Graziani"], "title": "Test-Time Tuned Language Models Enable End-to-end De Novo Molecular Structure Generation from MS/MS Spectra", "comment": null, "summary": "Tandem Mass Spectrometry enables the identification of unknown compounds in\ncrucial fields such as metabolomics, natural product discovery and\nenvironmental analysis. However, current methods rely on database matching from\npreviously observed molecules, or on multi-step pipelines that require\nintermediate fragment or fingerprint prediction. This makes finding the correct\nmolecule highly challenging, particularly for compounds absent from reference\ndatabases. We introduce a framework that, by leveraging test-time tuning,\nenhances the learning of a pre-trained transformer model to address this gap,\nenabling end-to-end de novo molecular structure generation directly from the\ntandem mass spectra and molecular formulae, bypassing manual annotations and\nintermediate steps. We surpass the de-facto state-of-the-art approach DiffMS on\ntwo popular benchmarks NPLIB1 and MassSpecGym by 100% and 20%, respectively.\nTest-time tuning on experimental spectra allows the model to dynamically adapt\nto novel spectra, and the relative performance gain over conventional\nfine-tuning is of 62% on MassSpecGym. When predictions deviate from the ground\ntruth, the generated molecular candidates remain structurally accurate,\nproviding valuable guidance for human interpretation and more reliable\nidentification.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u65b0\u6846\u67b6\uff0c\u5229\u7528\u6d4b\u8bd5\u65f6\u95f4\u8c03\u4f18\uff0c\u589e\u5f3a\u9884\u8bad\u7ec3\u8f6c\u6362\u5668\u6a21\u578b\uff0c\u5b9e\u73b0\u4ece\u4e32\u8054\u8d28\u8c31\u548c\u5206\u5b50\u5f0f\u76f4\u63a5\u751f\u6210\u65b0\u5206\u5b50\u7ed3\u6784\u3002\u8be5\u6846\u67b6\u5728\u4e24\u4e2a\u6d41\u884c\u57fa\u51c6\u4e0a\u7684\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5DiffMS\uff0c\u540c\u65f6\u63d0\u9ad8\u4e86\u6a21\u578b\u5bf9\u672a\u77e5\u5206\u5b50\u7684\u9002\u5e94\u6027\u548c\u51c6\u786e\u5ea6\u3002", "motivation": "\u73b0\u6709\u7684\u5206\u5b50\u8bc6\u522b\u65b9\u6cd5\u4f9d\u8d56\u4e8e\u6570\u636e\u5e93\u5339\u914d\u6216\u591a\u6b65\u9aa4\u6d41\u6c34\u7ebf\uff0c\u5bfc\u81f4\u5bf9\u4e8e\u672a\u77e5\u5316\u5408\u7269\u96be\u4ee5\u627e\u5230\u6b63\u786e\u5339\u914d\u3002\u4e3a\u4e86\u514b\u670d\u8fd9\u4e2a\u95ee\u9898\uff0c\u5f15\u5165\u4e86\u4e00\u79cd\u65b0\u7684\u6846\u67b6\uff0c\u65e8\u5728\u76f4\u63a5\u4ece\u4e32\u8054\u8d28\u8c31\u548c\u5206\u5b50\u5f0f\u4e2d\u751f\u6210\u65b0\u5206\u5b50\u7ed3\u6784\uff0c\u4e0d\u9700\u8981\u624b\u52a8\u6ce8\u91ca\u6216\u4e2d\u95f4\u6b65\u9aa4\uff0c\u4ee5\u63d0\u9ad8\u8bc6\u522b\u672a\u77e5\u5316\u5408\u7269\u7684\u80fd\u529b\u3002", "method": "\u901a\u8fc7\u6d4b\u8bd5\u65f6\u95f4\u8c03\u4f18\u6280\u672f\uff0c\u8be5\u6846\u67b6\u80fd\u591f\u589e\u5f3a\u9884\u8bad\u7ec3\u7684\u8f6c\u6362\u5668\u6a21\u578b\uff0c\u5b9e\u73b0\u7aef\u5230\u7aef\u7684\u65b0\u5206\u5b50\u7ed3\u6784\u751f\u6210\u3002\u8fd9\u79cd\u65b9\u6cd5\u907f\u514d\u4e86\u4f20\u7edf\u7684\u591a\u6b65\u9aa4\u6d41\u7a0b\u548c\u6570\u636e\u5e93\u5339\u914d\u7684\u65b9\u6cd5\uff0c\u4f7f\u5f97\u5206\u5b50\u7ed3\u6784\u8bc6\u522b\u66f4\u52a0\u9ad8\u6548\u548c\u7cbe\u51c6\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u4e24\u4e2a\u57fa\u51c6\u6570\u636e\u96c6NPLIB1\u548cMassSpecGym\u4e0a\u7684\u8868\u73b0\u5206\u522b\u6bd4\u73b0\u6709\u7684SOTA\u65b9\u6cd5DiffMS\u9ad8\u51fa100%\u548c20%\uff0c\u5e76\u4e14\u5728\u5b9e\u9a8c\u8c31\u56fe\u4e0a\u7684\u6d4b\u8bd5\u65f6\u95f4\u8c03\u4f18\u63d0\u9ad8\u4e86\u6a21\u578b\u7684\u52a8\u6001\u9002\u5e94\u6027\uff0c\u5176\u6027\u80fd\u6bd4\u4f20\u7edf\u5fae\u8c03\u9ad8\u51fa62%\u3002", "conclusion": "\u6d4b\u8bd5\u65f6\u95f4\u8c03\u4f18\u4f7f\u5f97\u6a21\u578b\u80fd\u591f\u66f4\u597d\u5730\u9002\u5e94\u672a\u77e5\u8c31\u56fe\uff0c\u5e76\u751f\u6210\u4e0e\u5b9e\u9645\u7ed3\u6784\u76f8\u8fd1\u7684\u5206\u5b50\u5019\u9009\u7ed3\u6784\uff0c\u4e3a\u4eba\u7c7b\u7406\u89e3\u548c\u7edf\u8ba1\u5305\u62ec\u4fee\u8865\u8bef\u5dee\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u6307\u5bfc\uff0c\u5927\u5927\u589e\u5f3a\u4e86\u5bf9\u672a\u77e5\u5316\u5408\u7269\u7684\u8bc6\u522b\u80fd\u529b\u548c\u51c6\u786e\u6027\u3002"}}
{"id": "2510.23624", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.23624", "abs": "https://arxiv.org/abs/2510.23624", "authors": ["Tiago Mendon\u00e7a dos Santos", "Rafael Izbicki", "Lu\u00eds Gustavo Esteves"], "title": "DiNo and RanBu: Lightweight Predictions from Shallow Random Forests", "comment": null, "summary": "Random Forest ensembles are a strong baseline for tabular prediction tasks,\nbut their reliance on hundreds of deep trees often results in high inference\nlatency and memory demands, limiting deployment in latency-sensitive or\nresource-constrained environments. We introduce DiNo (Distance with Nodes) and\nRanBu (Random Bushes), two shallow-forest methods that convert a small set of\ndepth-limited trees into efficient, distance-weighted predictors. DiNo measures\ncophenetic distances via the most recent common ancestor of observation pairs,\nwhile RanBu applies kernel smoothing to Breiman's classical proximity measure.\nBoth approaches operate entirely after forest training: no additional trees are\ngrown, and tuning of the single bandwidth parameter $h$ requires only\nlightweight matrix-vector operations. Across three synthetic benchmarks and 25\npublic datasets, RanBu matches or exceeds the accuracy of full-depth random\nforests-particularly in high-noise settings-while reducing training plus\ninference time by up to 95\\%. DiNo achieves the best bias-variance trade-off in\nlow-noise regimes at a modest computational cost. Both methods extend directly\nto quantile regression, maintaining accuracy with substantial speed gains. The\nimplementation is available as an open-source R/C++ package at\nhttps://github.com/tiagomendonca/dirf. We focus on structured tabular random\nsamples (i.i.d.), leaving extensions to other modalities for future work.", "AI": {"tldr": "\u4ecb\u7ecd\u4e86\u4e00\u79cd\u65b0\u7684\u6d45\u5c42\u68ee\u6797\u65b9\u6cd5DiNo\u548cRanBu\uff0c\u8fd9\u4e9b\u65b9\u6cd5\u63d0\u4f9b\u4e86\u4e0e\u6df1\u68ee\u6797\u76f8\u4f3c\u7684\u9884\u6d4b\u80fd\u529b\uff0c\u4f46\u5177\u6709\u66f4\u5feb\u7684\u63a8\u65ad\u901f\u5ea6\u548c\u66f4\u4f4e\u7684\u5185\u5b58\u9700\u6c42\u3002\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\uff0cRanBu\u5728\u51c6\u786e\u6027\u4e0a\u5339\u914d\u6216\u8d85\u8fc7\u5b8c\u6574\u6df1\u5ea6\u7684\u968f\u673a\u68ee\u6797\uff0c\u5e76\u51cf\u5c11\u8bad\u7ec3\u548c\u63a8\u65ad\u65f6\u95f4\u6700\u591a\u8fbe95\uff05\u3002DiNo\u5728\u4f4e\u566a\u58f0\u73af\u5883\u4e2d\u63d0\u4f9b\u6700\u4f73\u504f\u5dee-\u65b9\u5dee\u6743\u8861\u3002\u4e24\u4e2a\u65b9\u6cd5\u5747\u9002\u7528\u4e8e\u5206\u4f4d\u6570\u56de\u5f52\uff0c\u5e76\u4fdd\u6301\u51c6\u786e\u6027\u7684\u540c\u65f6\u5e26\u6765\u5de8\u5927\u7684\u901f\u5ea6\u63d0\u5347\u3002\u76f8\u5173\u7684\u5f00\u6e90\u5b9e\u73b0\u53ef\u5728GitHub\u4e0a\u627e\u5230\u3002", "motivation": "\u539f\u59cb\u7684\u968f\u673a\u68ee\u6797\u867d\u7136\u5728\u6807\u6ce8\u9884\u6d4b\u4efb\u52a1\u4e2d\u8868\u73b0\u826f\u597d\uff0c\u4f46\u662f\u7531\u4e8e\u4f9d\u8d56\u4e8e\u5927\u91cf\u6df1\u5ea6\u6811\uff0c\u4f1a\u5bfc\u81f4\u63a8\u7406\u5ef6\u65f6\u8fc7\u9ad8\u548c\u5185\u5b58\u9700\u6c42\u6fc0\u589e\uff0c\u4e0d\u9002\u5408\u5728\u5ef6\u8fdf\u654f\u611f\u6216\u8d44\u6e90\u53d7\u9650\u7684\u73af\u5883\u4e2d\u90e8\u7f72\u3002\u5728\u6b64\u80cc\u666f\u4e0b\uff0c\u7814\u7a76\u8005\u4ecb\u7ecd\u4e86DiNo\uff08\u57fa\u4e8e\u8282\u70b9\u7684\u8ddd\u79bb\uff09\u548cRanBu\uff08\u968f\u673a\u704c\u6728\u4e1b\uff09\uff0c\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u6311\u6218\u3002", "method": "\u65b0\u7684\u65b9\u6cd5\u5c06\u4e00\u7ec4\u6df1\u5ea6\u53d7\u9650\u7684\u6811\u8f6c\u6362\u4e3a\u9ad8\u6548\u7684\u3001\u57fa\u4e8e\u8ddd\u79bb\u52a0\u6743\u7684\u9884\u6d4b\u5668\u3002DiNo\u901a\u8fc7\u8ba1\u7b97\u89c2\u6d4b\u5bf9\u7684\u6700\u8fd1\u516c\u5171\u7956\u5148\u6765\u6d4b\u91cf\u79d1\u82ac\u5c3c\u514b\u8ddd\u79bb\uff0c\u800cRanBu\u5219\u901a\u8fc7\u5e94\u7528\u6838\u5e73\u6ed1\u5230Breiman\u7684\u7ecf\u5178\u90bb\u8fd1\u5ea6\u6d4b\u91cf\u6765\u5b9e\u73b0\u3002\u8fd9\u4e9b\u65b9\u6cd5\u5b8c\u5168\u5728\u68ee\u6797\u8bad\u7ec3\u540e\u64cd\u4f5c\uff0c\u65e0\u9700\u751f\u957f\u989d\u5916\u7684\u6811\uff0c\u4e14\u5e26\u5bbd\u53c2\u6570$h$\u7684\u8c03\u6574\u4ec5\u9700\u8981\u8f7b\u91cf\u7ea7\u7684\u77e9\u9635-\u5411\u91cf\u8fd0\u7b97\u3002\u6b64\u5916\uff0c\u8fd9\u4e24\u4e2a\u65b9\u6cd5\u5747\u53ef\u4ee5\u5e94\u7528\u4e8e\u5206\u4f4d\u6570\u56de\u5f52\u3002", "result": "\u5728\u4e09\u4e2a\u5408\u6210\u57fa\u51c6\u548c25\u4e2a\u516c\u5171\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cRanBu\u4e0d\u4ec5\u8fbe\u5230\u4e86\u751a\u81f3\u8d85\u8fc7\u4e86\u5b8c\u6574\u6df1\u5ea6\u7684\u968f\u673a\u68ee\u6797\u7684\u51c6\u786e\u6027\uff0c\u5c24\u5176\u662f\u5728\u9ad8\u566a\u58f0\u73af\u5883\u4e0b\uff0c\u540c\u65f6\u8fd8\u5b9e\u73b0\u4e86\u6700\u591a95%\u7684\u8bad\u7ec3\u52a0\u63a8\u65ad\u65f6\u95f4\u7684\u51cf\u5c11\u3002DiNo\u5728\u4f4e\u566a\u58f0\u573a\u666f\u4e2d\u8fbe\u5230\u4e86\u6700\u4f73\u7684\u504f\u5dee-\u65b9\u5dee\u6743\u8861\uff0c\u4e14\u53ea\u9700\u5f88\u5c0f\u7684\u8ba1\u7b97\u6210\u672c\u3002\u4e24\u79cd\u65b9\u6cd5\u5747\u9002\u7528\u4e8e\u5206\u4f4d\u6570\u56de\u5f52\uff0c\u5e76\u53ef\u4ee5\u5728\u4fdd\u6301\u51c6\u786e\u6027\u7684\u540c\u65f6\u5e26\u6765\u5de8\u5927\u7684\u901f\u5ea6\u63d0\u5347\u3002", "conclusion": "\u65b0\u7684\u6d45\u5c42\u68ee\u6797\u65b9\u6cd5\u4e0d\u4ec5\u5728\u51c6\u786e\u6027\u4e0a\u53ef\u4ee5\u5339\u914d\u6216\u8d85\u8fc7\u539f\u59cb\u7684\u968f\u673a\u68ee\u6797\uff0c\u540c\u65f6\u5728\u5ef6\u8fdf\u548c\u5185\u5b58\u8981\u6c42\u4e0a\u8981\u4f4e\u5f97\u591a\u3002\u8fd9\u4f7f\u5176\u53ef\u4ee5\u66f4\u6709\u6548\u5730\u90e8\u7f72\u5728\u5ef6\u8fdf\u654f\u611f\u6216\u8d44\u6e90\u53d7\u9650\u7684\u73af\u5883\u4e2d\u3002\u6b64\u5916\uff0c\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u5728\u4fdd\u6301\u51c6\u786e\u6027\u7684\u540c\u65f6\uff0c\u4e24\u79cd\u65b9\u6cd5\u5728\u63a8\u65ad\u901f\u5ea6\u4e0a\u6709\u663e\u8457\u63d0\u5347\u3002"}}
{"id": "2510.23633", "categories": ["cs.LG", "cs.AI", "cs.CV", "eess.IV"], "pdf": "https://arxiv.org/pdf/2510.23633", "abs": "https://arxiv.org/abs/2510.23633", "authors": ["Xun Su", "Hiroyuki Kasai"], "title": "Noise is All You Need: Solving Linear Inverse Problems by Noise Combination Sampling with Diffusion Models", "comment": "9 pages", "summary": "Pretrained diffusion models have demonstrated strong capabilities in\nzero-shot inverse problem solving by incorporating observation information into\nthe generation process of the diffusion models. However, this presents an\ninherent dilemma: excessive integration can disrupt the generative process,\nwhile insufficient integration fails to emphasize the constraints imposed by\nthe inverse problem. To address this, we propose \\emph{Noise Combination\nSampling}, a novel method that synthesizes an optimal noise vector from a noise\nsubspace to approximate the measurement score, replacing the noise term in the\nstandard Denoising Diffusion Probabilistic Models process. This enables\nconditional information to be naturally embedded into the generation process\nwithout reliance on step-wise hyperparameter tuning. Our method can be applied\nto a wide range of inverse problem solvers, including image compression, and,\nparticularly when the number of generation steps $T$ is small, achieves\nsuperior performance with negligible computational overhead, significantly\nimproving robustness and stability.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u65b9\u6cd5\uff0c\u540d\u4e3aNoise Combination Sampling\uff0c\u8be5\u65b9\u6cd5\u53ef\u4ee5\u5c06\u6761\u4ef6\u4fe1\u606f\u81ea\u7136\u5d4c\u5165\u6269\u6563\u6a21\u578b\u7684\u751f\u6210\u8fc7\u7a0b\u4e2d\uff0c\u89e3\u51b3\u4e86\u8fc7\u5ea6\u548c\u4e0d\u8db3\u878d\u5408\u89c2\u6d4b\u4fe1\u606f\u7684\u96be\u9898\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u9006\u95ee\u9898\u6c42\u89e3\u4efb\u52a1\uff0c\u7279\u522b\u662f\u5728\u6b65\u9aa4\u6570\u8f83\u5c11\u65f6\u6027\u80fd\u66f4\u4f18\u4e14\u8ba1\u7b97\u5f00\u9500\u51e0\u4e4e\u53ef\u4ee5\u5ffd\u7565\u4e0d\u8ba1\uff0c\u63d0\u9ad8\u4e86\u6a21\u578b\u7684\u9c81\u68d2\u6027\u548c\u7a33\u5b9a\u6027\u3002", "motivation": "\u9884\u8bad\u7ec3\u6269\u6563\u6a21\u578b\u901a\u8fc7\u5c06\u89c2\u6d4b\u4fe1\u606f\u878d\u5165\u751f\u6210\u8fc7\u7a0b\u6765\u89e3\u51b3\u9006\u95ee\u9898\uff0c\u4f46\u8fc7\u5ea6\u6216\u4e0d\u8db3\u7684\u878d\u5408\u5bfc\u81f4\u6a21\u578b\u6027\u80fd\u4e0d\u4f73\u3002\u4e3a\u4e86\u5e73\u8861\u8fd9\u4e24\u8005\uff0c\u63d0\u51fa\u4e86\u4e00\u4e2a\u65b0\u7684\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u7684\u65b9\u6cd5\u662fNoise Combination Sampling\uff0c\u8be5\u65b9\u6cd5\u4ece\u4e00\u4e2a\u566a\u58f0\u5b50\u7a7a\u95f4\u5408\u6210\u4e00\u4e2a\u6700\u4f73\u7684\u566a\u58f0\u5411\u91cf\uff0c\u6765\u8fd1\u4f3c\u6d4b\u91cf\u5f97\u5206\u3002\u66ff\u4ee3\u6807\u51c6\u7684\u53bb\u566a\u6269\u6563\u6982\u7387\u6a21\u578b\u4e2d\u7684\u566a\u58f0\u9879\uff0c\u4ece\u800c\u4f7f\u6761\u4ef6\u4fe1\u606f\u81ea\u7136\u7eb3\u5165\u751f\u6210\u8fc7\u7a0b\u4e2d\u3002\u6b64\u65b9\u6cd5\u9002\u7528\u4e8e\u5404\u79cd\u9006\u95ee\u9898\u6c42\u89e3\u5668\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u9006\u95ee\u9898\u6c42\u89e3\u4e2d\u5b9e\u73b0\u4e86\u4f18\u5f02\u7684\u6027\u80fd\uff0c\u5c24\u5176\u662f\u5728\u751f\u6210\u6b65\u9aa4\u8f83\u5c11\u65f6\uff0c\u4e14\u8ba1\u7b97\u5f00\u9500\u51e0\u4e4e\u53ef\u4ee5\u5ffd\u7565\u4e0d\u8ba1\uff0c\u540c\u65f6\u63d0\u9ad8\u4e86\u6a21\u578b\u7684\u9c81\u68d2\u6027\u548c\u7a33\u5b9a\u6027\u3002", "conclusion": "Noise Combination Sampling \u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u65b9\u6cd5\u6765\u514b\u670d\u9884\u8bad\u7ec3\u6269\u6563\u6a21\u578b\u4e2d\u7684\u96be\u9898\uff0c\u9002\u7528\u4e8e\u5e7f\u6cdb\u7684\u9006\u95ee\u9898\u6c42\u89e3\u4efb\u52a1\uff0c\u5982\u56fe\u50cf\u538b\u7f29\u7b49\u3002"}}
{"id": "2510.23880", "categories": ["cs.CV", "cs.GR"], "pdf": "https://arxiv.org/pdf/2510.23880", "abs": "https://arxiv.org/abs/2510.23880", "authors": ["Hanke Chen", "Yuan Liu", "Minchen Li"], "title": "TRELLISWorld: Training-Free World Generation from Object Generators", "comment": null, "summary": "Text-driven 3D scene generation holds promise for a wide range of\napplications, from virtual prototyping to AR/VR and simulation. However,\nexisting methods are often constrained to single-object generation, require\ndomain-specific training, or lack support for full 360-degree viewability. In\nthis work, we present a training-free approach to 3D scene synthesis by\nrepurposing general-purpose text-to-3D object diffusion models as modular tile\ngenerators. We reformulate scene generation as a multi-tile denoising problem,\nwhere overlapping 3D regions are independently generated and seamlessly blended\nvia weighted averaging. This enables scalable synthesis of large, coherent\nscenes while preserving local semantic control. Our method eliminates the need\nfor scene-level datasets or retraining, relies on minimal heuristics, and\ninherits the generalization capabilities of object-level priors. We demonstrate\nthat our approach supports diverse scene layouts, efficient generation, and\nflexible editing, establishing a simple yet powerful foundation for\ngeneral-purpose, language-driven 3D scene construction.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u4e0d\u4f9d\u8d56\u4e8e\u573a\u666f\u7ea7\u522b\u6570\u636e\u96c6\u6216\u91cd\u65b0\u8bad\u7ec3\u7684\u6587\u672c\u9a71\u52a8\u76843D\u573a\u666f\u751f\u6210\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c06\u73b0\u6709\u7684\u6587\u672c\u52303D\u5bf9\u8c61\u6269\u6563\u6a21\u578b\u4f5c\u4e3a\u6a21\u5757\u5316\u8d34\u56fe\u751f\u6210\u5668\uff0c\u91cd\u65b0\u5b9a\u4e49\u573a\u666f\u751f\u6210\u4e3a\u591a\u8d34\u56fe\u53bb\u566a\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u5927\u8303\u56f4\u3001\u8fde\u8d2f\u7684\u573a\u666f\u5408\u6210\uff0c\u540c\u65f6\u4fdd\u6301\u5c40\u90e8\u8bed\u4e49\u63a7\u5236\u3002\u8be5\u65b9\u6cd5\u652f\u6301\u591a\u6837\u5316\u7684\u573a\u666f\u5e03\u5c40\u3001\u9ad8\u6548\u7684\u751f\u6210\u548c\u7075\u6d3b\u7684\u7f16\u8f91\u3002", "motivation": "\u73b0\u6709\u76843D\u573a\u666f\u751f\u6210\u65b9\u6cd5\u5f80\u5f80\u5c40\u9650\u4e8e\u5355\u4e2a\u5bf9\u8c61\u751f\u6210\uff0c\u9700\u8981\u7279\u5b9a\u9886\u57df\u7684\u8bad\u7ec3\u6570\u636e\uff0c\u6216\u8005\u4e0d\u652f\u6301360\u5ea6\u7684\u5168\u666f\u89c6\u56fe\u3002\u8fd9\u4e2a\u5de5\u4f5c\u5e0c\u671b\u901a\u8fc7\u4e00\u79cd\u4e0d\u9700\u8981\u8bad\u7ec3\u7684\u65b9\u6cd5\u6765\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u4ee5\u5408\u6210\u590d\u6742\u76843D\u573a\u666f\u3002", "method": "\u8be5\u65b9\u6cd5\u5c06\u73b0\u6709\u7684\u6587\u672c\u52303D\u5bf9\u8c61\u6269\u6563\u6a21\u578b\u91cd\u65b0\u5b9a\u4e49\u4e3a\u751f\u6210\u6a21\u5757\u5316\u8d34\u56fe\u7684\u5de5\u5177\uff0c\u5c06\u573a\u666f\u751f\u6210\u95ee\u9898\u8f6c\u5316\u4e3a\u591a\u8d34\u56fe\u53bb\u566a\u95ee\u9898\uff0c\u901a\u8fc7\u91cd\u53e0\u76843D\u533a\u57df\u7684\u72ec\u7acb\u751f\u6210\u548c\u52a0\u6743\u5e73\u5747\u7684\u65e0\u7f1d\u878d\u5408\u6765\u751f\u6210\u573a\u666f\u3002\u8fd9\u4f7f\u5f97\u751f\u6210\u5927\u578b\u8fde\u8d2f\u7684\u573a\u666f\u53d8\u5f97\u53ef\u80fd\uff0c\u540c\u65f6\u4fdd\u6301\u5c40\u90e8\u8bed\u4e49\u7684\u63a7\u5236\u3002\u8be5\u65b9\u6cd5\u57fa\u4e8e\u5bf9\u8c61\u7ea7\u522b\u7684\u5148\u9a8c\u77e5\u8bc6\uff0c\u9700\u8981\u7684\u542f\u53d1\u5f0f\u6700\u5c11\uff0c\u4e0d\u9700\u8981\u573a\u666f\u7ea7\u522b\u6570\u636e\u96c6\u6216\u91cd\u65b0\u8bad\u7ec3\u3002", "result": "\u5c55\u793a\u4e86\u4e00\u79cd\u652f\u6301\u591a\u6837\u5316\u573a\u666f\u5e03\u5c40\u3001\u9ad8\u6548\u751f\u6210\u548c\u7075\u6d3b\u7f16\u8f91\u7684\u65b9\u6cd5\uff0c\u4e3a\u901a\u7528\u7684\u3001\u8bed\u8a00\u9a71\u52a8\u76843D\u573a\u666f\u6784\u5efa\u5960\u5b9a\u7b80\u5355\u800c\u5f3a\u5927\u7684\u57fa\u7840\u3002\u8be5\u65b9\u6cd5\u80fd\u591f\u5408\u6210\u5927\u578b\u7684\u3001\u8fde\u8d2f\u7684\u573a\u666f\uff0c\u540c\u65f6\u4fdd\u6301\u5bf9\u573a\u666f\u5c40\u90e8\u7ec6\u8282\u7684\u63a7\u5236\u80fd\u529b\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u3001\u4e0d\u9700\u8981\u573a\u666f\u7ea7\u522b\u6570\u636e\u96c6\u6216\u91cd\u8bad\u7ec3\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u6587\u672c\u9a71\u52a8\u76843D\u573a\u666f\u751f\u6210\u3002\u8be5\u65b9\u6cd5\u57fa\u4e8e\u5bf9\u8c61\u7ea7\u522b\u5148\u9a8c\uff0c\u63d0\u4f9b\u4e86\u573a\u666f\u5e03\u5c40\u591a\u6837\u5316\u652f\u6301\u3001\u9ad8\u6548\u751f\u6210\u53ca\u7f16\u8f91\u529f\u80fd\uff0c\u662f\u901a\u7528\u8bed\u8a00\u9a71\u52a83D\u573a\u666f\u6784\u5efa\u7684\u57fa\u7840\u3002"}}
{"id": "2510.23772", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.23772", "abs": "https://arxiv.org/abs/2510.23772", "authors": ["Vivek Veeriah", "Federico Barbero", "Marcus Chiam", "Xidong Feng", "Michael Dennis", "Ryan Pachauri", "Thomas Tumiel", "Johan Obando-Ceron", "Jiaxin Shi", "Shaobo Hou", "Satinder Singh", "Nenad Toma\u0161ev", "Tom Zahavy"], "title": "Evaluating In Silico Creativity: An Expert Review of AI Chess Compositions", "comment": "Accepted at the Creative AI Track, NeurIPS 2025", "summary": "The rapid advancement of Generative AI has raised significant questions\nregarding its ability to produce creative and novel outputs. Our recent work\ninvestigates this question within the domain of chess puzzles and presents an\nAI system designed to generate puzzles characterized by aesthetic appeal,\nnovelty, counter-intuitive and unique solutions. We briefly discuss our method\nbelow and refer the reader to the technical paper for more details. To assess\nour system's creativity, we presented a curated booklet of AI-generated puzzles\nto three world-renowned experts: International Master for chess compositions\nAmatzia Avni, Grandmaster Jonathan Levitt, and Grandmaster Matthew Sadler. All\nthree are noted authors on chess aesthetics and the evolving role of computers\nin the game. They were asked to select their favorites and explain what made\nthem appealing, considering qualities such as their creativity, level of\nchallenge, or aesthetic design.", "AI": {"tldr": "\u7814\u7a76\u63a2\u7d22\u4e86\u4e00\u79cdAI\u7cfb\u7edf\uff0c\u53ef\u4ee5\u751f\u6210\u5177\u6709\u7f8e\u5b66\u5438\u5f15\u529b\u3001\u65b0\u9896\u6027\u548c\u53cd\u76f4\u89c9\u89e3\u51b3\u65b9\u6848\u7684\u56fd\u9645\u8c61\u68cb\u96be\u9898\uff0c\u5e76\u9080\u8bf7\u56fd\u9645\u5927\u5e08\u5bf9\u8fd9\u4e9b\u96be\u9898\u8fdb\u884c\u8bc4\u4f30\u3002", "motivation": "\u7814\u7a76\u7684\u52a8\u673a\u662f\u63a2\u8ba8\u751f\u6210\u5f0fAI\u5728\u521b\u9020\u6027\u548c\u65b0\u9896\u8f93\u51fa\u65b9\u9762\u7684\u80fd\u529b\uff0c\u7279\u522b\u662f\u5728\u56fd\u9645\u8c61\u68cb\u96be\u9898\u751f\u6210\u4e2d\u7684\u5e94\u7528\u3002", "method": "\u7814\u7a76\u7b80\u8981\u63d0\u53ca\u7814\u7a76\u65b9\u6cd5\uff0c\u4f46\u8981\u6c42\u8bfb\u8005\u53c2\u8003\u6280\u672f\u8bba\u6587\u4ee5\u83b7\u53d6\u66f4\u591a\u7ec6\u8282\u3002\u7814\u7a76\u5c06AI\u751f\u6210\u7684\u95ee\u9898\u96c6\u5c55\u793a\u7ed9\u56fd\u9645\u8c61\u68cb\u6743\u5a01\u5927\u5e08\u8bc4\u5ba1\uff0c\u4ee5\u6b64\u65b9\u6cd5\u8bc4\u4f30\u7cfb\u7edf\u7684\u521b\u610f\u80fd\u529b\u3002", "result": "\u56fd\u9645\u8c61\u68cb\u4e13\u5bb6\u4eec\u9009\u51fa\u4e86\u4ed6\u4eec\u6700\u559c\u6b22\u7684\u95ee\u9898\uff0c\u5e76\u5bf9\u8fd9\u4e9b\u95ee\u9898\u8fdb\u884c\u4e86\u8bc4\u4ef7\uff0c\u8003\u8651\u5230\u8bf8\u5982\u521b\u9020\u6027\u3001\u96be\u5ea6\u6c34\u5e73\u6216\u7f8e\u5b66\u8bbe\u8ba1\u7b49\u65b9\u9762\u3002\u7ed3\u679c\u5c55\u793a\u4e86AI\u7cfb\u7edf\u5728\u751f\u6210\u5177\u6709\u7f8e\u5b66\u5438\u5f15\u529b\u548c\u65b0\u9896\u6027\u7684\u56fd\u9645\u8c61\u68cb\u96be\u9898\u65b9\u9762\u7684\u6f5c\u529b\u3002", "conclusion": "\u7814\u7a76\u8bc1\u660e\u4e86AI\u7cfb\u7edf\u5728\u751f\u6210\u5177\u6709\u7f8e\u5b66\u4ef7\u503c\u548c\u65b0\u9896\u6027\u7684\u95ee\u9898\u65b9\u9762\u7684\u6709\u6548\u6027\uff0c\u8fd9\u4e5f\u662f\u672a\u6765\u56fd\u9645\u8c61\u68cb\u96be\u9898\u751f\u6210\u7814\u7a76\u7684\u65b9\u5411\u3002"}}
{"id": "2510.23626", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.23626", "abs": "https://arxiv.org/abs/2510.23626", "authors": ["Shuang Geng", "Wenli Zhang", "Jiaheng Xie", "Rui Wang", "Sudha Ram"], "title": "From Detection to Discovery: A Closed-Loop Approach for Simultaneous and Continuous Medical Knowledge Expansion and Depression Detection on Social Media", "comment": "Presented at SWAIB2025 and HICSS2026", "summary": "Social media user-generated content (UGC) provides real-time, self-reported\nindicators of mental health conditions such as depression, offering a valuable\nsource for predictive analytics. While prior studies integrate medical\nknowledge to improve prediction accuracy, they overlook the opportunity to\nsimultaneously expand such knowledge through predictive processes. We develop a\nClosed-Loop Large Language Model (LLM)-Knowledge Graph framework that\nintegrates prediction and knowledge expansion in an iterative learning cycle.\nIn the knowledge-aware depression detection phase, the LLM jointly performs\ndepression detection and entity extraction, while the knowledge graph\nrepresents and weights these entities to refine prediction performance. In the\nknowledge refinement and expansion phase, new entities, relationships, and\nentity types extracted by the LLM are incorporated into the knowledge graph\nunder expert supervision, enabling continual knowledge evolution. Using\nlarge-scale UGC, the framework enhances both predictive accuracy and medical\nunderstanding. Expert evaluations confirmed the discovery of clinically\nmeaningful symptoms, comorbidities, and social triggers complementary to\nexisting literature. We conceptualize and operationalize\nprediction-through-learning and learning-through-prediction as mutually\nreinforcing processes, advancing both methodological and theoretical\nunderstanding in predictive analytics. The framework demonstrates the\nco-evolution of computational models and domain knowledge, offering a\nfoundation for adaptive, data-driven knowledge systems applicable to other\ndynamic risk monitoring contexts.", "AI": {"tldr": "\u672c\u6587\u5f00\u53d1\u4e86\u4e00\u79cd\u95ed\u73af\u7684\u5927\u89c4\u6a21\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09-\u77e5\u8bc6\u56fe\u8c31\u6846\u67b6\uff0c\u901a\u8fc7\u9884\u6d4b\u548c\u77e5\u8bc6\u6269\u5c55\u7684\u8fed\u4ee3\u5b66\u4e60\u5faa\u73af\u6765\u6539\u5584\u6291\u90c1\u75c7\u68c0\u6d4b\u548c\u533b\u5b66\u77e5\u8bc6\u3002\n", "motivation": "\u73b0\u6709\u7684\u7814\u7a76\u5ffd\u7565\u4e86\u901a\u8fc7\u9884\u6d4b\u8fc7\u7a0b\u540c\u65f6\u6269\u5c55\u533b\u5b66\u77e5\u8bc6\u7684\u673a\u4f1a\u3002\n", "method": "\u8be5\u6846\u67b6\u5305\u542b\u77e5\u8bc6\u9a71\u52a8\u7684\u6291\u90c1\u68c0\u6d4b\u548c\u77e5\u8bc6\u63d0\u70bc\u4e0e\u6269\u5c55\u4e24\u4e2a\u9636\u6bb5\u3002\u5728\u7b2c\u4e00\u4e2a\u9636\u6bb5\uff0cLLM\u8fdb\u884c\u6291\u90c1\u68c0\u6d4b\u5e76\u63d0\u53d6\u5b9e\u4f53\uff0c\u77e5\u8bc6\u56fe\u8c31\u88ab\u7528\u6765\u4ee3\u8868\u548c\u52a0\u6743\u8fd9\u4e9b\u5b9e\u4f53\uff0c\u63d0\u9ad8\u9884\u6d4b\u6027\u80fd\u3002\u5728\u7b2c\u4e8c\u4e2a\u9636\u6bb5\uff0c\u65b0\u7684\u5b9e\u4f53\u3001\u5173\u7cfb\u548c\u5b9e\u4f53\u7c7b\u578b\u5728\u4e13\u5bb6\u76d1\u7763\u4e0b\u5e76\u5165\u77e5\u8bc6\u56fe\u8c31\uff0c\u4f7f\u77e5\u8bc6\u80fd\u591f\u4e0d\u65ad\u8fdb\u5316\u3002\n", "result": "\u5229\u7528\u5927\u89c4\u6a21\u7684UGC\uff0c\u8be5\u6846\u67b6\u63d0\u9ad8\u4e86\u9884\u6d4b\u7684\u51c6\u786e\u6027\u5e76\u6df1\u5316\u4e86\u533b\u5b66\u7406\u89e3\u3002\n", "conclusion": "\u4e13\u5bb6\u8bc4\u4f30\u786e\u8ba4\u4e86\u8be5\u6846\u67b6\u53d1\u73b0\u65b0\u7684\u4e34\u5e8a\u6709\u610f\u4e49\u7684\u75c7\u72b6\u3001\u5171\u75c5\u548c\u793e\u4ea4\u89e6\u53d1\u56e0\u7d20\u3002\u8fd9\u79cd\u9884\u6d4b\u548c\u5b66\u4e60\u7684\u76f8\u4e92\u5f3a\u5316\u8fc7\u7a0b\u65e2\u63a8\u8fdb\u4e86\u65b9\u6cd5\u8bba\u7406\u89e3\u4e5f\u63a8\u8fdb\u4e86\u7406\u8bba\u7406\u89e3\u3002\u8be5\u6846\u67b6\u5c55\u793a\u4e86\u8ba1\u7b97\u6a21\u578b\u4e0e\u9886\u57df\u77e5\u8bc6\u7684\u5171\u540c\u8fdb\u5316\uff0c\u53ef\u4ee5\u5e94\u7528\u4e8e\u5176\u4ed6\u52a8\u6001\u98ce\u9669\u7ba1\u7406\u573a\u5408\u3002\n"}}
{"id": "2510.23807", "categories": ["cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.23807", "abs": "https://arxiv.org/abs/2510.23807", "authors": ["Hamid R. Tizhoosh"], "title": "Why Foundation Models in Pathology Are Failing", "comment": null, "summary": "In non-medical domains, foundation models (FMs) have revolutionized computer\nvision and language processing through large-scale self-supervised and\nmultimodal learning. Consequently, their rapid adoption in computational\npathology was expected to deliver comparable breakthroughs in cancer diagnosis,\nprognostication, and multimodal retrieval. However, recent systematic\nevaluations reveal fundamental weaknesses: low diagnostic accuracy, poor\nrobustness, geometric instability, heavy computational demands, and concerning\nsafety vulnerabilities. This short paper examines these shortcomings and argues\nthat they stem from deeper conceptual mismatches between the assumptions\nunderlying generic foundation modeling in mainstream AI and the intrinsic\ncomplexity of human tissue. Seven interrelated causes are identified:\nbiological complexity, ineffective self-supervision, overgeneralization,\nexcessive architectural complexity, lack of domain-specific innovation,\ninsufficient data, and a fundamental design flaw related to tissue patch size.\nThese findings suggest that current pathology foundation models remain\nconceptually misaligned with the nature of tissue morphology and call for a\nfundamental rethinking of the paradigm itself.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u5728\u8ba1\u7b97\u75c5\u7406\u5b66\u4e2d\u57fa\u7840\u6a21\u578b(FMs)\u7684\u4e0d\u8db3\uff0c\u5e76\u6307\u51fa\u8fd9\u4e9b\u4e0d\u8db3\u6e90\u81ea\u57fa\u7840\u5efa\u6a21\u4e0e\u4eba\u7c7b\u7ec4\u7ec7\u56fa\u6709\u590d\u6742\u6027\u4e4b\u95f4\u7684\u6982\u5ff5\u6027\u4e0d\u5339\u914d\u3002\u6307\u51fa\u4e86\u4e03\u79cd\u4e0e\u57fa\u7840\u6a21\u578b\u5728\u8ba1\u7b97\u75c5\u7406\u5b66\u5e94\u7528\u4e2d\u5931\u6548\u76f8\u5173\u7684\u56e0\u7d20\uff0c\u547c\u5401\u91cd\u65b0\u601d\u8003\u57fa\u7840\u6a21\u578b\u5728\u75c5\u7406\u5b66\u8fd9\u4e2a\u9886\u57df\u7684\u5e94\u7528\u601d\u8def\u3002", "motivation": "\u8ba1\u7b97\u673a\u89c6\u89c9\u548c\u8bed\u8a00\u5904\u7406\u9886\u57df\u4e2d\uff0c\u57fa\u7840\u6a21\u578b\u901a\u8fc7\u5927\u89c4\u6a21\u81ea\u6211\u76d1\u7763\u548c\u591a\u6a21\u6001\u5b66\u4e60\u9769\u65b0\u4e86\u975e\u533b\u5b66\u9886\u57df\uff0c\u4eba\u4eec\u671f\u671b\u8fd9\u79cd\u6a21\u5f0f\u53ef\u4ee5\u7528\u4e8e\u764c\u75c7\u8bca\u65ad\u3001\u9884\u540e\u548c\u591a\u6a21\u6001\u68c0\u7d22\u4e2d\u5e26\u6765\u76f8\u4f3c\u7684\u7a81\u7834\uff0c\u4f46\u662f\u7cfb\u7edf\u8bc4\u4f30\u663e\u793a\uff0c\u57fa\u7840\u6a21\u578b\u5728\u533b\u5b66\u9886\u57df\u5b58\u5728\u660e\u663e\u7684\u4e0d\u8db3\uff0c\u672c\u6587\u6b32\u63a2\u8ba8\u5176\u539f\u56e0\u3002", "method": "\u901a\u8fc7\u7cfb\u7edf\u8bc4\u4f30\u8bc6\u522b\u8ba1\u7b97\u75c5\u7406\u5b66\u4e2d\u57fa\u7840\u6a21\u578b\u7684\u4e0d\u8db3\uff0c\u6df1\u5165\u63a2\u8ba8\u8fd9\u79cd\u6a21\u5f0f\u5728\u533b\u5b66\u9886\u57df\u7684\u5e94\u7528\u6311\u6218\uff0c\u8bc6\u522b\u57fa\u7840\u6a21\u578b\u5728\u533b\u5b66\u5e94\u7528\u4e2d\u7684\u6982\u5ff5\u6027\u4e0d\u5339\u914d\u3002", "result": "\u672c\u6587\u8bc6\u522b\u4e86\u8ba1\u7b97\u75c5\u7406\u5b66\u4e2d\u57fa\u7840\u6a21\u578b\u7684\u4e03\u79cd\u5931\u6548\u56e0\u7d20\uff0c\u5305\u62ec\u751f\u7269\u590d\u6742\u6027\u3001\u81ea\u6211\u76d1\u7763\u65e0\u5173\u7d27\u8981\u3001\u8fc7\u5ea6\u63a8\u5e7f\u7b49\u3002\u4e3b\u8981\u7ed3\u8bba\u662f\u57fa\u7840\u6a21\u578b\u4e0e\u75c5\u7406\u5b66\u4e4b\u95f4\u7684\u5339\u914d\u5ea6\u4e0d\u8db3\u3002", "conclusion": "\u5f53\u524d\u75c5\u7406\u5b66\u57fa\u7840\u6a21\u578b\u5728\u6982\u5ff5\u4e0a\u4e0e\u7ec4\u7ec7\u5f62\u6001\u5b66\u7684\u672c\u8d28\u7279\u6027\u4e0d\u4e00\u81f4\uff0c\u547c\u5401\u5bf9\u6b64\u8fdb\u884c\u6839\u672c\u6027\u7684\u53cd\u601d\u3002"}}
{"id": "2510.23629", "categories": ["cs.LG", "cs.AI", "cs.PL"], "pdf": "https://arxiv.org/pdf/2510.23629", "abs": "https://arxiv.org/abs/2510.23629", "authors": ["Nuo Chen", "Zehua Li", "Keqin Bao", "Junyang Lin", "Dayiheng Liu"], "title": "Chain of Execution Supervision Promotes General Reasoning in Large Language Models", "comment": null, "summary": "Building robust and general reasoning ability is a central goal in the\ndevelopment of large language models (LLMs). Recent efforts increasingly turn\nto code as a rich training source, given its inherent logical structure and\ndiverse reasoning paradigms such as divide-and-conquer, topological ordering,\nand enumeration. However, reasoning in code is often expressed implicitly and\nentangled with syntactic or implementation noise, making direct training on raw\ncode suboptimal.To address this, we introduce TracePile, a large-scale corpus\nof 2.6 million samples that transforms code execution into explicit,\nstep-by-step chain-of-thought-style rationales, which we call Chain of\nExecution (CoE). The corpus spans domains including mathematics, classical\nalgorithms and algorithmic competition, and is enriched with variable-tracing\nquestions and code rewritings to enhance logical granularity and code\ndiversity. We evaluate TracePile using three training setups:\ncontinue-pretraining, instruction tuning after pretraining, and two-stage\nfinetuning. Experiments across four base models (LLaMA 3, LLaMA 3.1, Qwen-2.5,\nand Qwen-2.5 Coder) and 20 benchmarks covering math, code, logic, and\nalgorithms demonstrate consistent improvements. Notably, TracePile boosts\nLLaMA3.1-8B by 7.1\\% on average across nine math datasets and delivers clear\ngains on LiveCodeBench, CRUX, and MMLU under two-stage fine-tuning.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86TracePile\uff0c\u4e00\u4e2a\u5c06\u4ee3\u7801\u6267\u884c\u8f6c\u5316\u4e3a\u660e\u786e\u7684\u3001\u9010\u6b65\u7684\u63a8\u7406\u94fe\u7684\u5927\u578b\u6570\u636e\u96c6\uff0c\u4ee5\u89e3\u51b3\u4ee3\u7801\u63a8\u7406\u4e2d\u9690\u5f0f\u8868\u8fbe\u548c\u5b9e\u65bd\u566a\u97f3\u5e26\u6765\u7684\u6311\u6218\u3002\u5b9e\u9a8c\u8868\u660e\uff0cTracePile\u80fd\u663e\u8457\u63d0\u5347\u6a21\u578b\u5728\u6570\u5b66\u3001\u4ee3\u7801\u3001\u903b\u8f91\u548c\u7b97\u6cd5\u7b49\u65b9\u9762\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u5927\u578b\u8bed\u8a00\u6a21\u578b\u901a\u8fc7\u4ee3\u7801\u8bad\u7ec3\u903b\u8f91\u63a8\u7406\u80fd\u529b\u65f6\uff0c\u9047\u5230\u4ee3\u7801\u63a8\u7406\u8868\u8fbe\u9690\u5f0f\u53ca\u4e0e\u8bed\u6cd5\u6216\u5b9e\u73b0\u566a\u97f3\u6df7\u6742\u7684\u95ee\u9898\uff0c\u76f4\u63a5\u5728\u539f\u59cb\u4ee3\u7801\u4e0a\u8bad\u7ec3\u6548\u679c\u4e0d\u4f73\uff0c\u56e0\u6b64\u63d0\u51fa\u4e86TracePile\u6765\u6539\u5584\u8fd9\u4e00\u72b6\u51b5\u3002", "method": "\u6784\u5efa\u4e86\u4e00\u4e2a\u547d\u540d\u4e3aTracePile\u7684\u5927\u89c4\u6a21\u6570\u636e\u96c6\uff0c\u8be5\u6570\u636e\u96c6\u5c06\u4ee3\u7801\u6267\u884c\u8f6c\u5316\u4e3a\u663e\u5f0f\u7684\u3001\u9010\u6b65\u7684\u63a8\u7406\u94fe\uff0c\u79f0\u4e3a\u6267\u884c\u94fe\uff08CoE\uff09\uff0c\u5e76\u4f7f\u7528\u4ece\u7ee7\u7eed\u9884\u8bad\u7ec3\u5230\u6307\u4ee4\u5fae\u8c03\u7684\u4e0d\u540c\u8bad\u7ec3\u8bbe\u7f6e\u8bc4\u4f30\u5176\u6548\u679c\u3002", "result": "\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\uff0cTracePile\u80fd\u663e\u8457\u63d0\u5347\u6a21\u578b\u5728\u6570\u5b66\u3001\u4ee3\u7801\u3001\u903b\u8f91\u548c\u7b97\u6cd5\u7b49\u9886\u57df\u7684\u8868\u73b0\uff0c\u7279\u522b\u662f\u5728\u6570\u5b66\u9886\u57df\u5b9e\u73b0\u4e867.1%\u7684\u5e73\u5747\u6027\u80fd\u63d0\u5347\u3002", "conclusion": "TracePile\u901a\u8fc7\u5c06\u4ee3\u7801\u6267\u884c\u8f6c\u5316\u4e3a\u63a8\u7406\u94fe\uff0c\u89e3\u51b3\u4e86\u4ee3\u7801\u63a8\u7406\u7684\u9690\u5f0f\u8868\u8fbe\u548c\u5b9e\u65bd\u566a\u97f3\u95ee\u9898\uff0c\u663e\u8457\u589e\u5f3a\u4e86\u6a21\u578b\u7684\u903b\u8f91\u63a8\u7406\u80fd\u529b\u3002"}}
{"id": "2510.23877", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2510.23877", "abs": "https://arxiv.org/abs/2510.23877", "authors": ["Zhentong Shao", "Nanpeng Yu"], "title": "Carbon-Aware Optimal Power Flow with Data-Driven Carbon Emission Tracing", "comment": null, "summary": "Quantifying locational carbon emissions in power grids is crucial for\nimplementing effective carbon reduction strategies for customers relying on\nelectricity. This paper presents a carbon-aware optimal power flow (OPF)\nframework that incorporates data-driven carbon tracing, enabling rapid\nestimation of nodal carbon emissions from electric loads. By developing\ngenerator-to-load carbon emission distribution factors through data-driven\ntechnique, the analytical formulas for both average and marginal carbon\nemissions can be derived and integrated seamlessly into DC OPF models as linear\nconstraints. The proposed carbon-aware OPF model enables market operators to\noptimize energy dispatch while reducing greenhouse gas emissions. Simulations\non IEEE test systems confirm the accuracy and computational efficiency of the\nproposed approach, highlighting its applicability for real-time carbon-aware\nsystem operations.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8003\u8651\u78b3\u6392\u653e\u7684\u6700\u4f18\u6f6e\u6d41\uff08OPF\uff09\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u901a\u8fc7\u6570\u636e\u9a71\u52a8\u7684\u78b3\u8ffd\u8e2a\u6280\u672f\uff0c\u80fd\u591f\u5feb\u901f\u4f30\u8ba1\u7535\u529b\u8d1f\u8377\u8282\u70b9\u7684\u78b3\u6392\u653e\u91cf\u3002\u901a\u8fc7\u5f00\u53d1\u53d1\u7535\u673a\u5230\u8d1f\u8377\u7684\u78b3\u6392\u653e\u5206\u914d\u7cfb\u6570\uff0c\u5c06\u5e73\u5747\u548c\u8fb9\u9645\u78b3\u6392\u653e\u7684\u516c\u5f0f\u96c6\u6210\u5230\u76f4\u6d41OPF\u6a21\u578b\u4e2d\u4f5c\u4e3a\u7ebf\u6027\u7ea6\u675f\u3002\u6240\u63d0\u51fa\u7684\u78b3\u611f\u77e5OPF\u6a21\u578b\u5141\u8bb8\u5e02\u573a\u8fd0\u8425\u5546\u5728\u4f18\u5316\u80fd\u6e90\u8c03\u5ea6\u7684\u540c\u65f6\u51cf\u5c11\u6e29\u5ba4\u6c14\u4f53\u6392\u653e\u3002\u901a\u8fc7\u5bf9IEEE\u6d4b\u8bd5\u7cfb\u7edf\u7684\u6a21\u62df\uff0c\u8bc1\u5b9e\u4e86\u8be5\u65b9\u6cd5\u7684\u51c6\u786e\u6027\u548c\u8ba1\u7b97\u6548\u7387\uff0c\u7a81\u663e\u4e86\u5176\u5b9e\u65f6\u78b3\u611f\u77e5\u7cfb\u7edf\u64cd\u4f5c\u7684\u5e94\u7528\u6f5c\u529b\u3002", "motivation": "\u91cf\u5316\u7535\u7f51\u4e2d\u7684\u4f4d\u7f6e\u78b3\u6392\u653e\u5bf9\u4e8e\u5b9e\u65bd\u6709\u6548\u7684\u51cf\u6392\u7b56\u7565\u4ee5\u670d\u52a1\u4e8e\u4f9d\u8d56\u7535\u529b\u7684\u5ba2\u6237\u81f3\u5173\u91cd\u8981\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u4e2a\u80fd\u591f\u5feb\u901f\u51c6\u786e\u4f30\u8ba1\u7535\u529b\u8d1f\u8377\u9020\u6210\u7684\u78b3\u6392\u653e\u91cf\u7684\u6846\u67b6\u3002\u901a\u8fc7\u672c\u7814\u7a76\uff0c\u53ef\u4ee5\u5b9e\u73b0\u78b3\u6392\u653e\u7684\u7cbe\u786e\u8ffd\u8e2a\u548c\u4f18\u5316\u8c03\u5ea6\uff0c\u4ece\u800c\u51cf\u5c11\u6e29\u5ba4\u6c14\u4f53\u6392\u653e\uff0c\u4fc3\u8fdb\u7eff\u8272\u80fd\u6e90\u7684\u4f7f\u7528\u3002", "method": "\u8be5\u65b9\u6cd5\u57fa\u4e8e\u6570\u636e\u9a71\u52a8\u7684\u78b3\u8ffd\u8e2a\u6280\u672f\uff0c\u901a\u8fc7\u751f\u6210\u53d1\u7535\u673a\u5230\u8d1f\u8377\u7684\u78b3\u6392\u653e\u5206\u5e03\u56e0\u5b50\uff0c\u63d0\u51fa\u4e86\u4e00\u4e2a\u4e0eDC OPF\u6a21\u578b\u517c\u5bb9\u7684\u96c6\u6210\u6846\u67b6\u3002\u8be5\u6846\u67b6\u80fd\u591f\u5728\u4f18\u5316\u80fd\u6e90\u8c03\u5ea6\u65f6\uff0c\u540c\u65f6\u5c06\u78b3\u6392\u653e\u7684\u7ea6\u675f\u6761\u4ef6\u8003\u8651\u8fdb\u53bb\u3002", "result": "\u901a\u8fc7\u5728IEEE\u6d4b\u8bd5\u7cfb\u7edf\u4e0a\u7684\u6a21\u62df\uff0c\u8be5\u65b9\u6cd5\u8bc1\u660e\u4e86\u5176\u5728\u89e3\u51b3\u78b3\u6392\u653e\u8ffd\u8e2a\u4e0e\u4f18\u5316\u8c03\u5ea6\u95ee\u9898\u4e2d\u7684\u9ad8\u6548\u6027\u548c\u51c6\u786e\u6027\u3002\u6a21\u62df\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u591f\u51c6\u786e\u5730\u8ba1\u7b97\u78b3\u6392\u653e\u91cf\uff0c\u5e76\u4e14\u8ba1\u7b97\u6548\u7387\u9ad8\uff0c\u9002\u7528\u4e8e\u5b9e\u65f6\u7684\u78b3\u611f\u77e5\u7cfb\u7edf\u64cd\u4f5c\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u78b3\u611f\u77e5OPF\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c06\u78b3\u6392\u653e\u6570\u636e\u6574\u5408\u5230\u73b0\u6709\u7684\u4f18\u5316\u8c03\u5ea6\u6846\u67b6\u4e2d\uff0c\u4f7f\u5f97\u5e02\u573a\u8fd0\u8425\u8005\u53ef\u4ee5\u5728\u51cf\u5c11\u78b3\u6392\u653e\u7684\u540c\u65f6\u4f18\u5316\u80fd\u6e90\u8c03\u5ea6\uff0c\u6709\u52a9\u4e8e\u5b9e\u73b0\u66f4\u4f4e\u7684\u78b3\u8db3\u8ff9\u548c\u66f4\u53ef\u6301\u7eed\u7684\u80fd\u6e90\u4f7f\u7528\u3002"}}
{"id": "2510.23907", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.23907", "abs": "https://arxiv.org/abs/2510.23907", "authors": ["Eddison Pham", "Prisha Priyadarshini", "Adrian Maliackel", "Kanishk Bandi", "Cristian Meo", "Kevin Zhu"], "title": "DynaStride: Dynamic Stride Windowing with MMCoT for Instructional Multi-Scene Captioning", "comment": "16 pages, 15 figures, 5 Tables, submitted to AAAI AI4ED Workshop 2026", "summary": "Scene-level captioning in instructional videos can enhance learning by\nrequiring an understanding of both visual cues and temporal structure. By\naligning visual cues with textual guidance, this understanding supports\nprocedural learning and multimodal reasoning, providing a richer context for\nskill acquisition. However, captions that fail to capture this structure may\nlack coherence and quality, which can create confusion and undermine the\nvideo's educational intent. To address this gap, we introduce DynaStride, a\npipeline to generate coherent, scene-level captions without requiring manual\nscene segmentation. Using the YouCookII dataset's scene annotations, DynaStride\nperforms adaptive frame sampling and multimodal windowing to capture key\ntransitions within each scene. It then employs a multimodal chain-of-thought\nprocess to produce multiple action-object pairs, which are refined and fused\nusing a dynamic stride window selection algorithm that adaptively balances\ntemporal context and redundancy. The final scene-level caption integrates\nvisual semantics and temporal reasoning in a single instructional caption.\nEmpirical evaluations against strong baselines, including VLLaMA3 and GPT-4o,\ndemonstrate consistent gains on both N-gram-based metrics (BLEU, METEOR) and\nsemantic similarity measures (BERTScore, CLIPScore). Qualitative analyses\nfurther show that DynaStride produces captions that are more temporally\ncoherent and informative, suggesting a promising direction for improving\nAI-powered instructional content generation.", "AI": {"tldr": "DynaStride\u662f\u4e00\u79cd\u751f\u6210\u8fde\u8d2f\u7684\u573a\u666f\u7ea7\u5b57\u5e55\u7684\u6d41\u6c34\u7ebf\uff0c\u65e0\u9700\u624b\u52a8\u573a\u666f\u5206\u5272\uff0c\u9002\u7528\u4e8e\u6559\u5b66\u89c6\u9891\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u5e27\u91c7\u6837\u3001\u591a\u6a21\u6001\u7a97\u53e3\u548c\u52a8\u6001\u6b65\u5e45\u7a97\u53e3\u9009\u62e9\u7b97\u6cd5\uff0c\u66f4\u597d\u5730\u6355\u6349\u89c6\u89c9\u8bed\u4e49\u548c\u65f6\u95f4\u63a8\u7406\uff0c\u751f\u6210\u9ad8\u8d28\u91cf\u7684\u5b57\u5e55\u3002", "motivation": "\u5f53\u524d\u7684\u6559\u5b66\u89c6\u9891\u5b57\u5e55\u7531\u4e8e\u7f3a\u4e4f\u7ed3\u6784\u800c\u53ef\u80fd\u7f3a\u4e4f\u8fde\u8d2f\u6027\u548c\u8d28\u91cf\uff0c\u9020\u6210\u6df7\u6dc6\uff0c\u7834\u574f\u89c6\u9891\u7684\u6559\u80b2\u76ee\u7684\u3002\u73b0\u6709\u6280\u672f\u9700\u8981\u624b\u52a8\u573a\u666f\u5206\u5272\uff0c\u8fd9\u65e2\u8017\u65f6\u53c8\u4e0d\u5207\u5b9e\u9645\u3002\u56e0\u6b64\uff0c\u5f15\u5165DynaStride\u6765\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u751f\u6210\u8fde\u8d2f\u7684\u573a\u666f\u7ea7\u5b57\u5e55\uff0c\u4ee5\u6539\u8fdbAI\u751f\u6210\u6559\u5b66\u5185\u5bb9\u7684\u80fd\u529b\u3002", "method": "DynaStride\u91c7\u7528\u81ea\u9002\u5e94\u5e27\u91c7\u6837\u548c\u591a\u6a21\u6001\u7a97\u53e3\u6765\u6355\u83b7\u6bcf\u4e2a\u573a\u666f\u7684\u5173\u952e\u8fc7\u6e21\uff0c\u5229\u7528\u591a\u6a21\u6001\u94fe\u5f0f\u601d\u7ef4\u8fc7\u7a0b\u751f\u6210\u591a\u4e2a\u52a8\u4f5c-\u5bf9\u8c61\u5bf9\uff0c\u7136\u540e\u901a\u8fc7\u52a8\u6001\u6b65\u5e45\u7a97\u53e3\u9009\u62e9\u7b97\u6cd5\u8fdb\u884c\u7ec6\u5316\u548c\u878d\u5408\uff0c\u4ea7\u751f\u6700\u540e\u7684\u573a\u666f\u7ea7\u6807\u9898\u3002", "result": "DynaStride\u5728\u57fa\u4e8en-gram\u7684\u6307\u6807\uff08\u5982BLEU\uff0cMETEOR\uff09\u548c\u8bed\u4e49\u76f8\u4f3c\u6027\u5ea6\u91cf\uff08\u5982BERTScore\uff0cCLIPScore\uff09\u4e0a\u90fd\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff08\u5982VLLaMA3\u548cGPT-4o\uff09\uff0c\u663e\u793a\u4e86\u5728\u63d0\u9ad8AI\u751f\u6210\u7684\u6559\u5b66\u5185\u5bb9\u8d28\u91cf\u65b9\u9762\u7684\u6f5c\u529b\u3002", "conclusion": "DynaStride\u751f\u6210\u7684\u5b57\u5e55\u5728\u65f6\u95f4\u548c\u4fe1\u606f\u65b9\u9762\u66f4\u4e3a\u8fde\u8d2f\uff0c\u63d0\u793a\u4e86\u4e00\u79cd\u6539\u8fdbAI\u751f\u6210\u6559\u5b66\u5185\u5bb9\u7684\u65b9\u6cd5\uff0c\u5177\u6709\u5f3a\u5927\u7684\u6f5c\u529b\u3002"}}
{"id": "2510.23822", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.23822", "abs": "https://arxiv.org/abs/2510.23822", "authors": ["Zhenyu Zhang", "Tianyi Chen", "Weiran Xu", "Alex Pentland", "Jiaxin Pei"], "title": "ReCAP: Recursive Context-Aware Reasoning and Planning for Large Language Model Agents", "comment": null, "summary": "Long-horizon tasks requiring multi-step reasoning and dynamic re-planning\nremain challenging for large language models (LLMs). Sequential prompting\nmethods are prone to context drift, loss of goal information, and recurrent\nfailure cycles, while hierarchical prompting methods often weaken cross-level\ncontinuity or incur substantial runtime overhead. We introduce ReCAP (Recursive\nContext-Aware Reasoning and Planning), a hierarchical framework with shared\ncontext for reasoning and planning in LLMs. ReCAP combines three key\nmechanisms: (i) plan-ahead decomposition, in which the model generates a full\nsubtask list, executes the first item, and refines the remainder; (ii)\nstructured re-injection of parent plans, maintaining consistent multi-level\ncontext during recursive return; and (iii) memory-efficient execution, bounding\nthe active prompt so costs scale linearly with task depth. Together these\nmechanisms align high-level goals with low-level actions, reduce redundant\nprompting, and preserve coherent context updates across recursion. Experiments\ndemonstrate that ReCAP substantially improves subgoal alignment and success\nrates on various long-horizon reasoning benchmarks, achieving a 32% gain on\nsynchronous Robotouille and a 29% improvement on asynchronous Robotouille under\nthe strict pass@1 protocol.", "AI": {"tldr": "ReCAP\u662f\u4e00\u79cd\u65b0\u7684\u5206\u5c42\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408\u8ba1\u5212\u5206\u89e3\u3001\u4fdd\u6301\u4e0a\u4e0b\u6587\u4e00\u81f4\u6027\u4ee5\u53ca\u51cf\u5c11\u5197\u4f59\u63d0\u793a\u7684\u673a\u5236\uff0c\u63d0\u9ad8\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u957f\u65f6\u95f4\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u3002\u5b9e\u9a8c\u663e\u793a\uff0c\u4e0e\u65e7\u65b9\u6cd5\u76f8\u6bd4\uff0cReCAP\u5728\u4e0d\u540c\u57fa\u51c6\u4e0a\u5b9e\u73b0\u4e86\u663e\u8457\u7684\u6539\u8fdb\uff0c\u7279\u522b\u662f\u5728\u540c\u6b65Robotouille\u4e2d\u5b9e\u73b0\u4e8632%\u7684\u6539\u8fdb\uff0c\u5728\u5f02\u6b65Robotouille\u4e2d\u5b9e\u73b0\u4e8629%\u7684\u6539\u8fdb\u3002", "motivation": "\u73b0\u6709\u7684\u65b9\u6cd5\u5b58\u5728\u4e0a\u4e0b\u6587\u6f02\u79fb\u3001\u76ee\u6807\u4fe1\u606f\u4e22\u5931\u4ee5\u53ca\u53cd\u590d\u5931\u8d25\u7b49\u6311\u6218\uff0c\u65e0\u6cd5\u6709\u6548\u5730\u5b9e\u73b0\u591a\u6b65\u9aa4\u63a8\u7406\u548c\u52a8\u6001\u91cd\u89c4\u5212\u7684\u4efb\u52a1\u3002\u4f5c\u8005\u56e0\u6b64\u63d0\u51fa\u4e86ReCAP\u6846\u67b6\uff0c\u65e8\u5728\u63d0\u9ad8\u5927\u8bed\u8a00\u6a21\u578b\u5904\u7406\u6b64\u7c7b\u4efb\u52a1\u7684\u80fd\u529b\uff0c\u4fdd\u6301\u591a\u7ea7\u4e0a\u4e0b\u6587\u7684\u4e00\u81f4\u6027\u548c\u51cf\u5c11\u5197\u4f59\u63d0\u793a\u3002", "method": "ReCAP\u6846\u67b6\u5229\u7528\u8ba1\u5212\u5206\u89e3\u3001\u5c42\u6b21\u4e0a\u4e0b\u6587\u91cd\u65b0\u6ce8\u5165\u548c\u5185\u5b58\u6548\u7387\u6267\u884c\u4e09\u79cd\u673a\u5236\uff0c\u8fd9\u4e9b\u673a\u5236\u51cf\u5c11\u4e86\u5197\u4f59\u63d0\u793a\uff0c\u4fdd\u6301\u4e86\u4e0a\u4e0b\u6587\u66f4\u65b0\u7684\u4e00\u81f4\u6027\uff0c\u5e76\u964d\u4f4e\u4e86\u4efb\u52a1\u6df1\u5ea6\u589e\u52a0\u7684\u6210\u672c\uff0c\u4ece\u800c\u63d0\u9ad8\u4e86\u957f\u65f6\u95f4\u4efb\u52a1\u4e2d\u5b50\u76ee\u6807\u7684\u4e00\u81f4\u6027\u548c\u6210\u529f\u7387\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0cReCAP\u76f8\u5bf9\u4e8e\u4ee5\u524d\u7684\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u5b50\u76ee\u6807\u7684\u4e00\u81f4\u6027\u548c\u6210\u529f\u7387\uff0c\u5728\u540c\u6b65\u548c\u5f02\u6b65\u7684Robotouille\u57fa\u51c6\u4e0a\u5206\u522b\u5b9e\u73b0\u4e8632%\u548c29%\u7684\u6539\u8fdb\u3002\u8fd9\u4e9b\u7ed3\u679c\u8bc1\u660e\u4e86ReCAP\u6846\u67b6\u5728\u5904\u7406\u591a\u7ea7\u4efb\u52a1\u548c\u987a\u5e8f\u4efb\u52a1\u65f6\u7684\u6709\u6548\u6027\u3002", "conclusion": "ReCAP\u901a\u8fc7\u6709\u6548\u7ed3\u5408\u8ba1\u5212\u5206\u89e3\u548c\u91cd\u65b0\u6ce8\u5165\u4e0a\u4e0b\u6587\uff0c\u63d0\u4f9b\u4e86\u4e00\u79cd\u65b0\u7684\u65b9\u6cd5\u6765\u5904\u7406\u957f\u65f6\u4efb\u52a1\uff0c\u663e\u793a\u51fa\u5f3a\u5927\u7684\u6027\u80fd\u63d0\u5347\u3002\u8fd9\u79cd\u65b9\u6cd5\u4e3a\u5904\u7406\u90a3\u4e9b\u9700\u8981\u5bc6\u96c6\u591a\u9636\u6bb5\u7684\u63a8\u7406\u548c\u8ba1\u5212\u7684\u4efb\u52a1\u63d0\u4f9b\u4e86\u4e00\u79cd\u65b0\u7684\u6709\u6548\u7684\u5904\u7406\u65b9\u5f0f\u3002"}}
{"id": "2510.23895", "categories": ["eess.SY", "cs.OS", "cs.RO", "cs.SY"], "pdf": "https://arxiv.org/pdf/2510.23895", "abs": "https://arxiv.org/abs/2510.23895", "authors": ["Hoora Sobhani", "Hyoseung Kim"], "title": "Modeling and Scheduling of Fusion Patterns in Autonomous Driving Systems (Extended Version)", "comment": null, "summary": "In Autonomous Driving Systems (ADS), Directed Acyclic Graphs (DAGs) are\nwidely used to model complex data dependencies and inter-task communication.\nHowever, existing DAG scheduling approaches oversimplify data fusion tasks by\nassuming fixed triggering mechanisms, failing to capture the diverse fusion\npatterns found in real-world ADS software stacks. In this paper, we propose a\nsystematic framework for analyzing various fusion patterns and their\nperformance implications in ADS. Our framework models three distinct fusion\ntask types: timer-triggered, wait-for-all, and immediate fusion, which\ncomprehensively represent real-world fusion behaviors. Our Integer Linear\nProgramming (ILP)-based approach enables an optimization of multiple real-time\nperformance metrics, including reaction time, time disparity, age of\ninformation, and response time, while generating deterministic offline\nschedules directly applicable to real platforms. Evaluation using real-world\nADS case studies, Raspberry Pi implementation, and randomly generated DAGs\ndemonstrates that our framework handles diverse fusion patterns beyond the\nscope of existing work, and achieves substantial performance improvements in\ncomparable scenarios.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u4e2d\u6570\u636e\u878d\u5408\u4efb\u52a1\u7684\u7cfb\u7edf\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u80fd\u591f\u5206\u6790\u548c\u4f18\u5316\u591a\u79cd\u6570\u636e\u878d\u5408\u6a21\u5f0f\u7684\u6027\u80fd\u3002\u4f7f\u7528\u6574\u6570\u7ebf\u6027\u89c4\u5212\u65b9\u6cd5\u4f18\u5316\u5b9e\u65f6\u6027\u80fd\u6307\u6807\uff0c\u5e76\u751f\u6210\u53ef\u4ee5\u76f4\u63a5\u5e94\u7528\u4e8e\u771f\u5b9e\u5e73\u53f0\u7684\u786e\u5b9a\u6027\u79bb\u7ebf\u8c03\u5ea6\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u6846\u67b6\u8d85\u8d8a\u4e86\u73b0\u6709\u5de5\u4f5c\u7684\u8303\u56f4\uff0c\u5b9e\u73b0\u4e86\u663e\u8457\u7684\u6027\u80fd\u6539\u8fdb\u3002", "motivation": "\u73b0\u6709\u6570\u636e\u878d\u5408\u4efb\u52a1\u7684\u8c03\u5ea6\u65b9\u6cd5\u8fc7\u4e8e\u7b80\u5316\uff0c\u65e0\u6cd5\u6355\u6349\u5230\u5b9e\u9645\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u4e2d\u6570\u636e\u878d\u5408\u7684\u591a\u6837\u6027\u6a21\u5f0f\u3002\u4ece\u800c\u5bfc\u81f4\u4e86\u5bf9\u4e8e\u771f\u5b9e\u5e94\u7528\u573a\u666f\u7684\u6027\u80fd\u4f18\u5316\u4e0d\u8db3\u3002\u56e0\u6b64\uff0c\u6211\u4eec\u9700\u8981\u4e00\u79cd\u80fd\u591f\u5168\u9762\u5206\u6790\u5e76\u4f18\u5316\u8fd9\u4e9b\u6a21\u5f0f\u7684\u6846\u67b6\u3002", "method": "\u91c7\u7528\u4e86\u6574\u6570\u7ebf\u6027\u89c4\u5212\u65b9\u6cd5\uff08ILP\uff09\u6765\u5904\u7406\u4e0d\u540c\u7c7b\u578b\u7684\u878d\u5408\u4efb\u52a1\uff0c\u5305\u62ec\u57fa\u4e8e\u5b9a\u65f6\u89e6\u53d1\u3001\u7b49\u5f85\u6240\u6709\u4efb\u52a1\u5b8c\u6210\u548c\u5373\u65f6\u878d\u5408\u3002\u8be5\u65b9\u6cd5\u53ef\u4ee5\u4f18\u5316\u591a\u4e2a\u5b9e\u65f6\u6027\u80fd\u6307\u6807\uff0c\u540c\u65f6\u751f\u6210\u53ef\u4ee5\u76f4\u63a5\u7528\u4e8e\u771f\u5b9e\u5e73\u53f0\u7684\u79bb\u7ebf\u8c03\u5ea6\u3002", "result": "\u76f8\u6bd4\u4e8e\u73b0\u6709\u5de5\u4f5c\uff0c\u8be5\u6846\u67b6\u80fd\u591f\u5904\u7406\u66f4\u590d\u6742\u7684\u878d\u5408\u6a21\u5f0f\uff0c\u5b9e\u73b0\u4e86\u66f4\u4f73\u7684\u6027\u80fd\u8868\u73b0\u3002\u901a\u8fc7\u771f\u5b9e\u6848\u4f8b\u548c\u968f\u673a\u751f\u6210\u7684 DAG \u6570\u636e\u96c6\u6d4b\u8bd5\uff0c\u8be5\u65b9\u6cd5\u5728\u53cd\u5e94\u65f6\u95f4\u3001\u65f6\u95f4\u5dee\u8ddd\u3001\u4fe1\u606f\u5e74\u9f84\u548c\u54cd\u5e94\u65f6\u95f4\u7b49\u6307\u6807\u4e0a\u53d6\u5f97\u4e86\u663e\u8457\u63d0\u5347\u3002", "conclusion": "\u8fd9\u4e00\u6846\u67b6\u4e3a\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u4e2d\u7684\u6570\u636e\u878d\u5408\u4efb\u52a1\u63d0\u4f9b\u4e86\u4e00\u79cd\u65b0\u7684\u4f18\u5316\u65b9\u6cd5\uff0c\u80fd\u591f\u66f4\u771f\u5b9e\u5730\u53cd\u6620\u548c\u4f18\u5316\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u6027\u80fd\u3002"}}
{"id": "2510.23929", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.23929", "abs": "https://arxiv.org/abs/2510.23929", "authors": ["Emily Kim", "Julieta Martinez", "Timur Bagautdinov", "Jessica Hodgins"], "title": "TurboPortrait3D: Single-step diffusion-based fast portrait novel-view synthesis", "comment": null, "summary": "We introduce TurboPortrait3D: a method for low-latency novel-view synthesis\nof human portraits. Our approach builds on the observation that existing\nimage-to-3D models for portrait generation, while capable of producing\nrenderable 3D representations, are prone to visual artifacts, often lack of\ndetail, and tend to fail at fully preserving the identity of the subject. On\nthe other hand, image diffusion models excel at generating high-quality images,\nbut besides being computationally expensive, are not grounded in 3D and thus\nare not directly capable of producing multi-view consistent outputs. In this\nwork, we demonstrate that image-space diffusion models can be used to\nsignificantly enhance the quality of existing image-to-avatar methods, while\nmaintaining 3D-awareness and running with low-latency. Our method takes a\nsingle frontal image of a subject as input, and applies a feedforward\nimage-to-avatar generation pipeline to obtain an initial 3D representation and\ncorresponding noisy renders. These noisy renders are then fed to a single-step\ndiffusion model which is conditioned on input image(s), and is specifically\ntrained to refine the renders in a multi-view consistent way. Moreover, we\nintroduce a novel effective training strategy that includes pre-training on a\nlarge corpus of synthetic multi-view data, followed by fine-tuning on\nhigh-quality real images. We demonstrate that our approach both qualitatively\nand quantitatively outperforms current state-of-the-art for portrait novel-view\nsynthesis, while being efficient in time.", "AI": {"tldr": "\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b9\u6cd5TurboPortrait3D\uff0c\u7528\u4e8e\u751f\u6210\u9ad8\u8d28\u91cf\u3001\u4f4e\u5ef6\u8fdf\u7684\u4eba\u8138\u591a\u89c6\u89d2\u5408\u6210\u56fe\u50cf\uff0c\u8be5\u65b9\u6cd5\u7ed3\u5408\u4e86\u56fe\u50cf\u6269\u6563\u6a21\u578b\u548c\u56fe\u50cf\u52303D\u6a21\u578b\u7684\u4f18\u70b9\uff0c\u65e2\u4fdd\u6301\u4e863D\u89c6\u89d2\u7684\u4e00\u81f4\u6027\uff0c\u53c8\u63d0\u9ad8\u4e86\u751f\u6210\u56fe\u50cf\u7684\u8d28\u91cf\u548c\u7ec6\u8282", "motivation": "\u73b0\u6709\u7684\u56fe\u50cf\u52303D\u6a21\u578b\u5728\u751f\u6210\u53ef\u6e32\u67d3\u76843D\u8868\u793a\u65f6\u5bb9\u6613\u4ea7\u751f\u89c6\u89c9\u4f2a\u5f71\uff0c\u7f3a\u4e4f\u7ec6\u8282\uff0c\u5e76\u4e14\u65e0\u6cd5\u5b8c\u5168\u4fdd\u7559\u5bf9\u8c61\u7684\u8eab\u4efd\u3002\u56fe\u50cf\u6269\u6563\u6a21\u578b\u5728\u751f\u6210\u9ad8\u8d28\u91cf\u56fe\u50cf\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u8ba1\u7b97\u6210\u672c\u9ad8\u4e14\u4e0d\u5177\u67093D\u57fa\u7840\uff0c\u4e0d\u80fd\u76f4\u63a5\u751f\u6210\u591a\u89c6\u89d2\u4e00\u81f4\u7684\u8f93\u51fa", "method": "\u8be5\u65b9\u6cd5\u9996\u5148\u901a\u8fc7\u56fe\u50cf\u52303D\u6a21\u578b\u751f\u6210\u521d\u59cb3D\u8868\u793a\u548c\u76f8\u5e94\u7684\u566a\u58f0\u6e32\u67d3\uff0c\u7136\u540e\u5c06\u8fd9\u4e9b\u566a\u58f0\u6e32\u67d3\u8f93\u5165\u5230\u5355\u6b65\u6269\u6563\u6a21\u578b\u8fdb\u884c\u7ec6\u5316\uff0c\u8be5\u6a21\u578b\u6839\u636e\u8f93\u5165\u56fe\u50cf\u8fdb\u884c\u6761\u4ef6\u8bad\u7ec3\uff0c\u4ee5\u5b9e\u73b0\u591a\u89c6\u89d2\u4e00\u81f4\u6027\u3002\u6b64\u5916\uff0c\u8be5\u65b9\u6cd5\u5f15\u5165\u4e86\u4e00\u79cd\u65b0\u578b\u6709\u6548\u7684\u8bad\u7ec3\u7b56\u7565\uff0c\u5305\u62ec\u5728\u5927\u91cf\u5408\u6210\u591a\u89c6\u89d2\u6570\u636e\u4e0a\u8fdb\u884c\u9884\u8bad\u7ec3\uff0c\u7136\u540e\u5728\u9ad8\u8d28\u91cf\u771f\u5b9e\u56fe\u50cf\u4e0a\u8fdb\u884c\u5fae\u8c03", "result": "\u8be5\u65b9\u6cd5\u5728\u8d28\u91cf\u4e0a\u548c\u6570\u91cf\u4e0a\u90fd\u4f18\u4e8e\u5f53\u524d\u6700\u5148\u8fdb\u7684\u4eba\u8138\u591a\u89c6\u89d2\u5408\u6210\u89e3\u51b3\u65b9\u6848\uff0c\u540c\u65f6\u5728\u65f6\u95f4\u6548\u7387\u4e0a\u4fdd\u6301\u9ad8\u6548", "conclusion": "TurboPortrait3D\u80fd\u591f\u5728\u4f4e\u5ef6\u8fdf\u4e0b\u751f\u6210\u9ad8\u8d28\u91cf\u7684\u4eba\u8138\u591a\u89c6\u89d2\u5408\u6210\u56fe\u50cf\uff0c\u5e76\u4e14\u5728\u8d28\u91cf\u548c\u6548\u7387\u4e0a\u90fd\u8d85\u8d8a\u4e86\u5f53\u524d\u7684\u6700\u5148\u8fdb\u6280\u672f"}}
{"id": "2510.23631", "categories": ["cs.LG", "cs.AI", "stat.ME", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.23631", "abs": "https://arxiv.org/abs/2510.23631", "authors": ["Yuxuan Tang", "Yifan Feng"], "title": "Beyond Pairwise: Empowering LLM Alignment With Ranked Choice Modeling", "comment": null, "summary": "Alignment of large language models (LLMs) has predominantly relied on\npairwise preference optimization, where annotators select the better of two\nresponses to a prompt. While simple, this approach overlooks the opportunity to\nlearn from richer forms of human feedback, such as multiwise comparisons and\ntop-$k$ rankings. We propose Ranked Choice Preference Optimization (RCPO), a\nunified framework that bridges preference optimization with (ranked) choice\nmodeling via maximum likelihood estimation. The framework is flexible,\nsupporting both utility-based and rank-based choice models. It subsumes several\nexisting pairwise methods (e.g., DPO, SimPO), while providing principled\ntraining objectives for richer feedback formats. We instantiate this framework\nwith two representative ranked choice models (Multinomial Logit and\nMallows-RMJ). Empirical studies on Llama-3-8B-Instruct and Gemma-2-9B-it across\nAlpacaEval 2 and Arena-Hard benchmarks show that RCPO consistently outperforms\ncompetitive baselines. RCPO shows how directly leveraging ranked preference\ndata, combined with the right choice models, yields more effective alignment.\nIt offers a versatile and extensible foundation for incorporating (ranked)\nchoice modeling into LLM training.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u4f18\u5316\u6846\u67b6 Ranked Choice Preference Optimization (RCPO)\uff0c\u5b83\u80fd\u591f\u5229\u7528\u6392\u540d\u504f\u597d\u6570\u636e\u8fdb\u884c\u8bed\u8a00\u6a21\u578b\u7684\u8bad\u7ec3\uff0c\u5b9e\u73b0\u4e86\u66f4\u597d\u7684\u6a21\u578b\u5bf9\u9f50\u6548\u679c\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4e3b\u8981\u91c7\u7528\u6210\u5bf9\u504f\u597d\u4f18\u5316\uff0c\u5ffd\u89c6\u4e86\u4ece\u66f4\u4e30\u5bcc\u7684\u7528\u6237\u53cd\u9988\uff08\u5982\u591a\u9009\u6bd4\u8f83\u548c\u524d k \u6392\u540d\uff09\u4e2d\u5b66\u4e60\u7684\u673a\u4f1a\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u5168\u9762\u7684\u6846\u67b6\u6765\u652f\u6301\u8fd9\u7c7b\u53cd\u9988\u683c\u5f0f\u7684\u5b66\u4e60\u3002", "method": "RCPO \u901a\u8fc7\u6700\u5927\u4f3c\u7136\u4f30\u8ba1\u5c06\u504f\u597d\u4f18\u5316\u4e0e\uff08\u6392\u540d\uff09\u9009\u62e9\u6a21\u578b\u7edf\u4e00\u8d77\u6765\uff0c\u652f\u6301\u5b9e\u7528\u6027\u548c\u7b49\u7ea7\u6027\u9009\u62e9\u6a21\u578b\uff0c\u5305\u62ec\u591a\u5143\u5bf9\u6570\u51e0\u7387\u6a21\u578b\u548c Mallows-RMJ \u6a21\u578b\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\uff0cRCPO \u7684\u6027\u80fd\u5747\u4f18\u4e8e\u73b0\u6709\u7684\u7ade\u4e89\u57fa\u7ebf\u6a21\u578b\u3002", "conclusion": "RCPO \u80fd\u591f\u76f4\u63a5\u5229\u7528\u6392\u540d\u504f\u597d\u7684\u6570\u636e\uff0c\u5e76\u7ed3\u5408\u9002\u5f53\u7684\u9009\u62e9\u6a21\u578b\uff0c\u4ece\u800c\u63d0\u5347\u8bed\u8a00\u6a21\u578b\u7684\u5bf9\u9f50\u6548\u679c\uff0c\u4e3a\u5c06\u9009\u62e9\u6a21\u578b\u7eb3\u5165 LLM \u8bad\u7ec3\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7075\u6d3b\u4e14\u53ef\u6269\u5c55\u7684\u57fa\u7840\u3002"}}
{"id": "2510.23910", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2510.23910", "abs": "https://arxiv.org/abs/2510.23910", "authors": ["Khadija Omar Said", "Yukta Pareek", "Satadru Dey", "Ashish Ranjan Kumar"], "title": "Dynamical Modeling of Temperature and Smoke Evolution in a Thermal-Runaway Event of a Large-Format Lithium-ion Battery in a Mine Tunnel", "comment": null, "summary": "Large-format lithium-ion batteries (LIBs) provide effective energy storage\nsolutions for high-power equipment used in underground mining operations. They\nhave high Columbic efficiency and minimal heat and emission footprints.\nHowever, improper use of LIBs, accidents, or other factors may increase the\nprobability of thermal runaway (TR), a rapid combustion reaction that\ndischarges toxic and flammable substances. Several such incidents have been\ndocumented in mines. Since repeatable TR experiments to uncover the\ntransient-state propagation of TR are expensive and hazardous, high-fidelity\nmodels are usually developed to mimic the impact of these events. They are\nresource-intensive and are impractical to develop for many scenarios that could\nbe observed in a mine. Therefore, dynamic models within a reduced-order\nframework were constructed to represent the transient-state combustion event.\nReduced order models (ROMs) reasonably replicate trends in temperature and\nsmoke, showing strong alignment with the ground-truth dataset.", "AI": {"tldr": "\u5927\u578b\u9502\u79bb\u5b50\u7535\u6c60\uff08LIB\uff09\u4e3a\u5730\u4e0b\u77ff\u5c71\u8bbe\u5907\u63d0\u4f9b\u6709\u6548\u7684\u80fd\u91cf\u5b58\u50a8\u89e3\u51b3\u65b9\u6848\uff0c\u4f46\u4e0d\u5f53\u4f7f\u7528\u53ef\u80fd\u5bfc\u81f4\u70ed\u5931\u63a7\uff08TR\uff09\uff0c\u91ca\u653e\u6709\u6bd2\u548c\u6613\u71c3\u6c14\u4f53\u3002\u4e3a\u7814\u7a76TR\u7684\u4f20\u64ad\uff0c\u6784\u5efa\u4e86\u7b80\u5316\u7684\u52a8\u6001\u6a21\u578b\uff0c\u8fd9\u4e9b\u6a21\u578b\u80fd\u591f\u5408\u7406\u5730\u590d\u5236\u6e29\u5ea6\u548c\u70df\u96fe\u7684\u53d8\u5316\u8d8b\u52bf\uff0c\u5e76\u4e0e\u5b9e\u9645\u6570\u636e\u9ad8\u5ea6\u4e00\u81f4\u3002", "motivation": "\u907f\u514dLIB\u4e0d\u5f53\u4f7f\u7528\u5bfc\u81f4\u7684\u70ed\u5931\u63a7\u98ce\u9669\uff0c\u9700\u8981\u7814\u7a76\u5176\u4f20\u64ad\u673a\u5236\u3002\u4f46\u662f\uff0c\u76f4\u63a5\u5b9e\u9a8c\u53ef\u80fd\u6210\u672c\u9ad8\u6602\u4e14\u5371\u9669\uff0c\u56e0\u6b64\u4f7f\u7528\u7b80\u5316\u7684\u52a8\u6001\u6a21\u578b\u6765\u6a21\u62df\u70ed\u5931\u63a7\u8fc7\u7a0b\u3002", "method": "\u5229\u7528\u964d\u9636\u6a21\u578b\uff08ROM\uff09\u5728\u7b80\u5316\u7684\u6846\u67b6\u5185\u6784\u5efa\u52a8\u6001\u6a21\u578b\uff0c\u7528\u4e8e\u6a21\u62df\u70ed\u5931\u63a7\u4e8b\u4ef6\u3002\u8be5\u6a21\u578b\u80fd\u591f\u6709\u6548\u5730\u590d\u5236\u6e29\u5ea6\u548c\u70df\u96fe\u7684\u53d8\u5316\u8d8b\u52bf\u3002", "result": "\u8be5\u6a21\u578b\u4e0e\u5b9e\u9645\u6570\u636e\u9ad8\u5ea6\u4e00\u81f4\uff0c\u6210\u529f\u5730\u590d\u5236\u4e86\u6e29\u5ea6\u548c\u70df\u96fe\u7684\u53d8\u5316\u8d8b\u52bf\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u7814\u7a76\u70ed\u5931\u63a7\u4e8b\u4ef6\u4e2d\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u63d0\u51fa\u7684\u7b80\u5316\u52a8\u6001\u6a21\u578b\u80fd\u591f\u6709\u6548\u6a21\u62df\u77ff\u7528\u5927\u578b\u9502\u79bb\u5b50\u7535\u6c60\u70ed\u5931\u63a7\u8fc7\u7a0b\uff0c\u4e3a\u7814\u7a76\u548c\u9884\u9632\u7c7b\u4f3c\u4e8b\u6545\u63d0\u4f9b\u4e86\u53ef\u9760\u7684\u65b9\u6cd5\u3002"}}
{"id": "2510.23930", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.23930", "abs": "https://arxiv.org/abs/2510.23930", "authors": ["Xirui Jin", "Renbiao Jin", "Boying Li", "Danping Zou", "Wenxian Yu"], "title": "PlanarGS: High-Fidelity Indoor 3D Gaussian Splatting Guided by Vision-Language Planar Priors", "comment": "Accepted by NeurIPS 2025. Project page: https://planargs.github.io", "summary": "Three-dimensional Gaussian Splatting (3DGS) has recently emerged as an\nefficient representation for novel-view synthesis, achieving impressive visual\nquality. However, in scenes dominated by large and low-texture regions, common\nin indoor environments, the photometric loss used to optimize 3DGS yields\nambiguous geometry and fails to recover high-fidelity 3D surfaces. To overcome\nthis limitation, we introduce PlanarGS, a 3DGS-based framework tailored for\nindoor scene reconstruction. Specifically, we design a pipeline for\nLanguage-Prompted Planar Priors (LP3) that employs a pretrained vision-language\nsegmentation model and refines its region proposals via cross-view fusion and\ninspection with geometric priors. 3D Gaussians in our framework are optimized\nwith two additional terms: a planar prior supervision term that enforces planar\nconsistency, and a geometric prior supervision term that steers the Gaussians\ntoward the depth and normal cues. We have conducted extensive experiments on\nstandard indoor benchmarks. The results show that PlanarGS reconstructs\naccurate and detailed 3D surfaces, consistently outperforming state-of-the-art\nmethods by a large margin. Project page: https://planargs.github.io", "AI": {"tldr": "\u63d0\u51fa\u4e86PlanarGS\u6846\u67b6\uff0c\u7528\u4e8e\u6539\u8fdb\u5ba4\u5185\u573a\u666f\u76843D\u91cd\u5efa\uff0c\u5c24\u5176\u662f\u5728\u5927\u9762\u79ef\u4f4e\u7eb9\u7406\u533a\u57df\u4e2d\u8868\u73b0\u66f4\u4f18\u3002\u8be5\u6846\u67b6\u7ed3\u5408\u4e86\u8bed\u8a00\u63d0\u793a\u7684\u5e73\u9762\u5148\u9a8c\u548c\u51e0\u4f55\u5148\u9a8c\uff0c\u4f18\u5316\u4e863D\u9ad8\u65af\u70b9\u4e91\u7684\u5e73\u9762\u4e00\u81f4\u6027\u548c\u51e0\u4f55\u7ec6\u8282\u3002\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u4e0e\u73b0\u6709\u65b9\u6cd5\u76f8\u6bd4\uff0cPlanarGS\u5728\u51c6\u786e\u6027\u548c\u7ec6\u8282\u4e30\u5bcc\u5ea6\u65b9\u9762\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u5149\u7167\u635f\u5931\u5728\u5927\u91cf\u4f4e\u7eb9\u7406\u533a\u57df\u7684\u5ba4\u5185\u573a\u666f\u4e2d\u5bfc\u81f43DGS\u7684\u51e0\u4f55\u8868\u793a\u51fa\u73b0\u6a21\u68f1\u4e24\u53ef\u7684\u60c5\u51b5\uff0c\u65e0\u6cd5\u6062\u590d\u9ad8\u8d28\u91cf\u76843D\u8868\u9762\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u65b0\u7684\u65b9\u6cd5\u6765\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u79cd\u57fa\u4e8e\u8bed\u8a00\u63d0\u793a\u7684\u5e73\u9762\u5148\u9a8c\u548c\u51e0\u4f55\u5148\u9a8c\u65b9\u6cd5\u6765\u4f18\u53163D\u9ad8\u65af\u70b9\u4e91\uff0c\u4ee5\u6b64\u6539\u8fdb\u5ba4\u5185\u573a\u666f\u76843D\u91cd\u5efa\u8868\u73b0\u3002\u901a\u8fc7\u7ed3\u5408\u9884\u8bad\u7ec3\u7684\u89c6\u89c9-\u8bed\u8a00\u5206\u5272\u6a21\u578b\u548c\u8de8\u89c6\u56fe\u878d\u5408\u6280\u672f\uff0c\u751f\u6210\u51c6\u786e\u7684\u5e73\u9762\u5148\u9a8c\u5e76\u6307\u5bfc3D\u9ad8\u65af\u7684\u4f18\u5316\u3002\u6b64\u5916\uff0c\u8fd8\u6dfb\u52a0\u4e86\u4e00\u4e2a\u51e0\u4f55\u5148\u9a8c\u76d1\u7763\u9879\uff0c\u4ee5\u5f15\u5bfc\u9ad8\u65af\u5206\u5e03\u7684\u6df1\u5ea6\u548c\u6cd5\u7ebf\u4fe1\u606f\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u4e0e\u5f53\u524d\u7684\u5148\u8fdb\u6280\u672f\u76f8\u6bd4\uff0cPlanarGS\u663e\u8457\u63d0\u9ad8\u4e86\u91cd\u5efa\u7684\u51c6\u786e\u6027\u548c\u7ec6\u8282\u4e30\u5bcc\u5ea6\uff0c\u7279\u522b\u662f\u5728\u5305\u542b\u5927\u9762\u79ef\u4f4e\u7eb9\u7406\u533a\u57df\u7684\u5ba4\u5185\u73af\u5883\u4e2d\u3002", "conclusion": "\u901a\u8fc7\u5f15\u5165\u8bed\u8a00\u63d0\u793a\u548c\u51e0\u4f55\u5148\u9a8c\u7684\u4f18\u5316\u65b9\u6cd5\uff0cPlanarGS\u6846\u67b6\u80fd\u591f\u6709\u6548\u5730\u63d0\u9ad83D\u91cd\u5efa\u8d28\u91cf\uff0c\u7279\u522b\u662f\u5728\u5ba4\u5185\u573a\u666f\u4e2d\u8868\u73b0\u66f4\u4f18\u3002"}}
{"id": "2510.23632", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.23632", "abs": "https://arxiv.org/abs/2510.23632", "authors": ["Guozhong Li", "Muhannad Alhumaidi", "Spiros Skiadopoulos", "Panos Kalnis"], "title": "LLMComp: A Language Modeling Paradigm for Error-Bounded Scientific Data Compression", "comment": null, "summary": "The rapid growth of high-resolution scientific simulations and observation\nsystems is generating massive spatiotemporal datasets, making efficient,\nerror-bounded compression increasingly important. Meanwhile, decoder-only large\nlanguage models (LLMs) have demonstrated remarkable capabilities in modeling\ncomplex sequential data. In this paper, we propose LLMCOMP, a novel lossy\ncompression paradigm that leverages decoder-only large LLMs to model scientific\ndata. LLMCOMP first quantizes 3D fields into discrete tokens, arranges them via\nZ-order curves to preserve locality, and applies coverage-guided sampling to\nenhance training efficiency. An autoregressive transformer is then trained with\nspatial-temporal embeddings to model token transitions. During compression, the\nmodel performs top-k prediction, storing only rank indices and fallback\ncorrections to ensure strict error bounds. Experiments on multiple reanalysis\ndatasets show that LLMCOMP consistently outperforms state-of-the-art\ncompressors, achieving up to 30% higher compression ratios under strict error\nbounds. These results highlight the potential of LLMs as general-purpose\ncompressors for high-fidelity scientific data.", "AI": {"tldr": "LLMCOMP \u662f\u4e00\u79cd\u5229\u7528\u89e3\u7801\u5668\u6a21\u578b\u5bf9\u79d1\u5b66\u6570\u636e\u8fdb\u884c\u538b\u7f29\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u4ee4\u724c\u91cf\u5316\u3001Z\u6392\u5e8f\u66f2\u7ebf\u548c\u8986\u76d6\u5f15\u5bfc\u91c7\u6837\u8fdb\u884c\u8bad\u7ec3\uff0c\u5b9e\u73b0\u9ad8\u6548\u4e14\u8bef\u5dee\u5141\u8bb8\u7684\u538b\u7f29\u3002\u5b9e\u9a8c\u8868\u660e\uff0cLLMCOMP \u5728\u4fdd\u6301\u4e25\u683c\u8bef\u5dee\u9650\u5236\u7684\u60c5\u51b5\u4e0b\uff0c\u6bd4\u73b0\u6709\u6700\u4f73\u538b\u7f29\u7b97\u6cd5\u9ad8\u51fa\u6700\u591a 30% \u7684\u538b\u7f29\u6bd4\u3002", "motivation": "\u57fa\u4e8e\u9ad8\u5206\u8fa8\u7387\u79d1\u5b66\u6a21\u62df\u548c\u89c2\u6d4b\u7cfb\u7edf\u4ea7\u751f\u7684\u5927\u91cf\u65f6\u7a7a\u6570\u636e\uff0c\u5f00\u53d1\u4e00\u79cd\u6548\u7387\u9ad8\u3001\u8bef\u5dee\u53ef\u63a7\u7684\u538b\u7f29\u65b9\u6cd5\u3002\u89e3\u7801\u5668\u5927\u578b\u8bed\u8a00\u6a21\u578b (LLM) \u5728\u5904\u7406\u590d\u6742\u5e8f\u5217\u6570\u636e\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u9002\u5408\u7528\u4e8e\u79d1\u5b66\u6570\u636e\u538b\u7f29\u3002", "method": "LLMCOMP \u5148\u5c063D\u573a\u91cf\u5316\u4e3a\u79bb\u6563\u6807\u8bb0\uff0c\u4f7f\u7528Z\u987a\u5e8f\u66f2\u7ebf\u4fdd\u7559\u5c40\u90e8\u76f8\u5173\u6027\uff0c\u5e76\u901a\u8fc7\u8986\u76d6\u5f15\u5bfc\u91c7\u6837\u63d0\u9ad8\u8bad\u7ec3\u6548\u7387\u3002\u968f\u540e\u4f7f\u7528\u5177\u6709\u65f6\u7a7a\u5d4c\u5165\u7684\u81ea\u56de\u5f52 Transformer \u6a21\u578b\u5bf9\u4ee4\u724c\u8f6c\u6362\u8fdb\u884c\u5efa\u6a21\u3002\u5728\u538b\u7f29\u8fc7\u7a0b\u4e2d\uff0c\u6a21\u578b\u901a\u8fc7Top-k\u9884\u6d4b\u4fdd\u5b58\u7b49\u7ea7\u7d22\u5f15\u5e76\u6dfb\u52a0\u6062\u590d\u66f4\u6b63\uff0c\u4ee5\u786e\u4fdd\u4e25\u683c\u7684\u9519\u8bef\u754c\u9650\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cLLMCOMP \u5728\u591a\u4e2a\u518d\u5206\u6790\u6570\u636e\u96c6\u4e0a\u5747\u80fd\u8d85\u8d8a\u73b0\u6709\u6700\u4f73\u538b\u7f29\u5668\uff0c\u5b9e\u73b0\u4e86\u6700\u9ad8\u8fbe 30% \u7684\u538b\u7f29\u6bd4\uff0c\u4e14\u6240\u6709\u7ed3\u679c\u90fd\u5728\u4e25\u683c\u7684\u8bef\u5dee\u9650\u5236\u5185\u3002\u8fd9\u8868\u660e\u5c06LLM\u4f5c\u4e3a\u7528\u4e8e\u9ad8\u4fdd\u771f\u79d1\u5b66\u6570\u636e\u7684\u4e00\u822c\u538b\u7f29\u5668\u5177\u6709\u6f5c\u5728\u4ef7\u503c\u3002", "conclusion": "LLMCOMP \u8bc1\u660e\u4e86\u57fa\u4e8e\u89e3\u7801\u5668\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\u53ef\u6709\u6548\u7528\u4e8e\u79d1\u5b66\u6570\u636e\u538b\u7f29\uff0c\u540c\u65f6\u5b9e\u73b0\u4e86\u9ad8\u5ea6\u7684\u538b\u7f29\u6548\u7387\u4e0e\u8bef\u5dee\u63a7\u5236\u3002"}}
{"id": "2510.23881", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.23881", "abs": "https://arxiv.org/abs/2510.23881", "authors": ["Xidong Feng", "Vivek Veeriah", "Marcus Chiam", "Michael Dennis", "Ryan Pachauri", "Thomas Tumiel", "Federico Barbero", "Johan Obando-Ceron", "Jiaxin Shi", "Satinder Singh", "Shaobo Hou", "Nenad Toma\u0161ev", "Tom Zahavy"], "title": "Generating Creative Chess Puzzles", "comment": null, "summary": "While Generative AI rapidly advances in various domains, generating truly\ncreative, aesthetic, and counter-intuitive outputs remains a challenge. This\npaper presents an approach to tackle these difficulties in the domain of chess\npuzzles. We start by benchmarking Generative AI architectures, and then\nintroduce an RL framework with novel rewards based on chess engine search\nstatistics to overcome some of those shortcomings. The rewards are designed to\nenhance a puzzle's uniqueness, counter-intuitiveness, diversity, and realism.\nOur RL approach dramatically increases counter-intuitive puzzle generation by\n10x, from 0.22\\% (supervised) to 2.5\\%, surpassing existing dataset rates\n(2.1\\%) and the best Lichess-trained model (0.4\\%). Our puzzles meet novelty\nand diversity benchmarks, retain aesthetic themes, and are rated by human\nexperts as more creative, enjoyable, and counter-intuitive than composed book\npuzzles, even approaching classic compositions. Our final outcome is a curated\nbooklet of these AI-generated puzzles, which is acknowledged for creativity by\nthree world-renowned experts.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5bf9\u6297\u751f\u6210AI\u5728\u68cb\u7c7b\u8c1c\u9898\u521b\u9020\u6027\u3001\u7f8e\u5b66\u548c\u53cd\u5e38\u8bc6\u8f93\u51fa\u65b9\u9762\u6311\u6218\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u5f15\u5165\u4e00\u4e2a\u57fa\u4e8e\u56fd\u9645\u8c61\u68cb\u5f15\u64ce\u641c\u7d22\u7edf\u8ba1\u7684\u65b0\u578b\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u663e\u8457\u63d0\u9ad8\u53cd\u5e38\u8bc6\u8c1c\u9898\u751f\u6210\u80fd\u529b\uff0c\u8fbe\u5230\u751a\u81f3\u8d85\u8fc7\u4eba\u7c7b\u4e13\u5bb6\u4f5c\u54c1\u7684\u6c34\u5e73\u3002", "motivation": "\u751f\u6210\u771f\u6b63\u5177\u6709\u521b\u9020\u6027\u3001\u7f8e\u5b66\u4ef7\u503c\u548c\u53cd\u5e38\u8bc6\u7684\u8f93\u51fa\u662f\u751f\u6210\u5f0fAI\u7684\u4e00\u5927\u6311\u6218\uff0c\u7279\u522b\u662f\u5728\u68cb\u7c7b\u8c1c\u9898\u9886\u57df\uff0c\u6587\u4e2d\u901a\u8fc7\u5f15\u5165\u7279\u5b9a\u7684\u65b9\u6cd5\u6765\u5e94\u5bf9\u8fd9\u4e00\u95ee\u9898\u3002 ", "method": "\u8be5\u65b9\u6cd5\u9996\u5148\u901a\u8fc7\u57fa\u51c6\u8bc4\u4f30\u751f\u6210\u5f0fAI\u67b6\u6784\uff0c\u7136\u540e\u5f15\u5165\u4e00\u4e2a\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u4e0e\u68cb\u7c7b\u5f15\u64ce\u641c\u7d22\u7edf\u8ba1\u7684\u65b0\u578b\u5956\u52b1\u673a\u5236\uff0c\u65e8\u5728\u589e\u5f3a\u8c1c\u9898\u7684\u72ec\u7279\u6027\uff0c\u53cd\u5e38\u8bc6\u6027\uff0c\u591a\u6837\u6027\uff0c\u548c\u771f\u5b9e\u6027\u3002", "result": "\u8be5\u7814\u7a76\u6210\u679c\u4f7f\u53cd\u5e38\u8bc6\u8c1c\u9898\u7684\u751f\u6210\u589e\u52a0\u4e8610\u500d\uff0c\u4ece0.22%\uff08\u76d1\u7763\uff09\u589e\u52a0\u52302.5%\uff0c\u8d85\u8fc7\u4e86\u73b0\u6709\u6570\u636e\u96c6\u7684\u7387\u548cLichess\u8bad\u7ec3\u6a21\u578b\u7684\u6c34\u5e73\u3002\u8fd9\u4e9b\u8c1c\u9898\u5728\u65b0\u9896\u6027\uff0c\u591a\u6837\u6027\uff0c\u771f\u5b9e\u6027\u7b49\u65b9\u9762\u6210\u679c\u663e\u8457\uff0c\u4ece\u4eba\u7c7b\u4e13\u5bb6\u89d2\u5ea6\u6765\u770b\uff0c\u8fd9\u4e9b\u751f\u6210\u7684\u8c1c\u9898\u6bd4\u4f20\u7edf\u5e03\u7f6e\u7684\u53d8\u5316\u66f4\u5177\u6709\u521b\u9020\u6027\uff0c\u66f4\u6709\u8da3\uff0c\u5e76\u4e14\u66f4\u5177\u6709\u53cd\u5e38\u8bc6\u6027\u3002", "conclusion": "\u751f\u6210\u65b9\u6cd5\u6253\u7834\u4e86\u4ee5\u5f80\u5bf9AI\u6240\u80fd\u4ea7\u751f\u7684\u6587\u672c\u7684\u9650\u5236\uff0c\u8fbe\u5230\u4e86\u63a5\u8fd1\u7ecf\u5178\u4f5c\u54c1\u7684\u6548\u679c\uff0c\u5e76\u83b7\u5f97\u4e86\u4e09\u540d\u4e16\u754c\u7ea7\u4e13\u5bb6\u7684\u8ba4\u53ef\u3002"}}
{"id": "2510.24191", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2510.24191", "abs": "https://arxiv.org/abs/2510.24191", "authors": ["Isabelle Krauss", "Victor G. Lopez", "Matthias A. M\u00fcller"], "title": "Sample-based Moving Horizon Estimation", "comment": null, "summary": "In this paper, we propose a sample-based moving horizon estimation (MHE)\nscheme for general nonlinear systems to estimate the current system state using\nirregularly and/or infrequently available measurements. The cost function of\nthe MHE optimization problem is suitably designed to accommodate these\nirregular output sequences. We also establish that, under a suitable\nsample-based detectability condition known as sample-based incremental\ninput/output-to-state stability (i-IOSS), the proposed sample-based MHE\nachieves robust global exponential stability (RGES). Additionally, for the case\nof linear systems, we draw connections between sample-based observability and\nsample-based i-IOSS. This demonstrates that previously established conditions\nfor linear systems to be sample-based observable can be utilized to verify or\ndesign sampling strategies that satisfy the conditions to guarantee RGES of the\nsample-based MHE. Finally, the effectiveness of the proposed sample-based MHE\nis illustrated through a simulation example.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6837\u672c\u7684\u79fb\u52a8\u5730\u5e73\u7ebf\u4f30\u8ba1\uff08MHE\uff09\u65b9\u6848\uff0c\u9002\u7528\u4e8e\u7528\u4e0d\u89c4\u5219\u4e14/\u6216\u4e0d\u9891\u7e41\u7684\u6d4b\u91cf\u4f30\u8ba1\u975e\u7ebf\u6027\u7cfb\u7edf\u5f53\u524d\u72b6\u6001\u3002\u8be5\u65b9\u6848\u5728\u6837\u672c\u589e\u91cf\u8f93\u5165/\u8f93\u51fa-\u72b6\u6001\u7a33\u5b9a\u6027\u6761\u4ef6\u4e0b\u80fd\u5b9e\u73b0\u9c81\u68d2\u5168\u5c40\u6307\u6570\u7a33\u5b9a\u6027\u3002\u5bf9\u4e8e\u7ebf\u6027\u7cfb\u7edf\uff0c\u63d0\u51fa\u4e86\u57fa\u4e8e\u6837\u672c\u7684\u53ef\u89c2\u5bdf\u6027\u548c\u589e\u91cf\u8f93\u5165/\u8f93\u51fa-\u72b6\u6001\u7a33\u5b9a\u6027\u7684\u8054\u7cfb\uff0c\u8bf4\u660e\u4e86\u6837\u672c\u53ef\u89c2\u5bdf\u6027\u7684\u6761\u4ef6\u53ef\u5e94\u7528\u4e8e\u9a8c\u8bc1\u6216\u8bbe\u8ba1\u786e\u4fdd\u6837\u672cMHE\u9c81\u68d2\u5168\u5c40\u6307\u6570\u7a33\u5b9a\u7684\u91c7\u6837\u7b56\u7565\u3002\u5e76\u7ed9\u51fa\u4e86\u4e00\u4e2a\u6a21\u62df\u793a\u4f8b\u6765\u5c55\u793a\u8be5\u65b9\u6848\u7684\u6709\u6548\u6027\u3002", "motivation": "\u73b0\u6709MHE\u65b9\u6848\u901a\u5e38\u5047\u8bbe\u6d4b\u91cf\u6570\u636e\u662f\u89c4\u5219\u4e14\u9891\u7e41\u7684\uff0c\u672a\u80fd\u5145\u5206\u5229\u7528\u4e0d\u89c4\u5219\u548c\u4e0d\u9891\u7e41\u7684\u6d4b\u91cf\u6570\u636e\u3002\u56e0\u6b64\u9700\u8981\u5f00\u53d1\u4e00\u79cd\u65b0\u7684MHE\u65b9\u6848\u4ee5\u9002\u5e94\u8fd9\u6837\u7684\u6d4b\u91cf\u6570\u636e\u3002\u672c\u6587\u7684\u52a8\u673a\u5728\u4e8e\u8bbe\u8ba1\u4e00\u79cd\u57fa\u4e8e\u6837\u672c\u7684MHE\u65b9\u6cd5\uff0c\u80fd\u591f\u5904\u7406\u4e0d\u89c4\u5219\u4e14/\u6216\u4e0d\u9891\u7e41\u7684\u6d4b\u91cf\u6570\u636e\u3002", "method": "\u901a\u8fc7\u8bbe\u8ba1\u5408\u9002\u7684\u6210\u672c\u51fd\u6570\u6765\u9002\u5e94\u4e0d\u89c4\u5219\u8f93\u51fa\u5e8f\u5217\uff0c\u63d0\u51fa\u4e86\u57fa\u4e8e\u6837\u672c\u7684MHE\u65b9\u6848\u3002\u540c\u65f6\uff0c\u7ed9\u51fa\u4e86\u8be5\u65b9\u6848\u5728\u6837\u672c\u589e\u91cf\u8f93\u5165/\u8f93\u51fa-\u72b6\u6001\u7a33\u5b9a\u6027\u6761\u4ef6\u4e0b\u7684\u9c81\u68d2\u5168\u5c40\u6307\u6570\u7a33\u5b9a\u6027\u5b9a\u7406\u3002\u5bf9\u4e8e\u7ebf\u6027\u7cfb\u7edf\uff0c\u63a2\u8ba8\u4e86\u6837\u672c\u53ef\u89c2\u5bdf\u6027\u4e0e\u6837\u672c\u589e\u91cf\u8f93\u5165/\u8f93\u51fa-\u72b6\u6001\u7a33\u5b9a\u6027\u7684\u8054\u7cfb\u3002\u6700\u540e\uff0c\u901a\u8fc7\u6a21\u62df\u5b9e\u9a8c\u5c55\u793a\u4e86\u6240\u63d0\u65b9\u6848\u7684\u6709\u6548\u6027\u3002", "result": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u9002\u5e94\u4e8e\u975e\u89c4\u5219\u53ca\u4e0d\u9891\u7e41\u6d4b\u91cf\u7684\u57fa\u4e8e\u6837\u672c\u7684MHE\u65b9\u6848\u3002\u8bc1\u660e\u4e86\u5728\u6837\u672c\u589e\u91cf\u8f93\u5165/\u8f93\u51fa-\u72b6\u6001\u7a33\u5b9a\u6027\u7684\u6761\u4ef6\u4e0b\uff0c\u8be5\u65b9\u6848\u80fd\u5b9e\u73b0\u9c81\u68d2\u5168\u5c40\u6307\u6570\u7a33\u5b9a\u6027\u3002\u540c\u65f6\uff0c\u5bf9\u4e8e\u7ebf\u6027\u7cfb\u7edf\uff0c\u63ed\u793a\u4e86\u6837\u672c\u53ef\u89c2\u5bdf\u6027\u548c\u6837\u672c\u589e\u91cf\u8f93\u5165/\u8f93\u51fa-\u72b6\u6001\u7a33\u5b9a\u6027\u4e4b\u95f4\u7684\u8054\u7cfb\uff0c\u540c\u65f6\u5c55\u793a\u4e86\u8be5\u65b9\u6848\u7684\u6548\u7528\u3002", "conclusion": "\u672c\u6587\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684MHE\u65b9\u6848\uff0c\u8be5\u65b9\u6848\u80fd\u591f\u5904\u7406\u975e\u89c4\u5219\u53ca\u4e0d\u9891\u7e41\u7684\u6d4b\u91cf\u6570\u636e\uff0c\u9002\u7528\u4e8e\u66f4\u4e3a\u5e7f\u6cdb\u7684\u7cfb\u7edf\u72b6\u6001\u4f30\u8ba1\u573a\u666f\u3002\u6b64\u65b9\u6848\u5728\u6ee1\u8db3\u6837\u672c\u589e\u91cf\u8f93\u5165/\u8f93\u51fa-\u72b6\u6001\u7a33\u5b9a\u6027\u7684\u524d\u63d0\u4e0b\uff0c\u53ef\u4ee5\u5b9e\u73b0\u9c81\u68d2\u5168\u5c40\u6307\u6570\u7a33\u5b9a\u6027\u3002"}}
{"id": "2510.23956", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.23956", "abs": "https://arxiv.org/abs/2510.23956", "authors": ["Alejandro Escontrela", "Shrinu Kushagra", "Sjoerd van Steenkiste", "Yulia Rubanova", "Aleksander Holynski", "Kelsey Allen", "Kevin Murphy", "Thomas Kipf"], "title": "Neural USD: An object-centric framework for iterative editing and control", "comment": "22 pages, 16 figures, 1 table", "summary": "Amazing progress has been made in controllable generative modeling,\nespecially over the last few years. However, some challenges remain. One of\nthem is precise and iterative object editing. In many of the current methods,\ntrying to edit the generated image (for example, changing the color of a\nparticular object in the scene or changing the background while keeping other\nelements unchanged) by changing the conditioning signals often leads to\nunintended global changes in the scene. In this work, we take the first steps\nto address the above challenges. Taking inspiration from the Universal Scene\nDescriptor (USD) standard developed in the computer graphics community, we\nintroduce the \"Neural Universal Scene Descriptor\" or Neural USD. In this\nframework, we represent scenes and objects in a structured, hierarchical\nmanner. This accommodates diverse signals, minimizes model-specific\nconstraints, and enables per-object control over appearance, geometry, and\npose. We further apply a fine-tuning approach which ensures that the above\ncontrol signals are disentangled from one another. We evaluate several design\nconsiderations for our framework, demonstrating how Neural USD enables\niterative and incremental workflows. More information at:\nhttps://escontrela.me/neural_usd .", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86Neural Universal Scene Descriptor\uff08Neural USD\uff09\uff0c\u4e00\u79cd\u7528\u4e8e\u7cbe\u786e\u3001\u8fed\u4ee3\u5730\u7f16\u8f91\u751f\u6210\u56fe\u50cf\u7684\u6280\u672f\u3002Neural USD\u4ee5\u7ed3\u6784\u5316\u7684\u65b9\u5f0f\u8868\u793a\u573a\u666f\u548c\u5bf9\u8c61\uff0c\u5e76\u901a\u8fc7\u7ec6\u8c03\u65b9\u6cd5\u786e\u4fdd\u4fe1\u53f7\u5206\u79bb\uff0c\u5b9e\u73b0\u4e86\u5bf9\u5bf9\u8c61\u5916\u89c2\u3001\u51e0\u4f55\u5f62\u72b6\u548c\u59ff\u6001\u7684\u72ec\u7acb\u63a7\u5236\u3002", "motivation": "\u73b0\u6709\u7684\u751f\u6210\u6a21\u578b\u5728\u7f16\u8f91\u751f\u6210\u7684\u56fe\u50cf\u65f6\u5f80\u5f80\u4f1a\u5bfc\u81f4\u5168\u5c40\u53d8\u5316\uff0c\u800c\u975e\u4ec5\u5728\u6307\u5b9a\u533a\u57df\u8fdb\u884c\u5c40\u90e8\u4fee\u6539\u3002\u672c\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u5c24\u5176\u662f\u5b9e\u73b0\u7cbe\u786e\u548c\u8fed\u4ee3\u7684\u5bf9\u8c61\u7f16\u8f91\u80fd\u529b\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6846\u67b6Neural USD\uff0c\u8be5\u6846\u67b6\u53ef\u4ee5\u7ed3\u6784\u5316\u5730\u8868\u793a\u573a\u666f\u548c\u5bf9\u8c61\uff0c\u652f\u6301\u591a\u6837\u5316\u7684\u4fe1\u53f7\u8f93\u5165\uff0c\u6700\u5c0f\u5316\u7279\u5b9a\u6a21\u578b\u7684\u7ea6\u675f\uff0c\u5e76\u5b9e\u73b0\u4e86\u5bf9\u5355\u4e2a\u5bf9\u8c61\u7684\u5916\u89c2\u3001\u51e0\u4f55\u5f62\u72b6\u548c\u59ff\u6001\u7684\u5206\u79bb\u63a7\u5236\u3002\u6587\u7ae0\u8fd8\u63d0\u51fa\u901a\u8fc7\u7ec6\u8c03\u65b9\u6cd5\u6765\u786e\u4fdd\u8fd9\u4e9b\u63a7\u5236\u4fe1\u53f7\u7684\u5206\u79bb\uff0c\u4ee5\u5b9e\u73b0\u66f4\u7cbe\u786e\u7684\u7f16\u8f91\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\uff0c\u8be5\u6846\u67b6\u80fd\u591f\u652f\u6301\u8fed\u4ee3\u548c\u589e\u91cf\u7684\u5de5\u4f5c\u6d41\u7a0b\uff0c\u4ece\u800c\u5b9e\u73b0\u4e86\u66f4\u7cbe\u786e\u548c\u63a7\u5236\u66f4\u5177\u4f53\u7684\u8fed\u4ee3\u5bf9\u8c61\u7f16\u8f91\u80fd\u529b\u3002", "conclusion": "\u672c\u6587\u63a8\u51fa\u4e86Neural USD\u6846\u67b6\uff0c\u89e3\u51b3\u4e86\u751f\u6210\u6a21\u578b\u5728\u7cbe\u786e\u548c\u8fed\u4ee3\u5bf9\u8c61\u7f16\u8f91\u65b9\u9762\u7684\u6311\u6218\uff0c\u4e3a\u5b9e\u73b0\u5bf9\u573a\u666f\u548c\u5bf9\u8c61\u7684\u66f4\u7ec6\u7c92\u5ea6\u63a7\u5236\u63d0\u4f9b\u4e86\u4e00\u79cd\u65b0\u7684\u65b9\u6cd5\u3002"}}
{"id": "2510.23882", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.23882", "abs": "https://arxiv.org/abs/2510.23882", "authors": ["Adil Rasheed", "Oscar Ravik", "Omer San"], "title": "Hybrid Modeling, Sim-to-Real Reinforcement Learning, and Large Language Model Driven Control for Digital Twins", "comment": null, "summary": "This work investigates the use of digital twins for dynamical system modeling\nand control, integrating physics-based, data-driven, and hybrid approaches with\nboth traditional and AI-driven controllers. Using a miniature greenhouse as a\ntest platform, four predictive models Linear, Physics-Based Modeling (PBM),\nLong Short Term Memory (LSTM), and Hybrid Analysis and Modeling (HAM) are\ndeveloped and compared under interpolation and extrapolation scenarios. Three\ncontrol strategies Model Predictive Control (MPC), Reinforcement Learning (RL),\nand Large Language Model (LLM) based control are also implemented to assess\ntrade-offs in precision, adaptability, and implementation effort. Results show\nthat in modeling HAM provides the most balanced performance across accuracy,\ngeneralization, and computational efficiency, while LSTM achieves high\nprecision at greater resource cost. Among controllers, MPC delivers robust and\npredictable performance, RL demonstrates strong adaptability, and LLM-based\ncontrollers offer flexible human-AI interaction when coupled with predictive\ntools.", "AI": {"tldr": "\u672c\u6587\u901a\u8fc7\u4e00\u4e2a\u5fae\u578b\u6e29\u5ba4\u6d4b\u8bd5\u5e73\u53f0\uff0c\u7814\u7a76\u4e86\u6570\u5b57\u5b6a\u751f\u5728\u52a8\u6001\u7cfb\u7edf\u5efa\u6a21\u548c\u63a7\u5236\u4e2d\u7684\u5e94\u7528\uff0c\u6bd4\u8f83\u4e86\u7ebf\u6027\u6a21\u578b\u3001\u57fa\u4e8e\u7269\u7406\u6a21\u578b\u3001LSTM\u548c\u6df7\u5408\u5206\u6790\u6a21\u578b\uff08HAM\uff09\u5728\u63d2\u503c\u548c\u5916\u63a8\u60c5\u51b5\u4e0b\u7684\u8868\u73b0\uff0c\u5e76\u8bc4\u4f30\u4e86\u6a21\u578b\u9884\u6d4b\u63a7\u5236\u3001\u5f3a\u5316\u5b66\u4e60\u548c\u8bed\u8a00\u6a21\u578b\u63a7\u5236\u7684\u6027\u80fd\u3002\u7ed3\u679c\u663e\u793a\uff0c\u5728\u5efa\u6a21\u65b9\u9762\uff0cHAM\u8868\u73b0\u6700\u4e3a\u5e73\u8861\uff0c\u800cLSTM\u7cbe\u5ea6\u6700\u9ad8\u4f46\u8ba1\u7b97\u8d44\u6e90\u6d88\u8017\u8f83\u5927\uff1b\u5728\u63a7\u5236\u5668\u65b9\u9762\uff0c\u6a21\u578b\u9884\u6d4b\u63a7\u5236\u7a33\u5065\u4e14\u53ef\u9884\u6d4b\uff0c\u5f3a\u5316\u5b66\u4e60\u5177\u6709\u5f88\u5f3a\u7684\u9002\u5e94\u6027\uff0c\u8bed\u8a00\u6a21\u578b\u63a7\u5236\u63d0\u4f9b\u4e86\u7075\u6d3b\u7684\u4eba\u673a\u4ea4\u4e92\u65b9\u5f0f\u3002", "motivation": "\u672c\u6587\u7684\u7814\u7a76\u52a8\u673a\u662f\u901a\u8fc7\u96c6\u6210\u57fa\u4e8e\u7269\u7406\u7684\u3001\u6570\u636e\u9a71\u52a8\u7684\u548c\u6df7\u5408\u7684\u65b9\u6cd5\uff0c\u4ee5\u53ca\u4f20\u7edf\u548cAI\u9a71\u52a8\u7684\u63a7\u5236\u5668\uff0c\u6765\u7814\u7a76\u6570\u5b57\u5b6a\u751f\u5728\u52a8\u6001\u7cfb\u7edf\u5efa\u6a21\u548c\u63a7\u5236\u4e2d\u7684\u5e94\u7528\u6548\u679c\uff0c\u4ece\u800c\u4e3a\u76f8\u5173\u95ee\u9898\u63d0\u4f9b\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u8be5\u7814\u7a76\u4f7f\u7528\u5fae\u578b\u6e29\u5ba4\u4f5c\u4e3a\u6d4b\u8bd5\u5e73\u53f0\uff0c\u5f00\u53d1\u5e76\u6bd4\u8f83\u4e86\u56db\u79cd\u9884\u6d4b\u6a21\u578b\uff1a\u7ebf\u6027\u6a21\u578b\u3001\u7269\u7406\u6a21\u578b\u3001LSTM\u548c\u6df7\u5408\u5206\u6790\u6a21\u578b\uff08HAM\uff09\uff0c\u5728\u63d2\u503c\u548c\u5916\u63a8\u60c5\u51b5\u4e0b\u8fdb\u884c\u6027\u80fd\u8bc4\u4f30\u3002\u540c\u65f6\uff0c\u5b9e\u65bd\u4e86\u4e09\u79cd\u63a7\u5236\u7b56\u7565\uff1a\u6a21\u578b\u9884\u6d4b\u63a7\u5236\uff08MPC\uff09\u3001\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u548c\u57fa\u4e8e\u8bed\u8a00\u6a21\u578b\u7684\u63a7\u5236\uff0c\u4ee5\u8bc4\u4f30\u7cbe\u5ea6\u3001\u9002\u5e94\u6027\u548c\u5b9e\u65bd\u52aa\u529b\u65b9\u9762\u7684\u6743\u8861\u3002", "result": "\u5efa\u6a21\u65b9\u9762\uff0cHAM\u6a21\u578b\u5b9e\u73b0\u4e86\u7cbe\u5ea6\u3001\u6cdb\u5316\u80fd\u529b\u548c\u8ba1\u7b97\u6548\u7387\u4e4b\u95f4\u7684\u6700\u5e73\u8861\u8868\u73b0\uff0c\u800cLSTM\u6a21\u578b\u5728\u8f83\u9ad8\u7684\u8d44\u6e90\u6d88\u8017\u4e0b\u83b7\u5f97\u4e86\u66f4\u9ad8\u7684\u7cbe\u5ea6\u3002\u63a7\u5236\u5668\u65b9\u9762\uff0cMPC\u63d0\u4f9b\u4e86\u7a33\u5065\u4e14\u53ef\u9884\u6d4b\u7684\u8868\u73b0\uff0cRL\u5c55\u793a\u4e86\u5f3a\u70c8\u7684\u9002\u5e94\u6027\uff0c\u57fa\u4e8e\u8bed\u8a00\u6a21\u578b\u7684\u63a7\u5236\u5668\u4e0e\u9884\u6d4b\u5de5\u5177\u7ed3\u5408\u65f6\u63d0\u4f9b\u4e86\u7075\u6d3b\u7684\u4eba\u673a\u4ea4\u4e92\u65b9\u5f0f\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0c\u6570\u5b57\u5b6a\u751f\u7ed3\u5408\u591a\u79cd\u65b9\u6cd5\u548c\u63a7\u5236\u5668\u53ef\u4ee5\u5728\u52a8\u6001\u7cfb\u7edf\u5efa\u6a21\u548c\u63a7\u5236\u4e2d\u63d0\u4f9b\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5176\u4e2dHAM\u6a21\u578b\u63d0\u4f9b\u6700\u4e3a\u5e73\u8861\u7684\u5efa\u6a21\u6027\u80fd\uff0c\u800cMPC\u63a7\u5236\u5668\u5219\u63d0\u4f9b\u4e86\u7a33\u5065\u548c\u53ef\u9884\u6d4b\u63a7\u5236\u7ed3\u679c\u3002\u8fd9\u4e9b\u6210\u679c\u4e3a\u672a\u6765\u7684\u7814\u7a76\u548c\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u53c2\u8003\u3002"}}
{"id": "2510.23634", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.23634", "abs": "https://arxiv.org/abs/2510.23634", "authors": ["Soutrik Sarangi", "Yonatan Sverdlov", "Nadav Dym", "Abir De"], "title": "Monotone and Separable Set Functions: Characterizations and Neural Models", "comment": null, "summary": "Motivated by applications for set containment problems, we consider the\nfollowing fundamental problem: can we design set-to-vector functions so that\nthe natural partial order on sets is preserved, namely $S\\subseteq T \\text{ if\nand only if } F(S)\\leq F(T) $. We call functions satisfying this property\nMonotone and Separating (MAS) set functions. % We establish lower and upper\nbounds for the vector dimension necessary to obtain MAS functions, as a\nfunction of the cardinality of the multisets and the underlying ground set. In\nthe important case of an infinite ground set, we show that MAS functions do not\nexist, but provide a model called our which provably enjoys a relaxed MAS\nproperty we name \"weakly MAS\" and is stable in the sense of Holder continuity.\nWe also show that MAS functions can be used to construct universal models that\nare monotone by construction and can approximate all monotone set functions.\nExperimentally, we consider a variety of set containment tasks. The experiments\nshow the benefit of using our our model, in comparison with standard set models\nwhich do not incorporate set containment as an inductive bias. Our code is\navailable in https://github.com/yonatansverdlov/Monotone-Embedding.", "AI": {"tldr": "\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u79f0\u4e3aMonotone and Separating (MAS)\u7684\u96c6\u5408\u5230\u5411\u91cf\u51fd\u6570\u7684\u6982\u5ff5\uff0c\u5e76\u63a2\u8ba8\u4e86\u5176\u5728\u96c6\u5408\u5305\u542b\u95ee\u9898\u4e2d\u7684\u5e94\u7528\uff0c\u540c\u65f6\u63d0\u4f9b\u4e86\u4e00\u4e2a\u540d\u4e3a'\u5f31MAS'\u7684\u66ff\u4ee3\u6a21\u578b\u3002\u6211\u4eec\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u4f7f\u7528\u8fd9\u79cd\u65b9\u6cd5\u53ef\u4ee5\u66f4\u597d\u5730\u5904\u7406\u96c6\u5408\u5305\u542b\u4efb\u52a1\uff0c\u5bf9\u6bd4\u4f20\u7edf\u7684\u4e0d\u5305\u542b\u96c6\u5408\u5305\u542b\u4f5c\u4e3a\u5f52\u7eb3\u504f\u7f6e\u7684\u6a21\u578b\u6709\u4e86\u660e\u663e\u6539\u8fdb\u3002\u6211\u4eec\u7684\u4ee3\u7801\u53ef\u5728https://github.com/yonatansverdlov/Monotone-Embedding\u83b7\u53d6", "motivation": "\u4e3a\u4e86\u5e94\u7528\u5230\u96c6\u5408\u5305\u542b\u95ee\u9898\uff0c\u7814\u7a76\u5e0c\u671b\u627e\u5230\u4e00\u79cd\u4fdd\u6301\u81ea\u7136\u504f\u5e8f\u5173\u7cfb\u7684\u96c6\u5408\u5230\u5411\u91cf\u51fd\u6570", "method": "\u8bbe\u8ba1\u548c\u5206\u6790MAS\u96c6\u5408\u51fd\u6570\u7684\u6982\u5ff5\uff0c\u8bc4\u4f30\u5176\u5728\u6709\u9650\u548c\u65e0\u9650\u57fa\u6570\u4e0b\u7684\u53ef\u884c\u6027\uff0c\u5e76\u63d0\u51fa\u4e00\u79cd'\u5f31MAS'\u66ff\u4ee3\u6a21\u578b\uff0c\u8861\u91cf\u5176\u4e0e\u539f\u59cbMAS\u51fd\u6570\u7684\u5dee\u5f02", "result": "\u8bc1\u660e\u4e86\u5728\u65e0\u9650\u57fa\u6570\u4e2dMAS\u51fd\u6570\u4e0d\u5b58\u5728\uff0c\u4f46\u662f'\u5f31MAS'\u51fd\u6570\u7ecf\u8bc1\u5177\u6709\u7a33\u5b9a\u6027\u548cHolder\u8fde\u7eed\u6027\u3002\u6b64\u5916\uff0c\u5b9e\u9a8c\u6570\u636e\u8868\u660e\u4f7f\u7528MAS\u51fd\u6570\u6784\u5efa\u7684\u6a21\u578b\u5728\u5904\u7406\u96c6\u5408\u5305\u542b\u4efb\u52a1\u4e0a\u5177\u6709\u660e\u663e\u7684\u4f18\u52bf", "conclusion": "\u7814\u7a76\u63ed\u793a\u4e86\u96c6\u5408-\u5411\u91cf\u51fd\u6570\u5728\u4fdd\u6301\u96c6\u5408\u5305\u542b\u5173\u7cfb\u4e0a\u5177\u6709\u91cd\u8981\u7684\u5e94\u7528\u4ef7\u503c\uff0c\u7279\u522b\u662f'\u5f31MAS'\u6a21\u578b\u5c55\u73b0\u4e86\u5f88\u597d\u7684\u5b9e\u7528\u6027\u548c\u7406\u8bba\u57fa\u7840"}}
{"id": "2510.23960", "categories": ["cs.CV", "cs.AI", "cs.CR"], "pdf": "https://arxiv.org/pdf/2510.23960", "abs": "https://arxiv.org/abs/2510.23960", "authors": ["Peiyang Xu", "Minzhou Pan", "Zhaorun Chen", "Shuang Yang", "Chaowei Xiao", "Bo Li"], "title": "SafeVision: Efficient Image Guardrail with Robust Policy Adherence and Explainability", "comment": "42 pages, 9 figures", "summary": "With the rapid proliferation of digital media, the need for efficient and\ntransparent safeguards against unsafe content is more critical than ever.\nTraditional image guardrail models, constrained by predefined categories, often\nmisclassify content due to their pure feature-based learning without semantic\nreasoning. Moreover, these models struggle to adapt to emerging threats,\nrequiring costly retraining for new threats. To address these limitations, we\nintroduce SafeVision, a novel image guardrail that integrates human-like\nreasoning to enhance adaptability and transparency. Our approach incorporates\nan effective data collection and generation framework, a policy-following\ntraining pipeline, and a customized loss function. We also propose a diverse QA\ngeneration and training strategy to enhance learning effectiveness. SafeVision\ndynamically aligns with evolving safety policies at inference time, eliminating\nthe need for retraining while ensuring precise risk assessments and\nexplanations. Recognizing the limitations of existing unsafe image benchmarks,\nwhich either lack granularity or cover limited risks, we introduce VisionHarm,\na high-quality dataset comprising two subsets: VisionHarm Third-party\n(VisionHarm-T) and VisionHarm Comprehensive(VisionHarm-C), spanning diverse\nharmful categories. Through extensive experiments, we show that SafeVision\nachieves state-of-the-art performance on different benchmarks. SafeVision\noutperforms GPT-4o by 8.6% on VisionHarm-T and by 15.5% on VisionHarm-C, while\nbeing over 16x faster. SafeVision sets a comprehensive, policy-following, and\nexplainable image guardrail with dynamic adaptation to emerging threats.", "AI": {"tldr": "SafeVision \u662f\u4e00\u79cd\u65b0\u578b\u7684\u56fe\u7247\u5b88\u62a4\u680f\u7cfb\u7edf\uff0c\u7ed3\u5408\u4e86\u7c7b\u4f3c\u4eba\u7c7b\u7684\u63a8\u7406\u80fd\u529b\uff0c\u63d0\u9ad8\u4e86\u9002\u5e94\u6027\u548c\u900f\u660e\u5ea6\u3002\u8be5\u7cfb\u7edf\u901a\u8fc7\u6709\u6548\u7684\u6570\u636e\u6536\u96c6\u548c\u751f\u6210\u6846\u67b6\u3001\u9075\u5faa\u7b56\u7565\u7684\u8bad\u7ec3\u6d41\u7a0b\u548c\u81ea\u5b9a\u4e49\u635f\u5931\u51fd\u6570\u63d0\u5347\u4e86\u6027\u80fd\u3002\u6b64\u5916\uff0cSafeVision\u901a\u8fc7\u4e0eVisionHarm\uff08\u9ad8\u6e05\u6570\u636e\u96c6\uff09\u7684\u5b9e\u9a8c\uff0c\u663e\u793a\u51fa\u6bd4GPT-4o\u7b49\u6a21\u578b\u66f4\u597d\u7684\u8868\u73b0\uff0c\u5e76\u4e14\u901f\u5ea6\u66f4\u5feb\uff0c\u9002\u5e94\u65b0\u7684\u5a01\u80c1\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u3002", "motivation": "\u73b0\u6709\u7684\u56fe\u7247\u5b88\u62a4\u680f\u6a21\u578b\u4f9d\u8d56\u9884\u5b9a\u4e49\u7684\u7c7b\u522b\uff0c\u5e76\u4e14\u4ec5\u4ec5\u57fa\u4e8e\u7279\u5f81\u8fdb\u884c\u5b66\u4e60\u800c\u6ca1\u6709\u8bed\u4e49\u63a8\u7406\uff0c\u5bfc\u81f4\u65e0\u6cd5\u9002\u5e94\u65b0\u7684\u5a01\u80c1\u5e76\u4e14\u51c6\u786e\u7387\u4f4e\u3002SafeVision\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u901a\u8fc7\u6a21\u4eff\u4eba\u7c7b\u7684\u63a8\u7406\u80fd\u529b\u63d0\u9ad8\u6a21\u578b\u7684\u9002\u5e94\u6027\u548c\u900f\u660e\u5ea6\u3002\u540c\u65f6\uff0cSafeVision\u63d0\u9ad8\u68c0\u6d4b\u901f\u5ea6\u548c\u6027\u80fd\uff0c\u540c\u65f6\u52a8\u6001\u9002\u5e94\u65b0\u7684\u5a01\u80c1\u3002", "method": "SafeVision \u5305\u62ec\u4e00\u4e2a\u6709\u6548\u7684\u6570\u636e\u6536\u96c6\u548c\u751f\u6210\u6846\u67b6\uff0c\u4e00\u4e2a\u9075\u5faa\u7b56\u7565\u7684\u8bad\u7ec3\u6d41\u6c34\u7ebf\u4ee5\u53ca\u4e00\u4e2a\u5b9a\u5236\u7684\u635f\u5931\u51fd\u6570\u3002\u6b64\u5916\u8fd8\u63d0\u51fa\u591a\u6837\u5316\u7684 QA \u751f\u6210\u548c\u8bad\u7ec3\u7b56\u7565\u6765\u589e\u5f3a\u5b66\u4e60\u6548\u679c\u3002 SafeVision \u5728\u63a8\u7406\u65f6\u53ef\u4ee5\u52a8\u6001\u5730\u4e0e\u4e0d\u65ad\u6f14\u53d8\u7684\u5b89\u5168\u7b56\u7565\u4fdd\u6301\u4e00\u81f4\uff0c\u4ece\u800c\u6d88\u9664\u4e86\u5bf9\u91cd\u65b0\u8bad\u7ec3\u7684\u9700\u6c42\uff0c\u540c\u65f6\u786e\u4fdd\u7cbe\u786e\u7684\u98ce\u9669\u8bc4\u4f30\u548c\u89e3\u91ca\u3002", "result": "\u901a\u8fc7\u5728 VisionHarm \u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\uff0cSafeVision \u5c55\u793a\u4e86\u5176\u4f18\u4e8e GPT-4o \u7684\u6027\u80fd\uff1a\u5728 VisionHarm-T \u6570\u636e\u96c6\u4e0a\u4f18\u4e8e GPT-4o 8.6%\uff0c\u5728 VisionHarm-C \u6570\u636e\u96c6\u4e0a\u4f18\u4e8e GPT-4o 15.5%\uff0c\u540c\u65f6\u901f\u5ea6\u4e5f\u5feb\u4e86 16 \u500d\u4ee5\u4e0a\u3002 SafeVision \u8bbe\u7f6e\u4e86\u5168\u9762\u3001\u7b56\u7565\u9075\u5faa\u548c\u53ef\u89e3\u91ca\u7684\u56fe\u50cf\u9632\u62a4\u680f\u5e76\u80fd\u591f\u52a8\u6001\u9002\u5e94\u65b0\u5a01\u80c1\u3002", "conclusion": "SafeVision \u662f\u4e00\u4e2a\u5168\u9762\u3001\u9075\u5faa\u7b56\u7565\u3001\u53ef\u89e3\u91ca\u7684\u56fe\u50cf\u4fdd\u62a4\u7cfb\u7edf\uff0c\u901a\u8fc7\u7ed3\u5408\u4eba\u7c7b\u822c\u7684\u63a8\u7406\u4f7f\u5f97\u6a21\u578b\u80fd\u591f\u9002\u5e94\u65b0\u7684\u5b89\u5168\u5a01\u80c1\u3002\u5b83\u5728\u5904\u7406\u4e0d\u65ad\u53d8\u5316\u7684\u5b89\u5168\u5a01\u80c1\u65f6\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\uff0c\u4fdd\u6301\u9ad8\u6548\u548c\u51c6\u786e\u7684\u6027\u80fd\u3002"}}
{"id": "2510.23635", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.23635", "abs": "https://arxiv.org/abs/2510.23635", "authors": ["Andrea Bontempelli", "Matteo Busso", "Leonardo Javier Malcotti", "Fausto Giunchiglia"], "title": "Help the machine to help you: an evaluation in the wild of egocentric data cleaning via skeptical learning", "comment": null, "summary": "Any digital personal assistant, whether used to support task performance,\nanswer questions, or manage work and daily life, including fitness schedules,\nrequires high-quality annotations to function properly. However, user\nannotations, whether actively produced or inferred from context (e.g., data\nfrom smartphone sensors), are often subject to errors and noise. Previous\nresearch on Skeptical Learning (SKEL) addressed the issue of noisy labels by\ncomparing offline active annotations with passive data, allowing for an\nevaluation of annotation accuracy. However, this evaluation did not include\nconfirmation from end-users, the best judges of their own context. In this\nstudy, we evaluate SKEL's performance in real-world conditions with actual\nusers who can refine the input labels based on their current perspectives and\nneeds. The study involves university students using the iLog mobile application\non their devices over a period of four weeks. The results highlight the\nchallenges of finding the right balance between user effort and data quality,\nas well as the potential benefits of using SKEL, which include reduced\nannotation effort and improved quality of collected data.", "AI": {"tldr": "\u672c\u7814\u7a76\u8bc4\u4f30\u4e86Skeptical Learning\uff08SKEL\uff09\u5728\u73b0\u5b9e\u6761\u4ef6\u4e0b\u7684\u6027\u80fd\uff0c\u7279\u522b\u662f\u7ed3\u5408\u5b9e\u9645\u7528\u6237\u7684\u89c6\u89d2\u548c\u9700\u6c42\u6765\u8c03\u6574\u8f93\u5165\u6807\u7b7e\u3002\u7814\u7a76\u6d89\u53ca\u7684\u53c2\u4e0e\u8005\u662f\u4f7f\u7528iLog\u79fb\u52a8\u5e94\u7528\u7a0b\u5e8f\u7684\u5927\u5b66\u751f\u3002\u7ed3\u679c\u5f3a\u8c03\u4e86\u5728\u7528\u6237\u52aa\u529b\u548c\u6570\u636e\u8d28\u91cf\u4e4b\u95f4\u627e\u5230\u5e73\u8861\u7684\u6311\u6218\uff0c\u4e5f\u63d0\u51fa\u4e86SKEL\u5728\u51cf\u5c11\u6807\u6ce8\u5de5\u4f5c\u91cf\u548c\u63d0\u5347\u6570\u636e\u8d28\u91cf\u65b9\u9762\u7684\u6f5c\u5728\u4f18\u52bf\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u4e2d\u7684Skeptical Learning\uff08SKEL\uff09\u901a\u8fc7\u5bf9\u6bd4\u79bb\u7ebf\u79ef\u6781\u6807\u6ce8\u4e0e\u88ab\u52a8\u6570\u636e\u6765\u89e3\u51b3\u566a\u58f0\u6807\u7b7e\u95ee\u9898\uff0c\u4f46\u662f\u5e76\u672a\u5f15\u5165\u6700\u7ec8\u7528\u6237\u7684\u786e\u8ba4\uff0c\u800c\u540e\u8005\u624d\u662f\u5224\u65ad\u81ea\u5df1\u6240\u5728\u7684\u73af\u5883\u7684\u6700\u4f73\u8bc4\u5224\u8005\u3002\u672c\u7814\u7a76\u65e8\u5728\u5f15\u5165\u6700\u7ec8\u7528\u6237\u7684\u53cd\u9988\uff0c\u4ee5\u8bc4\u4f30SKEL\u5728\u771f\u5b9e\u6761\u4ef6\u4e0b\u7684\u6027\u80fd\u3002", "method": "\u672c\u7814\u7a76\u901a\u8fc7\u8ba9\u4f7f\u7528iLog\u79fb\u52a8\u5e94\u7528\u7a0b\u5e8f\u7684\u5927\u5b66\u751f\u63d0\u4f9b\u6570\u636e\uff0c\u89c2\u5bdf\u5e76\u9a8c\u8bc1SKEL\u5728\u5b9e\u9645\u4e2d\u7684\u5e94\u7528\u60c5\u51b5\uff0c\u4e3a\u671f\u56db\u5468\u3002\u53c2\u4e0e\u8005\u4f1a\u6839\u636e\u81ea\u5df1\u7684\u5904\u5883\u548c\u9700\u6c42\u8c03\u6574\u8f93\u5165\u6807\u7b7e\uff0c\u4ee5\u4fbf\u8bc4\u4f30\u6570\u636e\u7684\u8d28\u91cf\u548c\u51c6\u786e\u6027\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u5bfb\u627e\u7528\u6237\u52aa\u529b\u4e0e\u6570\u636e\u8d28\u91cf\u4e4b\u95f4\u7684\u5e73\u8861\u5177\u6709\u6311\u6218\u6027\uff0c\u5e76\u4e14SKEL\u6709\u53ef\u80fd\u964d\u4f4e\u6807\u6ce8\u5de5\u4f5c\u91cf\u4ee5\u53ca\u63d0\u5347\u6570\u636e\u8d28\u91cf\u3002", "conclusion": "\u7814\u7a76\u5f3a\u8c03\u4e86\u4f7f\u7528SKEL\u6280\u672f\u4ee5\u7cbe\u7b80\u7528\u6237\u53c2\u4e0e\u7684\u6570\u636e\u6807\u6ce8\u8fc7\u7a0b\u548c\u63d0\u9ad8\u6570\u636e\u8d28\u91cf\u7684\u91cd\u8981\u6027\uff0c\u540c\u65f6\u4e5f\u6307\u51fa\u4e86\u8fc7\u7a0b\u4e2d\u9762\u4e34\u7684\u6311\u6218\u3002"}}
{"id": "2510.24272", "categories": ["eess.SY", "cs.AI", "cs.SY"], "pdf": "https://arxiv.org/pdf/2510.24272", "abs": "https://arxiv.org/abs/2510.24272", "authors": ["Maximilian Bloor", "Max Mowbray", "Ehecatl Antonio Del Rio Chanona", "Calvin Tsay"], "title": "Survey and Tutorial of Reinforcement Learning Methods in Process Systems Engineering", "comment": null, "summary": "Sequential decision making under uncertainty is central to many Process\nSystems Engineering (PSE) challenges, where traditional methods often face\nlimitations related to controlling and optimizing complex and stochastic\nsystems. Reinforcement Learning (RL) offers a data-driven approach to derive\ncontrol policies for such challenges. This paper presents a survey and tutorial\non RL methods, tailored for the PSE community. We deliver a tutorial on RL,\ncovering fundamental concepts and key algorithmic families including\nvalue-based, policy-based and actor-critic methods. Subsequently, we survey\nexisting applications of these RL techniques across various PSE domains, such\nas in fed-batch and continuous process control, process optimization, and\nsupply chains. We conclude with PSE focused discussion of specialized\ntechniques and emerging directions. By synthesizing the current state of RL\nalgorithm development and implications for PSE this work identifies successes,\nchallenges, trends, and outlines avenues for future research at the interface\nof these fields.", "AI": {"tldr": "\u672c\u6587\u7efc\u8ff0\u5e76\u4ecb\u7ecd\u4e86\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u65b9\u6cd5\uff0c\u4e3a\u8fc7\u7a0b\u7cfb\u7edf\u5de5\u7a0b\uff08PSE\uff09\u9886\u57df\u63d0\u4f9b\u6559\u7a0b\u548c\u5e94\u7528\u6848\u4f8b\u5206\u6790\uff0c\u6db5\u76d6\u4e86\u4ef7\u503c\u578b\u3001\u7b56\u7565\u578b\u548cActor-Critic\u65b9\u6cd5\uff0c\u5e76\u63a2\u8ba8\u4e86PSE\u9886\u57df\u5185\u7684\u7279\u6b8a\u6280\u672f\u548c\u65b0\u5174\u65b9\u5411\uff0c\u63d0\u51fa\u4e86\u672a\u6765\u7814\u7a76\u7684\u65b9\u5411\u548c\u6311\u6218\u3002", "motivation": "\u4f20\u7edf\u7684\u63a7\u5236\u548c\u4f18\u5316\u65b9\u6cd5\u5728\u5904\u7406\u590d\u6742\u548c\u5177\u6709\u4e0d\u786e\u5b9a\u6027\u7684\u8fc7\u7a0b\u7cfb\u7edf\u5de5\u7a0b\u6311\u6218\u65f6\u5e38\u5e38\u663e\u5f97\u529b\u4e0d\u4ece\u5fc3\uff0c\u5f3a\u5316\u5b66\u4e60\u4f5c\u4e3a\u4e00\u79cd\u6570\u636e\u9a71\u52a8\u7684\u65b9\u6cd5\u53ef\u4ee5\u5f88\u597d\u5730\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u4e3a\u6b64\u672c\u8bba\u6587\u8fdb\u884c\u4e86\u7efc\u8ff0\u548c\u4ecb\u7ecd\uff0c\u4ee5\u62d3\u5bbdPSE\u9886\u57df\u7684\u7814\u7a76\u8303\u56f4\u548c\u6280\u672f\u5e94\u7528\u3002", "method": "\u672c\u8bba\u6587\u9996\u5148\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5173\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u6559\u7a0b\uff0c\u63a5\u7740\u4ecb\u7ecd\u4e86\u5404\u79cdPSE\u9886\u57df\u7684\u5e94\u7528\u6848\u4f8b\uff0c\u6700\u540e\u63a2\u8ba8\u4e86\u7279\u6b8a\u6280\u672f\u548c\u65b0\u5174\u65b9\u5411\u3002\u8986\u76d6\u7684\u5185\u5bb9\u5305\u62ec\u4ef7\u503c\u578b\u3001\u7b56\u7565\u578b\u548cActor-Critic\u65b9\u6cd5\u3002", "result": "\u603b\u7ed3\u4e86\u5f53\u524d\u7684\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\u53d1\u5c55\u72b6\u6001\uff0c\u4e3aPSE\u9886\u57df\u7684\u5e94\u7528\u63d0\u4f9b\u4e86\u53c2\u8003\uff0c\u5e76\u63d0\u51fa\u4e86\u6210\u529f\u7684\u6848\u4f8b\u3001\u5b58\u5728\u7684\u6311\u6218\u548c\u672a\u6765\u7684\u7814\u7a76\u65b9\u5411\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0c\u5f3a\u5316\u5b66\u4e60\u5728\u5904\u7406\u590d\u6742\u4e14\u5145\u6ee1\u4e0d\u786e\u5b9a\u6027\u7684\u8fc7\u7a0b\u7cfb\u7edf\u5de5\u7a0b\u95ee\u9898\u4e2d\u6f5c\u529b\u5de8\u5927\uff0c\u6307\u51fa\u6b64\u9886\u57df\u7684\u7814\u7a76\u5177\u6709\u91cd\u8981\u7684\u73b0\u5b9e\u610f\u4e49\u548c\u5e7f\u9614\u7684\u53d1\u5c55\u524d\u666f\u3002"}}
{"id": "2510.23968", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.23968", "abs": "https://arxiv.org/abs/2510.23968", "authors": ["Andriy Myronenko", "Dong Yang", "Baris Turkbey", "Mariam Aboian", "Sena Azamat", "Esra Akcicek", "Hongxu Yin", "Pavlo Molchanov", "Marc Edgar", "Yufan He", "Pengfei Guo", "Yucheng Tang", "Daguang Xu"], "title": "Reasoning Visual Language Model for Chest X-Ray Analysis", "comment": "NV-Reason-CXR-3B", "summary": "Vision-language models (VLMs) have shown strong promise for medical image\nanalysis, but most remain opaque, offering predictions without the transparent,\nstepwise reasoning clinicians rely on. We present a framework that brings\nchain-of-thought (CoT) reasoning to chest X-ray interpretation. Inspired by\nreasoning-first training paradigms, our approach is designed to learn how\nexperts reason, not just what they conclude, by aligning intermediate steps\nwith observable image evidence and radiology workflow. Beyond accuracy, the\nexplicit reasoning traces support clinical auditability: they reveal why a\nconclusion was reached, which alternatives were considered, and where\nuncertainty remains, enabling quality assurance, error analysis, and safer\nhuman-AI collaboration.\n  Our model couples high-fidelity visual encoding with a two-stage training\nrecipe: a reasoning-style supervised fine-tuning (SFT) followed by\nreinforcement learning (RL) that uses verifiable rewards over a list of X-ray\nabnormalities. The model outputs reasoning that mirrors radiologists systematic\nthought process, uncertainty, and differential diagnosis. In\nout-of-distribution evaluation, the approach achieves competitive multi-label\nclassification while improving interpretability. In a reader study with expert\nradiologists, full reasoning traces increased confidence, supported error\nauditing, and reduced time to finalize reports. We release code and the model\nNV-Reason-CXR-3B to support community progress toward trustworthy, explainable\nAI in chest radiography and other medical imaging tasks where reasoning quality\nis as critical as prediction quality.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5c06\u94fe\u5f0f\u601d\u7ef4\uff08CoT\uff09\u63a8\u7406\u5e94\u7528\u4e8e\u80f8\u90e8X\u5149\u5f71\u50cf\u89e3\u8bfb\u7684\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u901a\u8fc7\u9ad8\u4fdd\u771f\u89c6\u89c9\u7f16\u7801\u548c\u4e24\u9636\u6bb5\u8bad\u7ec3\u65b9\u6cd5\uff0c\u63d0\u9ad8\u4e86\u6a21\u578b\u7684\u900f\u660e\u6027\u548c\u53ef\u89e3\u91ca\u6027\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u9884\u6d4b\u51c6\u786e\u6027\u3002\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u5b8c\u6574\u7684\u63a8\u7406\u8e2a\u8ff9\u80fd\u589e\u52a0\u4e13\u5bb6\u653e\u5c04\u79d1\u533b\u751f\u7684\u4fe1\u5fc3\uff0c\u652f\u6301\u9519\u8bef\u5ba1\u8ba1\uff0c\u5e76\u51cf\u5c11\u6700\u7ec8\u62a5\u544a\u7684\u65f6\u95f4\uff0c\u4ece\u800c\u63d0\u9ad8\u4e86\u4eba\u673a\u5408\u4f5c\u7684\u5b89\u5168\u6027\u3002", "motivation": "\u73b0\u6709\u8bb8\u591a\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u533b\u5b66\u56fe\u50cf\u5206\u6790\u65b9\u9762\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u6f5c\u529b\uff0c\u4f46\u591a\u6570\u6a21\u578b\u7f3a\u4e4f\u900f\u660e\u7684\u3001\u9010\u6b65\u63a8\u7406\u8fc7\u7a0b\uff0c\u800c\u8fd9\u662f\u4e34\u5e8a\u533b\u751f\u6240\u4f9d\u8d56\u7684\u3002\u56e0\u6b64\uff0c\u7814\u7a76\u52a8\u673a\u5728\u4e8e\u5f00\u53d1\u4e00\u79cd\u80fd\u591f\u63d0\u4f9b\u900f\u660e\u3001\u9010\u6b65\u63a8\u7406\u8fc7\u7a0b\u7684\u6a21\u578b\uff0c\u4ee5\u652f\u6301\u4e34\u5e8a\u53ef\u5ba1\u8ba1\u6027\u3001\u8bef\u5dee\u5206\u6790\u53ca\u66f4\u5b89\u5168\u7684\u4eba\u673a\u5408\u4f5c\u3002", "method": "\u8be5\u7814\u7a76\u5f15\u5165\u4e86\u4e00\u79cd\u7ed3\u5408\u9ad8\u4fdd\u771f\u89c6\u89c9\u7f16\u7801\u548c\u4e24\u9636\u6bb5\u8bad\u7ec3\u65b9\u6cd5\u7684\u6a21\u578b\uff0c\u7b2c\u4e00\u9636\u6bb5\u4e3a\u5e26\u6709\u63a8\u7406\u98ce\u683c\u7684\u76d1\u7763\u5fae\u8c03\uff08SFT\uff09\uff0c\u7b2c\u4e8c\u9636\u6bb5\u4e3a\u9002\u7528\u4e8e\u80f8\u90e8X\u5149\u5f02\u5e38\u73b0\u8c61\u5217\u8868\u7684\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u3002\u8be5\u6a21\u578b\u8f93\u51fa\u7684\u63a8\u7406\u8fc7\u7a0b\u53cd\u6620\u4e86\u653e\u5c04\u79d1\u533b\u751f\u7684\u7cfb\u7edf\u6027\u601d\u7ef4\u8fc7\u7a0b\uff0c\u5305\u62ec\u4e0d\u786e\u5b9a\u6027\u4ee5\u53ca\u9274\u522b\u8bca\u65ad\u3002", "result": "\u5728\u5206\u5e03\u5916\u8bc4\u4f30\u4e2d\uff0c\u8be5\u65b9\u6cd5\u5728\u4fdd\u6301\u9884\u6d4b\u51c6\u786e\u6027\u7684\u540c\u65f6\uff0c\u63d0\u9ad8\u4e86\u6a21\u578b\u7684\u900f\u660e\u6027\u548c\u53ef\u89e3\u91ca\u6027\u3002\u5728\u4e0e\u4e13\u5bb6\u653e\u5c04\u79d1\u533b\u751f\u8fdb\u884c\u7684\u8bfb\u8005\u7814\u7a76\u4e2d\uff0c\u5b8c\u6574\u7684\u63a8\u7406\u8e2a\u8ff9\u589e\u52a0\u4e86\u533b\u751f\u7684\u4fe1\u5fc3\uff0c\u652f\u6301\u4e86\u9519\u8bef\u5ba1\u8ba1\uff0c\u51cf\u5c11\u4e86\u6700\u7ec8\u62a5\u544a\u7684\u65f6\u95f4\u3002", "conclusion": "\u7814\u7a76\u7ed3\u8bba\u652f\u6301\u8fdb\u4e00\u6b65\u7814\u7a76\u548c\u5e94\u7528\u5728\u80f8\u90e8\u653e\u5c04\u6210\u50cf\u548c\u5176\u4ed6\u533b\u7597\u5f71\u50cf\u4efb\u52a1\u4e2d\uff0c\u5176\u4e2d\u63a8\u7406\u8d28\u91cf\u4e0e\u9884\u6d4b\u8d28\u91cf\u4e00\u6837\u91cd\u8981\u7684\u53ef\u9760\u3001\u89e3\u91ca\u6027AI\u7684\u53d1\u5c55\u3002\u5df2\u516c\u5f00\u6a21\u578bNV-Reason-CXR-3B\u4f9b\u793e\u533a\u4f7f\u7528\uff0c\u4ee5\u671d\u5411\u8fd9\u4e00\u76ee\u6807\u8fc8\u8fdb\u3002"}}
{"id": "2510.23925", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.23925", "abs": "https://arxiv.org/abs/2510.23925", "authors": ["Guohao Sun", "Hang Hua", "Jian Wang", "Jiebo Luo", "Sohail Dianat", "Majid Rabbani", "Raghuveer Rao", "Zhiqiang Tao"], "title": "Latent Chain-of-Thought for Visual Reasoning", "comment": "NeurIPS 2025", "summary": "Chain-of-thought (CoT) reasoning is critical for improving the\ninterpretability and reliability of Large Vision-Language Models (LVLMs).\nHowever, existing training algorithms such as SFT, PPO, and GRPO may not\ngeneralize well across unseen reasoning tasks and heavily rely on a biased\nreward model. To address this challenge, we reformulate reasoning in LVLMs as\nposterior inference and propose a scalable training algorithm based on\namortized variational inference. By leveraging diversity-seeking reinforcement\nlearning algorithms, we introduce a novel sparse reward function for\ntoken-level learning signals that encourage diverse, high-likelihood latent\nCoT, overcoming deterministic sampling limitations and avoiding reward hacking.\nAdditionally, we implement a Bayesian inference-scaling strategy that replaces\ncostly Best-of-N and Beam Search with a marginal likelihood to efficiently rank\noptimal rationales and answers. We empirically demonstrate that the proposed\nmethod enhances the state-of-the-art LVLMs on seven reasoning benchmarks, in\nterms of effectiveness, generalization, and interpretability.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8fd1\u4f3c\u53d8\u5206\u63a8\u7406\u7684\u53ef\u6269\u5c55\u8bad\u7ec3\u7b97\u6cd5\uff0c\u7528\u4e8e\u6539\u5584LVLMs\u7684\u63a8\u7406\u80fd\u529b\uff0c\u901a\u8fc7\u5f15\u5165\u7a00\u758f\u5956\u52b1\u51fd\u6570\u548c\u8d1d\u53f6\u65af\u63a8\u7406\u7b56\u7565\uff0c\u63d0\u9ad8\u4e86\u6a21\u578b\u7684\u6709\u6548\u6027\u3001\u6cdb\u5316\u6027\u548c\u53ef\u89e3\u91ca\u6027", "motivation": "\u4f20\u7edf\u8bad\u7ec3\u7b97\u6cd5\u53ef\u80fd\u4e0d\u9002\u7528\u4e8e\u672a\u89c1\u7684\u63a8\u7406\u4efb\u52a1\uff0c\u5e76\u4e14\u4f9d\u8d56\u4e8e\u6709\u504f\u7684\u5956\u52b1\u6a21\u578b", "method": "\u5c06LVLMs\u4e2d\u7684\u63a8\u7406\u89c6\u4e3a\u540e\u9a8c\u63a8\u7406\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8fd1\u4f3c\u53d8\u5206\u63a8\u7406\u7684\u53ef\u6269\u5c55\u8bad\u7ec3\u7b97\u6cd5\uff0c\u7ed3\u5408\u5206\u6563\u5bfb\u6c42\u578b\u5f3a\u5316\u5b66\u4e60\u548c\u8d1d\u53f6\u65af\u7f29\u653e\u7b56\u7565", "result": "\u5728\u4e03\u4e2a\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u8d85\u8fc7\u5f53\u524d\u6700\u4f73LVLMs", "conclusion": "\u65b9\u6cd5\u63d0\u9ad8\u4e86LVLMs\u7684\u6709\u6548\u6027\u3001\u6cdb\u5316\u80fd\u529b\u548c\u53ef\u89e3\u91ca\u6027"}}
{"id": "2510.24370", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2510.24370", "abs": "https://arxiv.org/abs/2510.24370", "authors": ["Yue Wu"], "title": "Mechanism-Guided Residual Lifting and Control Consistent Modeling for Pneumatic Drying Processes", "comment": "6figs,4tables", "summary": "Pneumatic drying processes in industries such as agriculture, chemicals,and\npharmaceuticals are notoriously difficult to model and control due to\nmulti-source disturbances,coupled stage dynamics, and significant measurement\ndelays. Traditional modeling paradigms often fail to simultaneously deliver\naccuracy, interpretability, and closed-loop applicability. To address this\nchallenge, this paper introduces a unified hybrid modeling framework, termed\nPhysics-Guided Residual Lifting with Control-Consistent Correction,which\nintegrates a transient mechanistic model with a stability-constrained\ndata-driven component. The framework covers the complete process chain of\ndrying, transport, and winnowing. On the mechanistic level, the model unifies\nmass transfer dynamics using the partial pressure difference of water vapor,\nincorporates water activity clamping and latent heat corrections for bound\nwater, and ensures energy closure with moisture-dependent specific heat. On the\ndata-driven level,we propose an orthogonal residual learning scheme. It\nleverages intermediate states from the mechanistic model as proxy variables to\nconstruct a physics-inspired dictionary, preventing parameter compensation and\noverfitting during ridge regression. Furthermore, to ensure suitability for\npredictive control, a Control-Consistent Extended Dynamic Mode Decomposition\nwith stability constraints is employed to learn the residual dynamics, for\nwhich we provide boundedness proofs and stability guarantees. The framework was\nvalidated on 10 industrial batches, comprising 63,000 samples. On unseen test\ndata, the hybrid model achieved a Mean Absolute Error of 0.016% for outlet\nmoisture and 0.015 {\\deg}C for outlet temperature, with values improving to\n0.986 and 0.995, respectively. The resulting prediction residuals exhibit\nwhite-noise characteristics, with significantly reduced spectral energy at low\nfrequencies.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7edf\u4e00\u7684\u6df7\u5408\u5efa\u6a21\u6846\u67b6\uff0c\u540d\u4e3a'\u7269\u7406\u5f15\u5bfc\u7684\u6b8b\u5dee\u63d0\u5347\u4e0e\u63a7\u5236\u4e00\u81f4\u4fee\u6b63'\uff0c\u80fd\u591f\u66f4\u51c6\u786e\u5730\u5bf9\u519c\u4e1a\u3001\u5316\u5de5\u548c\u5236\u836f\u7b49\u884c\u4e1a\u4e2d\u7684\u6c14\u52a8\u5e72\u71e5\u8fc7\u7a0b\u8fdb\u884c\u5efa\u6a21\u548c\u63a7\u5236\uff0c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u8be5\u6846\u67b6\u7684\u6709\u6548\u6027", "motivation": "\u4f20\u7edf\u5efa\u6a21\u65b9\u6cd5\u5728\u540c\u65f6\u4fdd\u8bc1\u51c6\u786e\u6027\u3001\u53ef\u89e3\u91ca\u6027\u548c\u95ed\u73af\u5e94\u7528\u6027\u65b9\u9762\u5b58\u5728\u56f0\u96be", "method": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u7ed3\u5408\u4e86\u77ac\u6001\u673a\u7406\u6a21\u578b\u548c\u7a33\u5b9a\u6027\u7ea6\u675f\u6570\u636e\u9a71\u52a8\u7ec4\u4ef6\u7684\u7edf\u4e00\u6df7\u5408\u5efa\u6a21\u6846\u67b6\uff0c\u6a21\u578b\u5728\u673a\u7406\u5c42\u9762\u4e0a\u7edf\u4e00\u4e86\u6c34\u84b8\u6c14\u5206\u538b\u5dee\u7684\u52a8\u6001\u8d28\u91cf\u8f6c\u79fb\uff0c\u5e76\u5f15\u5165\u4e86\u63a7\u5236\u4e00\u81f4\u6269\u5c55\u7684\u52a8\u6001\u6a21\u5f0f\u5206\u89e3\u65b9\u6cd5\u6765\u4fee\u6b63\u6b8b\u5dee\u52a8\u529b\u5b66", "result": "\u8be5\u6df7\u5408\u6a21\u578b\u5728\u672a\u89c1\u6d4b\u8bd5\u6570\u636e\u96c6\u4e2d\u7684\u5e73\u5747\u7edd\u5bf9\u8bef\u5dee\u5206\u522b\u4e3a\u51fa\u53e3\u542b\u6c34\u91cf0.016%\u548c\u51fa\u53e3\u6e29\u5ea60.015\u00b0C\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u9884\u6d4b\u7684\u51c6\u786e\u6027", "conclusion": "\u9a8c\u8bc1\u4e86\u8be5\u6846\u67b6\u7684\u6709\u6548\u6027\u548c\u53ef\u9760\u6027\uff0c\u8bc1\u660e\u4e86\u5176\u80fd\u591f\u6709\u6548\u5730\u5e94\u7528\u4e8e\u6c14\u52a8\u5e72\u71e5\u8fc7\u7a0b\u7684\u5efa\u6a21\u548c\u63a7\u5236"}}
{"id": "2510.23942", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.23942", "abs": "https://arxiv.org/abs/2510.23942", "authors": ["Sridhar Mahadevan"], "title": "Decentralized Causal Discovery using Judo Calculus", "comment": "54 pages", "summary": "We describe a theory and implementation of an intuitionistic decentralized\nframework for causal discovery using judo calculus, which is formally defined\nas j-stable causal inference using j-do-calculus in a topos of sheaves. In\nreal-world applications -- from biology to medicine and social science --\ncausal effects depend on regime (age, country, dose, genotype, or lab\nprotocol). Our proposed judo calculus formalizes this context dependence\nformally as local truth: a causal claim is proven true on a cover of regimes,\nnot everywhere at once. The Lawvere-Tierney modal operator j chooses which\nregimes are relevant; j-stability means the claim holds constructively and\nconsistently across that family. We describe an algorithmic and implementation\nframework for judo calculus, combining it with standard score-based,\nconstraint-based, and gradient-based causal discovery methods. We describe\nexperimental results on a range of domains, from synthetic to real-world\ndatasets from biology and economics. Our experimental results show the\ncomputational efficiency gained by the decentralized nature of sheaf-theoretic\ncausal discovery, as well as improved performance over classical causal\ndiscovery methods.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u57fa\u4e8e\u4f50\u6b66\u5c14\u592b-\u6cf0\u5c14\u5c3c(y)\u6a21\u6001\u7b97\u5b50\u7684\u76f4\u89c2\u4e3b\u4e49\u5206\u6563\u5f0f\u56e0\u679c\u53d1\u73b0\u6846\u67b6\uff0c\u5229\u7528\u67d4\u9053(judo)\u5fae\u79ef\u5206\uff0c\u53ef\u4ee5\u66f4\u52a0\u7cbe\u786e\u5730\u5904\u7406\u56e0\u679c\u6548\u5e94\u7684\u4e0a\u4e0b\u6587\u4f9d\u8d56\u6027\uff0c\u4e14\u5b9e\u9a8c\u663e\u793a\u5176\u5728\u8ba1\u7b97\u6548\u7387\u548c\u6027\u80fd\u4e0a\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5", "motivation": "\u56e0\u679c\u6548\u5e94\u5728\u4e0d\u540c\u7684\u4e0a\u4e0b\u6587\u4e2d\u662f\u4e0d\u540c\u7684\uff0c\u73b0\u6709\u7684\u56e0\u679c\u53d1\u73b0\u65b9\u6cd5\u96be\u4ee5\u7cbe\u786e\u5904\u7406\u8fd9\u79cd\u4f9d\u8d56\u6027\uff0c\u63d0\u51fa\u4e86\u5e26\u6709\u4e0a\u4e0b\u6587\u4f9d\u8d56\u6027\u7684\u67d4\u9053\u5fae\u79ef\u5206\u4ee5\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898", "method": "\u63d0\u51fa\u57fa\u4e8e\u4f50\u6b66\u5c14\u592b-\u6cf0\u5c14\u5c3c(y)\u6a21\u6001\u7b97\u5b50\u7684\u76f4\u89c2\u4e3b\u4e49\u5206\u6563\u5f0f\u56e0\u679c\u53d1\u73b0\u6846\u67b6\uff0c\u5229\u7528\u67d4\u9053\u5fae\u79ef\u5206\uff0c\u5e76\u7ed3\u5408\u8bc4\u5206\u3001\u7ea6\u675f\u548c\u68af\u5ea6\u4f18\u5316\u7684\u56e0\u679c\u53d1\u73b0\u65b9\u6cd5\uff0c\u8fdb\u884c\u5b9e\u9a8c\u9a8c\u8bc1", "result": "\u5c55\u793a\u4e86\u7b97\u6cd5\u5728\u591a\u9886\u57df\u7684\u6709\u6548\u6027\u4ee5\u53ca\u5728\u8ba1\u7b97\u6548\u7387\u548c\u6027\u80fd\u4e0a\u7684\u4f18\u52bf", "conclusion": "\u901a\u8fc7\u67d4\u9053\u5fae\u79ef\u5206\u548c\u5206\u6563\u5f0f\u7684\u56e0\u679c\u53d1\u73b0\u6846\u67b6\uff0c\u53ef\u4ee5\u66f4\u7cbe\u786e\u5730\u5904\u7406\u56e0\u679c\u6548\u5e94\u7684\u4e0a\u4e0b\u6587\u4f9d\u8d56\u6027\uff0c\u5e76\u4e14\u5728\u8ba1\u7b97\u6548\u7387\u548c\u6027\u80fd\u4e0a\u8d85\u8fc7\u4e86\u4f20\u7edf\u65b9\u6cd5"}}
{"id": "2510.24389", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2510.24389", "abs": "https://arxiv.org/abs/2510.24389", "authors": ["Lamine Chalal", "Ahmed Rachid"], "title": "Development of a Digital Twin for an Electric Vehicle Emulator Modeling, Control, and Experimental Validation", "comment": "6 pages, Accepted at CODIT 2025 (Conference on Decision and Control\n  in Intelligent Technology)", "summary": "This paper presents the development and validation of a digital twin for a\nscaled-down electric vehicle (EV) emulator, designed to replicate longitudinal\nvehicle dynamics under diverse operating conditions. The emulator integrates a\nseparately excited DC motor (SEDCM), a four-quadrant DC-DC converter, a battery\nemulator, and a mechanical load emulator. The system models tractive effort,\naerodynamic drag, and gradient resistance using Newton's second law. In\ncontrast to conventional graphical modeling tools (e.g., block diagrams and\nbond graphs), the adopted Energetic Macroscopic Representation (EMR) framework\noffers clear advantages by explicitly representing energy interactions and\nfacilitating the systematic derivation of control structures. A control\nstrategy developed within this framework governs energy flow across the\npowertrain, enabling accurate speed control via armature voltage regulation.\nExperimental tests conducted on a Lucas-Nulle test bench show strong\ncorrelation with simulation results. The study also introduces a methodology to\ncompute the maximum admissible vehicle mass - determined to be 13.5 kg for a\n180 W motor operating at 1900 rpm - based on acceleration and slope\nconstraints. Furthermore, a switching algorithm for the bidirectional converter\nensures reliable four quadrant operation. Overall, the proposed framework\nprovides a scalable and effective approach for EV emulation, control design,\nand energy management validation.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u7535\u52a8\u6c7d\u8f66\uff08EV\uff09\u6a21\u62df\u5668\u7684\u6570\u5b57\u5b6a\u751f\u5f00\u53d1\u548c\u9a8c\u8bc1\u65b9\u6cd5\uff0c\u8be5\u6a21\u62df\u5668\u80fd\u591f\u6a21\u62df\u4e0d\u540c\u7684\u64cd\u4f5c\u6761\u4ef6\u4e0b\u7684\u7eb5\u5411\u8f66\u8f86\u52a8\u6001\u3002\u8be5\u65b9\u6cd5\u4f7f\u7528\u80fd\u91cf\u5b8f\u89c2\u8868\u793a\uff08EMR\uff09\u6846\u67b6\uff0c\u4f18\u4e8e\u4f20\u7edf\u7684\u56fe\u5f62\u5efa\u6a21\u5de5\u5177\uff0c\u901a\u8fc7\u80fd\u91cf\u4ea4\u4e92\u7684\u660e\u786e\u8868\u793a\u548c\u63a7\u5236\u7ed3\u6784\u7684\u7cfb\u7edf\u5bfc\u51fa\uff0c\u5b9e\u73b0\u4e86\u51c6\u786e\u7684\u901f\u5ea6\u63a7\u5236\u3002\u5b9e\u9a8c\u7ed3\u679c\u4e0e\u6a21\u62df\u7ed3\u679c\u9ad8\u5ea6\u543b\u5408\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u52a0\u901f\u5ea6\u548c\u5761\u5ea6\u7ea6\u675f\u7684\u6700\u5927\u5141\u8bb8\u8f66\u8f86\u8d28\u91cf\u7684\u8ba1\u7b97\u65b9\u6cd5\u4ee5\u53ca\u7528\u4e8e\u53cc\u5411\u8f6c\u6362\u5668\u7684\u5207\u6362\u7b97\u6cd5\uff0c\u786e\u4fdd\u4e86\u53ef\u9760\u7684\u56db\u8c61\u9650\u64cd\u4f5c\u3002", "motivation": "\u5f53\u524d\u7535\u52a8\u6c7d\u8f66\u6a21\u62df\u4e2d\u5b58\u5728\u7684\u95ee\u9898\u9700\u8981\u4e00\u79cd\u65b0\u7684\u65b9\u6cd5\u6765\u63d0\u9ad8\u6a21\u62df\u7cbe\u5ea6\u548c\u63a7\u5236\u8bbe\u8ba1\u7684\u590d\u6742\u6027\uff0c\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u80fd\u91cf\u5b8f\u89c2\u8868\u793a\uff08EMR\uff09\u6846\u67b6\u7684\u65b9\u6cd5\u6765\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u5b83\u662f\u4e13\u95e8\u4e3a\u7535\u52a8\u6c7d\u8f66\u6a21\u62df\u5668\u7684\u9a8c\u8bc1\u548c\u80fd\u91cf\u7ba1\u7406\u9a8c\u8bc1\u800c\u8bbe\u8ba1\u7684\u3002\u8fd9\u4e2a\u6846\u67b6\u63d0\u9ad8\u4e86\u6548\u7387\u548c\u51c6\u786e\u6027\uff0c\u907f\u514d\u4e86\u4f20\u7edf\u5efa\u6a21\u65b9\u6cd5\u8bf8\u5982\u56fe\u5757\u56fe\u6a21\u578b\u7684\u5c40\u9650\u6027\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u79cd\u6570\u5b57\u5b6a\u751f\u65b9\u6cd5\u6765\u6a21\u62df\u7535\u52a8\u6c7d\u8f66\u7684\u7eb5\u5411\u52a8\u6001\uff0c\u57fa\u4e8e\u80fd\u91cf\u5b8f\u89c2\u8868\u793a\uff08EMR\uff09\u6a21\u578b\u5efa\u7acb\u4e86\u7cfb\u7edf\u67b6\u6784\uff0c\u4f7f\u7528\u80fd\u91cf\u6d41\u63a7\u5236\u65b9\u6cd5\uff0c\u5e76\u63d0\u4f9b\u4e86\u5bf9\u53cc\u5411\u8f6c\u6362\u5668\u56db\u8c61\u9650\u64cd\u4f5c\u7684\u5207\u6362\u7b97\u6cd5\u3002", "result": "\u5b9e\u9a8c\u6d4b\u8bd5\u663e\u793a\u6570\u503c\u6a21\u62df\u548c\u5b9e\u9a8c\u6570\u636e\u9ad8\u5ea6\u543b\u5408\uff1b\u57fa\u4e8e\u52a0\u901f\u548c\u5761\u5ea6\u7ea6\u675f\uff0c\u6700\u5927\u5141\u8bb8\u8f66\u8f86\u8d28\u91cf\u8ba1\u7b97\u4e3a13.5 kg\uff1b\u5f00\u53d1\u7684\u53cc\u5411\u8f6c\u6362\u5668\u4e0e\u5207\u6362\u7b97\u6cd5\uff0c\u663e\u793a\u4e86\u53ef\u9760\u7684\u56db\u8c61\u9650\u64cd\u4f5c\uff0c\u9a8c\u8bc1\u4e86\u8be5\u65b0\u65b9\u6cd5\u7684\u51c6\u786e\u6027\u548c\u6548\u7387\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684\u57fa\u4e8eEMR\u6846\u67b6\u7684\u65b9\u6cd5\u5bf9\u4e8e\u7535\u52a8\u6c7d\u8f66\u7684\u4eff\u771f\uff0c\u63a7\u5236\u5668\u8bbe\u8ba1\u548c\u80fd\u91cf\u7ba1\u7406\u7684\u9a8c\u8bc1\uff0c\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u62d3\u5c55\u4e14\u6709\u6548\u7684\u65b9\u6cd5\uff0c\u4e0e\u4f20\u7edf\u5efa\u6a21\u5de5\u5177\u76f8\u6bd4\u5177\u6709\u660e\u663e\u7684\u4f18\u70b9\u3002"}}
{"id": "2510.23981", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.23981", "abs": "https://arxiv.org/abs/2510.23981", "authors": ["Jiaqi Yan", "Ruilong Ren", "Jingren Liu", "Shuning Xu", "Ling Wang", "Yiheng Wang", "Yun Wang", "Long Zhang", "Xiangyu Chen", "Changzhi Sun", "Jixiang Luo", "Dell Zhang", "Hao Sun", "Chi Zhang", "Xuelong Li"], "title": "TeleEgo: Benchmarking Egocentric AI Assistants in the Wild", "comment": null, "summary": "Egocentric AI assistants in real-world settings must process multi-modal\ninputs (video, audio, text), respond in real time, and retain evolving\nlong-term memory. However, existing benchmarks typically evaluate these\nabilities in isolation, lack realistic streaming scenarios, or support only\nshort-term tasks. We introduce \\textbf{TeleEgo}, a long-duration, streaming,\nomni-modal benchmark for evaluating egocentric AI assistants in realistic daily\ncontexts. The dataset features over 14 hours per participant of synchronized\negocentric video, audio, and text across four domains: work \\& study, lifestyle\n\\& routines, social activities, and outings \\& culture. All data is aligned on\na unified global timeline and includes high-quality visual narrations and\nspeech transcripts, curated through human refinement.TeleEgo defines 12\ndiagnostic subtasks across three core capabilities: Memory (recalling past\nevents), Understanding (interpreting the current moment), and Cross-Memory\nReasoning (linking distant events). It contains 3,291 human-verified QA items\nspanning multiple question formats (single-choice, binary, multi-choice, and\nopen-ended), evaluated strictly in a streaming setting. We propose two key\nmetrics -- Real-Time Accuracy and Memory Persistence Time -- to jointly assess\ncorrectness, temporal responsiveness, and long-term retention. TeleEgo provides\na realistic and comprehensive evaluation to advance the development of\npractical AI assistants.", "AI": {"tldr": "TeleEgo\u662f\u4e00\u4e2a\u8bc4\u4ef7\u60c5\u5883\u667a\u80fd\u52a9\u624b\u5728\u65e5\u5e38\u751f\u6d3b\u4e2d\u7684\u8868\u73b0\u7684\u6570\u636e\u96c6\uff0c\u5305\u542b14\u5c0f\u65f6\u7684\u89c6\u9891\u3001\u97f3\u9891\u548c\u6587\u672c\u4fe1\u606f\uff0c\u7528\u4e8e\u8bc4\u4f30\u80cc\u666f\u8bb0\u5fc6\u3001\u7406\u89e3\u80fd\u529b\u548c\u8de8\u80cc\u666f\u63a8\u7406\u80fd\u529b\uff0c\u5e76\u8bbe\u5b9a\u4e86\u771f\u5b9e\u65f6\u95f4\u51c6\u786e\u6027\u548c\u8bb0\u5fc6\u6301\u4e45\u65f6\u95f4\u8fd9\u4e24\u4e2a\u5173\u952e\u6307\u6807", "motivation": "\u73b0\u6709\u7684\u57fa\u51c6\u6d4b\u8bd5\u5f80\u5f80\u5728\u5355\u72ec\u8bc4\u4f30\u8fd9\u4e9b\u80fd\u529b\u3001\u7f3a\u4e4f\u73b0\u5b9e\u7684\u6d41\u8ba1\u7b97\u573a\u666f\u6216\u4ec5\u652f\u6301\u77ed\u671f\u4efb\u52a1\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u56e0\u6b64\u63d0\u51faTeleEgo\u6765\u8bc4\u4f30\u60c5\u5883\u4eba\u5de5\u667a\u80fd\u52a9\u624b\u5728\u771f\u5b9e\u73af\u5883\u4e2d\u7684\u8868\u73b0", "method": "\u521b\u5efa\u4e00\u4e2a\u5305\u542b\u591a\u6a21\u6001\u6570\u636e\u96c6TeleEgo\uff0c\u5305\u62ec\u89c6\u9891\u3001\u97f3\u9891\u548c\u6587\u672c\u7b49\u957f\u671f\u5b9e\u65f6\u6570\u636e\uff0c\u5e76\u5b9a\u4e49\u4e8612\u4e2a\u8bca\u65ad\u5b50\u4efb\u52a1\u4ee5\u53ca\u4e24\u4e2a\u8bc4\u4f30\u6307\u6807\uff1a\u771f\u5b9e\u65f6\u95f4\u51c6\u786e\u6027\u53ca\u8bb0\u5fc6\u6301\u4e45\u65f6\u95f4", "result": "\u8be5\u6570\u636e\u96c6\u80fd\u591f\u5728\u6d41\u8ba1\u7b97\u8bbe\u7f6e\u4e0b\u4e25\u683c\u8bc4\u6d4b\uff0c\u5305\u542b3291\u4e2a\u4eba\u9a8c\u8bc1\u7684\u95ee\u9898\u6db5\u76d6\u591a\u79cd\u5f62\u5f0f\uff0c\u5177\u6709\u9ad8\u5ea6\u771f\u5b9e\u6027\u548c\u7efc\u5408\u8bc4\u4f30", "conclusion": "TeleEgo\u4e3a\u5b9e\u9645\u60c5\u5883\u667a\u80fd\u52a9\u624b\u7684\u53d1\u5c55\u63d0\u4f9b\u4e86\u4e00\u4e2a\u73b0\u5b9e\u548c\u5168\u9762\u7684\u8bc4\u4f30\u624b\u6bb5"}}
{"id": "2510.23965", "categories": ["cs.AI", "cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.23965", "abs": "https://arxiv.org/abs/2510.23965", "authors": ["Aymane El Gadarri", "Ali Aouad", "Vivek F. Farias"], "title": "The Sign Estimator: LLM Alignment in the Face of Choice Heterogeneity", "comment": null, "summary": "Traditional LLM alignment methods are vulnerable to heterogeneity in human\npreferences. Fitting a na\\\"ive probabilistic model to pairwise comparison data\n(say over prompt-completion pairs) yields an inconsistent estimate of the\npopulation-average utility -a canonical measure of social welfare. We propose a\nnew method, dubbed the sign estimator, that provides a simple, provably\nconsistent, and efficient estimator by replacing cross-entropy with binary\nclassification loss in the aggregation step. This simple modification recovers\nconsistent ordinal alignment under mild assumptions and achieves the first\npolynomial finite-sample error bounds in this setting. In realistic simulations\nof LLM alignment using digital twins, the sign estimator substantially reduces\npreference distortion over a panel of simulated personas, cutting (angular)\nestimation error by nearly 35% and decreasing disagreement with true population\npreferences from 12% to 8% compared to standard RLHF. Our method also compares\nfavorably to panel data heuristics that explicitly model user heterogeneity and\nrequire tracking individual-level preference data-all while maintaining the\nimplementation simplicity of existing LLM alignment pipelines.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u7b26\u53f7\u4f30\u8ba1\u5668\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u5728\u805a\u5408\u6b65\u9aa4\u4e2d\u5c06\u4ea4\u53c9\u71b5\u66ff\u6362\u4e3a\u4e8c\u5143\u5206\u7c7b\u635f\u5931\uff0c\u63d0\u4f9b\u4e86\u4e00\u79cd\u7b80\u5355\u3001\u6709\u4fdd\u8bc1\u7684\u4e00\u81f4\u6027\u548c\u9ad8\u6548\u7684\u4f30\u8ba1\u65b9\u6cd5\u3002\u8fd9\u79cd\u65b9\u6cd5\u5728\u8f7b\u5fae\u5047\u8bbe\u4e0b\u6062\u590d\u4e86\u4e00\u81f4\u7684\u5e8f\u6570\u5bf9\u9f50\uff0c\u5e76\u5728\u8be5\u73af\u5883\u4e2d\u9996\u6b21\u5b9e\u73b0\u4e86\u591a\u9879\u5f0f\u6709\u9650\u6837\u672c\u8bef\u5dee\u754c\u9650\u3002\u5728\u4f7f\u7528\u6570\u5b57\u53cc\u80de\u80ce\u6a21\u62df\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5bf9\u9f50\u7684\u73b0\u5b9e\u60c5\u666f\u4e2d\uff0c\u7b26\u53f7\u4f30\u8ba1\u5668\u663e\u8457\u51cf\u5c11\u4e86\u6a21\u62df\u4eba\u7269\u504f\u597d\u7684\u626d\u66f2\uff0c\u5c06\uff08\u89d2\u5ea6\uff09\u4f30\u8ba1\u8bef\u5dee\u51cf\u5c11\u4e86\u8fd135%\uff0c\u5728\u4e0e\u771f\u5b9e\u7684\u4eba\u53e3\u504f\u597d\u4e0d\u7b26\u7684\u60c5\u51b5\u4e0b\u4ece12%\u51cf\u5c11\u52308%\u3002\u4e0e\u663e\u5f0f\u5efa\u6a21\u7528\u6237\u5f02\u8d28\u6027\u548c\u9700\u8981\u8ddf\u8e2a\u4e2a\u4eba\u7ea7\u504f\u597d\u6570\u636e\u7684\u966a\u5ba1\u56e2\u6570\u636e\u542f\u53d1\u5f0f\u65b9\u6cd5\u76f8\u6bd4\uff0c\u8be5\u65b9\u6cd5\u5728\u4fdd\u6301\u73b0\u6709\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5bf9\u9f50\u7ba1\u9053\u7684\u5b9e\u73b0\u7b80\u6613\u6027\u7684\u540c\u65f6\u8868\u73b0\u826f\u597d\u3002", "motivation": "\u4f20\u7edfLLM\u5bf9\u9f50\u65b9\u6cd5\u5728\u9762\u5bf9\u4eba\u7c7b\u504f\u597d\u7684\u5f02\u8d28\u6027\u65f6\u5b58\u5728\u8106\u5f31\u6027\u3002\u62df\u5408\u6734\u7d20\u6982\u7387\u6a21\u578b\u5230\u6210\u5bf9\u6bd4\u8f83\u6570\u636e\u4e0a\u4f1a\u83b7\u5f97\u4eba\u53e3\u5e73\u5747\u6548\u7528\u4e0d\u4e00\u81f4\u7684\u4f30\u8ba1\uff0c\u8fd9\u662f\u4e00\u4e2a\u793e\u4f1a\u798f\u5229\u7684\u6807\u51c6\u5ea6\u91cf\u3002\u56e0\u6b64\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u514b\u670d\u8fd9\u4e9b\u8106\u5f31\u6027\u7684\u65b0\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u7684\u65b9\u6cd5\u79f0\u4e3a\u7b26\u53f7\u4f30\u8ba1\u5668\uff0c\u901a\u8fc7\u5728\u805a\u5408\u6b65\u9aa4\u4e2d\u7528\u4e8c\u5143\u5206\u7c7b\u635f\u5931\u66ff\u6362\u4ea4\u53c9\u71b5\u6765\u5b9e\u73b0\u4eba\u53e3\u5e73\u5747\u6548\u7528\u7684\u4f30\u8ba1\u3002\u8be5\u65b9\u6cd5\u8bc1\u660e\u4e86\u4e00\u81f4\u6027\u548c\u6548\u7387\uff0c\u5e76\u4e14\u5728\u8f7b\u5fae\u5047\u8bbe\u4e0b\u6062\u590d\u4e00\u81f4\u7684\u5e8f\u6570\u5bf9\u9f50\u3002\u5b83\u9002\u7528\u4e8e\u771f\u5b9e\u6a21\u62df\u573a\u666f\uff0c\u7279\u522b\u662f\u5728\u4f7f\u7528\u6570\u5b57\u53cc\u80de\u80ce\u6a21\u62df\u7684LLM\u5bf9\u9f50\u4e2d\u5177\u6709\u663e\u8457\u7684\u6548\u679c\u3002\u5728\u7406\u8bba\u4e0a\uff0c\u8fd9\u79cd\u65b9\u6cd5\u63d0\u4f9b\u4e86\u591a\u9879\u5f0f\u6709\u9650\u6837\u672c\u8bef\u5dee\u754c\u3002", "result": "\u5728\u4e00\u7cfb\u5217\u6a21\u62df\u573a\u666f\u4e2d\uff0c\u4e0e\u6807\u51c6RLHF\u76f8\u6bd4\uff0c\u7b26\u53f7\u4f30\u8ba1\u5668\u901a\u8fc7\u51cf\u5c1135%\u7684\uff08\u89d2\u5ea6\uff09\u4f30\u8ba1\u8bef\u5dee\uff0c\u964d\u4f4e\u4e86\u6a21\u62df\u4eba\u7269\u504f\u597d\u7684\u626d\u66f2\u3002\u6b64\u5916\uff0c\u4e0e\u771f\u5b9e\u4eba\u53e3\u504f\u597d\u4e0d\u4e00\u81f4\u7684\u6bd4\u4f8b\u4ece12%\u51cf\u5c11\u5230\u4e868%\u3002\u76f8\u6bd4\u4e8e\u5176\u4ed6\u9700\u8981\u8003\u8651\u7528\u6237\u5f02\u8d28\u6027\u7684\u65b9\u6cd5\uff0c\u8fd9\u79cd\u65b0\u7684\u7b26\u53f7\u4f30\u8ba1\u5668\u65b9\u6cd5\u5728\u4e0d\u589e\u52a0\u590d\u6742\u5ea6\u7684\u60c5\u51b5\u4e0b\u63d0\u4f9b\u4e86\u4e00\u79cd\u66f4\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002", "conclusion": "\u65b0\u7684\u7b26\u53f7\u4f30\u8ba1\u5668\u65b9\u6cd5\u63d0\u4f9b\u4e86\u4e00\u79cd\u7b80\u5355\u3001\u9ad8\u6548\u4e14\u6709\u4fdd\u8bc1\u7684\u4f30\u8ba1\u65b9\u6cd5\uff0c\u80fd\u591f\u5728LLM\u5bf9\u9f50\u8fc7\u7a0b\u4e2d\u5904\u7406\u4eba\u7c7b\u504f\u597d\u5f02\u8d28\u6027\u95ee\u9898\u3002\u8be5\u65b9\u6cd5\u5728\u6a21\u62df\u5b9e\u9a8c\u4e2d\u6210\u529f\u5730\u51cf\u5c11\u4e86\u4f30\u8ba1\u8bef\u5dee\uff0c\u5e76\u4e14\u5728\u5904\u7406\u590d\u6742\u504f\u597d\u60c5\u51b5\u65f6\u63d0\u4f9b\u4e86\u6bd4\u6807\u51c6\u65b9\u6cd5\u66f4\u597d\u7684\u6548\u679c\u3002"}}
{"id": "2510.23639", "categories": ["cs.LG", "cs.AI", "q-bio.QM"], "pdf": "https://arxiv.org/pdf/2510.23639", "abs": "https://arxiv.org/abs/2510.23639", "authors": ["Jonathan Amar", "Edward Liu", "Alessandra Breschi", "Liangliang Zhang", "Pouya Kheradpour", "Sylvia Li", "Lisa Soleymani Lehmann", "Alessandro Giulianelli", "Matt Edwards", "Yugang Jia", "David Nola", "Raghav Mani", "Pankaj Vats", "Jesse Tetreault", "T. J. Chen", "Cory Y. McLean"], "title": "Integrating Genomics into Multimodal EHR Foundation Models", "comment": null, "summary": "This paper introduces an innovative Electronic Health Record (EHR) foundation\nmodel that integrates Polygenic Risk Scores (PRS) as a foundational data\nmodality, moving beyond traditional EHR-only approaches to build more holistic\nhealth profiles. Leveraging the extensive and diverse data from the All of Us\n(AoU) Research Program, this multimodal framework aims to learn complex\nrelationships between clinical data and genetic predispositions. The\nmethodology extends advancements in generative AI to the EHR foundation model\nspace, enhancing predictive capabilities and interpretability. Evaluation on\nAoU data demonstrates the model's predictive value for the onset of various\nconditions, particularly Type 2 Diabetes (T2D), and illustrates the interplay\nbetween PRS and EHR data. The work also explores transfer learning for custom\nclassification tasks, showcasing the architecture's versatility and efficiency.\nThis approach is pivotal for unlocking new insights into disease prediction,\nproactive health management, risk stratification, and personalized treatment\nstrategies, laying the groundwork for more personalized, equitable, and\nactionable real-world evidence generation in healthcare.", "AI": {"tldr": "\u8be5\u8bba\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u65b0\u7684\u533b\u7597\u7535\u5b50\u8bb0\u5f55\uff08EHR\uff09\u57fa\u7840\u6a21\u578b\uff0c\u6574\u5408\u4e86\u591a\u57fa\u56e0\u98ce\u9669\u8bc4\u5206\uff08PRS\uff09\uff0c\u8d85\u8d8a\u4e86\u4f20\u7edf\u7684\u4ec5\u4f9d\u8d56EHR\u7684\u65b9\u6cd5\uff0c\u6784\u5efa\u4e86\u66f4\u5168\u9762\u7684\u5065\u5eb7\u6863\u6848\u3002\u901a\u8fc7\u4ece\u201c\u6240\u6709\u4eba\u7684\u7814\u7a76\u8ba1\u5212\u201d\uff08AoU\uff09\u4e2d\u83b7\u5f97\u7684\u6570\u636e\uff0c\u8be5\u591a\u6a21\u6001\u6846\u67b6\u65e8\u5728\u5b66\u4e60\u4e34\u5e8a\u6570\u636e\u548c\u9057\u4f20\u6613\u611f\u6027\u4e4b\u95f4\u7684\u590d\u6742\u5173\u7cfb\uff0c\u5e76\u5728EHR\u57fa\u7840\u6a21\u578b\u9886\u57df\u63a8\u8fdb\u4e86\u751f\u6210\u5f0fAI\u7684\u5e94\u7528\uff0c\u63d0\u5347\u4e86\u9884\u6d4b\u80fd\u529b\u548c\u53ef\u89e3\u91ca\u6027\u3002\u5728AoU\u6570\u636e\u4e0a\u7684\u8bc4\u4f30\u8868\u660e\uff0c\u8be5\u6a21\u578b\u5bf9\u4e8e\u5404\u79cd\u75be\u75c5\u7684\u9884\u6d4b\u5177\u6709\u4ef7\u503c\uff0c\u7279\u522b\u662f2\u578b\u7cd6\u5c3f\u75c5\uff08T2D\uff09\uff0c\u540c\u65f6\u5c55\u793a\u4e86PRS\u548cEHR\u6570\u636e\u4e4b\u95f4\u7684\u76f8\u4e92\u4f5c\u7528\u3002\u8fd9\u9879\u5de5\u4f5c\u8fd8\u63a2\u7d22\u4e86\u4e3a\u5b9a\u5236\u5206\u7c7b\u4efb\u52a1\u8fdb\u884c\u8fc1\u79fb\u5b66\u4e60\uff0c\u5c55\u793a\u4e86\u8be5\u67b6\u6784\u7684\u591a\u529f\u80fd\u6027\u548c\u6548\u7387\u3002\u8be5\u65b9\u6cd5\u4e3a\u75be\u75c5\u9884\u6d4b\u3001\u4e3b\u52a8\u5065\u5eb7\u7ba1\u7406\u548c\u4e2a\u6027\u5316\u6cbb\u7597\u7b56\u7565\u5960\u5b9a\u4e86\u57fa\u7840\uff0c\u5f00\u542f\u4e86\u66f4\u4e2a\u6027\u5316\u3001\u66f4\u516c\u5e73\u548c\u66f4\u5177\u64cd\u4f5c\u6027\u7684\u73b0\u5b9e\u4e16\u754c\u8bc1\u636e\u751f\u6210\u7684\u524d\u666f\u3002", "motivation": "\u4f20\u7edf\u57fa\u4e8eEHR\u7684\u6570\u636e\u5206\u6790\u65b9\u6cd5\u5177\u6709\u5c40\u9650\u6027\uff0c\u800c\u591a\u57fa\u56e0\u98ce\u9669\u8bc4\u5206\uff08PRS\uff09\u7684\u5f15\u5165\u53ef\u4ee5\u586b\u8865\u9057\u4f20\u4fe1\u606f\u65b9\u9762\u7684\u7a7a\u767d\u3002\u8bba\u6587\u52a8\u673a\u5728\u4e8e\u521b\u65b0\u6027\u5730\u5c06PRS\u6574\u5408\u5230EHR\u4e2d\uff0c\u6784\u5efa\u66f4\u5168\u9762\u7684\u5065\u5eb7\u6863\u6848\uff0c\u5e76\u901a\u8fc7\u63d0\u5347\u9884\u6d4b\u80fd\u529b\u548c\u53ef\u89e3\u91ca\u6027\uff0c\u5b9e\u73b0\u66f4\u597d\u7684\u75be\u75c5\u7ba1\u7406\u548c\u4e2a\u6027\u5316\u533b\u7597\u3002\u4e3a\u4e86\u5b9e\u73b0\u8fd9\u4e00\u76ee\u6807\uff0c\u8bba\u6587\u4f5c\u8005\u91c7\u7528\u4e86\u591a\u6a21\u6001\u6846\u67b6\u548c\u5148\u8fdb\u7684\u7535\u5b50\u5065\u5eb7\u8bb0\u5f55\uff08EHR\uff09\u57fa\u7840\u6a21\u578b\uff0c\u5229\u7528\u751f\u6210\u5f0fAI\u6280\u672f\uff0c\u5e76\u8fdb\u884c\u4e86\u5e7f\u6cdb\u7684\u5b9e\u9a8c\u9a8c\u8bc1\u4ee5\u5c55\u793a\u5176\u4f18\u52bf\u3002\u8fd9\u9879\u5de5\u4f5c\u7684\u6700\u7ec8\u76ee\u6807\u662f\u4e3a\u75be\u75c5\u7684\u9884\u6d4b\u3001\u7ba1\u7406\u53ca\u6cbb\u7597\u63d0\u4f9b\u66f4\u5f3a\u6709\u529b\u7684\u5de5\u5177\uff0c\u63a8\u52a8\u533b\u5b66\u7814\u7a76\u7684\u8fdb\u6b65\u3002", "method": "\u5229\u7528All of Us (AoU) \u7814\u7a76\u8ba1\u5212\u4e2d\u4e30\u5bcc\u7684\u591a\u6a21\u6001\u6570\u636e\uff0c\u7814\u7a76\u56e2\u961f\u5f00\u53d1\u4e86\u4e00\u79cd\u5c06\u591a\u57fa\u56e0\u98ce\u9669\u8bc4\u5206 (PRS) \u6574\u5408\u4e3a\u7535\u5b50\u5065\u5eb7\u8bb0\u5f55\uff08EHR\uff09\u57fa\u7840\u6a21\u578b\u4e00\u90e8\u5206\u7684\u6280\u672f\u3002\u8be5\u65b9\u6cd5\u4e0d\u4ec5\u5229\u7528EHR\u91cc\u7684\u4e34\u5e8a\u6570\u636e\uff0c\u8fd8\u5c06\u5176\u4e0e\u9057\u4f20\u6570\u636e\u76f8\u7ed3\u5408\uff0c\u901a\u8fc7\u8fd0\u7528\u5148\u8fdb\u7684\u4eba\u5de5\u667a\u80fd\u6280\u672f\u8fdb\u884c\u6570\u636e\u6574\u5408\u4e0e\u6a21\u5f0f\u8bc6\u522b\u3002\u6b64\u5916\uff0c\u8fd8\u5728EHR\u57fa\u7840\u6a21\u578b\u4e2d\u6fc0\u6d3b\u4e86\u751f\u6210\u5f0fAI\u6280\u672f\uff0c\u4ee5\u63d0\u5347\u6574\u4f53\u5206\u6790\u7684\u9884\u6d4b\u6027\u548c\u89e3\u91ca\u6027\u3002\u6700\u540e\uff0c\u5bf9AoU\u6570\u636e\u8fdb\u884c\u4e86\u5b9e\u9a8c\u8bc4\u4f30\uff0c\u5448\u73b0\u4e86\u5229\u7528\u65b0\u6a21\u5f0f\u8fdb\u884c\u5065\u5eb7\u72b6\u51b5\u9884\u6d4b\u7684\u5b9e\u9645\u6548\u679c\uff0c\u5e76\u63a2\u8ba8\u4e86\u5176\u5728\u4e0d\u540c\u5e94\u7528\u573a\u666f\u4e2d\u5e94\u7528\u7684\u6f5c\u529b\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u5c06PRS\u6574\u5408\u5230EHR\u4e2d\u7684\u65b9\u6cd5\u80fd\u591f\u63d0\u5347\u5bf9\u8bf8\u5982T2D\u8fd9\u6837\u7684\u75be\u75c5\u53d1\u751f\u7387\u7684\u9884\u6d4b\u51c6\u786e\u6027\uff0c\u6709\u6548\u7684\u7ed3\u5408\u4e86\u4e34\u5e8a\u548c\u9057\u4f20\u7684\u6570\u636e\uff0c\u660e\u767d\u5c55\u73b0\u4e24\u8005\u4e4b\u95f4\u590d\u6742\u7684\u5173\u7cfb\u3002\u6a21\u578b\u4e0d\u4ec5\u80fd\u591f\u826f\u597d\u5730\u9884\u6d4b\u75be\u75c5\u7684\u53d1\u751f\uff0c\u8fd8\u5c55\u793a\u4e86\u5b83\u5728\u5b9a\u5236\u6d41\u7a0b\u4e2d\u7684\u9002\u7528\u6027\u548c\u591a\u529f\u80fd\u6027\uff0c\u5305\u62ec\u6f5c\u5728\u7684\u8f6c\u79fb\u5b66\u4e60\u5e94\u7528\u4ee5\u652f\u6301\u66f4\u4e3a\u4e13\u4e1a\u5316\u7684\u7279\u5b9a\u4efb\u52a1\u5206\u7c7b\u3002\u6b64\u5916\uff0c\u521d\u6b65\u7814\u7a76\u7ed3\u679c\u8868\u660e\u8be5\u6a21\u578b\u53ef\u80fd\u4f1a\u5728\u9884\u6d4b\u5176\u4ed6\u7c7b\u578b\u7684\u75be\u75c5\u4e0a\u4e5f\u6709\u8f83\u4e3a\u5e7f\u6cdb\u7684\u5e94\u7528\u573a\u666f\u3002", "conclusion": "\u5c06PRS\u6570\u636e\u6574\u5408\u8fdbEHR\u57fa\u7840\u6a21\u578b\u4e2d\uff0c\u4e3a\u75be\u75c5\u9884\u6d4b\u63d0\u4f9b\u4e86\u65b0\u7684\u65b9\u5411\uff0c\u6709\u671b\u8fdb\u4e00\u6b65\u63a8\u52a8\u4e3b\u52a8\u5065\u5eb7\u7ba1\u7406\u548c\u4e2a\u6027\u5316\u6cbb\u7597\u7b56\u7565\u7684\u53d1\u5c55\u3002\u4e5f\u9884\u793a\u7740\u66f4\u52a0\u4e2a\u6027\u5316\u3001\u7cbe\u51c6\u548c\u53ef\u884c\u7684\u533b\u7597\u4fdd\u5065\u6570\u636e\u751f\u6210\u3002\u540c\u65f6\uff0c\u8fd9\u9879\u7814\u7a76\u4e5f\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u65b9\u5411\uff0c\u5373\u901a\u8fc7\u66f4\u591a\u7684\u6570\u636e\u548c\u66f4\u591a\u7684\u7b97\u6cd5\u8fed\u4ee3\u6765\u63d0\u9ad8\u6a21\u578b\u7684\u51c6\u786e\u6027\u3001\u7a33\u5065\u6027\u548c\u89e3\u91ca\u6027\u3002"}}
{"id": "2510.24000", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.24000", "abs": "https://arxiv.org/abs/2510.24000", "authors": ["Heethanjan Kanagalingam", "Thenukan Pathmanathan", "Mokeeshan Vathanakumar", "Tharmakulasingam Mukunthan"], "title": "AdvBlur: Adversarial Blur for Robust Diabetic Retinopathy Classification and Cross-Domain Generalization", "comment": null, "summary": "Diabetic retinopathy (DR) is a leading cause of vision loss worldwide, yet\nearly and accurate detection can significantly improve treatment outcomes.\nWhile numerous Deep learning (DL) models have been developed to predict DR from\nfundus images, many face challenges in maintaining robustness due to\ndistributional variations caused by differences in acquisition devices,\ndemographic disparities, and imaging conditions. This paper addresses this\ncritical limitation by proposing a novel DR classification approach, a method\ncalled AdvBlur. Our method integrates adversarial blurred images into the\ndataset and employs a dual-loss function framework to address domain\ngeneralization. This approach effectively mitigates the impact of unseen\ndistributional variations, as evidenced by comprehensive evaluations across\nmultiple datasets. Additionally, we conduct extensive experiments to explore\nthe effects of factors such as camera type, low-quality images, and dataset\nsize. Furthermore, we perform ablation studies on blurred images and the loss\nfunction to ensure the validity of our choices. The experimental results\ndemonstrate the effectiveness of our proposed method, achieving competitive\nperformance compared to state-of-the-art domain generalization DR models on\nunseen external datasets.", "AI": {"tldr": "\u8fd9\u9879\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u7cd6\u5c3f\u75c5\u89c6\u7f51\u819c\u75c5\u53d8\uff08DR\uff09\u5206\u7c7b\u65b9\u6cd5\uff0c\u901a\u8fc7\u96c6\u6210\u5bf9\u6297\u6a21\u7cca\u56fe\u50cf\u548c\u4f7f\u7528\u53cc\u91cd\u635f\u5931\u51fd\u6570\u6846\u67b6\u6765\u63d0\u9ad8\u5bf9\u4e0d\u540c\u5206\u5e03\u53d8\u5316\u7684\u9c81\u68d2\u6027\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u672a\u89c1\u8fc7\u7684\u5916\u90e8\u6570\u636e\u96c6\u4e0a\u8fbe\u5230\u4e86\u4e0e\u6700\u5148\u8fdb\u7684\u9886\u57df\u6cdb\u5316DR\u6a21\u578b\u76f8\u5f53\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5728\u9884\u6d4bDR\u65f6\uff0c\u7531\u4e8e\u6210\u50cf\u8bbe\u5907\u3001\u4eba\u53e3\u7edf\u8ba1\u5b66\u5dee\u5f02\u548c\u6210\u50cf\u6761\u4ef6\u7684\u4e0d\u540c\u5bfc\u81f4\u7684\u6570\u636e\u5206\u5e03\u53d8\u5316\u800c\u96be\u4ee5\u4fdd\u6301\u9c81\u68d2\u6027\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e00\u91cd\u8981\u9650\u5236\uff0c\u63d0\u51fa\u4e86AdvBlur\u65b9\u6cd5\u3002", "method": "\u65b9\u6cd5\u5305\u62ec\u5c06\u5bf9\u6297\u6a21\u7cca\u56fe\u50cf\u96c6\u6210\u5230\u6570\u636e\u96c6\u4e2d\uff0c\u5e76\u5e94\u7528\u53cc\u91cd\u635f\u5931\u51fd\u6570\u6846\u67b6\uff0c\u4ee5\u89e3\u51b3\u9886\u57df\u6cdb\u5316\u7684\u95ee\u9898\u3002\u901a\u8fc7\u5e7f\u6cdb\u7684\u5b9e\u9a8c\u6765\u63a2\u7d22\u5982\u76f8\u673a\u7c7b\u578b\u3001\u4f4e\u8d28\u91cf\u56fe\u50cf\u548c\u6570\u636e\u96c6\u5927\u5c0f\u7b49\u4e0d\u540c\u56e0\u7d20\u7684\u5f71\u54cd\uff0c\u5e76\u8fdb\u884c\u6d88\u878d\u7814\u7a76\uff0c\u9a8c\u8bc1\u4e86\u6240\u9009\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u6709\u6548\u5730\u51cf\u8f7b\u4e86\u672a\u89c1\u5206\u5e03\u53d8\u5316\u7684\u5f71\u54cd\uff0c\u5e76\u5728\u672a\u89c1\u8fc7\u7684\u5916\u90e8\u6570\u636e\u96c6\u4e0a\u8fbe\u5230\u4e86\u4e0e\u6700\u5148\u8fdb\u7684\u9886\u57df\u6cdb\u5316DR\u6a21\u578b\u76f8\u5f53\u7684\u6027\u80fd\u3002", "conclusion": "\u901a\u8fc7\u63d0\u51fa\u7684AdvBlur\u65b9\u6cd5\uff0c\u53ef\u4ee5\u66f4\u6709\u6548\u5730\u9884\u6d4bDR\uff0c\u7279\u522b\u5728\u89e3\u51b3\u9886\u57df\u6cdb\u5316\u548c\u5404\u79cd\u5206\u5e03\u53d8\u5316\u95ee\u9898\u4e0a\u5177\u6709\u663e\u8457\u7684\u4f18\u8d8a\u6027\u3002"}}
{"id": "2510.23989", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.23989", "abs": "https://arxiv.org/abs/2510.23989", "authors": ["Shangde Gao", "Zelin Xu", "Zhe Jiang"], "title": "Learning Individual Movement Shifts After Urban Disruptions with Social Infrastructure Reliance", "comment": null, "summary": "Shifts in individual movement patterns following disruptive events can reveal\nchanging demands for community resources. However, predicting such shifts\nbefore disruptive events remains challenging for several reasons. First,\nmeasures are lacking for individuals' heterogeneous social infrastructure\nresilience (SIR), which directly influences their movement patterns, and\ncommonly used features are often limited or unavailable at scale, e.g.,\nsociodemographic characteristics. Second, the complex interactions between\nindividual movement patterns and spatial contexts have not been sufficiently\ncaptured. Third, individual-level movement may be spatially sparse and not\nwell-suited to traditional decision-making methods for movement predictions.\nThis study incorporates individuals' SIR into a conditioned deep learning model\nto capture the complex relationships between individual movement patterns and\nlocal spatial context using large-scale, sparse individual-level data. Our\nexperiments demonstrate that incorporating individuals' SIR and spatial context\ncan enhance the model's ability to predict post-event individual movement\npatterns. The conditioned model can capture the divergent shifts in movement\npatterns among individuals who exhibit similar pre-event patterns but differ in\nSIR.", "AI": {"tldr": "\u8be5\u7814\u7a76\u5c06\u4e2a\u4eba\u7684\u793e\u4f1a\u57fa\u7840\u8bbe\u65bd\u97e7\u6027(SIR)\u6574\u5408\u5230\u4e00\u4e2a\u6761\u4ef6\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u4e2d\uff0c\u4ee5\u6355\u6349\u5927\u89c4\u6a21\u7a00\u758f\u7684\u4e2a\u4eba\u7ea7\u6570\u636e\u4e2d\u4e2a\u4eba\u8fd0\u52a8\u6a21\u5f0f\u4e0e\u5c40\u90e8\u7a7a\u95f4\u73af\u5883\u4e4b\u95f4\u7684\u590d\u6742\u5173\u7cfb\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u6a21\u578b\u80fd\u591f\u66f4\u51c6\u786e\u5730\u9884\u6d4b\u4e8b\u4ef6\u540e\u7684\u4e2a\u4eba\u8fd0\u52a8\u6a21\u5f0f\uff0c\u7279\u522b\u662f\u5bf9\u4e8e\u5728\u4e8b\u4ef6\u524d\u8868\u73b0\u51fa\u76f8\u4f3c\u8fd0\u52a8\u6a21\u5f0f\u4f46SIR\u4e0d\u540c\u7684\u4e2a\u4f53\u4e4b\u95f4\u7684\u5206\u5316\u8fd0\u52a8\u6a21\u5f0f\u53d8\u5316\u7684\u9884\u6d4b\u5177\u6709\u4f18\u52bf\u3002", "motivation": "\u7814\u7a76\u65e8\u5728\u9884\u6d4b\u7834\u574f\u6027\u4e8b\u4ef6\u540e\u4e2a\u4eba\u8fd0\u52a8\u6a21\u5f0f\u7684\u53d8\u5316\uff0c\u4f46\u8fd9\u4e00\u76ee\u6807\u7684\u5b9e\u73b0\u9762\u4e34\u4e09\u5927\u6311\u6218\uff1a\u7f3a\u4e4f\u8861\u91cf\u4e2a\u4eba\u5f02\u8d28SIR\u7684\u6307\u6807\uff0c\u96be\u4ee5\u6355\u6349\u4e2a\u4eba\u8fd0\u52a8\u6a21\u5f0f\u4e0e\u7a7a\u95f4\u80cc\u666f\u7684\u590d\u6742\u4e92\u52a8\u5173\u7cfb\uff0c\u4ee5\u53ca\u4e2a\u4eba\u7ea7\u7a7a\u95f4\u7a00\u758f\u8fd0\u52a8\u6570\u636e\u4e0d\u9002\u7528\u4e8e\u4f20\u7edf\u51b3\u7b56\u7b97\u6cd5\u3002\u57fa\u4e8e\u8fd9\u4e9b\u6311\u6218\uff0c\u672c\u6587\u63d0\u51fa\u4e86\u6574\u5408\u4e2a\u4ebaSIR\u548c\u7a7a\u95f4\u80cc\u666f\u7684\u5bf9\u7b56\u3002", "method": "\u672c\u6587\u5f00\u53d1\u4e86\u4e00\u79cd\u6761\u4ef6\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff0c\u8be5\u6a21\u578b\u5c06\u4e2a\u4f53\u7684SIR\u548c\u7a7a\u95f4\u80cc\u666f\u6574\u5408\u8fdb\u53bb\uff0c\u901a\u8fc7\u5927\u89c4\u6a21\u3001\u7a00\u758f\u7684\u4e2a\u4eba\u7ea7\u6570\u636e\u6355\u6349\u4e2a\u4eba\u8fd0\u52a8\u6a21\u5f0f\u4e0e\u5c40\u90e8\u7a7a\u95f4\u73af\u5883\u7684\u5173\u7cfb\uff0c\u4ece\u800c\u66f4\u51c6\u786e\u5730\u9884\u6d4b\u4e8b\u4ef6\u540e\u7684\u4e2a\u4eba\u8fd0\u52a8\u6a21\u5f0f\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u5c06\u4e2a\u4f53SIR\u548c\u7a7a\u95f4\u80cc\u666f\u5143\u7d20\u6574\u5408\u5230\u6a21\u578b\u4e2d\uff0c\u53ef\u4ee5\u663e\u8457\u63d0\u5347\u6a21\u578b\u9884\u6d4b\u4e8b\u4ef6\u540e\u4e2a\u4eba\u8fd0\u52a8\u6a21\u5f0f\u7684\u80fd\u529b\uff0c\u7279\u522b\u662f\u4e00\u4e9b\u5728\u4e8b\u4ef6\u524d\u6709\u8fc7\u76f8\u4f3c\u8fd0\u52a8\u6a21\u5f0f\u4f46\u5728SIR\u4e0a\u6709\u5dee\u5f02\u7684\u4e2a\u4f53\u4e4b\u95f4\u7684\u5206\u5316\u8fd0\u52a8\u6a21\u5f0f\u53d8\u5316\u3002", "conclusion": "\u8be5\u7814\u7a76\u901a\u8fc7\u6574\u5408\u4e2a\u4ebaSIR\u548c\u7a7a\u95f4\u80cc\u666f\uff0c\u8bc1\u660e\u4e86\u6761\u4ef6\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5728\u9884\u6d4b\u7834\u88c2\u4e8b\u4ef6\u540e\u4e2a\u4eba\u8fd0\u52a8\u6a21\u5f0f\u53d8\u5316\u65b9\u9762\u7684\u80fd\u529b\uff0c\u6b64\u65b9\u6cd5\u5bf9\u63d0\u9ad8\u4e2a\u4eba\u8fd0\u52a8\u6a21\u5f0f\u9884\u6d4b\u7684\u51c6\u786e\u6027\u5177\u6709\u91cd\u8981\u8d21\u732e\u3002"}}
{"id": "2510.23640", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.23640", "abs": "https://arxiv.org/abs/2510.23640", "authors": ["Zihao Jing", "Yan Sun", "Yan Yi Li", "Sugitha Janarthanan", "Alana Deng", "Pingzhao Hu"], "title": "Structure-Aware Fusion with Progressive Injection for Multimodal Molecular Representation Learning", "comment": "Accepted by NeurIPS 2025", "summary": "Multimodal molecular models often suffer from 3D conformer unreliability and\nmodality collapse, limiting their robustness and generalization. We propose\nMuMo, a structured multimodal fusion framework that addresses these challenges\nin molecular representation through two key strategies. To reduce the\ninstability of conformer-dependent fusion, we design a Structured Fusion\nPipeline (SFP) that combines 2D topology and 3D geometry into a unified and\nstable structural prior. To mitigate modality collapse caused by naive fusion,\nwe introduce a Progressive Injection (PI) mechanism that asymmetrically\nintegrates this prior into the sequence stream, preserving modality-specific\nmodeling while enabling cross-modal enrichment. Built on a state space\nbackbone, MuMo supports long-range dependency modeling and robust information\npropagation. Across 29 benchmark tasks from Therapeutics Data Commons (TDC) and\nMoleculeNet, MuMo achieves an average improvement of 2.7% over the\nbest-performing baseline on each task, ranking first on 22 of them, including a\n27% improvement on the LD50 task. These results validate its robustness to 3D\nconformer noise and the effectiveness of multimodal fusion in molecular\nrepresentation. The code is available at: github.com/selmiss/MuMo.", "AI": {"tldr": "MuMo \u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u6784\u5316\u591a\u6a21\u6001\u878d\u5408\u6846\u67b6\uff0c\u901a\u8fc7\u51cf\u5c11\u4f9d\u8d56\u4e8e\u6784\u8c61\u7684\u878d\u5408\u4e0d\u7a33\u5b9a\u6027\u548c\u907f\u514d\u6a21\u6001\u584c\u9677\uff0c\u6539\u8fdb\u4e86\u5206\u5b50\u8868\u793a\u3002\u8be5\u6846\u67b6\u5728\u591a\u57fa\u51c6\u6d4b\u8bd5\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u9a8c\u8bc1\u4e86\u5176\u5728\u5206\u5b50\u8868\u793a\u4e2d\u7684\u591a\u6a21\u6001\u878d\u5408\u7684\u6709\u6548\u6027\u548c\u9c81\u68d2\u6027\u3002", "motivation": "\u591a\u6a21\u6001\u5206\u5b50\u6a21\u578b\u7ecf\u5e38\u53d7\u52303D\u6784\u8c61\u4e0d\u7a33\u5b9a\u6027\u548c\u6a21\u6001\u584c\u9677\u7684\u9650\u5236\uff0c\u5f71\u54cd\u5176\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002\u56e0\u6b64\u63d0\u51fa\u4e86MuMo\u6846\u67b6\u6765\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "MuMo\u6846\u67b6\u8bbe\u8ba1\u4e86\u7ed3\u6784\u5316\u878d\u5408\u7ba1\u9053(SFP)\uff0c\u5c062D\u62d3\u6251\u548c3D\u51e0\u4f55\u7ed3\u5408\u4e3a\u7edf\u4e00\u4e14\u7a33\u5b9a\u7684\u7ed3\u6784\u6027\u5148\u9a8c\uff1b\u8fd8\u5f15\u5165\u4e86\u6e10\u8fdb\u6ce8\u5165\uff08PI\uff09\u673a\u5236\uff0c\u4e0d\u5bf9\u79f0\u5730\u5c06\u8fd9\u79cd\u5148\u9a8c\u5408\u5e76\u5230\u5e8f\u5217\u6d41\u4e2d\uff0c\u4fdd\u6301\u6a21\u6001\u7279\u5b9a\u5efa\u6a21\u540c\u65f6\u4fc3\u8fdb\u8de8\u6a21\u5f0f\u4e30\u5bcc\u6027\u3002\u6846\u67b6\u57fa\u4e8e\u72b6\u6001\u7a7a\u95f4\u9aa8\u5e72\uff0c\u652f\u6301\u957f\u7a0b\u4f9d\u8d56\u5efa\u6a21\u548c\u9c81\u68d2\u4fe1\u606f\u4f20\u64ad\u3002", "result": "\u572829\u4e2a\u57fa\u51c6\u4efb\u52a1\u4e2d\uff0cMuMo\u76f8\u8f83\u6700\u4f73\u57fa\u7ebf\u6a21\u578b\u5e73\u5747\u6539\u8fdb\u4e862.7%\u7684\u6027\u80fd\uff0c\u5176\u4e2d\u5728LD50\u4efb\u52a1\u4e2d\u63d0\u5347\u4e8627%\uff0c\u572822\u4e2a\u4efb\u52a1\u4e2d\u6392\u540d\u9996\u4f4d\u3002\u7ed3\u679c\u9a8c\u8bc1\u4e86\u6846\u67b6\u5bf93D\u6784\u8c61\u566a\u58f0\u7684\u9c81\u68d2\u6027\u548c\u591a\u6a21\u6001\u878d\u5408\u7684\u6709\u6548\u6027\u3002", "conclusion": "MuMo\u901a\u8fc7\u63d0\u4f9b\u4e00\u79cd\u7a33\u5b9a\u4e14\u6709\u6548\u7684\u591a\u6a21\u6001\u878d\u5408\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u5206\u5b50\u8868\u793a\u7684\u8d28\u91cf\uff0c\u5c55\u793a\u4e86\u5176\u5728\u836f\u7269\u7814\u53d1\u7b49\u9886\u57df\u7684\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2510.24416", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2510.24416", "abs": "https://arxiv.org/abs/2510.24416", "authors": ["Nikhat Khan", "E. M. H. E. B. Ekanayake", "Nicolas Casilli", "Cristian Cassella", "Luke Theogarajan", "Nikhil Shukla"], "title": "Analyzing Parametric Oscillator Ising Machines through the Kuramoto Lens", "comment": null, "summary": "Networks of coupled nonlinear oscillators are emerging as powerful physical\nplatforms for implementing Ising machines. Yet the relationship between\nparametric-oscillator implementations and traditional oscillator-based Ising\nmachines remains underexplored. In this work, we develop a Kuramoto-style,\ncanonical phase description of parametric oscillator Ising machines by starting\nfrom the Stuart-Landau oscillator model- the canonical normal form near a Hopf\nbifurcation, and a natural reduced description for many parametric oscillator\nimplementations such as the degenerate optical parametric oscillator (DOPO)\namong others. The resulting phase dynamics combine the usual phase-difference\ncoupling observed in the standard Kuramoto model along with an intrinsic phase\nsum term that is generated when conjugate coupling is considered. Moreover, our\nformulation helps explain why explicit second-harmonic driving is unnecessary\nin parametric oscillators and also reveals how quasi-steady amplitude\nheterogeneity scales the original strength of the spin interaction with\npotentially adverse impacts on the solution quality. Our work helps develop a\nunifying view of the oscillator-based approach to designing Ising machines.", "AI": {"tldr": "\u672c\u6587\u5f00\u53d1\u4e86\u4e00\u4e2a\u57fa\u4e8eKuramoto\u6a21\u578b\u7684\u76f8\u4f4d\u63cf\u8ff0\uff0c\u7528\u4e8e\u63cf\u8ff0\u53c2\u6570\u632f\u8361\u5668\u4f0a\u8f9b\u673a\uff0c\u5e76\u89e3\u91ca\u4e86\u4e3a\u4ec0\u4e48\u5728\u53c2\u6570\u632f\u8361\u5668\u4e2d\u4e0d\u9700\u8981\u663e\u5f0f\u7684\u4e8c\u6b21\u8c10\u6ce2\u9a71\u52a8\u3002\u6b64\u5916\uff0c\u8fd8\u63a2\u8ba8\u4e86\u51c6\u7a33\u6001\u632f\u5e45\u5f02\u8d28\u6027\u5982\u4f55\u5f71\u54cd\u89e3\u51b3\u65b9\u6848\u7684\u8d28\u91cf\uff0c\u63d0\u4f9b\u4e86\u6784\u9020\u4f0a\u8f9b\u673a\u7684\u65b0\u89c6\u89d2\u3002", "motivation": "\u5f53\u524d\u53c2\u6570\u632f\u8361\u5668\u5b9e\u73b0\u548c\u4f20\u7edf\u632f\u8361\u5668\u4f0a\u8f9b\u673a\u4e4b\u95f4\u7684\u5173\u7cfb\u5c1a\u5f85\u6df1\u5165\u7814\u7a76\u3002\u4e3a\u4e86\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u4f5c\u8005\u5e0c\u671b\u5f00\u53d1\u4e00\u4e2a\u901a\u7528\u7684\u7406\u8bba\u6846\u67b6\u6765\u6307\u5bfc\u53c2\u6570\u632f\u8361\u5668\u4f0a\u8f9b\u673a\u7684\u8bbe\u8ba1\u548c\u7406\u89e3\u3002", "method": "\u4eceStuart-Landau\u632f\u5b50\u6a21\u578b\u51fa\u53d1\uff0c\u53d1\u5c55\u4e86\u4e00\u4e2aKuramoto\u578b\u3001\u89c4\u8303\u76f8\u4f4d\u63cf\u8ff0\u7684\u624b\u6cd5\uff0c\u7528\u4ee5\u9610\u91ca\u53c2\u6570\u632f\u8361\u5668\u4f0a\u8f9b\u673a\u7684\u5de5\u4f5c\u539f\u7406\uff0c\u5e76\u4ee5\u6b64\u63ed\u793a\u53c2\u6570\u632f\u8361\u5668\u4e2d\u76f8\u5f02\u7b56\u7565\u7684\u539f\u7406\u4e0e\u7ed3\u679c\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u5728\u53c2\u6570\u632f\u8361\u5668\u4e2d\u4e0d\u4f7f\u7528\u663e\u5f0f\u7684\u4e8c\u6b21\u8c10\u6ce2\u9a71\u52a8\u662f\u5408\u7406\u7684\uff0c\u51c6\u7a33\u6001\u632f\u5e45\u5f02\u8d28\u6027\u5bf9\u89e3\u51b3\u8d28\u91cf\u6709\u6f5c\u5728\u7684\u8d1f\u9762\u5f71\u54cd\u3002\u672c\u6587\u7684\u5206\u6790\u7ed3\u679c\u4e3a\u7406\u89e3\u53c2\u6570\u632f\u8361\u5668\u4f0a\u8f9b\u673a\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7edf\u4e00\u7684\u89c6\u89d2\u3002", "conclusion": "\u6211\u4eec\u7684\u7814\u7a76\u4e3a\u7406\u89e3\u53c2\u6570\u632f\u8361\u5668\u4f0a\u8f9b\u673a\u63d0\u4f9b\u4e86\u4e00\u4e2a\u65b0\u9896\u7684\u7406\u8bba\u6846\u67b6\uff0c\u6709\u5229\u4e8e\u5f00\u53d1\u66f4\u6709\u6548\u7684Ising\u673a\u5668\u8bbe\u8ba1\u3002"}}
{"id": "2510.24009", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.24009", "abs": "https://arxiv.org/abs/2510.24009", "authors": ["Yuan Jin", "Antonio Pepe", "Gian Marco Melito", "Yuxuan Chen", "Yunsu Byeon", "Hyeseong Kim", "Kyungwon Kim", "Doohyun Park", "Euijoon Choi", "Dosik Hwang", "Andriy Myronenko", "Dong Yang", "Yufan He", "Daguang Xu", "Ayman El-Ghotni", "Mohamed Nabil", "Hossam El-Kady", "Ahmed Ayyad", "Amr Nasr", "Marek Wodzinski", "Henning M\u00fcller", "Hyeongyu Kim", "Yejee Shin", "Abbas Khan", "Muhammad Asad", "Alexander Zolotarev", "Caroline Roney", "Anthony Mathur", "Martin Benning", "Gregory Slabaugh", "Theodoros Panagiotis Vagenas", "Konstantinos Georgas", "George K. Matsopoulos", "Jihan Zhang", "Zhen Zhang", "Liqin Huang", "Christian Mayer", "Heinrich M\u00e4chler", "Jan Egger"], "title": "Towards the Automatic Segmentation, Modeling and Meshing of the Aortic Vessel Tree from Multicenter Acquisitions: An Overview of the SEG.A. 2023 Segmentation of the Aorta Challenge", "comment": null, "summary": "The automated analysis of the aortic vessel tree (AVT) from computed\ntomography angiography (CTA) holds immense clinical potential, but its\ndevelopment has been impeded by a lack of shared, high-quality data. We\nlaunched the SEG.A. challenge to catalyze progress in this field by introducing\na large, publicly available, multi-institutional dataset for AVT segmentation.\nThe challenge benchmarked automated algorithms on a hidden test set, with\nsubsequent optional tasks in surface meshing for computational simulations. Our\nfindings reveal a clear convergence on deep learning methodologies, with 3D\nU-Net architectures dominating the top submissions. A key result was that an\nensemble of the highest-ranking algorithms significantly outperformed\nindividual models, highlighting the benefits of model fusion. Performance was\nstrongly linked to algorithmic design, particularly the use of customized\npost-processing steps, and the characteristics of the training data. This\ninitiative not only establishes a new performance benchmark but also provides a\nlasting resource to drive future innovation toward robust, clinically\ntranslatable tools.", "AI": {"tldr": "SEGA\u6311\u6218\u8d5b\u63a8\u51fa\u4e86\u4e00\u4e2a\u5927\u578b\u7684\u516c\u5171\u591a\u673a\u6784\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u4e3b\u52a8\u8109\u8840\u7ba1\u6811\u7684\u5206\u5272\uff0c\u5438\u5f15\u4e86\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u65b9\u6cd5\uff0c\u7279\u522b\u662f3D U-Net\uff0c\u8868\u73b0\u51fa\u8272\u3002\u6a21\u578b\u878d\u5408\u663e\u793a\u51fa\u6bd4\u5355\u4e00\u6a21\u578b\u66f4\u597d\u7684\u6027\u80fd\u3002\u8fd9\u9879\u7814\u7a76\u4e3a\u672a\u6765\u7684\u7814\u7a76\u63d0\u4f9b\u4e86\u5b9d\u8d35\u7684\u8d44\u6e90\u548c\u65b0\u7684\u6027\u80fd\u57fa\u51c6\uff0c\u4fc3\u8fdb\u4e86\u4e34\u5e8a\u53ef\u7ffb\u8bd1\u5de5\u5177\u7684\u53d1\u5c55\u3002", "motivation": "\u81ea\u52a8\u5316\u5206\u6790\u4e3b\u52a8\u8109\u8840\u7ba1\u6811(AVT)\u5728\u4e34\u5e8a\u4e2d\u5177\u6709\u5de8\u5927\u6f5c\u529b\uff0c\u4f46\u7531\u4e8e\u7f3a\u4e4f\u9ad8\u8d28\u91cf\u7684\u5171\u4eab\u6570\u636e\uff0c\u5176\u53d1\u5c55\u53d7\u5230\u963b\u788d\u3002\u6b64\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u5f15\u5165\u65b0\u7684\u6570\u636e\u96c6\u6765\u63a8\u52a8AVT\u5206\u5272\u9886\u57df\u7684\u8fdb\u6b65\u3002", "method": "SEGA\u6311\u6218\u8d5b\u901a\u8fc7\u516c\u5f00\u5927\u578b\u591a\u673a\u6784\u6570\u636e\u96c6\uff0c\u4ee5\u53ca\u5bf9\u9690\u85cf\u6d4b\u8bd5\u96c6\u7684\u6027\u80fd\u8bc4\u4f30\uff0c\u6765\u4fc3\u8fdb\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u7684\u53d1\u5c55\uff0c\u7279\u522b\u662f3D U-Net\u5728AVT\u5206\u5272\u4e2d\u7684\u5e94\u7528\uff0c\u540c\u65f6\u4e5f\u63a2\u7d22\u4e86\u6a21\u578b\u878d\u5408\u7684\u65b9\u6cd5\u3002", "result": "\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\uff0c\u7279\u522b\u662f3D U-Net\u67b6\u6784\uff0c\u8868\u73b0\u6700\u4f18\u3002\u6a21\u578b\u878d\u5408\u76f8\u6bd4\u4e8e\u5355\u4e2a\u6a21\u578b\u6709\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\u3002\u53d1\u73b0\u7b97\u6cd5\u7684\u8bbe\u8ba1\uff0c\u7279\u522b\u662f\u5b9a\u5236\u7684\u540e\u5904\u7406\u6b65\u9aa4\uff0c\u4ee5\u53ca\u8bad\u7ec3\u6570\u636e\u7684\u7279\u6027\u662f\u5f71\u54cd\u6027\u80fd\u7684\u91cd\u8981\u56e0\u7d20\u3002", "conclusion": "\u6b64\u6b21\u7814\u7a76\u4e3aAVT\u5206\u5272\u63d0\u4f9b\u4e86\u65b0\u7684\u6027\u80fd\u57fa\u51c6\uff0c\u5e76\u4e14\u4e3a\u672a\u6765\u7684\u7814\u7a76\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6301\u4e45\u7684\u8d44\u6e90\uff0c\u6709\u52a9\u4e8e\u5f00\u53d1\u66f4\u7a33\u5065\u7684\u4e34\u5e8a\u53ef\u7ffb\u8bd1\u5de5\u5177\u3002"}}
{"id": "2510.23641", "categories": ["cs.LG", "cs.AI", "hep-ex", "physics.ins-det"], "pdf": "https://arxiv.org/pdf/2510.23641", "abs": "https://arxiv.org/abs/2510.23641", "authors": ["Aaron Wang", "Zihan Zhao", "Subash Katel", "Vivekanand Gyanchand Sahu", "Elham E Khoda", "Abhijith Gandrakota", "Jennifer Ngadiuba", "Richard Cavanaugh", "Javier Duarte"], "title": "Spatially Aware Linear Transformer (SAL-T) for Particle Jet Tagging", "comment": null, "summary": "Transformers are very effective in capturing both global and local\ncorrelations within high-energy particle collisions, but they present\ndeployment challenges in high-data-throughput environments, such as the CERN\nLHC. The quadratic complexity of transformer models demands substantial\nresources and increases latency during inference. In order to address these\nissues, we introduce the Spatially Aware Linear Transformer (SAL-T), a\nphysics-inspired enhancement of the linformer architecture that maintains\nlinear attention. Our method incorporates spatially aware partitioning of\nparticles based on kinematic features, thereby computing attention between\nregions of physical significance. Additionally, we employ convolutional layers\nto capture local correlations, informed by insights from jet physics. In\naddition to outperforming the standard linformer in jet classification tasks,\nSAL-T also achieves classification results comparable to full-attention\ntransformers, while using considerably fewer resources with lower latency\nduring inference. Experiments on a generic point cloud classification dataset\n(ModelNet10) further confirm this trend. Our code is available at\nhttps://github.com/aaronw5/SAL-T4HEP.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6539\u8fdb\u7684\u53d8\u538b\u5668\u67b6\u6784\u2014\u2014\u7a7a\u95f4\u611f\u77e5\u7ebf\u6027\u53d8\u538b\u5668(SAL-T)\uff0c\u8be5\u67b6\u6784\u65e8\u5728\u89e3\u51b3\u9ad8\u80fd\u7c92\u5b50\u78b0\u649e\u5206\u6790\u4e2d\u5728\u9ad8\u6570\u636e\u541e\u5410\u91cf\u73af\u5883\u4e0b\u7684\u5e94\u7528\u96be\u9898\u3002SAL-T\u5229\u7528\u4e86\u7269\u7406\u5b66\u77e5\u8bc6\uff0c\u5e76\u901a\u8fc7\u7a7a\u95f4\u611f\u77e5\u5206\u533a\u548c\u5377\u79ef\u5c42\u6765\u964d\u4f4e\u8ba1\u7b97\u590d\u6742\u5ea6\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u9ad8\u51c6\u786e\u5ea6\u3002\u5b9e\u9a8c\u8868\u660e\uff0cSAL-T\u5728\u8d44\u6e90\u6d88\u8017\u548c\u63a8\u65ad\u5ef6\u8fdf\u4e0a\u4f4e\u4e8e\u6807\u51c6\u7684\u7ebf\u6027\u53d8\u538b\u5668\uff0c\u800c\u5728\u51c6\u786e\u5ea6\u4e0a\u5219\u4e0e\u5168\u6ce8\u610f\u529b\u53d8\u538b\u5668\u76f8\u5f53\uff0c\u5e76\u5728\u4e00\u5b9a\u7a0b\u5ea6\u4e0a\u8d85\u8d8a\u4e86\u5b83\u4eec\u3002", "motivation": "\u4f20\u7edf\u7684\u53d8\u538b\u5668\u6a21\u578b\u5728\u5904\u7406\u9ad8\u6570\u636e\u541e\u5410\u91cf\uff08\u5982CERN LHC\u5b9e\u9a8c\uff09\u65f6\u5b58\u5728\u8ba1\u7b97\u590d\u6742\u5ea6\u8fc7\u9ad8\u3001\u8d44\u6e90\u9700\u6c42\u5927\u4ee5\u53ca\u63a8\u65ad\u5ef6\u8fdf\u7b49\u95ee\u9898\uff0c\u4ece\u800c\u963b\u788d\u4e86\u5b83\u4eec\u5728\u9ad8\u80fd\u7269\u7406\u5206\u6790\u4e2d\u7684\u5e94\u7528\u3002\u56e0\u6b64\uff0c\u4f5c\u8005\u63d0\u51fa\u4e86SAL-T\uff0c\u65e8\u5728\u964d\u4f4e\u8ba1\u7b97\u590d\u6742\u5ea6\u540c\u65f6\u4fdd\u6301\u6216\u63d0\u9ad8\u6a21\u578b\u7684\u9884\u6d4b\u6027\u80fd\uff0c\u4ee5\u6b64\u89e3\u51b3\u8fd9\u4e9b\u6311\u6218\u3002\u6b64\u5916\uff0c\u4f5c\u8005\u5e0c\u671b\u901a\u8fc7\u5c06\u7269\u7406\u77e5\u8bc6\u878d\u5165\u6a21\u578b\u8bbe\u8ba1\u4e2d\uff0c\u53ef\u4ee5\u63d0\u5347\u6a21\u578b\u5728\u9ad8\u80fd\u7c92\u5b50\u78b0\u649e\u6570\u636e\u4e2d\u7684\u5e94\u7528\u6548\u679c\u3002", "method": "SAL-T\u7ed3\u5408\u7269\u7406\u5b66\u539f\u7406\uff0c\u901a\u8fc7\u7a7a\u95f4\u611f\u77e5\u5206\u533a\uff0c\u57fa\u4e8e\u9897\u7c92\u7269\u7684\u8fd0\u52a8\u5b66\u7279\u6027\uff0c\u5212\u5206\u7269\u7406\u663e\u8457\u533a\u57df\uff0c\u5e76\u4f7f\u7528\u5377\u79ef\u5c42\u6355\u6349\u5c40\u90e8\u76f8\u5173\u6027\u3002\u8be5\u65b9\u6cd5\u5de7\u5999\u878d\u5408\u4e86\u7269\u7406\u77e5\u8bc6\uff0c\u964d\u4f4e\u4e86\u8ba1\u7b97\u590d\u6742\u5ea6\uff0c\u5e76\u4fdd\u7559\u4e86\u7ebf\u6027\u6ce8\u610f\u673a\u5236\u3002\u5728\u5177\u4f53\u7684\u4efb\u52a1\u4e2d\uff0cSAL-T\u5c55\u793a\u4e86\u8f83\u597d\u7684\u6027\u80fd\uff0c\u5c24\u5176\u662f\u5728\u8d44\u6e90\u5206\u914d\u548c\u63a8\u65ad\u5ef6\u8fdf\u4e0a\u76f8\u8f83\u4e8e\u6807\u51c6\u7684\u7ebf\u6027\u53d8\u538b\u5668\u6a21\u578b\u6709\u663e\u8457\u63d0\u5347\u3002\u6b64\u5916\uff0c\u5b9e\u9a8c\u7ed3\u679c\u4e5f\u9a8c\u8bc1\u4e86\u8fd9\u79cd\u65b0\u7684\u67b6\u6784\u5728\u975e\u4e13\u95e8\u5316\u7684\u70b9\u4e91\u5206\u7c7b\u4efb\u52a1\u4e0a\u7684\u6f5c\u529b\u3002", "result": "SAL-T\u5728\u8d44\u6e90\u6d88\u8017\u3001\u63a8\u65ad\u8fc7\u7a0b\u7684\u5ef6\u8fdf\u4ee5\u53ca\u6027\u80fd\u4e0a\u76f8\u8f83\u4e8e\u4f20\u7edf\u7684\u7ebf\u6027\u53d8\u538b\u5668\u6a21\u578b\u6709\u663e\u8457\u4f18\u52bf\uff0c\u540c\u65f6\u5728\u4e00\u4e9b\u7279\u5b9a\u4efb\u52a1\u4e0a\u7684\u6027\u80fd\u751a\u81f3\u80fd\u8fbe\u5230\u5168\u6ce8\u610f\u529b\u53d8\u538b\u5668\u7684\u6548\u679c\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u5728\u9ad8\u80fd\u7c92\u5b50\u78b0\u649e\u7684\u6570\u636e\u5206\u6790\u4e0a\uff0c\u4ee5\u53ca\u5728\u901a\u7528\u7684\u70b9\u4e91\u5206\u7c7b\u6570\u636e\u96c6\u4e0a\uff08\u6bd4\u5982ModelNet10\uff09\uff0cSAL-T\u90fd\u663e\u793a\u51fa\u4f18\u8d8a\u7684\u8868\u73b0\u3002", "conclusion": "SAL-T\u4f5c\u4e3a\u4e00\u79cd\u6539\u8fdb\u7684\u7269\u7406\u611f\u77e5\u53d8\u538b\u5668\u67b6\u6784\uff0c\u5c55\u793a\u4e86\u5728\u9ad8\u80fd\u7c92\u5b50\u78b0\u649e\u5206\u6790\u548c\u901a\u7528\u6570\u636e\u96c6\u4e0a\u7684\u5e94\u7528\u6f5c\u529b\uff0c\u5b83\u7684\u521b\u65b0\u8bbe\u8ba1\u6709\u671b\u89e3\u51b3\u5927\u89c4\u6a21\u6570\u636e\u96c6\u5206\u6790\u4e2d\u7684\u590d\u6742\u6027\u548c\u6548\u7387\u95ee\u9898\uff0c\u4e3a\u9ad8\u80fd\u7269\u7406\u5b66\u4ee5\u53ca\u5176\u4ed6\u9700\u8981\u5904\u7406\u5927\u89c4\u6a21\u6570\u636e\u7684\u5e94\u7528\u9886\u57df\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.24010", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.24010", "abs": "https://arxiv.org/abs/2510.24010", "authors": ["Mirali Purohit", "Bimal Gajera", "Vatsal Malaviya", "Irish Mehta", "Kunal Kasodekar", "Jacob Adler", "Steven Lu", "Umaa Rebbapragada", "Hannah Kerner"], "title": "Mars-Bench: A Benchmark for Evaluating Foundation Models for Mars Science Tasks", "comment": "Accepted at NeurIPS 2025", "summary": "Foundation models have enabled rapid progress across many specialized domains\nby leveraging large-scale pre-training on unlabeled data, demonstrating strong\ngeneralization to a variety of downstream tasks. While such models have gained\nsignificant attention in fields like Earth Observation, their application to\nMars science remains limited. A key enabler of progress in other domains has\nbeen the availability of standardized benchmarks that support systematic\nevaluation. In contrast, Mars science lacks such benchmarks and standardized\nevaluation frameworks, which have limited progress toward developing foundation\nmodels for Martian tasks. To address this gap, we introduce Mars-Bench, the\nfirst benchmark designed to systematically evaluate models across a broad range\nof Mars-related tasks using both orbital and surface imagery. Mars-Bench\ncomprises 20 datasets spanning classification, segmentation, and object\ndetection, focused on key geologic features such as craters, cones, boulders,\nand frost. We provide standardized, ready-to-use datasets and baseline\nevaluations using models pre-trained on natural images, Earth satellite data,\nand state-of-the-art vision-language models. Results from all analyses suggest\nthat Mars-specific foundation models may offer advantages over general-domain\ncounterparts, motivating further exploration of domain-adapted pre-training.\nMars-Bench aims to establish a standardized foundation for developing and\ncomparing machine learning models for Mars science. Our data, models, and code\nare available at: https://mars-bench.github.io/.", "AI": {"tldr": "Mars-Bench \u662f\u9996\u4e2a\u4e13\u95e8\u4e3a\u706b\u661f\u76f8\u5173\u4efb\u52a1\u8bbe\u8ba1\u7684\u8bc4\u4f30\u57fa\u51c6\uff0c\u6db5\u76d6\u5206\u7c7b\u3001\u5206\u5272\u548c\u76ee\u6807\u68c0\u6d4b\u7b49\u4efb\u52a1\uff0c\u9996\u6b21\u63d0\u4f9b\u4e86\u7cfb\u7edf\u6027\u8bc4\u4f30\u706b\u661f\u4efb\u52a1\u76f8\u5173\u6a21\u578b\u7684\u6807\u51c6\u6846\u67b6\u548c\u6570\u636e\u96c6", "motivation": "\u5728\u5730\u7403\u89c2\u6d4b\u7b49\u9886\u57df\uff0c\u6807\u51c6\u5316\u57fa\u51c6\u7684\u7f3a\u4e4f\u9650\u5236\u4e86\u706b\u661f\u79d1\u5b66\u7814\u7a76\u7684\u53d1\u5c55\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u5f15\u5165 Mars-Bench \u6765\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u63a8\u52a8\u706b\u661f\u9886\u57df\u57fa\u7840\u6a21\u578b\u7684\u53d1\u5c55", "method": "\u63d0\u4f9b\u6807\u51c6\u5316\u3001\u5373\u7528\u578b\u7684\u6570\u636e\u96c6\u4ee5\u53ca\u57fa\u4e8e\u81ea\u7136\u56fe\u50cf\u3001\u5730\u7403\u536b\u661f\u6570\u636e\u548c\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u7684\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u57fa\u7ebf\u8bc4\u4f30", "result": "\u6240\u6709\u5206\u6790\u7ed3\u679c\u8868\u660e\uff0c\u7279\u5b9a\u9886\u57df\u7684\u57fa\u7840\u6a21\u578b\u5bf9\u706b\u661f\u4efb\u52a1\u53ef\u80fd\u6709\u4f18\u52bf\uff0c\u8fd9\u9f13\u52b1\u4e86\u5bf9\u9886\u57df\u9002\u5e94\u6027\u9884\u8bad\u7ec3\u7684\u8fdb\u4e00\u6b65\u63a2\u7d22", "conclusion": "Mars-Bench \u5efa\u7acb\u4e86\u5f00\u53d1\u548c\u6bd4\u8f83\u706b\u661f\u79d1\u5b66\u7814\u7a76\u7528\u673a\u5668\u5b66\u4e60\u6a21\u578b\u7684\u6807\u51c6\u6846\u67b6\u3002\u6570\u636e\u3001\u6a21\u578b\u548c\u4ee3\u7801\u53ef\u4ece\u9879\u76ee\u5b98\u7f51\u4e0b\u8f7d"}}
{"id": "2510.24028", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24028", "abs": "https://arxiv.org/abs/2510.24028", "authors": ["Tingyue Pan", "Mingyue Cheng", "Shilong Zhang", "Zhiding Liu", "Xiaoyu Tao", "Yucong Luo", "Jintao Zhang", "Qi Liu"], "title": "OneCast: Structured Decomposition and Modular Generation for Cross-Domain Time Series Forecasting", "comment": null, "summary": "Cross-domain time series forecasting is a valuable task in various web\napplications. Despite its rapid advancement, achieving effective generalization\nacross heterogeneous time series data remains a significant challenge. Existing\nmethods have made progress by extending single-domain models, yet often fall\nshort when facing domain-specific trend shifts and inconsistent periodic\npatterns. We argue that a key limitation lies in treating temporal series as\nundifferentiated sequence, without explicitly decoupling their inherent\nstructural components. To address this, we propose OneCast, a structured and\nmodular forecasting framework that decomposes time series into seasonal and\ntrend components, each modeled through tailored generative pathways.\nSpecifically, the seasonal component is captured by a lightweight projection\nmodule that reconstructs periodic patterns via interpretable basis functions.\nIn parallel, the trend component is encoded into discrete tokens at segment\nlevel via a semantic-aware tokenizer, and subsequently inferred through a\nmasked discrete diffusion mechanism. The outputs from both branches are\ncombined to produce a final forecast that captures seasonal patterns while\ntracking domain-specific trends. Extensive experiments across eight domains\ndemonstrate that OneCast mostly outperforms state-of-the-art baselines.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aOneCast\u7684\u8de8\u57df\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u901a\u8fc7\u5206\u89e3\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u4e3a\u5b63\u8282\u6027\u548c\u8d8b\u52bf\u6027\u7ec4\u6210\u90e8\u5206\uff0c\u5e76\u5206\u522b\u91c7\u7528\u7279\u5b9a\u7684\u751f\u6210\u65b9\u6cd5\u6765\u9884\u6d4b\u3002\u5b9e\u9a8c\u663e\u793aOneCast\u5728\u591a\u4e2a\u6570\u636e\u57df\u4e0a\u7684\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u5f53\u524d\u7684\u8de8\u57df\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u65b9\u6cd5\u5728\u9762\u5bf9\u7279\u5b9a\u9886\u57df\u7684\u8d8b\u52bf\u53d8\u5316\u548c\u4e0d\u4e00\u81f4\u7684\u5468\u671f\u6a21\u5f0f\u65f6\u8868\u73b0\u4e0d\u4f73\uff0c\u8fd9\u662f\u56e0\u4e3a\u5b83\u4eec\u672a\u80fd\u660e\u786e\u5730\u533a\u5206\u65f6\u95f4\u5e8f\u5217\u7684\u57fa\u672c\u7ed3\u6784\u7ec4\u4ef6\u3002\u4e3a\u4e86\u514b\u670d\u8fd9\u4e2a\u9650\u5236\uff0c\u63d0\u51fa\u4e86\u4e00\u4e2a\u7ed3\u6784\u5316\u7684\u9884\u6d4b\u6846\u67b6\u6765\u63d0\u5347\u9884\u6d4b\u7684\u51c6\u786e\u6027\u548c\u9002\u5e94\u6027\u3002", "method": "OneCast\u901a\u8fc7\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u7684\u6295\u5f71\u6a21\u5757\u548c\u4e00\u4e2a\u8bed\u4e49\u611f\u77e5\u7684\u6807\u8bb0\u5668\u5206\u522b\u63d0\u53d6\u65f6\u95f4\u5e8f\u5217\u4e2d\u7684\u5b63\u8282\u6027\u548c\u8d8b\u52bf\u6027\u7ec4\u6210\u90e8\u5206\u3002\u5b63\u8282\u6027\u90e8\u5206\u901a\u8fc7\u53ef\u89e3\u91ca\u7684\u57fa\u51fd\u6570\u6765\u91cd\u5efa\u5468\u671f\u6a21\u5f0f\uff0c\u800c\u8d8b\u52bf\u6027\u90e8\u5206\u5219\u88ab\u8f6c\u5316\u4e3a\u79bb\u6563\u6807\u8bb0\u5e76\u901a\u8fc7\u63a9\u7801\u79bb\u6563\u6269\u6563\u673a\u5236\u6765\u9884\u6d4b\u3002\u6700\u540e\u5c06\u8fd9\u4e24\u90e8\u5206\u7684\u8f93\u51fa\u7ed3\u5408\u8d77\u6765\u751f\u6210\u6700\u7ec8\u7684\u9884\u6d4b\u3002", "result": "\u5728\u516b\u4e2a\u4e0d\u540c\u9886\u57df\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u4e2d\uff0cOneCast\u7684\u8868\u73b0\u6bd4\u5f53\u524d\u6700\u5148\u8fdb\u7684\u57fa\u51c6\u65b9\u6cd5\u66f4\u597d\u3002\u8fd9\u662f\u9996\u6b21\u901a\u8fc7\u5206\u89e3\u65f6\u95f4\u5e8f\u5217\u7684\u7ed3\u6784\u7ec4\u5206\u6765\u4f18\u5316\u8de8\u57df\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u7684\u8868\u73b0\u3002", "conclusion": "OneCast\u6846\u67b6\u63d0\u4f9b\u4e86\u4e00\u79cd\u57fa\u4e8e\u7ed3\u6784\u5316\u7684\u8de8\u57df\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u65b9\u6cd5\uff0c\u8fd9\u79cd\u65b9\u6cd5\u4e0d\u4ec5\u63d0\u9ad8\u4e86\u9884\u6d4b\u7684\u51c6\u786e\u7387\uff0c\u8fd8\u5177\u6709\u66f4\u5f3a\u7684\u8de8\u9886\u57df\u9002\u5e94\u6027\u3002"}}
{"id": "2510.24674", "categories": ["cs.LG", "cs.AI", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2510.24674", "abs": "https://arxiv.org/abs/2510.24674", "authors": ["Bram De Cooman", "Johan Suykens"], "title": "Learning to Drive Safely with Hybrid Options", "comment": null, "summary": "Out of the many deep reinforcement learning approaches for autonomous\ndriving, only few make use of the options (or skills) framework. That is\nsurprising, as this framework is naturally suited for hierarchical control\napplications in general, and autonomous driving tasks in specific. Therefore,\nin this work the options framework is applied and tailored to autonomous\ndriving tasks on highways. More specifically, we define dedicated options for\nlongitudinal and lateral manoeuvres with embedded safety and comfort\nconstraints. This way, prior domain knowledge can be incorporated into the\nlearning process and the learned driving behaviour can be constrained more\neasily. We propose several setups for hierarchical control with options and\nderive practical algorithms following state-of-the-art reinforcement learning\ntechniques. By separately selecting actions for longitudinal and lateral\ncontrol, the introduced policies over combined and hybrid options obtain the\nsame expressiveness and flexibility that human drivers have, while being easier\nto interpret than classical policies over continuous actions. Of all the\ninvestigated approaches, these flexible policies over hybrid options perform\nthe best under varying traffic conditions, outperforming the baseline policies\nover actions.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u5728\u9ad8\u901f\u516c\u8def\u81ea\u52a8\u9a7e\u9a76\u4efb\u52a1\u4e2d\u4f7f\u7528\u9009\u9879\u6846\u67b6\u7684\u65b9\u6cd5\uff0c\u5e76\u901a\u8fc7\u7ed3\u5408\u7eb5\u5411\u548c\u6a2a\u5411\u63a7\u5236\u7684\u52a8\u4f5c\u9009\u62e9\uff0c\u63d0\u9ad8\u4e86\u7075\u6d3b\u6027\u548c\u53ef\u89e3\u91ca\u6027\uff0c\u4e14\u5728\u4e0d\u540c\u7684\u4ea4\u901a\u6761\u4ef6\u4e0b\u4f18\u4e8e\u57fa\u7840\u52a8\u4f5c\u7b56\u7565\u3002", "motivation": "\u73b0\u6709\u7684\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u5f88\u5c11\u4f7f\u7528\u9009\u9879\u6846\u67b6\uff0c\u867d\u540e\u8005\u975e\u5e38\u9002\u5408\u5206\u5c42\u63a7\u5236\u5e94\u7528\u4e14\u6613\u4e8e\u7eb3\u5165\u5148\u9a8c\u77e5\u8bc6\u3002\u56e0\u6b64\uff0c\u672c\u7814\u7a76\u8bd5\u56fe\u5c06\u9009\u9879\u6846\u67b6\u5e94\u7528\u5230\u9ad8\u901f\u516c\u8def\u81ea\u52a8\u9a7e\u9a76\u4efb\u52a1\u4e2d\uff0c\u5e76\u901a\u8fc7\u5f15\u5165\u5d4c\u5165\u5b89\u5168\u6027\u548c\u8212\u9002\u6027\u7ea6\u675f\u7684\u4e13\u95e8\u9009\u9879\u6765\u4f18\u5316\u5b66\u4e60\u8fc7\u7a0b\u548c\u63a7\u5236\u884c\u4e3a\u3002", "method": "\u5b9a\u4e49\u4e86\u7eb5\u5411\u548c\u6a2a\u5411\u8fd0\u52a8\u63a7\u5236\u7684\u52a8\u4f5c\uff0c\u5e76\u7528\u8fd9\u4e9b\u52a8\u4f5c\u8bbe\u7f6e\u4e86\u5206\u5c42\u63a7\u5236\u7684\u51e0\u79cd\u4f53\u7cfb\uff0c\u6267\u884c\u57fa\u4e8e\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u6280\u672f\u7684\u5b9e\u7528\u7b97\u6cd5\uff0c\u4ee5\u5206\u522b\u9009\u62e9\u7eb5\u5411\u548c\u6a2a\u5411\u63a7\u5236\u7684\u52a8\u4f5c\u3002\u8fd9\u4e9b\u65b9\u6848\u5177\u6709\u4e0e\u4eba\u7c7b\u53f8\u673a\u76f8\u540c\u7684\u8868\u8fbe\u80fd\u529b\u548c\u7075\u6d3b\u6027\u3002", "result": "\u5728\u5404\u79cd\u4ea4\u901a\u6761\u4ef6\u4e0b\uff0c\u7075\u6d3b\u7684\u6df7\u5408\u9009\u9879\u7b56\u7565\u4f18\u4e8e\u57fa\u7840\u52a8\u4f5c\u7b56\u7565\u3002", "conclusion": "\u901a\u8fc7\u5f15\u5165\u5d4c\u5165\u5148\u9a8c\u77e5\u8bc6\u7684\u4e13\u95e8\u9009\u9879\uff0c\u53ef\u4ee5\u589e\u5f3a\u81ea\u52a8\u9a7e\u9a76\u51b3\u7b56\u7cfb\u7edf\u7684\u8868\u8fbe\u80fd\u529b\u548c\u7075\u6d3b\u6027\uff0c\u63d0\u5347\u5176\u6027\u80fd\uff0c\u540c\u65f6\u7ef4\u62a4\u6613\u89e3\u91ca\u6027\uff0c\u8fd9\u4e3a\u81ea\u52a8\u9a7e\u9a76\u4efb\u52a1\u63d0\u4f9b\u4e86\u4e00\u79cd\u65b0\u9896\u4e14\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.24034", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.24034", "abs": "https://arxiv.org/abs/2510.24034", "authors": ["Yufan Liu", "Wanqian Zhang", "Huashan Chen", "Lin Wang", "Xiaojun Jia", "Zheng Lin", "Weiping Wang"], "title": "AutoPrompt: Automated Red-Teaming of Text-to-Image Models via LLM-Driven Adversarial Prompts", "comment": "Accepted by ICCV 2025", "summary": "Despite rapid advancements in text-to-image (T2I) models, their safety\nmechanisms are vulnerable to adversarial prompts, which maliciously generate\nunsafe images. Current red-teaming methods for proactively assessing such\nvulnerabilities usually require white-box access to T2I models, and rely on\ninefficient per-prompt optimization, as well as inevitably generate\nsemantically meaningless prompts easily blocked by filters. In this paper, we\npropose APT (AutoPrompT), a black-box framework that leverages large language\nmodels (LLMs) to automatically generate human-readable adversarial suffixes for\nbenign prompts. We first introduce an alternating optimization-finetuning\npipeline between adversarial suffix optimization and fine-tuning the LLM\nutilizing the optimized suffix. Furthermore, we integrates a dual-evasion\nstrategy in optimization phase, enabling the bypass of both perplexity-based\nfilter and blacklist word filter: (1) we constrain the LLM generating\nhuman-readable prompts through an auxiliary LLM perplexity scoring, which\nstarkly contrasts with prior token-level gibberish, and (2) we also introduce\nbanned-token penalties to suppress the explicit generation of banned-tokens in\nblacklist. Extensive experiments demonstrate the excellent red-teaming\nperformance of our human-readable, filter-resistant adversarial prompts, as\nwell as superior zero-shot transferability which enables instant adaptation to\nunseen prompts and exposes critical vulnerabilities even in commercial APIs\n(e.g., Leonardo.Ai.).", "AI": {"tldr": "\u4ecb\u7ecd\u4e86\u4e00\u79cd\u540d\u4e3aAPTP\u7684\u65b0\u6846\u67b6\uff0c\u901a\u8fc7\u4f7f\u7528\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u81ea\u52a8\u751f\u6210\u5bf9\u4eba\u7c7b\u53ef\u8bfb\u7684\u5bf9\u6297\u6027\u540e\u7f00\uff0c\u6765\u4e3b\u52a8\u8bc4\u4f30\u6587\u672c\u5230\u56fe\u50cf\uff08T2I\uff09\u6a21\u578b\u7684\u5b89\u5168\u6f0f\u6d1e\u3002\u6b64\u65b9\u6cd5\u63d0\u9ad8\u4e86\u5bf9\u6297\u6027\u751f\u6210\u7684\u5b89\u5168\u6027\u4e0e\u6709\u6548\u6027\u3002", "motivation": "\u73b0\u6709\u591a\u6570\u6587\u672c\u5230\u56fe\u50cf\u6a21\u578b\u7684\u5b89\u5168\u673a\u5236\u5bb9\u6613\u53d7\u5230\u5bf9\u6297\u6027\u63d0\u793a\u7684\u5f71\u54cd\uff0c\u751f\u6210\u4e0d\u5b89\u5168\u7684\u56fe\u50cf\u3002\u5f53\u524d\u7684\u7ea2\u961f\u6d4b\u8bd5\u65b9\u6cd5\u4f9d\u8d56\u4e8e\u767d\u76d2\u8bbf\u95ee\u548c\u8017\u65f6\u7684\u9010\u63d0\u793a\u4f18\u5316\uff0c\u9700\u8981\u6539\u8fdb\u7684\u751f\u6210\u7b56\u7565\u3002\u63d0\u51faAPTP\u65b9\u6cd5\uff0c\u4ee5\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u901a\u8fc7\u5927\u8bed\u8a00\u6a21\u578b\u81ea\u52a8\u751f\u6210\u6709\u6548\u7684\u5bf9\u6297\u6027\u63d0\u793a\uff0c\u63d0\u9ad8\u4e86\u7ea2\u961f\u6d4b\u8bd5\u7684\u6548\u7387\u548c\u5b9e\u7528\u6027\u3002", "method": "APTP\u4f7f\u7528\u5927\u8bed\u8a00\u6a21\u578b\u751f\u6210\u5bf9\u6297\u6027\u540e\u7f00\uff0c\u901a\u8fc7\u4ea4\u66ff\u4f18\u5316\u548c\u5fae\u8c03\u6a21\u578b\u7b56\u7565\u63d0\u9ad8\u751f\u6210\u5bf9\u6297\u6027\u63d0\u793a\u7684\u6709\u6548\u6027\u3002\u5728\u4f18\u5316\u8fc7\u7a0b\u4e2d\uff0c\u91c7\u7528\u4e86\u53cc\u89c4\u907f\u7b56\u7565\uff0c\u65e2\u4fdd\u8bc1\u751f\u6210\u7684\u4eba\u7c7b\u53ef\u8bfb\u6027\uff0c\u4e5f\u7ed5\u8fc7\u4e86\u57fa\u4e8e\u56f0\u60d1\u5ea6\u548c\u9ed1\u540d\u5355\u8bcd\u7684\u8fc7\u6ee4\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u5229\u7528\u8f85\u52a9\u7684\u5927\u8bed\u8a00\u6a21\u578b\u56f0\u60d1\u5ea6\u8bc4\u5206\u6765\u7ea6\u675f\u751f\u6210\u7684\u4eba\u7c7b\u53ef\u8bfb\u63d0\u793a\uff0c\u5e76\u5f15\u5165\u7981\u6b62\u8bcd\u60e9\u7f5a\u6765\u6291\u5236\u9ed1\u540d\u5355\u8bcd\u7684\u751f\u6210\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0cAPTP\u751f\u6210\u7684\u5bf9\u6297\u6027\u63d0\u793a\u8868\u73b0\u51fa\u4e86\u4f18\u79c0\u7684\u7ea2\u961f\u6d4b\u8bd5\u6027\u80fd\uff0c\u5177\u6709\u66f4\u597d\u7684\u96f6\u6837\u672c\u8fc1\u79fb\u80fd\u529b\uff0c\u80fd\u591f\u5bf9\u672a\u89c1\u63d0\u793a\u8fdb\u884c\u5373\u65f6\u9002\u5e94\uff0c\u5e76\u5c55\u793a\u4e86\u73b0\u6709\u5546\u4e1aAPI\uff08\u4f8b\u5982Leonardo.Ai\uff09\u4e2d\u7684\u5173\u952e\u6f0f\u6d1e\u3002", "conclusion": "APTP\u5f15\u5165\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u7ea2\u961f\u6d4b\u8bd5\u65b9\u6cd5\uff0c\u901a\u8fc7\u5927\u8bed\u8a00\u6a21\u578b\u81ea\u52a8\u751f\u6210\u5bf9\u6297\u6027\u63d0\u793a\uff0c\u9a8c\u8bc1\u4e86\u5176\u5728\u8bc4\u4f30T2I\u6a21\u578b\u5b89\u5168\u6027\u80fd\u65b9\u9762\u7684\u4f18\u8d8a\u6027\u3002"}}
{"id": "2510.23650", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.23650", "abs": "https://arxiv.org/abs/2510.23650", "authors": ["Wei Xia"], "title": "Beyond Hidden-Layer Manipulation: Semantically-Aware Logit Interventions for Debiasing LLMs", "comment": null, "summary": "We proposed Static and Dynamic -- two zero-shot logits-layer debiasing\nmethods. Dynamic reduces bias by up to 70% with minimal fluency loss. Logits\nintervention outperforms hidden-layer approaches. We show semantic-aware logits\nintervention is stable and effective for debiasing aligned LLMs.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e24\u79cd\u96f6\u6837\u672c logits \u5c42\u53bb\u504f\u65b9\u6cd5\uff1a\u9759\u6001\u548c\u52a8\u6001\u3002\u52a8\u6001\u65b9\u6cd5\u53ef\u4ee5\u5728\u51e0\u4e4e\u4e0d\u635f\u5931\u6d41\u7545\u5ea6\u7684\u60c5\u51b5\u4e0b\u51cf\u5c11\u9ad8\u8fbe 70% \u7684\u504f\u5dee\u3002\u672c\u6587\u8bc1\u660e\u4e86\u57fa\u4e8e\u8bed\u4e49\u7684 logits \u5e72\u9884\u65b9\u6cd5\u5bf9\u4e8e\u53bb\u504f\u5bf9\u9f50\u7684\u5927\u8bed\u8a00\u6a21\u578b\u662f\u7a33\u5b9a\u4e14\u6709\u6548\u7684\uff1b", "motivation": "\u52a8\u673a\u5728\u4e8e\u5f00\u53d1\u4e00\u79cd\u53ef\u4ee5\u6709\u6548\u51cf\u5c11\u5927\u8bed\u8a00\u6a21\u578b\u9884\u6d4b\u504f\u89c1\u7684\u65b9\u6cd5\uff0c\u540c\u65f6\u4fdd\u6301\u5176\u751f\u6210\u6587\u672c\u7684\u6d41\u7545\u6027\uff1b", "method": "\u63d0\u51fa\u4e86\u9759\u6001\u548c\u52a8\u6001\u4e24\u79cd\u96f6\u6837\u672c logits \u5c42\u53bb\u504f\u65b9\u6cd5\uff0c\u5176\u4e2d\u52a8\u6001\u65b9\u6cd5\u901a\u8fc7\u51cf\u5c11\u504f\u5dee\u540c\u65f6\u4fdd\u6301\u8f83\u9ad8\u6d41\u7545\u5ea6\u663e\u793a\u51fa\u4f18\u8d8a\u6027\uff1b", "result": "\u52a8\u6001\u65b9\u6cd5\u663e\u8457\u51cf\u5c11\u4e86\u5927\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u9884\u6d4b\u504f\u89c1\uff0c\u51cf\u5c11\u9ad8\u8fbe 70%\uff0c\u540c\u65f6\u4e5f\u5c55\u793a\u4e86\u57fa\u4e8e\u8bed\u4e49\u7684 logits \u5e72\u9884\u7684\u6709\u6548\u6027\u548c\u7a33\u5b9a\u6027\uff1b", "conclusion": "\u96f6\u6837\u672c logits \u5c42\u53bb\u504f\u65b9\u6cd5\u662f\u4e00\u79cd\u7a33\u5b9a\u4e14\u6709\u6548\u7684\u65b9\u5f0f\uff0c\u80fd\u591f\u5b9e\u73b0\u5927\u8bed\u8a00\u6a21\u578b\u7684\u53bb\u504f\u4e14\u4e0d\u663e\u8457\u5f71\u54cd\u5176\u751f\u6210\u6587\u672c\u7684\u8d28\u91cf\u3002"}}
{"id": "2510.24085", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.24085", "abs": "https://arxiv.org/abs/2510.24085", "authors": ["Md. Shihab Uddin", "Md Nazmus Shakib", "Rahul Bhadani"], "title": "Modeling Electric Vehicle Car-Following Behavior: Classical vs Machine Learning Approach", "comment": null, "summary": "The increasing adoption of electric vehicles (EVs) necessitates an\nunderstanding of their driving behavior to enhance traffic safety and develop\nsmart driving systems. This study compares classical and machine learning\nmodels for EV car following behavior. Classical models include the Intelligent\nDriver Model (IDM), Optimum Velocity Model (OVM), Optimal Velocity Relative\nVelocity (OVRV), and a simplified CACC model, while the machine learning\napproach employs a Random Forest Regressor. Using a real world dataset of an EV\nfollowing an internal combustion engine (ICE) vehicle under varied driving\nconditions, we calibrated classical model parameters by minimizing the RMSE\nbetween predictions and real data. The Random Forest model predicts\nacceleration using spacing, speed, and gap type as inputs. Results demonstrate\nthe Random Forest's superior accuracy, achieving RMSEs of 0.0046 (medium gap),\n0.0016 (long gap), and 0.0025 (extra long gap). Among physics based models,\nCACC performed best, with an RMSE of 2.67 for long gaps. These findings\nhighlight the machine learning model's performance across all scenarios. Such\nmodels are valuable for simulating EV behavior and analyzing mixed autonomy\ntraffic dynamics in EV integrated environments.", "AI": {"tldr": "\u6b64\u7814\u7a76\u6bd4\u8f83\u4e86\u7ecf\u5178\u6a21\u578b\u548c\u673a\u5668\u5b66\u4e60\u6a21\u578b\u5728\u7535\u52a8\u6c7d\u8f66\u8ddf\u968f\u884c\u4e3a\u9884\u6d4b\u4e2d\u7684\u8868\u73b0\uff0c\u4f7f\u7528\u968f\u673a\u68ee\u6797\u56de\u5f52\u5668\u7684\u673a\u5668\u5b66\u4e60\u6a21\u578b\u5728\u9884\u6d4b\u52a0\u901f\u65f6\u8868\u73b0\u51fa\u66f4\u9ad8\u7684\u7cbe\u5ea6\uff0c\u4f18\u4e8e\u7ecf\u5178\u7269\u7406\u6a21\u578b\u3002\u7ed3\u679c\u663e\u793a\uff0c\u968f\u673a\u68ee\u6797\u6a21\u578b\u5728\u5404\u79cd\u60c5\u51b5\u4e0b\u8868\u73b0\u6700\u4f73\u3002\u8fd9\u79cd\u6a21\u578b\u5bf9\u4e8e\u6a21\u62df\u7535\u52a8\u6c7d\u8f66\u884c\u4e3a\u548c\u5206\u6790\u6df7\u5408\u81ea\u6cbb\u4ea4\u901a\u52a8\u6001\u975e\u5e38\u91cd\u8981\u3002", "motivation": "\u968f\u7740\u7535\u52a8\u6c7d\u8f66\u7684\u589e\u591a\uff0c\u4e86\u89e3\u5176\u9a7e\u9a76\u884c\u4e3a\u5bf9\u4e8e\u63d0\u9ad8\u4ea4\u901a\u5b89\u5168\u53ca\u5f00\u53d1\u667a\u80fd\u9a7e\u9a76\u7cfb\u7edf\u81f3\u5173\u91cd\u8981\u3002\u672c\u7814\u7a76\u65e8\u5728\u6bd4\u8f83\u7ecf\u5178\u7684\u7269\u7406\u5b66\u6a21\u578b\u548c\u57fa\u4e8e\u673a\u5668\u5b66\u4e60\u7684\u6a21\u578b\u5728\u7535\u52a8\u6c7d\u8f66\u8ddf\u968f\u884c\u4e3a\u9884\u6d4b\u4e2d\u7684\u8868\u73b0\u3002", "method": "\u7814\u7a76\u91c7\u7528\u4e86\u4e00\u4e2a\u5305\u542b\u7535\u52a8\u6c7d\u8f66\u8ddf\u968f\u5185\u71c3\u673a\u8f66\u8f86\u7684\u5b9e\u9645\u6570\u636e\u96c6\uff0c\u901a\u8fc7\u6700\u5c0f\u5316\u9884\u6d4b\u503c\u4e0e\u5b9e\u9645\u6570\u636e\u4e4b\u95f4\u7684RMSE\u6765\u6821\u51c6\u7ecf\u5178\u6a21\u578b\u7684\u53c2\u6570\u3002\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u91c7\u7528\u968f\u673a\u68ee\u6797\u56de\u5f52\u5668\uff0c\u901a\u8fc7\u95f4\u8ddd\u3001\u901f\u5ea6\u548c\u95f4\u9699\u7c7b\u578b\u4f5c\u4e3a\u8f93\u5165\u6765\u9884\u6d4b\u52a0\u901f\u3002", "result": "\u968f\u673a\u68ee\u6797\u6a21\u578b\u5728\u4e0d\u540c\u7c7b\u578b\u7684\u95f4\u9699\u4e0b\u8868\u73b0\u4f18\u4e8e\u7ecf\u5178\u7269\u7406\u6a21\u578b\uff0c\u5982\u957f\u95f4\u9699\u4e0bRMSE\u4e3a0.0016\uff0c\u800cCACC\u6a21\u578b\u7684RMSE\u4e3a2.67\u3002\u8fd9\u4e9b\u53d1\u73b0\u8868\u660e\u673a\u5668\u5b66\u4e60\u6a21\u578b\u5728\u6240\u6709\u573a\u666f\u4e2d\u5747\u8868\u73b0\u51fa\u8272\u3002", "conclusion": "\u672c\u7814\u7a76\u8bc1\u660e\u4e86\u673a\u5668\u5b66\u4e60\u6a21\u578b\u5728\u7535\u52a8\u6c7d\u8f66\u8ddf\u968f\u884c\u4e3a\u9884\u6d4b\u4e2d\u7684\u4f18\u52bf\uff0c\u5bf9\u4e8e\u6a21\u62df\u7535\u52a8\u6c7d\u8f66\u884c\u4e3a\u53ca\u5206\u6790\u6df7\u5408\u81ea\u6cbb\u4ea4\u901a\u52a8\u6001\u5177\u6709\u91cd\u8981\u4ef7\u503c\u3002"}}
{"id": "2510.23652", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.23652", "abs": "https://arxiv.org/abs/2510.23652", "authors": ["Yao Lu", "Yuqi Li", "Wenbin Xie", "Shanqing Yu", "Qi Xuan", "Zhaowei Zhu", "Shiping Wen"], "title": "The Structural Scalpel: Automated Contiguous Layer Pruning for Large Language Models", "comment": null, "summary": "Although large language models (LLMs) have achieved revolutionary\nbreakthroughs in many fields, their large model size and high computational\ncost pose significant challenges for practical deployment on\nresource-constrained edge devices. To this end, layer pruning has been proposed\nto reduce the computational overhead by directly removing redundant layers.\nHowever, existing layer pruning methods typically rely on hand-crafted metrics\nto evaluate and remove individual layers, while ignoring the dependencies\nbetween layers. This can disrupt the model's information flow and severely\ndegrade performance. To address these issues, we propose CLP, a novel\ncontinuous layer pruning framework that introduces two key innovations: a\ndifferentiable concave gate algorithm that automatically identifies the best\ncontinuous layer segments for pruning via gradient-based optimization; and a\ncutoff endpoint tuning strategy that effectively restores model performance by\nfine-tuning only the layers adjacent to the pruned segments. Extensive\nexperiments across multiple model architectures (including LLaMA2, LLaMA3 and\nQwen) and sizes (from $7$B to $70$B parameters) show that CLP significantly\noutperforms existing state-of-the-art baselines. For example, at a pruning rate\nof $20\\%$, CLP achieves an average performance retention of $95.34\\%$ on\nLLaMA3-70B, outperforming baselines by $4.29\\%$-$30.52\\%$. Furthermore, CLP can\nbe seamlessly combined with quantization to further compress the model with\nonly a slight performance loss.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u79f0\u4e3aCLP\u7684\u8fde\u7eed\u5c42\u526a\u679d\u6846\u67b6\uff0c\u4ee5\u89e3\u51b3\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u8fb9\u7f18\u8bbe\u5907\u90e8\u7f72\u65f6\u7684\u8ba1\u7b97\u6210\u672c\u9ad8\u7684\u95ee\u9898\u3002CLP\u4f7f\u7528\u53ef\u5fae\u5206\u7684\u51f9\u95e8\u7b97\u6cd5\u81ea\u52a8\u8bc6\u522b\u6700\u4f73\u7684\u8fde\u7eed\u5c42\u6bb5\u8fdb\u884c\u526a\u679d\uff0c\u5e76\u901a\u8fc7\u8c03\u6574\u526a\u679d\u6bb5\u9644\u8fd1\u7684\u5c42\u6765\u6062\u590d\u6a21\u578b\u6027\u80fd\u3002\u5b9e\u9a8c\u8868\u660e\uff0cCLP\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u4e14\u5728\u4e0e\u91cf\u5316\u7ed3\u5408\u65f6\u80fd\u591f\u8fdb\u4e00\u6b65\u538b\u7f29\u6a21\u578b\uff0c\u4ec5\u9020\u6210\u5fae\u5c0f\u7684\u6027\u80fd\u4e0b\u964d\u3002", "motivation": "\u73b0\u6709\u7684\u5c42\u526a\u679d\u65b9\u6cd5\u4f9d\u8d56\u4e8e\u624b\u5de5\u5236\u5b9a\u7684\u5ea6\u91cf\u6765\u8bc4\u4f30\u548c\u53bb\u9664\u5355\u4e2a\u5c42\uff0c\u8fd9\u4f1a\u5bfc\u81f4\u6a21\u578b\u4fe1\u606f\u6d41\u7684\u7834\u574f\u5e76\u4e25\u91cd\u964d\u4f4e\u6027\u80fd\u3002CLP\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u901a\u8fc7\u5f15\u5165\u8fde\u7eed\u5c42\u526a\u679d\u6846\u67b6\uff0c\u6539\u8fdb\u5c42\u526a\u679d\u6280\u672f\uff0c\u4ee5\u66f4\u597d\u5730\u9002\u5e94\u8d44\u6e90\u53d7\u9650\u7684\u8fb9\u7f18\u8bbe\u5907\u4e0a\u90e8\u7f72\u7684\u5927\u6a21\u578b\u3002", "method": "CLP\u6846\u67b6\u6709\u4e24\u4e2a\u5173\u952e\u521b\u65b0\u70b9\uff1a\u4e00\u79cd\u53ef\u5fae\u5206\u7684\u51f9\u95e8\u7b97\u6cd5\u7528\u4e8e\u81ea\u52a8\u8bc6\u522b\u6700\u4f73\u8fde\u7eed\u5c42\u6bb5\u8fdb\u884c\u526a\u679d\uff1b\u4ee5\u53ca\u4e00\u79cd\u526a\u679d\u672b\u70b9\u8c03\u6574\u7b56\u7565\uff0c\u901a\u8fc7\u5fae\u8c03\u526a\u679d\u6bb5\u9644\u8fd1\u7684\u5c42\u6765\u6062\u590d\u6a21\u578b\u6027\u80fd\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\uff0cCLP\u5728\u591a\u4e2a\u6a21\u578b\u67b6\u6784\u548c\u5927\u5c0f\u4e0a\u5747\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5\u3002\u5982\u572820%\u7684\u526a\u679d\u7387\u4e0b\uff0cCLP\u5728LLaMA3-70B\u6a21\u578b\u4e0a\u5b9e\u73b0\u4e8695.34%\u7684\u6027\u80fd\u4fdd\u7559\u7387\uff0c\u8d85\u8fc7\u57fa\u7ebf\u65b9\u6cd54.29%-30.52%\u3002\u6b64\u5916\uff0cCLP\u8fd8\u53ef\u4ee5\u4e0e\u91cf\u5316\u6280\u672f\u7ed3\u5408\uff0c\u8fdb\u4e00\u6b65\u538b\u7f29\u6a21\u578b\uff0c\u540c\u65f6\u4fdd\u6301\u63a5\u8fd1\u539f\u59cb\u6a21\u578b\u7684\u6027\u80fd\u3002", "conclusion": "CLP\u662f\u4e00\u79cd\u6709\u6548\u4e14\u6027\u80fd\u4f18\u8d8a\u7684\u8fde\u7eed\u5c42\u526a\u679d\u6846\u67b6\uff0c\u9002\u7528\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u8d44\u6e90\u53d7\u9650\u8fb9\u7f18\u8bbe\u5907\u4e0a\u7684\u90e8\u7f72\u3002"}}
{"id": "2510.24037", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.24037", "abs": "https://arxiv.org/abs/2510.24037", "authors": ["Shufan Shen", "Junshu Sun", "Shuhui Wang", "Qingming Huang"], "title": "Kernelized Sparse Fine-Tuning with Bi-level Parameter Competition for Vision Models", "comment": null, "summary": "Parameter-efficient fine-tuning (PEFT) aims to adapt pre-trained vision\nmodels to downstream tasks. Among PEFT paradigms, sparse tuning achieves\nremarkable performance by adjusting only the weights most relevant to\ndownstream tasks, rather than densely tuning the entire weight matrix. Current\nmethods follow a two-stage paradigm. First, it locates task-relevant weights by\ngradient information, which overlooks the parameter adjustments during\nfine-tuning and limits the performance. Second, it updates only the located\nweights by applying a sparse mask to the gradient of the weight matrix, which\nresults in high memory usage due to the storage of all weight matrices in the\noptimizer. In this paper, we propose a one-stage method named SNELLA to\novercome the above limitations. For memory usage, SNELLA selectively updates\nthe weight matrix by adding it to another sparse matrix that is merged by two\nlow-rank learnable matrices. We extend the low-rank decomposition by\nintroducing nonlinear kernel functions, thereby increasing the rank of the\nresulting merged matrix to prevent the interdependency among weight updates,\nenabling better adaptation to downstream tasks. For locating task-relevant\nweights, we propose an adaptive bi-level sparsity allocation mechanism that\nencourages weights to compete across and inside layers based on their\nimportance scores in an end-to-end manner. Extensive experiments are conducted\non classification, segmentation, and generation tasks using different\npre-trained vision models. The results show that SNELLA achieves SOTA\nperformance with low memory usage. Notably, SNELLA obtains 1.8% (91.9% v.s.\n90.1%) higher Top-1 accuracy on the FGVC benchmark compared to SPT-LoRA.\nCompared to previous methods, SNELLA achieves a memory reduction of 31.1%-39.9%\nacross models with parameter scales from 86M to 632M. Our source codes are\navailable at https://github.com/ssfgunner/SNELL.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aSNELLA\u7684\u4e00\u9636\u6bb5\u65b9\u6cd5\uff0c\u4ee5\u89e3\u51b3\u5f53\u524d\u7a00\u758f\u8c03\u4f18\u65b9\u6cd5\u7684\u9650\u5236\uff0c\u901a\u8fc7\u5f15\u5165\u975e\u7ebf\u6027\u6838\u51fd\u6570\u6765\u6269\u5c55\u4f4e\u79e9\u5206\u89e3\uff0c\u91c7\u7528\u81ea\u9002\u5e94\u53cc\u5c42\u7a00\u758f\u5206\u914d\u673a\u5236\u6765\u5bfb\u627e\u4e0e\u4efb\u52a1\u76f8\u5173\u8054\u7684\u6743\u91cd\uff0c\u5b9e\u9a8c\u8868\u660eSNELLA\u5728\u4f4e\u5185\u5b58\u4f7f\u7528\u7684\u60c5\u51b5\u4e0b\u8fbe\u5230\u4e86SOTA\u6027\u80fd\u3002", "motivation": "\u76ee\u524d\u7684\u7a00\u758f\u8c03\u4f18\u65b9\u6cd5\u5b58\u5728\u4e24\u4e2a\u9636\u6bb5\uff0c\u5373\u5b9a\u4f4d\u4efb\u52a1\u76f8\u5173\u7684\u6743\u91cd\u548c\u66f4\u65b0\u6743\u91cd\uff0c\u524d\u8005\u7684\u53c2\u6570\u8c03\u6574\u5728\u5fae\u8c03\u8fc7\u7a0b\u4e2d\u88ab\u5ffd\u89c6\uff0c\u540e\u8005\u7684\u7a00\u758f\u63a9\u7801\u5bfc\u81f4\u9ad8\u5185\u5b58\u4f7f\u7528\u3002\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4e00\u9636\u6bb5\u65b9\u6cd5SNELLA\u6765\u514b\u670d\u8fd9\u4e9b\u9650\u5236\u3002", "method": "SNELLA\u9009\u62e9\u6027\u66f4\u65b0\u6743\u91cd\u77e9\u9635\uff0c\u901a\u8fc7\u5c06\u5176\u6dfb\u52a0\u5230\u7531\u4e24\u4e2a\u4f4e\u79e9\u5b66\u4e60\u77e9\u9635\u5408\u5e76\u800c\u6210\u7684\u7a00\u758f\u77e9\u9635\u4e2d\uff0c\u5e76\u5f15\u5165\u975e\u7ebf\u6027\u6838\u51fd\u6570\u6765\u589e\u52a0\u5408\u5e76\u77e9\u9635\u7684\u79e9\uff0c\u4ece\u800c\u9632\u6b62\u6743\u91cd\u66f4\u65b0\u4e4b\u95f4\u7684\u76f8\u4e92\u4f9d\u5b58\u5173\u7cfb\u3002\u6b64\u5916\uff0cSNELLA\u8fd8\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u9002\u5e94\u53cc\u5c42\u7a00\u758f\u5206\u914d\u673a\u5236\uff0c\u4ee5\u9f13\u52b1\u6743\u91cd\u5728\u5c42\u95f4\u548c\u5c42\u5185\u6839\u636e\u4ed6\u4eec\u7684\u91cd\u8981\u6027\u5206\u6570\u8fdb\u884c\u7ade\u4e89\u3002", "result": "\u5728\u4e0d\u540c\u7684\u56fe\u50cf\u9884\u8bad\u7ec3\u6a21\u578b\u4e0a\u8fdb\u884c\u5206\u7c7b\u3001\u5206\u5272\u548c\u751f\u6210\u4efb\u52a1\u5b9e\u9a8c\u540e\uff0c\u7ed3\u679c\u663e\u793a\uff0cSNELLA\u5728\u4f4e\u5185\u5b58\u4f7f\u7528\u7684\u60c5\u51b5\u4e0b\u5b9e\u73b0\u4e86SOTA\u6027\u80fd\uff0c\u5c24\u5176\u5728FGVC\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u6bd4SPT-LoRA\u83b7\u5f97\u4e86\u66f4\u9ad8\u7684Top-1\u51c6\u786e\u6027\u3002", "conclusion": "SNELLA\u901a\u8fc7\u5408\u7406\u7684\u65b9\u6cd5\u514b\u670d\u4e86\u73b0\u6709\u7a00\u758f\u8c03\u4f18\u65b9\u6cd5\u4e2d\u7684\u95ee\u9898\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u8bc1\u660e\u4e86\u5176\u5728\u4f4e\u5185\u5b58\u4f7f\u7528\u60c5\u51b5\u4e0b\u8fbe\u5230\u7684SOTA\u6027\u80fd\u3002"}}
{"id": "2510.24115", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.24115", "abs": "https://arxiv.org/abs/2510.24115", "authors": ["Sandeep Vissapragada", "Vikrant Sahu", "Gagan Raj Gupta", "Vandita Singh"], "title": "HistoLens: An Interactive XAI Toolkit for Verifying and Mitigating Flaws in Vision-Language Models for Histopathology", "comment": null, "summary": "For doctors to truly trust artificial intelligence, it can't be a black box.\nThey need to understand its reasoning, almost as if they were consulting a\ncolleague. We created HistoLens1 to be that transparent, collaborative partner.\nIt allows a pathologist to simply ask a question in plain English about a\ntissue slide--just as they would ask a trainee. Our system intelligently\ntranslates this question into a precise query for its AI engine, which then\nprovides a clear, structured report. But it doesn't stop there. If a doctor\never asks, \"Why?\", HistoLens can instantly provide a 'visual proof' for any\nfinding--a heatmap that points to the exact cells and regions the AI used for\nits analysis. We've also ensured the AI focuses only on the patient's tissue,\njust like a trained pathologist would, by teaching it to ignore distracting\nbackground noise. The result is a workflow where the pathologist remains the\nexpert in charge, using a trustworthy AI assistant to verify their insights and\nmake faster, more confident diagnoses.", "AI": {"tldr": "HistoLens\u662f\u4e00\u4e2a\u900f\u660e\u534f\u4f5c\u7684AI\u7cfb\u7edf\uff0c\u533b\u751f\u53ef\u4ee5\u901a\u8fc7\u63d0\u95ee\u6765\u83b7\u53d6\u5173\u4e8e\u7ec4\u7ec7\u5207\u7247\u7684\u5206\u6790\u62a5\u544a\uff0c\u5e76\u4e14\u80fd\u591f\u83b7\u5f97'\u89c6\u89c9\u8bc1\u636e'\u6765\u7406\u89e3AI\u7684\u5224\u65ad\u4f9d\u636e\u3002\u786e\u4fddAI\u4e13\u6ce8\u4e8e\u75c5\u4eba\u7684\u7ec4\u7ec7\uff0c\u5ffd\u7565\u80cc\u666f\u566a\u58f0\uff0c\u534f\u52a9\u533b\u751f\u66f4\u5feb\u66f4\u81ea\u4fe1\u5730\u505a\u51fa\u8bca\u65ad\u3002\n", "motivation": "\u4e3a\u4e86\u4f7f\u533b\u751f\u80fd\u591f\u4fe1\u4efbAI\uff0c\u5e76\u7406\u89e3\u5b83\u7684\u63a8\u7406\u8fc7\u7a0b\uff0c\u5c31\u50cf\u54a8\u8be2\u4e00\u4f4d\u540c\u4e8b\u4e00\u6837\u3002\n", "method": "\u521b\u5efa\u4e86\u4e00\u4e2a\u540d\u4e3aHistoLens\u7684\u7cfb\u7edf\uff0c\u652f\u6301\u533b\u751f\u7528\u81ea\u7136\u8bed\u8a00\u63d0\u95ee\uff0c\u7cfb\u7edf\u5c06\u5176\u8f6c\u5316\u4e3aAI\u5f15\u64ce\u7684\u7cbe\u786e\u67e5\u8be2\uff0cAI\u63d0\u4f9b\u8be6\u7ec6\u7684\u62a5\u544a\uff1b\u5e76\u4e14\u53ef\u4ee5\u5c55\u793a'\u89c6\u89c9\u8bc1\u636e', \u5373\u70ed\u56fe\uff0c\u663e\u793a\u51faAI\u5728\u5206\u6790\u65f6\u6240\u4f7f\u7528\u7684\u5177\u4f53\u7ec6\u80de\u548c\u533a\u57df\u3002\n", "result": "\u533b\u751f\u53ef\u4ee5\u901a\u8fc7\u8fd9\u79cd\u65b9\u5f0f\u83b7\u5f97AI\u8f85\u52a9\u7684\u5206\u6790\uff0c\u540c\u65f6\u5f97\u5230\u53ef\u89c6\u5316\u7684\u89e3\u91ca\uff0c\u8fd9\u5e2e\u52a9\u533b\u751f\u66f4\u5feb\u66f4\u81ea\u4fe1\u5730\u505a\u51fa\u8bca\u65ad\u3002\n", "conclusion": "\u901a\u8fc7HistoLens\uff0c\u533b\u751f\u53ef\u4ee5\u4f7f\u7528\u4e00\u4e2a\u503c\u5f97\u4fe1\u8d56\u7684AI\u52a9\u624b\u6765\u9a8c\u8bc1\u4ed6\u4eec\u7684\u89c1\u89e3\uff0c\u7ef4\u6301\u4e13\u5bb6\u7684\u89d2\u8272\u540c\u65f6\u63d0\u9ad8\u6548\u7387\u3002\n"}}
{"id": "2510.24038", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.24038", "abs": "https://arxiv.org/abs/2510.24038", "authors": ["Xingyu Zhu", "Beier Zhu", "Shuo Wang", "Kesen Zhao", "Hanwang Zhang"], "title": "Enhancing CLIP Robustness via Cross-Modality Alignment", "comment": "NeurIPS 2025 Spotlight", "summary": "Vision-language models (VLMs) such as CLIP demonstrate strong generalization\nin zero-shot classification but remain highly vulnerable to adversarial\nperturbations. Existing methods primarily focus on adversarial fine-tuning or\nprompt optimization; they often overlook the gaps in CLIP's encoded features,\nwhich is shown as the text and image features lie far apart from each other.\nThis misalignment is significantly amplified under adversarial perturbations,\nleading to severe degradation in classification performance. To address this\nproblem, we propose Cross-modality Alignment, dubbed COLA, an optimal\ntransport-based framework that explicitly addresses adversarial misalignment by\nrestoring both global image-text alignment and local structural consistency in\nthe feature space. (1) COLA first projects adversarial image embeddings onto a\nsubspace spanned by class text features, effectively filtering out non-semantic\ndistortions while preserving discriminative information. (2) It then models\nimages and texts as discrete distributions over multiple augmented views and\nrefines their alignment via OT, with the subspace projection seamlessly\nintegrated into the cost computation. This design ensures stable cross-modal\nalignment even under adversarial conditions. COLA is training-free and\ncompatible with existing fine-tuned models. Extensive evaluations across 14\nzero-shot classification benchmarks demonstrate the effectiveness of COLA,\nespecially with an average improvement of 6.7% on ImageNet and its variants\nunder PGD adversarial attacks, while maintaining high accuracy on clean\nsamples.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86COLA\u6846\u67b6\uff0c\u901a\u8fc7\u6700\u4f18\u4f20\u8f93\u89e3\u51b3\u4e86CLIP\u7b49\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u5728\u5bf9\u6297\u653b\u51fb\u4e0b\u6027\u80fd\u4e25\u91cd\u4e0b\u964d\u7684\u95ee\u9898\u3002COLA\u53ef\u901a\u8fc7\u6295\u5f71\u5bf9\u6297\u56fe\u50cf\u5d4c\u5165\u548c\u7a33\u5b9a\u4ea4\u53c9\u6a21\u6001\u5bf9\u9f50\u6765\u63d0\u9ad8\u5176\u5bf9\u6297\u7a33\u5065\u6027\uff0c\u5b9e\u9a8c\u8bc1\u660e\u5176\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u90fd\u6709\u663e\u8457\u6539\u8fdb\u3002", "motivation": "\u73b0\u6709\u7684\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u5728\u5bf9\u6297\u653b\u51fb\u4e0b\u6027\u80fd\u663e\u8457\u4e0b\u964d\uff0c\u73b0\u6709\u65b9\u6cd5\u4e3b\u8981\u662f\u5fae\u8c03\u6216\u63d0\u793a\u4f18\u5316\uff0c\u672a\u80fd\u89e3\u51b3\u7f16\u7801\u7279\u5f81\u95f4\u5b58\u5728\u5dee\u8ddd\u7684\u95ee\u9898\uff0c\u7279\u522b\u662f\u5bf9\u6297\u6027\u6270\u52a8\u4f1a\u52a0\u5267\u6587\u672c\u548c\u56fe\u50cf\u7279\u5f81\u4e4b\u95f4\u7684\u9519\u4f4d\u3002\u56e0\u6b64\uff0c\u672c\u6587\u63d0\u51fa\u4e86COLA\u6846\u67b6\uff0c\u4ee5\u89e3\u51b3\u5bf9\u6297\u6027\u5bf9\u9f50\u95ee\u9898\u3002", "method": "COLA\u9996\u5148\u5c06\u5bf9\u6297\u56fe\u50cf\u5d4c\u5165\u6295\u5f71\u5230\u7531\u7c7b\u522b\u6587\u672c\u7279\u5f81\u5f20\u6210\u7684\u5b50\u7a7a\u95f4\u4e2d\uff0c\u4ece\u800c\u8fc7\u6ee4\u6389\u975e\u8bed\u4e49\u626d\u66f2\uff0c\u4fdd\u5b58\u4e86\u5224\u522b\u4fe1\u606f\u3002\u7136\u540e\uff0c\u5b83\u5c06\u56fe\u50cf\u548c\u6587\u672c\u89c6\u4e3a\u591a\u4e2a\u589e\u5f3a\u89c6\u56fe\u7684\u79bb\u6563\u5206\u5e03\uff0c\u5e76\u901a\u8fc7\u6700\u4f18\u4f20\u8f93\u4f18\u5316\u5b83\u4eec\u7684\u5bf9\u9f50\u3002\u8fd9\u79cd\u8bbe\u8ba1\u786e\u4fdd\u4e86\u5373\u4f7f\u5728\u5bf9\u6297\u6761\u4ef6\u4e0b\uff0c\u8de8\u6a21\u6001\u5bf9\u9f50\u4e5f\u662f\u7a33\u5b9a\u7684\uff0c\u5e76\u4e14\u8be5\u6280\u672f\u5bb9\u8bb8\u4ec5\u4f7f\u7528\u73b0\u6210\u7684\u5fae\u8c03\u6a21\u578b\uff0c\u4e0d\u9700\u8981\u5bf9\u5176\u8fdb\u884c\u518d\u8bad\u7ec3\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cCOLA\u5728\u591a\u79cd\u96f6\u6837\u672c\u5206\u7c7b\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u90fd\u8868\u73b0\u51fa\u8272\uff0c\u5e76\u4e14\u5728PGD\u5bf9\u6297\u653b\u51fb\u4e0b\u5e73\u5747\u63d0\u9ad8\u4e86ImageNet\u548c\u5176\u4ed6\u53d8\u4f536.7%\u7684\u6027\u80fd\u3002\u540c\u65f6\uff0c\u5b83\u4e5f\u5177\u5907\u9ad8\u7cbe\u5ea6\u7ef4\u62a4\uff0c\u4ee3\u8868\u6027\u6848\u4f8b\u662f\u5728\u5e72\u51c0\u6837\u672c\u4e0a\u7684\u9ad8\u51c6\u786e\u6027\u3002", "conclusion": "\u901a\u8fc7\u4f7f\u7528COLA\u6846\u67b6\uff0c\u53ef\u4ee5\u6709\u6548\u5730\u89e3\u51b3\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u5728\u5bf9\u6297\u6027\u6270\u52a8\u4e0b\u7684\u6027\u80fd\u4e0b\u964d\u95ee\u9898\uff0c\u8be5\u6846\u67b6\u5177\u6709\u8bad\u7ec3\u81ea\u7531\u6027\u548c\u4e0e\u73b0\u6210\u6a21\u578b\u7684\u517c\u5bb9\u6027\u4f18\u70b9\uff0c\u5373\u4f7f\u6ca1\u6709\u989d\u5916\u7684\u8bad\u7ec3\u9700\u6c42\u4e5f\u80fd\u591f\u663e\u8457\u63d0\u9ad8\u6a21\u578b\u7684\u7a33\u5065\u6027\u548c\u6027\u80fd\u3002"}}
{"id": "2510.24145", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24145", "abs": "https://arxiv.org/abs/2510.24145", "authors": ["Yu Luo", "Jiamin Jiang", "Jingfei Feng", "Lei Tao", "Qingliang Zhang", "Xidao Wen", "Yongqian Sun", "Shenglin Zhang", "Jielong Huang", "Nan Qi", "Dan Pei"], "title": "From Observability Data to Diagnosis: An Evolving Multi-agent System for Incident Management in Cloud Systems", "comment": null, "summary": "Incident management (IM) is central to the reliability of large-scale cloud\nsystems. Yet manual IM, where on-call engineers examine metrics, logs, and\ntraces is labor-intensive and error-prone in the face of massive and\nheterogeneous observability data. Existing automated IM approaches often\nstruggle to generalize across systems, provide limited interpretability, and\nincur high deployment costs, which hinders adoption in practice. In this paper,\nwe present OpsAgent, a lightweight, self-evolving multi-agent system for IM\nthat employs a training-free data processor to convert heterogeneous\nobservability data into structured textual descriptions, along with a\nmulti-agent collaboration framework that makes diagnostic inference transparent\nand auditable. To support continual capability growth, OpsAgent also introduces\na dual self-evolution mechanism that integrates internal model updates with\nexternal experience accumulation, thereby closing the deployment loop.\nComprehensive experiments on the OPENRCA benchmark demonstrate state-of-the-art\nperformance and show that OpsAgent is generalizable, interpretable,\ncost-efficient, and self-evolving, making it a practically deployable and\nsustainable solution for long-term operation in real-world cloud systems.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u7684\u81ea\u6211\u8fdb\u5316\u591a\u667a\u80fd\u4f53\u7cfb\u7edfOpsAgent\uff0c\u7528\u4e8e\u81ea\u52a8\u5904\u7406\u5927\u89c4\u6a21\u4e91\u7cfb\u7edf\u7684\u4e8b\u6545\u7ba1\u7406\u3002OpsAgent\u5305\u62ec\u4e00\u4e2a\u8bad\u7ec3\u81ea\u7531\u7684\u6570\u636e\u5904\u7406\u5668\uff0c\u5c06\u5f02\u6784\u7684\u53ef\u89c2\u6d4b\u6027\u6570\u636e\u8f6c\u6362\u4e3a\u7ed3\u6784\u5316\u6587\u672c\u63cf\u8ff0\uff0c\u4ee5\u53ca\u4e00\u4e2a\u591a\u667a\u80fd\u4f53\u5408\u4f5c\u6846\u67b6\uff0c\u4f7f\u8bca\u65ad\u63a8\u7406\u53d8\u5f97\u900f\u660e\u548c\u53ef\u5ba1\u8ba1\u3002\u5b9e\u9a8c\u8868\u660e\uff0cOpsAgent\u5177\u6709\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u80fd\u591f\u81ea\u6211\u8fdb\u5316\uff0c\u5177\u6709\u89e3\u91ca\u6027\uff0c\u6210\u672c\u6548\u76ca\u9ad8\u6548\uff0c\u9002\u7528\u4e8e\u5b9e\u9645\u90e8\u7f72\u548c\u957f\u671f\u8fd0\u8425\u3002", "motivation": "\u5927\u89c4\u6a21\u4e91\u7cfb\u7edf\u4e2d\u7684\u4e8b\u6545\u7ba1\u7406\u4f9d\u8d56\u4e8e\u624b\u52a8\u64cd\u4f5c\uff0c\u8fd9\u65e2\u8017\u65f6\u53c8\u5bb9\u6613\u51fa\u9519\uff0c\u56e0\u4e3a\u8981\u5904\u7406\u5927\u91cf\u7684\u5f02\u6784\u6570\u636e\u3002\u76ee\u524d\u7684\u81ea\u52a8\u5316\u89e3\u51b3\u65b9\u6848\u5728\u6cdb\u5316\u6027\u3001\u89e3\u91ca\u6027\u548c\u90e8\u7f72\u6210\u672c\u4e0a\u5b58\u5728\u4e0d\u8db3\u3002\u56e0\u6b64\uff0c\u5f15\u5165OpsAgent\u6765\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "OpsAgent\u5229\u7528\u8bad\u7ec3\u81ea\u7531\u7684\u6570\u636e\u5904\u7406\u5668\u5c06\u5f02\u6784\u89c2\u6d4b\u6570\u636e\u8f6c\u5316\u4e3a\u7ed3\u6784\u5316\u3001\u6613\u4e8e\u7406\u89e3\u7684\u6587\u672c\u63cf\u8ff0\uff0c\u5e76\u5f15\u5165\u591a\u667a\u80fd\u4f53\u534f\u540c\u6846\u67b6\u4ee5\u4fbf\u8bca\u65ad\u63a8\u7406\u66f4\u4e3a\u900f\u660e\u548c\u53ef\u5ba1\u8ba1\u3002\u540c\u65f6OpsAgent\u5177\u5907\u81ea\u6211\u5347\u7ea7\u673a\u5236\uff0c\u5b9e\u73b0\u4e86\u5185\u90e8\u6a21\u578b\u66f4\u65b0\u548c\u5916\u90e8\u7ecf\u9a8c\u79ef\u7d2f\u7684\u95ed\u5408\u90e8\u7f72\u5faa\u73af\uff0c\u4ece\u800c\u5b9e\u73b0\u6301\u7eed\u7684\u80fd\u529b\u63d0\u5347\u3002", "result": "\u57fa\u4e8eOPENRCA\u57fa\u51c6\u5b9e\u9a8c\uff0cOpsAgent\u5c55\u793a\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u81ea\u8eab\u8fdb\u5316\u3001\u89e3\u91ca\u6027\u3001\u6210\u672c\u6548\u76ca\u548c\u53ef\u90e8\u7f72\u6027\u65b9\u9762\u7684\u4f18\u52bf\uff0c\u7279\u522b\u9002\u7528\u4e8e\u5927\u89c4\u6a21\u4e91\u7cfb\u7edf\u4e2d\u7684\u957f\u671f\u53ef\u9760\u8fd0\u8425\u3002", "conclusion": "OpsAgent\u4f5c\u4e3a\u4e00\u6b3e\u8f7b\u91cf\u7ea7\u4e14\u81ea\u6211\u8fdb\u5316\u7684\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\uff0c\u80fd\u591f\u6709\u6548\u89e3\u51b3\u5927\u89c4\u6a21\u4e91\u7cfb\u7edf\u4e2d\u7684\u4e8b\u6545\u7ba1\u7406\u95ee\u9898\uff0c\u5177\u6709\u5e7f\u6cdb\u7684\u9002\u7528\u6027\u548c\u5b9e\u8df5\u6027\u3002"}}
{"id": "2510.23657", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.23657", "abs": "https://arxiv.org/abs/2510.23657", "authors": ["Saklain Niam", "Tashfiqur Rahman", "Md. Amjad Patwary", "Mukarram Hossain"], "title": "A machine learning framework integrating seed traits and plasma parameters for predicting germination uplift in crops", "comment": null, "summary": "Cold plasma (CP) is an eco-friendly method to enhance seed germination, yet\noutcomes remain difficult to predict due to complex seed--plasma--environment\ninteractions. This study introduces the first machine learning framework to\nforecast germination uplift in soybean, barley, sunflower, radish, and tomato\nunder dielectric barrier discharge (DBD) plasma. Among the models tested (GB,\nXGB, ET, and hybrids), Extra Trees (ET) performed best (R\\textsuperscript{2} =\n0.919; RMSE = 3.21; MAE = 2.62), improving to R\\textsuperscript{2} = 0.925\nafter feature reduction. Engineering analysis revealed a hormetic response:\nnegligible effects at $<$7 kV or $<$200 s, maximum germination at 7--15 kV for\n200--500 s, and reduced germination beyond 20 kV or prolonged exposures.\nDischarge power was also a dominant factor, with germination rate maximizing at\n$\\geq$100 W with low exposure time. Species and cultivar-level predictions\nshowed radish (MAE = 1.46) and soybean (MAE = 2.05) were modeled with high\nconsistency, while sunflower remained slightly higher variable (MAE = 3.80).\nAmong cultivars, Williams (MAE = 1.23) and Sari (1.33) were well predicted,\nwhile Arian (2.86) and Ny\\'{\\i}rs\\'{e}gi fekete (3.74) were comparatively\npoorly captured. This framework was also embedded into MLflow, providing a\ndecision-support tool for optimizing CP seed germination in precision\nagriculture.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2510.24093", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.24093", "abs": "https://arxiv.org/abs/2510.24093", "authors": ["Agus Gunawan", "Samuel Teodoro", "Yun Chen", "Soo Ye Kim", "Jihyong Oh", "Munchurl Kim"], "title": "OmniText: A Training-Free Generalist for Controllable Text-Image Manipulation", "comment": "The first two authors contributed equally to this work. The last two\n  authors are co-corresponding authors", "summary": "Recent advancements in diffusion-based text synthesis have demonstrated\nsignificant performance in inserting and editing text within images via\ninpainting. However, despite the potential of text inpainting methods, three\nkey limitations hinder their applicability to broader Text Image Manipulation\n(TIM) tasks: (i) the inability to remove text, (ii) the lack of control over\nthe style of rendered text, and (iii) a tendency to generate duplicated\nletters. To address these challenges, we propose OmniText, a training-free\ngeneralist capable of performing a wide range of TIM tasks. Specifically, we\ninvestigate two key properties of cross- and self-attention mechanisms to\nenable text removal and to provide control over both text styles and content.\nOur findings reveal that text removal can be achieved by applying\nself-attention inversion, which mitigates the model's tendency to focus on\nsurrounding text, thus reducing text hallucinations. Additionally, we\nredistribute cross-attention, as increasing the probability of certain text\ntokens reduces text hallucination. For controllable inpainting, we introduce\nnovel loss functions in a latent optimization framework: a cross-attention\ncontent loss to improve text rendering accuracy and a self-attention style loss\nto facilitate style customization. Furthermore, we present OmniText-Bench, a\nbenchmark dataset for evaluating diverse TIM tasks. It includes input images,\ntarget text with masks, and style references, covering diverse applications\nsuch as text removal, rescaling, repositioning, and insertion and editing with\nvarious styles. Our OmniText framework is the first generalist method capable\nof performing diverse TIM tasks. It achieves state-of-the-art performance\nacross multiple tasks and metrics compared to other text inpainting methods and\nis comparable with specialist methods.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aOmniText\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u89e3\u51b3\u5f53\u524d\u57fa\u4e8e\u6269\u6563\u7684\u6587\u672c\u5408\u6210\u65b9\u6cd5\u5728\u6587\u672c\u53bb\u9664\u3001\u6587\u672c\u98ce\u683c\u63a7\u5236\u4ee5\u53ca\u907f\u514d\u6587\u672c\u5e7b\u89c9\u7b49\u65b9\u9762\u7684\u4e0d\u8db3\uff0c\u80fd\u591f\u5b9e\u73b0\u591a\u79cd\u6587\u672c\u56fe\u50cf\u64cd\u7eb5\u4efb\u52a1\u3002", "motivation": "\u73b0\u6709\u7684\u57fa\u4e8e\u6269\u6563\u7684\u6587\u672c\u5408\u6210\u65b9\u6cd5\u867d\u7136\u5728\u63d2\u5165\u548c\u7f16\u8f91\u56fe\u50cf\u4e2d\u7684\u6587\u672c\u65f6\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5b58\u5728\u65e0\u6cd5\u53bb\u9664\u6587\u672c\u3001\u7f3a\u4e4f\u5bf9\u6e32\u67d3\u6587\u672c\u6837\u5f0f\u7684\u63a7\u5236\u4ee5\u53ca\u751f\u6210\u91cd\u590d\u5b57\u6bcd\u8fd9\u4e09\u4e2a\u4e3b\u8981\u9650\u5236\u3002\u56e0\u6b64\uff0c\u63d0\u51faOmniText\u4ee5\u89e3\u51b3\u8fd9\u4e9b\u6311\u6218\uff0c\u6269\u5c55\u66f4\u591a\u6587\u672c\u56fe\u50cf\u64cd\u7eb5\u4efb\u52a1\u7684\u652f\u6301\u3002", "method": "\u901a\u8fc7\u7814\u7a76\u4ea4\u53c9\u548c\u81ea\u6211\u6ce8\u610f\u673a\u5236\u7684\u6027\u8d28\uff0c\u63d0\u51faOmniText\u53ef\u4ee5\u53bb\u9664\u6587\u672c\u5e76\u901a\u8fc7\u65b0\u9896\u7684\u635f\u5931\u51fd\u6570\u63a7\u5236\u6587\u672c\u6837\u5f0f\u548c\u5185\u5bb9\u3002\u5177\u4f53\u5305\u62ec\u81ea\u6211\u6ce8\u610f\u53cd\u8f6c\u4ee5\u53bb\u9664\u6587\u672c\uff0c\u4ea4\u53c9\u6ce8\u610f\u5185\u5bb9\u635f\u5931\u4ee5\u63d0\u9ad8\u6587\u672c\u6e32\u67d3\u51c6\u786e\u6027\uff0c\u4ee5\u53ca\u81ea\u6211\u6ce8\u610f\u98ce\u683c\u635f\u5931\u4ee5\u5b9e\u73b0\u6837\u5f0f\u5b9a\u5236\u3002\u6b64\u5916\uff0c\u8fd8\u521b\u5efa\u4e86\u4e00\u4e2a\u5305\u542b\u591a\u79cdTIM\u4efb\u52a1\u7684OmniText-Bench\u6570\u636e\u96c6\u3002", "result": "OmniText\u65b9\u6cd5\u53ef\u4ee5\u89e3\u51b3\u5f53\u524d\u6587\u672c\u56fe\u50cf\u64cd\u7eb5\u4efb\u52a1\u4e2d\u7684\u4e09\u4e2a\u4e3b\u8981\u9650\u5236\uff0c\u5b9e\u73b0\u4e86\u6587\u672c\u53bb\u9664\u548c\u6587\u672c\u98ce\u683c\u63a7\u5236\uff0c\u540c\u65f6\u964d\u4f4e\u4e86\u6587\u672c\u5e7b\u89c9\u7684\u4ea7\u751f\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u591a\u79cd\u4efb\u52a1\u4e0a\u8868\u73b0\u4f18\u8d8a\uff0c\u4e14\u968f\u7740\u4efb\u52a1\u7c7b\u578b\u548c\u65b9\u6cd5\u7684\u589e\u52a0\uff0c\u8be5\u65b9\u6cd5\u7684\u4f18\u8d8a\u6027\u8d8a\u4e3a\u660e\u663e\u3002", "conclusion": "OmniText\u662f\u7b2c\u4e00\u4e2a\u80fd\u591f\u5904\u7406\u591a\u79cd\u6587\u672c\u56fe\u50cf\u64cd\u7eb5\u4efb\u52a1\u7684\u65b9\u6cd5\uff0c\u4e0e\u4e13\u95e8\u7684\u65b9\u6cd5\u76f8\u6bd4\uff0c\u5176\u6027\u80fd\u5728\u591a\u9879\u4efb\u52a1\u548c\u5ea6\u91cf\u6807\u51c6\u4e0a\u90fd\u5904\u4e8e\u6700\u4f73\u6c34\u5e73\u3002"}}
{"id": "2510.24161", "categories": ["cs.AI", "cs.MM", "cs.RO"], "pdf": "https://arxiv.org/pdf/2510.24161", "abs": "https://arxiv.org/abs/2510.24161", "authors": ["Wentao Tan", "Bowen Wang", "Heng Zhi", "Chenyu Liu", "Zhe Li", "Jian Liu", "Zengrong Lin", "Yukun Dai", "Yipeng Chen", "Wenjie Yang", "Enci Xie", "Hao Xue", "Baixu Ji", "Chen Xu", "Zhibin Wang", "Tianshi Wang", "Lei Zhu", "Heng Tao Shen"], "title": "BLM$_1$: A Boundless Large Model for Cross-Space, Cross-Task, and Cross-Embodiment Learning", "comment": null, "summary": "Multimodal large language models (MLLMs) have advanced vision-language\nreasoning and are increasingly deployed in embodied agents. However,\nsignificant limitations remain: MLLMs generalize poorly across digital-physical\nspaces and embodiments; vision-language-action models (VLAs) produce low-level\nactions yet lack robust high-level embodied reasoning; and most embodied large\nlanguage models (ELLMs) are constrained to digital-space with poor\ngeneralization to the physical world. Thus, unified models that operate\nseamlessly across digital and physical spaces while generalizing across\nembodiments and tasks remain absent. We introduce the \\textbf{Boundless Large\nModel (BLM$_1$)}, a multimodal spatial foundation model that preserves\ninstruction following and reasoning, incorporates embodied knowledge, and\nsupports robust cross-embodiment control. BLM$_1$ integrates three key\ncapabilities -- \\textit{cross-space transfer, cross-task learning, and\ncross-embodiment generalization} -- via a two-stage training paradigm. Stage I\ninjects embodied knowledge into the MLLM through curated digital corpora while\nmaintaining language competence. Stage II trains a policy module through an\nintent-bridging interface that extracts high-level semantics from the MLLM to\nguide control, without fine-tuning the MLLM backbone. This process is supported\nby a self-collected cross-embodiment demonstration suite spanning four robot\nembodiments and six progressively challenging tasks. Evaluations across digital\nand physical benchmarks show that a single BLM$_1$ instance outperforms four\nmodel families -- MLLMs, ELLMs, VLAs, and GMLMs -- achieving\n$\\sim\\!\\textbf{6%}$ gains in digital tasks and $\\sim\\!\\textbf{3%}$ in physical\ntasks.", "AI": {"tldr": "\u4ecb\u7ecd\u4e86\u4e00\u79cd\u540d\u4e3aBoundless Large Model (BLM$_1$) \u7684\u65b0\u578b\u591a\u6a21\u6001\u7a7a\u95f4\u57fa\u7840\u6a21\u578b\uff0c\u8be5\u6a21\u578b\u517c\u5177\u8de8\u6570\u5b57-\u7269\u7406\u7a7a\u95f4\u65e0\u7f1d\u64cd\u4f5c\u3001\u4efb\u52a1\u6cdb\u5316\u548c\u8eab\u4f53\u6cdb\u5316\u80fd\u529b\u3002\u7ecf\u8fc7\u4e24\u4e2a\u9636\u6bb5\u7684\u8bad\u7ec3\uff0cBLM$_1$\u5728\u5404\u79cd\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u4e8e\u4f20\u7edf\u6a21\u578b\u5bb6\u65cf\uff0c\u7279\u522b\u662f\u5728\u6570\u5b57\u548c\u7269\u7406\u4efb\u52a1\u65b9\u9762\u5206\u522b\u63d0\u9ad8\u4e866%\u548c3%\u7684\u6027\u80fd\u3002", "motivation": "\u5f53\u524d\u7684\u591a\u6a21\u6001\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08MLLM\uff09\u5728\u6570\u5b57-\u7269\u7406\u7a7a\u95f4\u8fc1\u79fb\u3001\u4efb\u52a1\u6cdb\u5316\u548c\u8eab\u4f53\u6cdb\u5316\u65b9\u9762\u5b58\u5728\u663e\u8457\u9650\u5236\uff0c\u6bd4\u5982\u5728\u771f\u5b9e\u4e16\u754c\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u800cBLM$_1$\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u901a\u8fc7\u878d\u5408\u8de8\u7a7a\u95f4\u8fc1\u79fb\u3001\u8de8\u4efb\u52a1\u5b66\u4e60\u548c\u8de8\u8eab\u4f53\u6cdb\u5316\u80fd\u529b\u6765\u6539\u5584\u6a21\u578b\u6027\u80fd\u3002", "method": "BLM$_1$\u91c7\u7528\u4e86\u4e24\u9636\u6bb5\u8bad\u7ec3\u7b56\u7565\uff1a\u7b2c\u4e00\u9636\u6bb5\u5c06\u8eab\u4f53\u77e5\u8bc6\u6ce8\u5165\u6570\u5b57\u8bed\u6599\u5e93\u4ee5\u7ef4\u6301\u8bed\u8a00\u80fd\u529b\uff0c\u7b2c\u4e8c\u9636\u6bb5\u901a\u8fc7\u610f\u56fe\u6865\u63a5\u63a5\u53e3\u8bad\u7ec3\u653f\u7b56\u6a21\u5757\uff0c\u4eceMLLM\u4e2d\u63d0\u53d6\u9ad8\u5c42\u6b21\u8bed\u4e49\u6765\u5f15\u5bfc\u63a7\u5236\uff0c\u4e14\u65e0\u9700\u5fae\u8c03MLLM\u4e3b\u4f53\u90e8\u5206\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\u5355\u4e2aBLM$_1$\u5b9e\u4f8b\u5728\u6570\u5b57\u548c\u7269\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5747\u8868\u73b0\u4f18\u4e8e\u56db\u79cd\u6a21\u578b\u5bb6\u65cf(MLLMs, ELLMs, VLAs, \u548cGMLMs)\uff0c\u5c24\u5176\u662f\u5728\u6570\u5b57\u548c\u7269\u7406\u4efb\u52a1\u4e0a\u5206\u522b\u63d0\u9ad8\u4e86\u7ea66%\u548c3%\u7684\u6027\u80fd\u3002", "conclusion": "BLM$_1$\u901a\u8fc7\u6574\u5408\u8de8\u7a7a\u95f4\u3001\u8de8\u4efb\u52a1\u548c\u8de8\u8eab\u4f53\u7684\u80fd\u529b\uff0c\u514b\u670d\u4e86\u73b0\u6709\u6a21\u578b\u5728\u6570\u5b57-\u7269\u7406\u7a7a\u95f4\u8fc1\u79fb\u548c\u6cdb\u5316\u65b9\u9762\u7684\u5c40\u9650\u6027\uff0c\u63d0\u5347\u4e86\u5728\u771f\u5b9e\u4e16\u754c\u4e2d\u7684\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2510.23659", "categories": ["cs.LG", "cs.CV", "cs.ET"], "pdf": "https://arxiv.org/pdf/2510.23659", "abs": "https://arxiv.org/abs/2510.23659", "authors": ["Md. Farhan Shahriyar", "Gazi Tanbhir", "Abdullah Md Raihan Chy"], "title": "Quantum Machine Learning for Image Classification: A Hybrid Model of Residual Network with Quantum Support Vector Machine", "comment": null, "summary": "Recently, there has been growing attention on combining quantum machine\nlearning (QML) with classical deep learning approaches, as computational\ntechniques are key to improving the performance of image classification tasks.\nThis study presents a hybrid approach that uses ResNet-50 (Residual Network)\nfor feature extraction and Quantum Support Vector Machines (QSVM) for\nclassification in the context of potato disease detection. Classical machine\nlearning as well as deep learning models often struggle with high-dimensional\nand complex datasets, necessitating advanced techniques like quantum computing\nto improve classification efficiency. In our research, we use ResNet-50 to\nextract deep feature representations from RGB images of potato diseases. These\nfeatures are then subjected to dimensionality reduction using Principal\nComponent Analysis (PCA). The resulting features are processed through QSVM\nmodels which apply various quantum feature maps such as ZZ, Z, and Pauli-X to\ntransform classical data into quantum states. To assess the model performance,\nwe compared it with classical machine learning algorithms such as Support\nVector Machine (SVM) and Random Forest (RF) using five-fold stratified\ncross-validation for comprehensive evaluation. The experimental results\ndemonstrate that the Z-feature map-based QSVM outperforms classical models,\nachieving an accuracy of 99.23 percent, surpassing both SVM and RF models. This\nresearch highlights the advantages of integrating quantum computing into image\nclassification and provides a potential disease detection solution through\nhybrid quantum-classical modeling.", "AI": {"tldr": "\u7814\u7a76\u7ed3\u5408\u4e86\u7ecf\u5178\u6df1\u5ea6\u5b66\u4e60\u548c\u91cf\u5b50\u673a\u5668\u5b66\u4e60\u7684\u65b9\u6cd5\uff0c\u4f7f\u7528ResNet-50\u63d0\u53d6\u7279\u5f81\uff0c\u7136\u540e\u901a\u8fc7\u91cf\u5b50\u652f\u6301\u5411\u91cf\u673a\u8fdb\u884c\u5206\u7c7b\uff0c\u7279\u522b\u662f\u5728\u9a6c\u94c3\u85af\u75be\u75c5\u68c0\u6d4b\u4e2d\u53d6\u5f97\u4e86\u5f88\u597d\u7684\u6548\u679c\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u57fa\u4e8eZ\u7279\u5f81\u6620\u5c04\u7684\u91cf\u5b50\u652f\u6301\u5411\u91cf\u673a\u8d85\u8d8a\u4e86\u4f20\u7edf\u6a21\u578b\uff0c\u51c6\u786e\u7387\u8fbe\u523099.23%\u3002", "motivation": "\u5728\u6b64\u7814\u7a76\u4e2d\uff0c\u4e3a\u4e86\u63d0\u9ad8\u590d\u6742\u9ad8\u7ef4\u6570\u636e\u96c6\u4e0b\u7684\u5206\u7c7b\u6548\u7387\uff0c\u7ed3\u5408\u4e86\u91cf\u5b50\u8ba1\u7b97\u548c\u4f20\u7edf\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\uff0c\u7279\u522b\u662f\u5728\u56fe\u50cf\u5206\u7c7b\u4efb\u52a1\u4e2d\u3002\u63d0\u51fa\u4e86\u5728\u9a6c\u94c3\u85af\u75be\u75c5\u68c0\u6d4b\u4e2d\u7684\u5177\u4f53\u5e94\u7528\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u8bc1\u5b9e\u4e86\u5176\u4f18\u8d8a\u6027\u3002", "method": "\u7814\u7a76\u4e2d\u91c7\u7528ResNet-50\u7528\u4e8e\u7279\u5f81\u63d0\u53d6\uff0c\u4e4b\u540e\u7528PCA\u964d\u7ef4\uff0c\u518d\u7ecf\u7531\u91cf\u5b50\u652f\u6301\u5411\u91cf\u673a\u8fdb\u884c\u5206\u7c7b\u3002\u91cf\u5b50\u652f\u6301\u5411\u91cf\u673a\u4f7f\u7528\u4e86ZZ\u3001Z\u548cPauli-X\u7b49\u7279\u5f81\u6620\u5c04\uff0c\u5c06\u7ecf\u5178\u6570\u636e\u8f6c\u6362\u4e3a\u91cf\u5b50\u72b6\u6001\u3002\u5b9e\u9a8c\u4e2d\u8fd8\u5bf9\u6bd4\u4e86\u4f20\u7edf\u652f\u6301\u5411\u91cf\u673a\u548c\u968f\u673a\u68ee\u6797\u6a21\u578b\uff0c\u8bc4\u4ef7\u65b9\u6cd5\u4e3a\u4e94\u6298\u4ea4\u53c9\u9a8c\u8bc1\u3002", "result": "\u7ecf\u8fc7\u4e94\u6298\u4ea4\u53c9\u9a8c\u8bc1\uff0c\u57fa\u4e8eZ\u7279\u5f81\u6620\u5c04\u7684\u91cf\u5b50\u652f\u6301\u5411\u91cf\u673a\u6a21\u578b\u53d6\u5f97\u4e86\u6700\u4f73\u6548\u679c\uff0c\u51c6\u786e\u7387\u8fbe99.23%\uff0c\u4f18\u4e8e\u652f\u6301\u5411\u91cf\u673a\u548c\u968f\u673a\u68ee\u6797\u6a21\u578b\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0c\u5c06\u91cf\u5b50\u8ba1\u7b97\u878d\u5165\u56fe\u50cf\u5206\u7c7b\u4efb\u52a1\u4e2d\u53ef\u4ee5\u63d0\u9ad8\u5206\u7c7b\u6548\u7387\uff0c\u5e76\u4e3a\u75be\u75c5\u7684\u68c0\u6d4b\u63d0\u4f9b\u4e86\u6f5c\u5728\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.24105", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.24105", "abs": "https://arxiv.org/abs/2510.24105", "authors": ["Shufan Shen", "Zhaobo Qi", "Junshu Sun", "Qingming Huang", "Qi Tian", "Shuhui Wang"], "title": "Enhancing Pre-trained Representation Classifiability can Boost its Interpretability", "comment": "ICLR 2025 (Spotlight)", "summary": "The visual representation of a pre-trained model prioritizes the\nclassifiability on downstream tasks, while the widespread applications for\npre-trained visual models have posed new requirements for representation\ninterpretability. However, it remains unclear whether the pre-trained\nrepresentations can achieve high interpretability and classifiability\nsimultaneously. To answer this question, we quantify the representation\ninterpretability by leveraging its correlation with the ratio of interpretable\nsemantics within the representations. Given the pre-trained representations,\nonly the interpretable semantics can be captured by interpretations, whereas\nthe uninterpretable part leads to information loss. Based on this fact, we\npropose the Inherent Interpretability Score (IIS) that evaluates the\ninformation loss, measures the ratio of interpretable semantics, and quantifies\nthe representation interpretability. In the evaluation of the representation\ninterpretability with different classifiability, we surprisingly discover that\nthe interpretability and classifiability are positively correlated, i.e.,\nrepresentations with higher classifiability provide more interpretable\nsemantics that can be captured in the interpretations. This observation further\nsupports two benefits to the pre-trained representations. First, the\nclassifiability of representations can be further improved by fine-tuning with\ninterpretability maximization. Second, with the classifiability improvement for\nthe representations, we obtain predictions based on their interpretations with\nless accuracy degradation. The discovered positive correlation and\ncorresponding applications show that practitioners can unify the improvements\nin interpretability and classifiability for pre-trained vision models. Codes\nare available at https://github.com/ssfgunner/IIS.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u8861\u91cf\u9884\u8bad\u7ec3\u6a21\u578b\u8868\u793a\u53ef\u89e3\u91ca\u6027\u7684\u65b9\u6cd5\uff0c\u5e76\u53d1\u73b0\u8868\u793a\u7684\u53ef\u89e3\u91ca\u6027\u548c\u5206\u7c7b\u6027\u80fd\u4e4b\u95f4\u5b58\u5728\u6b63\u76f8\u5173\u6027\u3002\u8fd9\u8868\u660e\u53ef\u4ee5\u901a\u8fc7\u589e\u52a0\u8868\u793a\u7684\u53ef\u89e3\u91ca\u6027\u6765\u63d0\u5347\u5176\u5206\u7c7b\u6027\u80fd\uff0c\u540c\u65f6\u4e5f\u51cf\u5c11\u4e86\u51c6\u786e\u6027\u7684\u635f\u5931\u3002\u8fd9\u4e00\u53d1\u73b0\u63d0\u9ad8\u4e86\u9884\u8bad\u7ec3\u89c6\u89c9\u6a21\u578b\u5728\u89e3\u91ca\u6027\u548c\u5206\u7c7b\u6027\u4e0a\u7684\u7edf\u4e00\u6539\u8fdb\u7684\u53ef\u80fd\u6027\u3002", "motivation": "\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u89c6\u89c9\u8868\u793a\u503e\u5411\u4e8e\u4e0b\u6e38\u4efb\u52a1\u7684\u5206\u7c7b\u6027\u80fd\uff0c\u4f46\u4e3a\u4e86\u63d0\u9ad8\u8868\u793a\u7684\u53ef\u89e3\u91ca\u6027\uff0c\u63d0\u51fa\u4e86\u65b0\u7684\u8861\u91cf\u65b9\u6cd5\uff0c\u63a2\u8ba8\u8868\u793a\u540c\u65f6\u63d0\u5347\u53ef\u89e3\u91ca\u6027\u548c\u5206\u7c7b\u6027\u7684\u53ef\u80fd\u6027\u3002", "method": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8861\u91cf\u9884\u8bad\u7ec3\u89c6\u89c9\u8868\u793a\u53ef\u89e3\u91ca\u6027\u7684\u65b0\u6307\u6807\u2014\u2014\u56fa\u6709\u53ef\u89e3\u91ca\u6027\u8bc4\u5206\uff08Inherent Interpretability Score, IIS\uff09\uff0c\u8be5\u6307\u6807\u901a\u8fc7\u8bc4\u4f30\u89e3\u91ca\u4e2d\u7684\u4fe1\u606f\u635f\u5931\u6765\u8861\u91cf\u8868\u793a\u4e2d\u53ef\u89e3\u91ca\u8bed\u4e49\u7684\u6bd4\u4f8b\u3002\u5e76\u57fa\u4e8e\u8be5\u6307\u6807\u8fdb\u884c\u8868\u793a\u7684\u53ef\u89e3\u91ca\u6027\u5206\u6790\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u53ef\u89e3\u91ca\u6027\u548c\u5206\u7c7b\u6027\u4e4b\u95f4\u5b58\u5728\u6b63\u76f8\u5173\u6027\uff0c\u8868\u660e\u901a\u8fc7\u589e\u52a0\u89e3\u91ca\u6027\u53ef\u4ee5\u63d0\u5347\u5206\u7c7b\u6027\u80fd\uff0c\u5e76\u51cf\u5c11\u51c6\u786e\u6027\u635f\u5931\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u8861\u91cf\u9884\u8bad\u7ec3\u6a21\u578b\u8868\u793a\u53ef\u89e3\u91ca\u6027\u7684\u65b0\u65b9\u6cd5\uff0c\u5e76\u8bc1\u660e\u4e86\u901a\u8fc7\u4f18\u5316\u89e3\u91ca\u6027\u53ef\u4ee5\u540c\u65f6\u63d0\u5347\u6a21\u578b\u7684\u5206\u7c7b\u6027\u80fd\u3002\u8fd9\u4e3a\u8fdb\u4e00\u6b65\u6539\u8fdb\u9884\u8bad\u7ec3\u89c6\u89c9\u6a21\u578b\u7684\u53ef\u89e3\u91ca\u6027\u548c\u5206\u7c7b\u6027\u63d0\u4f9b\u4e86\u4f9d\u636e\u3002"}}
{"id": "2510.24116", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.24116", "abs": "https://arxiv.org/abs/2510.24116", "authors": ["Fengming Yu", "Haiwei Pan", "Kejia Zhang", "Jian Guan", "Haiying Jiang"], "title": "UHKD: A Unified Framework for Heterogeneous Knowledge Distillation via Frequency-Domain Representations", "comment": "14 pages, 4 figures", "summary": "Knowledge distillation (KD) is an effective model compression technique that\ntransfers knowledge from a high-performance teacher to a lightweight student,\nreducing cost while maintaining accuracy. In visual applications, where\nlarge-scale image models are widely used, KD enables efficient deployment.\nHowever, architectural diversity introduces semantic discrepancies that hinder\nthe use of intermediate representations. Most existing KD methods are designed\nfor homogeneous models and degrade in heterogeneous scenarios, especially when\nintermediate features are involved. Prior studies mainly focus on the logits\nspace, making limited use of the semantic information in intermediate layers.\nTo address this limitation, Unified Heterogeneous Knowledge Distillation (UHKD)\nis proposed as a framework that leverages intermediate features in the\nfrequency domain for cross-architecture transfer. Fourier transform is applied\nto capture global feature information, alleviating representational\ndiscrepancies between heterogeneous teacher-student pairs. A Feature\nTransformation Module (FTM) produces compact frequency-domain representations\nof teacher features, while a learnable Feature Alignment Module (FAM) projects\nstudent features and aligns them via multi-level matching. Training is guided\nby a joint objective combining mean squared error on intermediate features with\nKullback-Leibler divergence on logits. Experiments on CIFAR-100 and ImageNet-1K\ndemonstrate gains of 5.59% and 0.83% over the latest method, highlighting UHKD\nas an effective approach for unifying heterogeneous representations and\nenabling efficient utilization of visual knowledge", "AI": {"tldr": "Unified Heterogeneous Knowledge Distillation (UHKD) \u63d0\u51fa\u4e86\u4e00\u79cd\u77e5\u8bc6\u84b8\u998f\u6846\u67b6\uff0c\u901a\u8fc7\u9891\u7387\u57df\u4e2d\u7684\u4e2d\u95f4\u7279\u5f81\u6765\u8fdb\u884c\u8de8\u67b6\u6784\u7684\u77e5\u8bc6\u8f6c\u79fb\uff0c\u7279\u522b\u9002\u7528\u4e8e\u5f02\u6784\u6a21\u578b\u7684\u573a\u666f\uff0c\u5b9e\u9a8c\u8868\u660e\u5176\u76f8\u8f83\u4e8e\u6700\u65b0\u65b9\u6cd5\u5728CIFAR-100\u548cImageNet-1K\u4e0a\u6709\u66f4\u597d\u7684\u6027\u80fd\u63d0\u5347\u3002", "motivation": "\u73b0\u6709\u7684\u77e5\u8bc6\u84b8\u998f\u65b9\u6cd5\u5927\u591a\u9488\u5bf9\u540c\u6784\u6a21\u578b\u8bbe\u8ba1\uff0c\u5728\u5904\u7406\u5f02\u6784\u6a21\u578b\u65f6\u6548\u679c\u4e0d\u4f73\uff0c\u5c24\u5176\u662f\u5728\u4e2d\u95f4\u5c42\u7279\u5f81\u6d89\u53ca\u65f6\u8868\u73b0\u66f4\u5dee\u3002\u4e3a\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u63d0\u51faUHKD\u65b9\u6cd5\uff0c\u65e8\u5728\u5229\u7528\u9891\u7387\u57df\u4e2d\u7684\u4e2d\u95f4\u7279\u5f81\u6765\u51cf\u5c11\u5f02\u6784\u6a21\u578b\u4e2d\u7684\u8868\u793a\u5dee\u5f02\uff0c\u5e76\u5b9e\u73b0\u77e5\u8bc6\u7684\u6709\u6548\u4f20\u8f93\u3002", "method": "UHKD\u6846\u67b6\u901a\u8fc7\u5085\u7acb\u53f6\u53d8\u6362\u6355\u6349\u5168\u5c40\u7279\u5f81\u4fe1\u606f\uff0c\u4f7f\u7528\u4e00\u4e2a\u7279\u5f81\u8f6c\u6362\u6a21\u5757(FTM)\u4ea7\u751f\u7d27\u51d1\u7684\u9891\u7387\u57df\u6559\u5e08\u7279\u5f81\u8868\u5f81\uff0c\u5e76\u4f7f\u7528\u53ef\u5b66\u4e60\u7684\u7279\u5f81\u5bf9\u9f50\u6a21\u5757(FAM)\u5c06\u5b66\u751f\u7279\u5f81\u6295\u5f71\u5e76\u8fdb\u884c\u591a\u7ea7\u5339\u914d\u5bf9\u9f50\u3002\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\uff0c\u901a\u8fc7\u4e2d\u95f4\u7279\u5f81\u7684\u5747\u65b9\u8bef\u5dee\u548c\u8f93\u51fa\u4e0a\u7684KL\u6563\u5ea6\u7684\u8054\u5408\u76ee\u6807\u51fd\u6570\u8fdb\u884c\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cUHKD\u5728CIFAR-100\u548cImageNet-1K\u6570\u636e\u96c6\u4e0a\u7684\u6027\u80fd\u5206\u522b\u6bd4\u6700\u65b0\u65b9\u6cd5\u9ad8\u51fa5.59%\u548c0.83%\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u7edf\u4e00\u5f02\u6784\u8868\u793a\u548c\u9ad8\u6548\u5229\u7528\u89c6\u89c9\u77e5\u8bc6\u65b9\u9762\u7684\u6709\u6548\u6027\u3002", "conclusion": "UHKD\u662f\u4e00\u79cd\u6709\u6548\u7684\u77e5\u8bc6\u84b8\u998f\u6846\u67b6\uff0c\u80fd\u591f\u901a\u8fc7\u9891\u7387\u57df\u4e2d\u7684\u4e2d\u95f4\u7279\u5f81\u5b9e\u73b0\u8de8\u67b6\u6784\u7684\u77e5\u8bc6\u8f6c\u79fb\uff0c\u7279\u522b\u9002\u7528\u4e8e\u5904\u7406\u5f02\u6784\u6a21\u578b\u7684\u573a\u666f\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u7684\u538b\u7f29\u6548\u7387\u548c\u8868\u793a\u80fd\u529b\u3002"}}
{"id": "2510.23663", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.23663", "abs": "https://arxiv.org/abs/2510.23663", "authors": ["Padmanabhan Jagannathan Prajesh", "Kaliaperumal Ragunath", "Miriam Gordon", "Bruce Rathgeber", "Suresh Neethirajan"], "title": "AI-Driven Carbon Monitoring: Transformer-Based Reconstruction of Atmospheric CO2 in Canadian Poultry Regions", "comment": null, "summary": "Accurate mapping of column-averaged CO2 (XCO2) over agricultural landscapes\nis essential for guiding emission mitigation strategies. We present a\nSpatiotemporal Vision Transformer with Wavelets (ST-ViWT) framework that\nreconstructs continuous, uncertainty-quantified XCO2 fields from OCO-2 across\nsouthern Canada, emphasizing poultry-intensive regions. The model fuses wavelet\ntime-frequency representations with transformer attention over meteorology,\nvegetation indices, topography, and land cover. On 2024 OCO-2 data, ST-ViWT\nattains R2 = 0.984 and RMSE = 0.468 ppm; 92.3 percent of gap-filled predictions\nlie within +/-1 ppm. Independent validation with TCCON shows robust\ngeneralization (bias = -0.14 ppm; r = 0.928), including faithful reproduction\nof the late-summer drawdown. Spatial analysis across 14 poultry regions reveals\na moderate positive association between facility density and XCO2 (r = 0.43);\nhigh-density areas exhibit larger seasonal amplitudes (9.57 ppm) and enhanced\nsummer variability. Compared with conventional interpolation and standard\nmachine-learning baselines, ST-ViWT yields seamless 0.25 degree CO2 surfaces\nwith explicit uncertainties, enabling year-round coverage despite sparse\nobservations. The approach supports integration of satellite constraints with\nnational inventories and precision livestock platforms to benchmark emissions,\nrefine region-specific factors, and verify interventions. Importantly,\ntransformer-based Earth observation enables scalable, transparent, spatially\nexplicit carbon accounting, hotspot prioritization, and policy-relevant\nmitigation assessment.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aSpatiotemporal Vision Transformer with Wavelets (ST-ViWT)\u7684\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u80fd\u591f\u901a\u8fc7\u878d\u5408\u5c0f\u6ce2\u65f6\u95f4\u9891\u7387\u8868\u793a\u548c\u53d8\u538b\u5668\u6ce8\u610f\u529b\uff0c\u4f7f\u7528\u8f83\u5c11\u7684\u89c2\u6d4b\u6570\u636e\u6765\u91cd\u5efa\u8fde\u7eed\u4e14\u53ef\u91cf\u5316\u7684XCO2\u573a\u3002\u8be5\u6a21\u578b\u57282024 OCO-2\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u5177\u5907\u826f\u597d\u7684\u63a8\u5e7f\u80fd\u529b\u3002\u7ed3\u679c\u663e\u793a\uff0c\u5728\u9ad8\u5bc6\u5ea6\u5bb6\u79bd\u517b\u6b96\u533a\uff0cXCO2\u6392\u653e\u66f4\u9ad8\uff0c\u5b63\u8282\u53d8\u5316\u66f4\u5927\uff0c\u590f\u5b63\u53d8\u5316\u66f4\u663e\u8457\u3002ST-ViWT\u4e3a\u536b\u661f\u6570\u636e\u4e0e\u56fd\u5bb6\u6392\u653e\u6e05\u5355\u548c\u7cbe\u51c6\u755c\u7267\u4e1a\u5e73\u53f0\u7684\u7ed3\u5408\u63d0\u4f9b\u4e86\u53ef\u80fd\uff0c\u6709\u52a9\u4e8e\u7cbe\u51c6\u6392\u653e\u5b9a\u4f4d\u548c\u653f\u7b56\u5236\u5b9a\u3002", "motivation": "\u51c6\u786e\u7ed8\u5236\u519c\u4e1a\u666f\u89c2\u4e2d\u7684CO2\u5e73\u5747\u67f1\u6d53\u5ea6\uff08XCO2\uff09\u5bf9\u4e8e\u5236\u5b9a\u51cf\u6392\u7b56\u7565\u81f3\u5173\u91cd\u8981\u3002\u672c\u6587\u65e8\u5728\u5f00\u53d1\u4e00\u79cd\u57fa\u4e8e\u5c0f\u6ce2\u65f6\u9891\u8868\u793a\u4e0e\u53d8\u538b\u5668\u6ce8\u610f\u529b\u7684\u6a21\u578b\uff0c\u4ee5\u5229\u7528\u7a00\u758f\u7684\u536b\u661f\u89c2\u6d4b\u6570\u636e\u6765\u91cd\u5efa\u8fde\u7eed\u3001\u91cf\u5316\u4e0d\u786e\u5b9a\u6027XCO2\u573a\uff0c\u5c24\u5176\u662f\u5728\u5bb6\u79bd\u5bc6\u96c6\u517b\u6b96\u5730\u533a", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aSpatiotemporal Vision Transformer with Wavelets (ST-ViWT)\u7684\u6846\u67b6\u3002\u8be5\u6846\u67b6\u7ed3\u5408\u4e86\u5c0f\u6ce2\u65f6\u95f4\u9891\u8c31\u8868\u793a\u548cTransformer\u6ce8\u610f\u673a\u5236\uff0c\u4f7f\u7528\u6c14\u8c61\u5b66\u3001\u690d\u88ab\u6307\u6570\u3001\u5730\u5f62\u548c\u571f\u5730\u8986\u76d6\u7b49\u591a\u6e90\u4fe1\u606f\u6765\u91cd\u5efaXCO2\u573a\u3002\u8be5\u6a21\u578b\u80fd\u5728\u8f83\u5c11\u7684\u89c2\u6d4b\u6570\u636e\u4e0b\uff0c\u751f\u6210\u8fde\u7eed\u4e14\u53ef\u91cf\u5316\u7684XCO2\u573a\uff0c\u5177\u5907\u826f\u597d\u7684\u63a8\u5e7f\u80fd\u529b\u548c\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u80fd\u529b", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u57282024 OCO-2\u6570\u636e\u96c6\u4e0a\uff0cST-ViWT\u6846\u67b6\u8868\u73b0\u51fa\u4e86\u663e\u8457\u7684\u4f18\u52bf\uff08R\u00b2 = 0.984, RMSE = 0.468 ppm\uff1b92.3%\u7684\u9884\u6d4b\u843d\u5728\u00b11 ppm\u8303\u56f4\u5185\uff09\u3002\u72ec\u7acb\u9a8c\u8bc1\u8868\u660e\u8be5\u6a21\u578b\u5177\u5907\u826f\u597d\u7684\u63a8\u5e7f\u80fd\u529b\uff08\u504f\u7f6e=-0.14 ppm\uff0cr=0.928\uff09\u3002\u5e76\u4e14\u5728\u9ad8\u5bc6\u5ea6\u5bb6\u79bd\u517b\u6b96\u533a\uff0cST-ViWT\u80fd\u591f\u66f4\u51c6\u786e\u5730\u6355\u6349\u5230XCO2\u7684\u5b63\u8282\u53d8\u5316\u548c\u590f\u5b63\u53d8\u5316", "conclusion": "\u901a\u8fc7ST-ViWT\u6a21\u578b\uff0c\u53ef\u4ee5\u4f9d\u636e\u7a00\u758f\u7684\u89c2\u6d4b\u6570\u636e\uff0c\u751f\u6210\u65e0\u7f1d\u9699\u76840.25\u5ea6CO2\u8868\u9762\uff0c\u5e76\u5177\u5907\u660e\u786e\u7684\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u3002\u8be5\u65b9\u6cd5\u652f\u6301\u5c06\u536b\u661f\u7ea6\u675f\u6761\u4ef6\u878d\u5165\u56fd\u5bb6\u6392\u653e\u6e05\u5355\u4e2d\uff0c\u5bf9\u7279\u5b9a\u533a\u57df\u7684\u6392\u653e\u8fdb\u884c\u7cbe\u786e\u91cf\u5316\uff0c\u5e76\u9a8c\u8bc1\u51cf\u6392\u63aa\u65bd\u3002\u6b64\u5916\uff0c\u8be5\u65b9\u6cd5\u6709\u52a9\u4e8e\u900f\u660e\u3001\u4e3b\u4f53\u660e\u786e\u7684\u78b3\u6838\u7b97\uff0c\u70ed\u70b9\u5730\u533a\u4f18\u5148\u8bc6\u522b\uff0c\u4ee5\u53ca\u5236\u5b9a\u5177\u6709\u653f\u7b56\u610f\u4e49\u7684\u51cf\u6392\u8bc4\u4f30"}}
{"id": "2510.23665", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.23665", "abs": "https://arxiv.org/abs/2510.23665", "authors": ["Juan C. Leon Alcazar", "Mattia Soldan", "Mohammad Saatialsoruji", "Alejandro Pardo", "Hani Itani", "Juan Camilo Perez", "Bernard Ghanem"], "title": "Transformers from Compressed Representations", "comment": null, "summary": "Compressed file formats are the corner stone of efficient data storage and\ntransmission, yet their potential for representation learning remains largely\nunderexplored. We introduce TEMPEST (TransformErs froM comPressed\nrEpreSenTations), a method that exploits the inherent byte-stream structure of\ncompressed files to design an effective tokenization and encoding strategy. By\nleveraging this compact encoding, a standard transformer can directly learn\nsemantic representations from compressed data streams, bypassing the need for\nraw byte-level processing or full media decoding. Our proposal substantially\nreduces the number of tokens required for semantic classification, thereby\nlowering both computational complexity and memory usage. Through extensive\nexperiments across diverse datasets, coding schemes, and modalities, we show\nthat TEMPEST achieves accuracy competitive wit the state-of-the-art while\ndelivering efficiency gains in memory and compute.", "AI": {"tldr": "\u6211\u4eec\u63d0\u51fa\u4e86TEMPEST\u65b9\u6cd5\uff0c\u5229\u7528\u538b\u7f29\u6587\u4ef6\u7684\u5b57\u8282\u6d41\u7ed3\u6784\u8bbe\u8ba1\u6709\u6548\u7684tokenization\u548c\u7f16\u7801\u7b56\u7565\uff0c\u4f7f\u5f97Transformer\u53ef\u4ee5\u76f4\u63a5\u4ece\u538b\u7f29\u6570\u636e\u6d41\u4e2d\u5b66\u4e60\u8bed\u4e49\u8868\u793a\uff0c\u4ece\u800c\u51cf\u5c11\u6240\u9700token\u6570\u91cf\uff0c\u964d\u4f4e\u8ba1\u7b97\u590d\u6742\u5ea6\u548c\u5185\u5b58\u4f7f\u7528\u3002\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0cTEMPEST\u5728\u51c6\u786e\u7387\u4e0a\u4e0e\u5f53\u524d\u6700\u4f18\u65b9\u6cd5\u76f8\u5f53\uff0c\u540c\u65f6\u5728\u5185\u5b58\u548c\u8ba1\u7b97\u8d44\u6e90\u4e0a\u66f4\u5177\u9ad8\u6548\u6027\u3002", "motivation": "\u76ee\u524d\u538b\u7f29\u6587\u4ef6\u683c\u5f0f\u5728\u6570\u636e\u5b58\u50a8\u548c\u4f20\u8f93\u4e2d\u7684\u9ad8\u6548\u6027\u5df2\u4e3a\u4eba\u6240\u77e5\uff0c\u7136\u800c\u5176\u5728\u8868\u793a\u5b66\u4e60\u4e2d\u7684\u6f5c\u529b\u5c1a\u672a\u88ab\u5145\u5206\u63a2\u7d22\u3002\u6211\u4eec\u5e0c\u671b\u5229\u7528\u538b\u7f29\u6587\u4ef6\u7684\u5b57\u8282\u6d41\u7ed3\u6784\u6765\u8bbe\u8ba1\u51fa\u4e00\u79cd\u65b0\u7684tokenization\u548c\u7f16\u7801\u7b56\u7565\uff0c\u4ee5\u63d0\u9ad8\u6570\u636e\u8868\u793a\u7684\u5b66\u4e60\u6548\u7387\u3002", "method": "\u63d0\u51fa\u4e86TEMPEST\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u901a\u8fc7\u5229\u7528\u538b\u7f29\u6587\u4ef6\u7684\u5b57\u8282\u6d41\u7ed3\u6784\uff0c\u8bbe\u8ba1\u4e86\u4e00\u79cd\u6709\u6548\u7684tokenization\u548c\u7f16\u7801\u7b56\u7565\uff0c\u4f7f\u5f97\u6807\u51c6\u7684Transformer\u53ef\u4ee5\u76f4\u63a5\u4ece\u538b\u7f29\u6570\u636e\u6d41\u4e2d\u5b66\u4e60\u8bed\u4e49\u8868\u793a\uff0c\u800c\u4e0d\u9700\u8981\u539f\u59cb\u5b57\u8282\u7ea7\u5904\u7406\u6216\u5b8c\u6574\u7684\u5a92\u4f53\u89e3\u7801\u8fc7\u7a0b\u3002\u8fd9\u51cf\u5c11\u4e86\u8bed\u4e49\u5206\u7c7b\u6240\u9700\u7684token\u6570\u91cf\uff0c\u4ece\u800c\u964d\u4f4e\u4e86\u8ba1\u7b97\u590d\u6742\u5ea6\u548c\u5185\u5b58\u4f7f\u7528\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cTEMPEST\u65b9\u6cd5\u5728\u591a\u4e2a\u4e0d\u540c\u6570\u636e\u96c6\u3001\u7f16\u7801\u65b9\u6848\u548c\u6a21\u6001\u4e0a\u53d6\u5f97\u4e86\u4e0e\u73b0\u6709\u6700\u4f18\u65b9\u6cd5\u76f8\u5f53\u7684\u51c6\u786e\u7387\uff0c\u540c\u65f6\u5728\u5185\u5b58\u548c\u8ba1\u7b97\u8d44\u6e90\u6548\u7387\u65b9\u9762\u5177\u6709\u663e\u8457\u4f18\u52bf\u3002", "conclusion": "\u6211\u4eec\u7684\u65b9\u6cd5\u901a\u8fc7\u5229\u7528\u538b\u7f29\u6587\u4ef6\u7684\u5b57\u8282\u6d41\u7ed3\u6784\uff0c\u53ef\u4ee5\u76f4\u63a5\u4ece\u538b\u7f29\u6570\u636e\u6d41\u4e2d\u8fdb\u884c\u8bed\u4e49\u8868\u793a\u5b66\u4e60\uff0c\u8fd9\u964d\u4f4e\u4e86\u8ba1\u7b97\u590d\u6742\u5ea6\u548c\u5185\u5b58\u4f7f\u7528\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u4e0e\u73b0\u6709\u6700\u4f18\u65b9\u6cd5\u76f8\u5f53\u7684\u51c6\u786e\u7387\u3002"}}
{"id": "2510.24129", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.24129", "abs": "https://arxiv.org/abs/2510.24129", "authors": ["Jiajian Xie", "Hubery Yin", "Chen Li", "Zhou Zhao", "Shengyu Zhang"], "title": "ETC: training-free diffusion models acceleration with Error-aware Trend Consistency", "comment": "17 pages, 10 figures", "summary": "Diffusion models have achieved remarkable generative quality but remain\nbottlenecked by costly iterative sampling. Recent training-free methods\naccelerate diffusion process by reusing model outputs. However, these methods\nignore denoising trends and lack error control for model-specific tolerance,\nleading to trajectory deviations under multi-step reuse and exacerbating\ninconsistencies in the generated results. To address these issues, we introduce\nError-aware Trend Consistency (ETC), a framework that (1) introduces a\nconsistent trend predictor that leverages the smooth continuity of diffusion\ntrajectories, projecting historical denoising patterns into stable future\ndirections and progressively distributing them across multiple approximation\nsteps to achieve acceleration without deviating; (2) proposes a model-specific\nerror tolerance search mechanism that derives corrective thresholds by\nidentifying transition points from volatile semantic planning to stable quality\nrefinement. Experiments show that ETC achieves a 2.65x acceleration over FLUX\nwith negligible (-0.074 SSIM score) degradation of consistency.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aError-aware Trend Consistency (ETC)\u7684\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u901a\u8fc7\u4e00\u81f4\u7684\u8d8b\u52bf\u9884\u6d4b\u5668\u548c\u6a21\u578b\u7279\u5b9a\u7684\u8bef\u5dee\u5bb9\u9650\u641c\u7d22\u673a\u5236\u6765\u52a0\u901f\u6269\u6563\u8fc7\u7a0b\uff0c\u540c\u65f6\u4fdd\u6301\u751f\u6210\u7ed3\u679c\u7684\u4e00\u81f4\u6027\uff0c\u5b9e\u9a8c\u8bc1\u660eETC\u53ef\u4ee5\u5b9e\u73b0\u6bd4FLUX\u5feb2.65\u500d\u7684\u52a0\u901f\uff0c\u540c\u65f6\u4fdd\u6301\u51e0\u4e4e\u4e0d\u53d8\u7684\u4e00\u81f4\u6027\uff08-0.074 SSIM\u5f97\u5206\uff09", "motivation": "\u4f20\u7edf\u7684\u8bad\u7ec3\u65e0\u6210\u672c\u65b9\u6cd5\u867d\u7136\u52a0\u901f\u4e86\u6269\u6563\u8fc7\u7a0b\uff0c\u4f46\u5ffd\u89c6\u4e86\u53bb\u566a\u8d8b\u52bf\uff0c\u7f3a\u4e4f\u6a21\u578b\u7279\u5b9a\u7684\u8bef\u5dee\u63a7\u5236\uff0c\u5bfc\u81f4\u8f68\u8ff9\u504f\u5dee\u548c\u7ed3\u679c\u4e0d\u4e00\u81f4\u3002ETC\u6846\u67b6\u63d0\u51fa\u4e86\u8d8b\u52bf\u9884\u6d4b\u5668\u548c\u8bef\u5dee\u5bb9\u9650\u641c\u7d22\u673a\u5236\u6765\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898", "method": "ETC\u6846\u67b6\u5305\u62ec\u4e00\u81f4\u7684\u8d8b\u52bf\u9884\u6d4b\u5668\u548c\u6a21\u578b\u7279\u5b9a\u7684\u8bef\u5dee\u5bb9\u9650\u641c\u7d22\u673a\u5236\uff0c\u8d8b\u52bf\u9884\u6d4b\u5668\u7528\u5386\u53f2\u53bb\u566a\u6a21\u5f0f\u9884\u6d4b\u672a\u6765\u7684\u7a33\u5b9a\u8d8b\u52bf\uff0c\u8bef\u5dee\u5bb9\u9650\u641c\u7d22\u673a\u5236\u8bc6\u522b\u4ece\u8bed\u4e49\u89c4\u5212\u5230\u8d28\u91cf\u7ec6\u5316\u7684\u8fc7\u6e21\u70b9\uff0c\u786e\u5b9a\u7ea0\u6b63\u9608\u503c", "result": "\u5b9e\u9a8c\u663e\u793a\uff0cETC\u53ef\u4ee5\u5b9e\u73b0\u6bd4FLUX\u5feb2.65\u500d\u7684\u52a0\u901f\uff0c\u540c\u65f6\u4fdd\u6301\u51e0\u4e4e\u4e0d\u53d8\u7684\u4e00\u81f4\u6027\uff08-0.074 SSIM\u5f97\u5206\uff09", "conclusion": "ETC\u6846\u67b6\u901a\u8fc7\u4e00\u81f4\u7684\u8d8b\u52bf\u9884\u6d4b\u5668\u548c\u6a21\u578b\u7279\u5b9a\u7684\u8bef\u5dee\u5bb9\u9650\u641c\u7d22\u673a\u5236\u6210\u529f\u89e3\u51b3\u4e86\u4f20\u7edf\u8bad\u7ec3\u65e0\u6210\u672c\u65b9\u6cd5\u7684\u95ee\u9898\uff0c\u5373\u5728\u52a0\u901f\u6269\u6563\u8fc7\u7a0b\u7684\u540c\u65f6\u4fdd\u6301\u7ed3\u679c\u7684\u4e00\u81f4\u6027\u548c\u8d28\u91cf"}}
{"id": "2510.24297", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24297", "abs": "https://arxiv.org/abs/2510.24297", "authors": ["Robin Schm\u00f6cker", "Alexander Dockhorn", "Bodo Rosenhahn"], "title": "Investigating Intra-Abstraction Policies For Non-exact Abstraction Algorithms", "comment": null, "summary": "One weakness of Monte Carlo Tree Search (MCTS) is its sample efficiency which\ncan be addressed by building and using state and/or action abstractions in\nparallel to the tree search such that information can be shared among nodes of\nthe same layer. The primary usage of abstractions for MCTS is to enhance the\nUpper Confidence Bound (UCB) value during the tree policy by aggregating visits\nand returns of an abstract node. However, this direct usage of abstractions\ndoes not take the case into account where multiple actions with the same parent\nmight be in the same abstract node, as these would then all have the same UCB\nvalue, thus requiring a tiebreak rule. In state-of-the-art abstraction\nalgorithms such as pruned On the Go Abstractions (pruned OGA), this case has\nnot been noticed, and a random tiebreak rule was implicitly chosen. In this\npaper, we propose and empirically evaluate several alternative\nintra-abstraction policies, several of which outperform the random policy\nacross a majority of environments and parameter settings.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u8499\u7279\u5361\u6d1b\u6811\u641c\u7d22(MCTS)\u7684\u4e00\u4e2a\u5f31\u70b9\uff0c\u5373\u6837\u672c\u6548\u7387\uff0c\u5e76\u901a\u8fc7\u4f7f\u7528\u72b6\u6001\u548c/\u6216\u52a8\u4f5c\u62bd\u8c61\u6765\u6539\u5584\u4e0a\u754c\u7f6e\u4fe1\u533a\u95f4(UCB)\u4f30\u503c\u3002\u540c\u65f6\u63d0\u51fa\u5e76\u5b9e\u8bc1\u8bc4\u4f30\u4e86\u51e0\u79cd\u66ff\u4ee3\u7684\u540c\u62bd\u8c61\u7b56\u7565\uff0c\u5176\u4e2d\u4e00\u4e9b\u7b56\u7565\u5728\u591a\u6570\u73af\u5883\u4e0b\u548c\u53c2\u6570\u8bbe\u7f6e\u4e0b\u4f18\u4e8e\u968f\u673a\u7b56\u7565\u3002", "motivation": "\u672c\u6587\u65e8\u5728\u89e3\u51b3\u8499\u7279\u5361\u6d1b\u6811\u641c\u7d22(MCTS)\u4e2d\u7684\u6837\u672c\u6548\u7387\u95ee\u9898\uff0c\u5e76\u9488\u5bf9\u4f7f\u7528\u76f8\u540c\u62bd\u8c61\u8282\u70b9\u65f6\u53ef\u80fd\u51fa\u73b0\u7684\u51b2\u7a81\u60c5\u51b5\u7ed9\u51fa\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u672c\u6587\u63d0\u51fa\u7684\u89e3\u51b3\u65b9\u6cd5\u662f\u91c7\u7528\u51e0\u79cd\u66ff\u4ee3\u7684\u540c\u62bd\u8c61\u7b56\u7565\uff0c\u8fd9\u4e9b\u65b9\u6cd5\u65e8\u5728\u6539\u8fdb\u4e0a\u754c\u7f6e\u4fe1\u533a\u95f4(UCB)\u4f30\u503c\uff0c\u5e76\u907f\u514d\u968f\u673a\u9009\u62e9\u5bfc\u81f4\u7684\u51b2\u7a81\u95ee\u9898\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\uff0c\u672c\u6587\u63d0\u51fa\u7684\u4e00\u4e9b\u65b9\u6cd5\u5728\u591a\u6570\u6d4b\u8bd5\u73af\u5883\u548c\u53c2\u6570\u8bbe\u7f6e\u4e0b\u4f18\u4e8e\u968f\u673a\u7b56\u7565\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u6f84\u6e05\u4e86\u73b0\u6709\u62bd\u8c61\u7b97\u6cd5\u4e2d\u7684\u4e00\u4e2a\u672a\u89e3\u51b3\u95ee\u9898\uff0c\u5e76\u63d0\u4f9b\u4e86\u6539\u8fdb\u8499\u7279\u5361\u6d1b\u6811\u641c\u7d22\u6837\u672c\u6548\u7387\u7684\u65b0\u9014\u5f84\u3002"}}
{"id": "2510.23667", "categories": ["cs.LG", "cs.AI", "cs.CE"], "pdf": "https://arxiv.org/pdf/2510.23667", "abs": "https://arxiv.org/abs/2510.23667", "authors": ["Amin Heyrani Nobari", "Lyle Regenwetter", "Cyril Picard", "Ligong Han", "Faez Ahmed"], "title": "Optimize Any Topology: A Foundation Model for Shape- and Resolution-Free Structural Topology Optimization", "comment": null, "summary": "Structural topology optimization (TO) is central to engineering design but\nremains computationally intensive due to complex physics and hard constraints.\nExisting deep-learning methods are limited to fixed square grids, a few\nhand-coded boundary conditions, and post-hoc optimization, preventing general\ndeployment. We introduce Optimize Any Topology (OAT), a foundation-model\nframework that directly predicts minimum-compliance layouts for arbitrary\naspect ratios, resolutions, volume fractions, loads, and fixtures. OAT combines\na resolution- and shape-agnostic autoencoder with an implicit neural-field\ndecoder and a conditional latent-diffusion model trained on OpenTO, a new\ncorpus of 2.2 million optimized structures covering 2 million unique\nboundary-condition configurations. On four public benchmarks and two\nchallenging unseen tests, OAT lowers mean compliance up to 90% relative to the\nbest prior models and delivers sub-1 second inference on a single GPU across\nresolutions from 64 x 64 to 256 x 256 and aspect ratios as high as 10:1. These\nresults establish OAT as a general, fast, and resolution-free framework for\nphysics-aware topology optimization and provide a large-scale dataset to spur\nfurther research in generative modeling for inverse design. Code & data can be\nfound at https://github.com/ahnobari/OptimizeAnyTopology.", "AI": {"tldr": "OAT\u662f\u4e00\u79cd\u57fa\u7840\u6a21\u578b\u6846\u67b6\uff0c\u7528\u4e8e\u9884\u6d4b\u4e0d\u540c\u6761\u4ef6\u4e0b\u7684\u6700\u5c0f\u5408\u89c4\u5e03\u5c40\uff0c\u63d0\u9ad8\u7ed3\u6784\u62d3\u6251\u4f18\u5316\u7684\u6548\u7387\u548c\u5e7f\u5ea6\uff0c\u4e0e\u73b0\u6709\u6700\u4f73\u6a21\u578b\u76f8\u6bd4\uff0c\u53ef\u5c06\u5e73\u5747\u5408\u89c4\u6027\u964d\u4f4e\u591a\u8fbe90%\u3002OAT\u5e94\u7528\u8303\u56f4\u5e7f\uff0c\u901f\u5ea6\u5feb\uff0c\u4e0d\u5206\u5206\u8fa8\u7387\uff0c\u9002\u7528\u4e8e\u7269\u7406\u611f\u77e5\u7684\u62d3\u6251\u4f18\u5316\u3002\u4ee3\u7801\u548c\u6570\u636e\u53ef\u5728https://github.com/ahnobari/OptimizeAnyTopology\u4e2d\u627e\u5230\u3002", "motivation": "\u73b0\u6709\u7684\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u5728\u56fa\u5b9a\u7684\u65b9\u683c\u7f51\u683c\u3001\u6709\u9650\u7684\u624b\u5de5\u7f16\u7801\u8fb9\u754c\u6761\u4ef6\u4ee5\u53ca\u540e\u671f\u4f18\u5316\u7684\u9650\u5236\u4e0b\uff0c\u96be\u4ee5\u5b9e\u73b0\u666e\u904d\u9002\u7528\u3002\u56e0\u6b64\uff0c\u7814\u7a76\u56e2\u961f\u63d0\u51fa\u4e86OAT\uff0c\u4ee5\u89e3\u51b3\u8fd9\u4e9b\u9650\u5236\u5e76\u5b9e\u73b0\u5bf9\u4efb\u610f\u7ed3\u6784\u7684\u9ad8\u6548\u4f18\u5316\u3002", "method": "OAT\u5c06\u5206\u8fa8\u7387\u548c\u5f62\u72b6\u65e0\u5173\u7684\u81ea\u52a8\u7f16\u7801\u5668\u4e0e\u9690\u5f0f\u795e\u7ecf\u573a\u89e3\u7801\u5668\u548c\u6761\u4ef6\u6f5c\u5728\u6269\u6563\u6a21\u578b\u76f8\u7ed3\u5408\uff0c\u8bad\u7ec3\u91c7\u7528\u4e86\u4e00\u4e2a\u65b0\u7684OpenTO\u6570\u636e\u96c6\uff0c\u8be5\u6570\u636e\u96c6\u5305\u542b220\u4e07\u4e2a\u4f18\u5316\u7ed3\u6784\uff0c\u8986\u76d6\u4e86200\u4e07\u4e2a\u72ec\u7279\u7684\u8fb9\u754c\u6761\u4ef6\u914d\u7f6e\u3002", "result": "\u5728\u56db\u4e2a\u516c\u5171\u57fa\u51c6\u6d4b\u8bd5\u548c\u4e24\u4e2a\u5177\u6709\u6311\u6218\u6027\u7684\u672a\u89c1\u8fc7\u7684\u6d4b\u8bd5\u4e2d\uff0cOAT\u76f8\u6bd4\u4e8e\u6700\u4f73\u7684\u5148\u524d\u65b9\u6cd5\uff0c\u5e73\u5747\u5408\u89c4\u6027\u964d\u4f4e\u4e86\u591a\u8fbe90%\u3002OAT\u5728\u5355\u4e2aGPU\u4e0a\u5b9e\u73b0\u4e86\u5c0f\u4e8e1\u79d2\u7684\u63a8\u7406\u65f6\u95f4\uff0c\u9002\u7528\u4e8e\u4ece64x64\u5230256x256\u7684\u5206\u8fa8\u7387\u4ee5\u53ca\u9ad8\u8fbe10:1\u7684\u5bbd\u9ad8\u6bd4\u3002", "conclusion": "OAT\u88ab\u786e\u7acb\u4e3a\u4e00\u79cd\u901a\u7528\u3001\u5feb\u901f\u4e14\u4e0d\u5206\u5206\u8fa8\u7387\u7684\u7269\u7406\u611f\u77e5\u62d3\u6251\u4f18\u5316\u6846\u67b6\uff0c\u5e76\u4e3a\u9006\u5411\u8bbe\u8ba1\u4e2d\u7684\u751f\u6210\u6a21\u578b\u7814\u7a76\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5927\u89c4\u6a21\u7684\u6570\u636e\u96c6\u3002"}}
{"id": "2510.24133", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24133", "abs": "https://arxiv.org/abs/2510.24133", "authors": ["Minsuk Ji", "Sanghyeok Lee", "Namhyuk Ahn"], "title": "Compositional Image Synthesis with Inference-Time Scaling", "comment": "projcet page: https://github.com/gcl-inha/ReFocus", "summary": "Despite their impressive realism, modern text-to-image models still struggle\nwith compositionality, often failing to render accurate object counts,\nattributes, and spatial relations. To address this challenge, we present a\ntraining-free framework that combines an object-centric approach with\nself-refinement to improve layout faithfulness while preserving aesthetic\nquality. Specifically, we leverage large language models (LLMs) to synthesize\nexplicit layouts from input prompts, and we inject these layouts into the image\ngeneration process, where a object-centric vision-language model (VLM) judge\nreranks multiple candidates to select the most prompt-aligned outcome\niteratively. By unifying explicit layout-grounding with self-refine-based\ninference-time scaling, our framework achieves stronger scene alignment with\nprompts compared to recent text-to-image models. The code are available at\nhttps://github.com/gcl-inha/ReFocus.", "AI": {"tldr": "\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u8bad\u7ec3\u65e0\u5173\u7684\u6846\u67b6\uff0c\u7ed3\u5408\u4e86\u5bf9\u8c61\u4e2d\u5fc3\u7684\u65b9\u6cd5\u548c\u81ea\u6211\u6539\u8fdb\uff0c\u4ee5\u63d0\u9ad8\u5e03\u5c40\u5fe0\u5b9e\u6027\uff0c\u540c\u65f6\u4fdd\u6301\u7f8e\u5b66\u8d28\u91cf\u3002\u901a\u8fc7\u5c06\u663e\u5f0f\u5e03\u5c40\u4e0e\u81ea\u6211\u6269\u5c55\u7684\u63a8\u65ad\u65f6\u95f4\u76f8\u7ed3\u5408\uff0c\u6211\u4eec\u7684\u6846\u67b6\u5728\u4e0e\u63d0\u793a\u7684\u573a\u666f\u5bf9\u9f50\u65b9\u9762\u4f18\u4e8e\u6700\u8fd1\u7684\u6587\u672c\u5230\u56fe\u50cf\u6a21\u578b\u3002\u4ee3\u7801\u5728https://github.com/gcl-inha/ReFocus\u4e0a\u53ef\u7528\u3002", "motivation": "\u5c3d\u7ba1\u73b0\u4ee3\u6587\u672c\u5230\u56fe\u50cf\u6a21\u578b\u5177\u6709\u4ee4\u4eba\u5370\u8c61\u6df1\u523b\u7684\u903c\u771f\u5ea6\uff0c\u4f46\u5b83\u4eec\u4ecd\u7136\u5728\u7ec4\u5408\u6027\u65b9\u9762\u9047\u5230\u56f0\u96be\uff0c\u7ecf\u5e38\u65e0\u6cd5\u51c6\u786e\u6e32\u67d3\u5bf9\u8c61\u8ba1\u6570\u3001\u5c5e\u6027\u548c\u7a7a\u95f4\u5173\u7cfb\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e00\u6311\u6218\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u4e2a\u7ed3\u5408\u4e86\u5bf9\u8c61\u4e2d\u5fc3\u7684\u65b9\u6cd5\u548c\u81ea\u6211\u6539\u8fdb\u7684\u8bad\u7ec3\u65e0\u5173\u6846\u67b6\u3002", "method": "\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b(LLMs)\u4ece\u8f93\u5165\u63d0\u793a\u4e2d\u5408\u6210\u663e\u5f0f\u5e03\u5c40\uff0c\u5e76\u5c06\u8fd9\u4e9b\u5e03\u5c40\u6ce8\u5165\u56fe\u50cf\u751f\u6210\u8fc7\u7a0b\uff0c\u5176\u4e2d\u5bf9\u8c61\u4e2d\u5fc3\u7684\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b(VLM judge)\u5728\u8fed\u4ee3\u8fc7\u7a0b\u4e2d\u91cd\u6392\u591a\u4e2a\u5019\u9009\u8005\u4ee5\u9009\u62e9\u4e0e\u63d0\u793a\u6700\u5bf9\u9f50\u7684\u7ed3\u679c\u3002", "result": "\u901a\u8fc7\u7edf\u4e00\u663e\u5f0f\u5e03\u5c40\u5bf9\u9f50\u548c\u57fa\u4e8e\u81ea\u6211\u6269\u5c55\u7684\u63a8\u65ad\u65f6\u95f4\uff0c\u6211\u4eec\u7684\u6846\u67b6\u5728\u4e0e\u63d0\u793a\u7684\u573a\u666f\u5bf9\u9f50\u65b9\u9762\u4f18\u4e8e\u6700\u8fd1\u7684\u6587\u672c\u5230\u56fe\u50cf\u6a21\u578b\u3002", "conclusion": "\u6211\u4eec\u7684\u6846\u67b6\u5728\u4e0d\u727a\u7272\u7f8e\u5b66\u8d28\u91cf\u7684\u60c5\u51b5\u4e0b\u63d0\u9ad8\u4e86\u5e03\u5c40\u5fe0\u5b9e\u6027\uff0c\u5e76\u4e14\u4f18\u4e8e\u6700\u8fd1\u7684\u6587\u672c\u5230\u56fe\u50cf\u6a21\u578b\u3002"}}
{"id": "2510.24299", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24299", "abs": "https://arxiv.org/abs/2510.24299", "authors": ["Jiayu Liu", "Wei Dai", "Zhenya Huang", "Ning Miao", "Enhong Chen"], "title": "Verifying Large Language Models' Reasoning Paths via Correlation Matrix Rank", "comment": null, "summary": "Despite the strong reasoning ability of large language models~(LLMs), they\nare prone to errors and hallucinations. As a result, how to check their outputs\neffectively and efficiently has become a critical problem in their\napplications. Existing checking methods heavily rely on external resources,\nsuch as trained verifiers (e.g., process/outcome reward models) or elaborate\nprompts, which lead to high computational overhead and are only applicable to\nspecific domains. In this paper, we investigate whether the internal behaviors\nof LLMs have already implied the credibility of their reasoning paths.\nSpecifically, we find that the rank of the correlation matrix between the input\nproblem and the output reasoning path is a robust indicator of reasoning\ncorrectness. Different from other correctness indicators for LLMs, the\ncalculation of the correlation matrix only relies on the LLM itself, which\navoids the hassle of training a separate model or designing complicated\nprompts. Based on it, we design a simple, plug-and-play Self-Indicator method\nto reweight candidate reasoning paths, which achieves significant performance\nimprovements than other voting and verification methods with very few\ncomputational overhead. Our experiments across multiple LLMs of varying scales\nand model families have further shown the effectiveness of Self-Indicator. It\nachieves over 75% accuracy in distinguishing correct reasoning paths from\nincorrect ones, and, in turn, improves the accuracies on three reasoning\nbenchmarks by more than 8%.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u7b80\u5355\u6709\u6548\u7684\u81ea\u6211\u6307\u793a\u5668\u65b9\u6cd5\uff0c\u5229\u7528LLM\u5185\u90e8\u884c\u4e3a\u6765\u8bc4\u4f30\u5176\u63a8\u7406\u8def\u5f84\u7684\u6b63\u786e\u6027\uff0c\u8fd9\u79cd\u65b9\u6cd5\u4ec5\u4f9d\u8d56\u4e8eLLM\u5185\u90e8\u6570\u636e\uff0c\u907f\u514d\u4e86\u989d\u5916\u7684\u8bad\u7ec3\u5f00\u9500\u548c\u590d\u6742\u7684\u63d0\u793a\u8bbe\u8ba1\uff0c\u5e76\u4e14\u5728\u591a\u4e2aLLM\u4e0a\u5b9e\u9a8c\u6709\u6548\uff0c\u63d0\u9ad8\u4e86\u63a8\u7406\u51c6\u786e\u7387\u3002", "motivation": "\u73b0\u6709\u7684\u68c0\u67e5LLM\u8f93\u51fa\u7684\u65b9\u6cd5\u4f9d\u8d56\u4e8e\u989d\u5916\u7684\u5916\u90e8\u8d44\u6e90\uff0c\u5982\u8bad\u7ec3\u597d\u7684\u9a8c\u8bc1\u5668\u6216\u590d\u6742\u7684\u63d0\u793a\uff0c\u8fd9\u5bfc\u81f4\u4e86\u9ad8\u8ba1\u7b97\u5f00\u9500\u4e14\u4ec5\u9002\u7528\u4e8e\u7279\u5b9a\u9886\u57df\u3002\u672c\u6587\u65e8\u5728\u63a2\u7a76\u5982\u4f55\u901a\u8fc7LLM\u5185\u90e8\u884c\u4e3a\u6765\u5224\u65ad\u5176\u63a8\u7406\u8def\u5f84\u7684\u6b63\u786e\u6027\uff0c\u4ece\u800c\u51cf\u5c11\u989d\u5916\u5f00\u9500\u5e76\u63d0\u9ad8\u901a\u7528\u6027\u3002", "method": "\u8be5\u65b9\u6cd5\u57fa\u4e8e\u8f93\u5165\u95ee\u9898\u4e0e\u8f93\u51fa\u63a8\u7406\u8def\u5f84\u4e4b\u95f4\u7684\u76f8\u5173\u77e9\u9635\u8fdb\u884c\u8ba1\u7b97\uff0c\u8fd9\u79cd\u8ba1\u7b97\u4ec5\u4f9d\u8d56\u4e8eLLM\u81ea\u8eab\u7684\u6570\u636e\u3002\u4f5c\u8005\u8bbe\u8ba1\u4e86\u4e00\u79cd\u7b80\u5355\u4e14\u6613\u4e8e\u63d2\u4ef6\u7684\u81ea\u6211\u6307\u793a\u5668\u65b9\u6cd5\uff0c\u7528\u4ee5\u91cd\u65b0\u52a0\u6743\u5019\u9009\u7684\u63a8\u7406\u8def\u5f84\uff0c\u4ece\u800c\u5b9e\u73b0\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\u7684\u540c\u65f6\u4fdd\u6301\u4f4e\u8ba1\u7b97\u5f00\u9500\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u81ea\u6211\u6307\u793a\u5668\u65b9\u6cd5\u80fd\u5c06\u6b63\u786e\u63a8\u7406\u8def\u5f84\u4e0e\u9519\u8bef\u63a8\u7406\u8def\u5f84\u533a\u5206\u7684\u51c6\u786e\u7387\u63d0\u9ad8\u5230\u8d85\u8fc775%\uff0c\u5728\u4e09\u4e2a\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e0a\uff0c\u5176\u51c6\u786e\u5ea6\u63d0\u5347\u8d85\u8fc78%\u3002", "conclusion": "\u63d0\u51fa\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u7684\u81ea\u6211\u6307\u793a\u5668\u65b9\u6cd5\uff0c\u80fd\u591f\u5728\u4e0d\u4f9d\u8d56\u5916\u90e8\u8d44\u6e90\u7684\u60c5\u51b5\u4e0b\u6709\u6548\u8bc4\u4f30LLM\u63a8\u7406\u8def\u5f84\u7684\u6b63\u786e\u6027\uff0c\u8fd9\u79cd\u65b9\u6cd5\u4e0d\u4ec5\u9002\u7528\u4e8e\u5404\u79cd\u89c4\u6a21\u548c\u5bb6\u65cf\u7684LLM\uff0c\u800c\u4e14\u80fd\u591f\u663e\u8457\u63d0\u5347\u63a8\u7406\u51c6\u786e\u5ea6\u3002"}}
{"id": "2510.23668", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.23668", "abs": "https://arxiv.org/abs/2510.23668", "authors": ["Fujiang Yuan", "Yangrui Fan", "Xiaohuan Bing", "Zhen Tian", "Chunhong Yuan", "Yankang Li"], "title": "Traffic flow forecasting, STL decomposition, Hybrid model, LSTM, ARIMA, XGBoost, Intelligent transportation systems", "comment": null, "summary": "Accurate traffic flow forecasting is essential for intelligent transportation\nsystems and urban traffic management. However, single model approaches often\nfail to capture the complex, nonlinear, and multi scale temporal patterns in\ntraffic flow data. This study proposes a decomposition driven hybrid framework\nthat integrates Seasonal Trend decomposition using Loess (STL) with three\ncomplementary predictive models. STL first decomposes the original time series\ninto trend, seasonal, and residual components. Then, a Long Short Term Memory\n(LSTM) network models long term trends, an Autoregressive Integrated Moving\nAverage (ARIMA) model captures seasonal periodicity, and an Extreme Gradient\nBoosting (XGBoost) algorithm predicts nonlinear residual fluctuations. The\nfinal forecast is obtained through multiplicative integration of the sub model\npredictions. Using 998 traffic flow records from a New York City intersection\nbetween November and December 2015, results show that the LSTM ARIMA XGBoost\nhybrid model significantly outperforms standalone models including LSTM, ARIMA,\nand XGBoost across MAE, RMSE, and R squared metrics. The decomposition strategy\neffectively isolates temporal characteristics, allowing each model to\nspecialize, thereby improving prediction accuracy, interpretability, and\nrobustness.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5206\u89e3\u7684\u6df7\u5408\u6846\u67b6\uff0c\u7528\u4e8e\u4ea4\u901a\u6d41\u91cf\u9884\u6d4b\uff0c\u8be5\u6846\u67b6\u7ed3\u5408\u4e86STL\u5206\u89e3\u548cLSTM\u3001ARIMA\u3001XGBoost\u4e09\u79cd\u6a21\u578b\uff0c\u5206\u522b\u7528\u4e8e\u957f\u65f6\u8d8b\u52bf\u3001\u5b63\u8282\u5468\u671f\u6027\u548c\u975e\u7ebf\u6027\u6b8b\u5dee\u6ce2\u52a8\u7684\u9884\u6d4b\uff0c\u63d0\u9ad8\u4e86\u9884\u6d4b\u7cbe\u5ea6\u548c\u9c81\u68d2\u6027", "motivation": "\u5355\u4e00\u6a21\u578b\u65b9\u6cd5\u96be\u4ee5\u6355\u6349\u4ea4\u901a\u6d41\u91cf\u6570\u636e\u7684\u590d\u6742\u975e\u7ebf\u6027\u548c\u8de8\u5c3a\u5ea6\u65f6\u95f4\u6a21\u5f0f\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u65b0\u7684\u65b9\u6cd5\u6765\u63d0\u9ad8\u4ea4\u901a\u6d41\u91cf\u9884\u6d4b\u7684\u51c6\u786e\u6027", "method": "\u91c7\u7528STL\u5206\u89e3\u5c06\u539f\u59cb\u65f6\u95f4\u5e8f\u5217\u5206\u89e3\u4e3a\u8d8b\u52bf\u3001\u5b63\u8282\u6027\u548c\u6b8b\u5dee\u6210\u5206\uff0c\u7136\u540e\u5206\u522b\u4f7f\u7528LSTM\u3001ARIMA\u548cXGBoost\u6a21\u578b\u8fdb\u884c\u9884\u6d4b\uff0c\u5e76\u5c06\u5404\u5b50\u6a21\u578b\u9884\u6d4b\u7ed3\u679c\u901a\u8fc7\u4e58\u6cd5\u96c6\u6210\u5f97\u5230\u6700\u7ec8\u9884\u6d4b\u7ed3\u679c", "result": "\u57fa\u4e8e\u7ebd\u7ea6\u5e02\u4e00\u4e2a\u4ea4\u53c9\u53e32015\u5e7411\u6708\u81f312\u6708\u7684998\u6761\u4ea4\u901a\u6d41\u91cf\u8bb0\u5f55\uff0c\u7ed3\u679c\u663e\u793aLSTM-ARIMA-XGBoost\u6df7\u5408\u6a21\u578b\u5728MAE\u3001RMSE\u548cR\u00b2\u7b49\u6307\u6807\u4e0a\u660e\u663e\u4f18\u4e8e\u5355\u72ec\u7684LSTM\u3001ARIMA\u548cXGBoost\u6a21\u578b", "conclusion": "\u5206\u89e3\u7b56\u7565\u6709\u6548\u9694\u79bb\u4e86\u65f6\u95f4\u7279\u5f81\uff0c\u4f7f\u5f97\u6bcf\u79cd\u6a21\u578b\u53ef\u4ee5\u4e13\u4e1a\u5316\uff0c\u4ece\u800c\u63d0\u9ad8\u4e86\u9884\u6d4b\u51c6\u786e\u6027\u3001\u53ef\u89e3\u91ca\u6027\u548c\u9c81\u68d2\u6027"}}
{"id": "2510.24134", "categories": ["cs.CV", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.24134", "abs": "https://arxiv.org/abs/2510.24134", "authors": ["Yang Du", "Zhuoran Lin", "Kaiqiang Song", "Biao Wang", "Zhicheng Zheng", "Tiezheng Ge", "Bo Zheng", "Qin Jin"], "title": "VC4VG: Optimizing Video Captions for Text-to-Video Generation", "comment": "Accepted by EMNLP 2025", "summary": "Recent advances in text-to-video (T2V) generation highlight the critical role\nof high-quality video-text pairs in training models capable of producing\ncoherent and instruction-aligned videos. However, strategies for optimizing\nvideo captions specifically for T2V training remain underexplored. In this\npaper, we introduce VC4VG (Video Captioning for Video Generation), a\ncomprehensive caption optimization framework tailored to the needs of T2V\nmodels.We begin by analyzing caption content from a T2V perspective,\ndecomposing the essential elements required for video reconstruction into\nmultiple dimensions, and proposing a principled caption design methodology. To\nsupport evaluation, we construct VC4VG-Bench, a new benchmark featuring\nfine-grained, multi-dimensional, and necessity-graded metrics aligned with\nT2V-specific requirements.Extensive T2V fine-tuning experiments demonstrate a\nstrong correlation between improved caption quality and video generation\nperformance, validating the effectiveness of our approach. We release all\nbenchmark tools and code at https://github.com/qyr0403/VC4VG to support further\nresearch.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86VC4VG\uff08\u89c6\u9891\u63cf\u8ff0\u4f18\u5316\u6846\u67b6\uff09\uff0c\u8be5\u6846\u67b6\u901a\u8fc7\u5206\u6790\u89c6\u9891\u63cf\u8ff0\u5e76\u8bbe\u8ba1\u4e00\u5957\u65b9\u6cd5\u6765\u4f18\u5316\u89c6\u9891\u751f\u6210\u6a21\u578b\u6240\u9700\u7684\u6587\u5b57\u63cf\u8ff0\uff0c\u4ece\u800c\u63d0\u5347\u89c6\u9891\u751f\u6210\u8d28\u91cf\u3002\u4e3a\u4e86\u9a8c\u8bc1\u6b64\u6846\u67b6\u7684\u6709\u6548\u6027\uff0c\u4f5c\u8005\u6784\u5efa\u4e86\u65b0\u7684\u57fa\u51c6VC4VG-Bench\uff0c\u5e76\u901a\u8fc7\u5927\u91cf\u5b9e\u9a8c\u5c55\u793a\u6539\u8fdb\u7684\u89c6\u9891\u63cf\u8ff0\u8d28\u91cf\u53ef\u4ee5\u63d0\u9ad8\u89c6\u9891\u751f\u6210\u7684\u8868\u73b0\u3002\u6240\u6709\u57fa\u51c6\u5de5\u5177\u548c\u4ee3\u7801\u90fd\u53ef\u5728GitHub\u4e0a\u83b7\u53d6\u3002", "motivation": "\u8fd1\u5e74\u6765\uff0c\u6587\u672c\u5230\u89c6\u9891\u751f\u6210\uff08T2V\uff09\u9886\u57df\u53d1\u5c55\u8fc5\u901f\uff0c\u9ad8\u8d28\u91cf\u7684\u89c6\u9891\u6587\u672c\u5bf9\u6a21\u578b\u751f\u6210\u4e00\u81f4\u6027\u548c\u6307\u4ee4\u7b26\u5408\u5ea6\u9ad8\u7684\u89c6\u9891\u81f3\u5173\u91cd\u8981\u3002\u7136\u800c\uff0c\u4e13\u95e8\u9488\u5bf9T2V\u8bad\u7ec3\u4f18\u5316\u89c6\u9891\u63cf\u8ff0\u7684\u65b9\u6cd5\u5c1a\u5904\u4e8e\u63a2\u7d22\u9636\u6bb5\u3002\u4e3a\u6b64\uff0c\u672c\u6587\u63d0\u51fa\u4e86VC4VG\uff0c\u65e8\u5728\u901a\u8fc7\u4f18\u5316\u89c6\u9891\u63cf\u8ff0\u6765\u63d0\u5347\u89c6\u9891\u751f\u6210\u8d28\u91cf\u3002", "method": "\u672c\u6587\u9996\u5148\u4eceT2V\u7684\u89d2\u5ea6\u5206\u6790\u4e86\u73b0\u6709\u89c6\u9891\u6587\u672c\u7684\u4e0d\u8db3\uff0c\u5c06\u7528\u4e8e\u89c6\u9891\u91cd\u5efa\u6240\u9700\u7684\u57fa\u672c\u8981\u7d20\u5206\u89e3\u4e3a\u591a\u4e2a\u7ef4\u5ea6\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u5957\u7cfb\u7edf\u7684\u63cf\u8ff0\u8bbe\u8ba1\u65b9\u6848\u3002\u4e3a\u4e86\u652f\u6301\u8bc4\u4f30\uff0c\u4f5c\u8005\u6784\u5efa\u4e86\u65b0\u7684\u57fa\u51c6VC4VG-Bench\uff0c\u8be5\u57fa\u51c6\u5305\u62ec\u7ec6\u7c92\u5ea6\u7684\u3001\u591a\u7ef4\u5ea6\u7684\u3001\u4e0eT2V\u7279\u5b9a\u9700\u6c42\u76f8\u914d\u7684\u5ea6\u91cf\u6807\u51c6\u3002", "result": "\u5927\u91cf\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u6539\u8fdb\u540e\u7684\u89c6\u9891\u63cf\u8ff0\u8d28\u91cf\u4e0e\u89c6\u9891\u751f\u6210\u8868\u73b0\u4e4b\u95f4\u5b58\u5728\u663e\u8457\u5173\u8054\uff0c\u9a8c\u8bc1\u4e86VC4VG\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684VC4VG\u6846\u67b6\u63d0\u4f9b\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u65b9\u6cd5\u6765\u4f18\u5316\u89c6\u9891\u63cf\u8ff0\uff0c\u4ee5\u652f\u6301\u66f4\u9ad8\u54c1\u8d28\u7684\u89c6\u9891\u751f\u6210\u3002\u5b9e\u9a8c\u7ed3\u679c\u9a8c\u8bc1\u4e86\u8fd9\u79cd\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002\u4e3a\u8fdb\u4e00\u6b65\u7684\u7814\u7a76\uff0c\u6240\u6709\u57fa\u51c6\u5de5\u5177\u548c\u4ee3\u7801\u90fd\u5df2\u516c\u5f00\u3002"}}
{"id": "2510.24337", "categories": ["cs.AI", "cs.SI"], "pdf": "https://arxiv.org/pdf/2510.24337", "abs": "https://arxiv.org/abs/2510.24337", "authors": ["Daria Kravets-Meinke", "Hannah Schmid-Petri", "Sonja Niemann", "Ute Schmid"], "title": "Generative Large Language Models (gLLMs) in Content Analysis: A Practical Guide for Communication Research", "comment": null, "summary": "Generative Large Language Models (gLLMs), such as ChatGPT, are increasingly\nbeing used in communication research for content analysis. Studies show that\ngLLMs can outperform both crowd workers and trained coders, such as research\nassistants, on various coding tasks relevant to communication science, often at\na fraction of the time and cost. Additionally, gLLMs can decode implicit\nmeanings and contextual information, be instructed using natural language,\ndeployed with only basic programming skills, and require little to no annotated\ndata beyond a validation dataset - constituting a paradigm shift in automated\ncontent analysis. Despite their potential, the integration of gLLMs into the\nmethodological toolkit of communication research remains underdeveloped. In\ngLLM-assisted quantitative content analysis, researchers must address at least\nseven critical challenges that impact result quality: (1) codebook development,\n(2) prompt engineering, (3) model selection, (4) parameter tuning, (5)\niterative refinement, (6) validation of the model's reliability, and\noptionally, (7) performance enhancement. This paper synthesizes emerging\nresearch on gLLM-assisted quantitative content analysis and proposes a\ncomprehensive best-practice guide to navigate these challenges. Our goal is to\nmake gLLM-based content analysis more accessible to a broader range of\ncommunication researchers and ensure adherence to established disciplinary\nquality standards of validity, reliability, reproducibility, and research\nethics.", "AI": {"tldr": "gLLMs \u5982 ChatGPT \u5728\u4f20\u64ad\u7814\u7a76\u4e2d\u7684\u5185\u5bb9\u5206\u6790\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5bf9\u5176\u6709\u6548\u96c6\u6210\u4ecd\u9700\u89e3\u51b3\u591a\u4e2a\u5173\u952e\u6311\u6218\u3002\u672c\u6587\u603b\u7ed3\u4e86 gLLM \u8f85\u52a9\u7684\u5b9a\u91cf\u5185\u5bb9\u5206\u6790\u7684\u65b0\u7814\u7a76\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u5957\u5168\u9762\u7684\u6700\u4f73\u5b9e\u8df5\u6307\u5357\uff0c\u65e8\u5728\u63d0\u5347 gLLM \u57fa\u4e8e\u5185\u5bb9\u5206\u6790\u7684\u53ef\u8fbe\u6027\u548c\u4fdd\u8bc1\u7814\u7a76\u8d28\u91cf\u6807\u51c6\u7684\u9075\u5b88\u3002", "motivation": "\u7814\u7a76\u53d1\u73b0 gLLMs \u5728\u591a\u79cd\u7f16\u7801\u4efb\u52a1\u4e2d\u4f18\u4e8e\u4f17\u5305\u5de5\u4eba\u548c\u8bad\u7ec3\u540e\u7684\u7f16\u7801\u8005\u3002\u5c3d\u7ba1\u6709\u6f5c\u529b\uff0c\u4f46 gLLM \u5728\u4f20\u64ad\u7814\u7a76\u4e2d\u7684\u96c6\u6210\u5c1a\u672a\u5b8c\u5168\u53d1\u5c55\u3002\u672c\u6587\u65e8\u5728\u89e3\u51b3\u5176\u6240\u9762\u4e34\u7684\u5173\u952e\u6311\u6218\uff0c\u4fc3\u8fdb gLLM \u5728\u5b9a\u91cf\u5185\u5bb9\u5206\u6790\u4e2d\u7684\u5e94\u7528\uff0c\u5e76\u786e\u4fdd\u7814\u7a76\u9075\u5b88\u5b66\u672f\u6807\u51c6\u3002", "method": "\u672c\u6587\u901a\u8fc7\u603b\u7ed3\u65b0\u5174\u7814\u7a76\uff0c\u63d0\u51fa\u4e86\u4e00\u4e2a\u5e94\u5bf9\u4e03\u4e2a\u5173\u952e\u6311\u6218\u7684\u7efc\u5408\u6700\u4f73\u5b9e\u8df5\u6307\u5357\uff0c\u8fd9\u4e9b\u6311\u6218\u5305\u62ec\u7f16\u7801\u672c\u5f00\u53d1\u3001\u63d0\u793a\u5de5\u7a0b\u3001\u6a21\u578b\u9009\u62e9\u3001\u53c2\u6570\u8c03\u6574\u3001\u8fed\u4ee3\u4f18\u5316\u3001\u6a21\u578b\u53ef\u9760\u6027\u7684\u9a8c\u8bc1\uff0c\u4ee5\u53ca\u53ef\u9009\u7684\u6027\u80fd\u589e\u5f3a\u3002", "result": "\u6587\u7ae0\u63d0\u51fa\u4e86\u4e00\u4e2a\u8be6\u5c3d\u7684\u65b9\u6cd5\u6765\u6307\u5bfc\u4f20\u64ad\u7814\u7a76\u4eba\u5458\u5728\u5b9e\u8df5\u4e2d\u4f7f\u7528 gLLM \u8fdb\u884c\u5185\u5bb9\u5206\u6790\uff0c\u8be5\u65b9\u6cd5\u8003\u8651\u4e86\u73b0\u6709\u7684\u548c\u672a\u6765\u7684\u6311\u6218\uff0c\u540c\u65f6\u4e5f\u786e\u4fdd\u4e86\u7814\u7a76\u7684\u53ef\u9760\u6027\u548c\u53ef\u91cd\u590d\u6027\u6807\u51c6\u7684\u9075\u5b88\u3002", "conclusion": "\u901a\u8fc7\u5b9e\u65bd\u6240\u63d0\u51fa\u7684\u6700\u4f73\u5b9e\u8df5\uff0c\u7814\u7a76\u4eba\u5458\u53ef\u4ee5\u66f4\u597d\u5730\u5229\u7528 gLLM \u7684\u4f18\u52bf\uff0c\u540c\u65f6\u786e\u4fdd\u5176\u5185\u5bb9\u5206\u6790\u7b26\u5408\u4f20\u64ad\u7814\u7a76\u7684\u9ad8\u8d28\u91cf\u6807\u51c6\u8981\u6c42\u3002"}}
{"id": "2510.23672", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.23672", "abs": "https://arxiv.org/abs/2510.23672", "authors": ["Xiangfei Qiu", "Xingjian Wu", "Hanyin Cheng", "Xvyuan Liu", "Chenjuan Guo", "Jilin Hu", "Bin Yang"], "title": "DBLoss: Decomposition-based Loss Function for Time Series Forecasting", "comment": "Accepted by NeurIPS 2025", "summary": "Time series forecasting holds significant value in various domains such as\neconomics, traffic, energy, and AIOps, as accurate predictions facilitate\ninformed decision-making. However, the existing Mean Squared Error (MSE) loss\nfunction sometimes fails to accurately capture the seasonality or trend within\nthe forecasting horizon, even when decomposition modules are used in the\nforward propagation to model the trend and seasonality separately. To address\nthese challenges, we propose a simple yet effective Decomposition-Based Loss\nfunction called DBLoss. This method uses exponential moving averages to\ndecompose the time series into seasonal and trend components within the\nforecasting horizon, and then calculates the loss for each of these components\nseparately, followed by weighting them. As a general loss function, DBLoss can\nbe combined with any deep learning forecasting model. Extensive experiments\ndemonstrate that DBLoss significantly improves the performance of\nstate-of-the-art models across diverse real-world datasets and provides a new\nperspective on the design of time series loss functions.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u635f\u5931\u51fd\u6570DBLoss\uff0c\u901a\u8fc7\u5206\u89e3\u65f6\u95f4\u5e8f\u5217\u4e2d\u7684\u5b63\u8282\u6027\u548c\u8d8b\u52bf\u6765\u6539\u8fdb\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u51c6\u786e\u6027\uff0c\u5e76\u5728\u591a\u79cd\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u5747\u65b9\u8bef\u5dee\uff08MSE\uff09\u7684\u635f\u5931\u51fd\u6570\u5728\u9884\u6d4b\u5b63\u8282\u6027\u548c\u8d8b\u52bf\u65f6\u5b58\u5728\u4e0d\u8db3\uff0c\u5373\u4f7f\u901a\u8fc7\u6b63\u5411\u4f20\u64ad\u4e2d\u7684\u5206\u89e3\u6a21\u5757\u5355\u72ec\u5efa\u6a21\u8fd9\u4e9b\u56e0\u7d20\u4e5f\u65e0\u6cd5\u5b8c\u5168\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002\u56e0\u6b64\uff0c\u63d0\u51fa\u4e00\u79cd\u80fd\u591f\u66f4\u51c6\u786e\u5730\u6355\u6349\u65f6\u95f4\u5e8f\u5217\u4e2d\u5b63\u8282\u6027\u548c\u8d8b\u52bf\u7684\u635f\u5931\u51fd\u6570\u662f\u5fc5\u8981\u7684\u3002", "method": "DBLoss\u4f7f\u7528\u6307\u6570\u79fb\u52a8\u5e73\u5747\u503c\u5c06\u65f6\u95f4\u5e8f\u5217\u5206\u89e3\u4e3a\u5b63\u8282\u6027\u548c\u8d8b\u52bf\u5206\u91cf\uff0c\u5728\u9884\u6d4b\u8303\u56f4\u5185\u5206\u522b\u8ba1\u7b97\u6bcf\u4e2a\u5206\u91cf\u7684\u635f\u5931\uff0c\u5e76\u5c06\u8fd9\u4e9b\u635f\u5931\u52a0\u6743\u3002\u4f5c\u4e3a\u4e00\u79cd\u901a\u7528\u7684\u635f\u5931\u51fd\u6570\uff0cDBLoss\u53ef\u4ee5\u4e0e\u4efb\u4f55\u6df1\u5ea6\u5b66\u4e60\u9884\u6d4b\u6a21\u578b\u7ed3\u5408\u4f7f\u7528\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cDBLoss\u663e\u8457\u6539\u5584\u4e86\u6700\u5148\u8fdb\u7684\u6a21\u578b\u5728\u6574\u4e2a\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u7684\u6027\u80fd\uff0c\u5c55\u793a\u4e86\u5728\u65f6\u95f4\u5e8f\u5217\u635f\u5931\u51fd\u6570\u8bbe\u8ba1\u4e0a\u7684\u65b0\u89c6\u89d2\u3002", "conclusion": "\u901a\u8fc7\u4f7f\u7528DBLoss\u635f\u5931\u51fd\u6570\uff0c\u53ef\u4ee5\u6709\u6548\u5730\u63d0\u9ad8\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u6a21\u578b\u7684\u6027\u80fd\uff0c\u7279\u522b\u662f\u5bf9\u4e8e\u6355\u6349\u65f6\u95f4\u5e8f\u5217\u4e2d\u7684\u5b63\u8282\u6027\u548c\u8d8b\u52bf\u5177\u6709\u663e\u8457\u4f18\u52bf\u3002"}}
{"id": "2510.24152", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24152", "abs": "https://arxiv.org/abs/2510.24152", "authors": ["Aodi Wu", "Xubo Luo"], "title": "Enhancing Vision-Language Models for Autonomous Driving through Task-Specific Prompting and Spatial Reasoning", "comment": "RoboSense Challenge with IROS 2025", "summary": "This technical report presents our solution for the RoboSense Challenge at\nIROS 2025, which evaluates Vision-Language Models (VLMs) on autonomous driving\nscene understanding across perception, prediction, planning, and corruption\ndetection tasks. We propose a systematic framework built on four core\ncomponents. First, a Mixture-of-Prompts router classifies questions and\ndispatches them to task-specific expert prompts, eliminating interference\nacross diverse question types. Second, task-specific prompts embed explicit\ncoordinate systems, spatial reasoning rules, role-playing,\nChain-of-Thought/Tree-of-Thought reasoning, and few-shot examples tailored to\neach task. Third, a visual assembly module composes multi-view images with\nobject crops, magenta markers, and adaptive historical frames based on question\nrequirements. Fourth, we configure model inference parameters (temperature,\ntop-p, message roles) per task to optimize output quality. Implemented on\nQwen2.5-VL-72B, our approach achieves 70.87% average accuracy on Phase-1 (clean\ndata) and 72.85% on Phase-2 (corrupted data), demonstrating that structured\nprompting and spatial grounding substantially enhance VLM performance on\nsafety-critical autonomous driving tasks. Code and prompt are available at\nhttps://github.com/wuaodi/UCAS-CSU-phase2.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7cfb\u7edf\u6846\u67b6\uff0c\u7528\u4e8e\u8bc4\u4f30\u81ea\u4e3b\u9a7e\u9a76\u573a\u666f\u7406\u89e3\u4e2d\u7684Vision-Language\u6a21\u578b\uff0c\u8be5\u6846\u67b6\u5305\u542b\u56db\u4e2a\u6838\u5fc3\u7ec4\u4ef6\uff1a\u4efb\u52a1\u5206\u7c7b\u4e0e\u8c03\u5ea6\u3001\u4efb\u52a1\u7279\u5b9a\u63d0\u793a\u3001\u89c6\u89c9\u7ec4\u88c5\u6a21\u5757\u548c\u6a21\u578b\u63a8\u7406\u53c2\u6570\u914d\u7f6e\uff0c\u5b9e\u73b0\u4e8670.87%\u7684\u5e73\u5747\u51c6\u786e\u7387\uff08\u5e72\u51c0\u6570\u636e\uff09\u548c72.85%\u7684\u51c6\u786e\u7387\uff08\u635f\u574f\u6570\u636e\uff09\uff0c\u8bc1\u660e\u4e86\u7ed3\u6784\u5316\u63d0\u793a\u548c\u7a7a\u95f4\u5b9a\u4f4d\u5bf9\u5b89\u5168\u5173\u952e\u4efb\u52a1\u4e2dVLM\u8868\u73b0\u7684\u663e\u8457\u63d0\u5347\u3002\u4ee3\u7801\u548c\u63d0\u793a\u53ef\u5728GitHub\u4e0a\u83b7\u53d6\u3002", "motivation": "\u4e3a\u4e86\u8bc4\u4f30Vision-Language\u6a21\u578b\u5728\u81ea\u4e3b\u9a7e\u9a76\u573a\u666f\u7406\u89e3\u4e2d\u7684\u6027\u80fd\uff0c\u7279\u522b\u662f\u5728\u611f\u77e5\u3001\u9884\u6d4b\u3001\u89c4\u5212\u548c\u5e72\u6270\u68c0\u6d4b\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u7cfb\u7edf\u5316\u7684\u89e3\u51b3\u65b9\u6848\u6846\u67b6\u3002\u901a\u8fc7\u91c7\u7528\u7ed3\u6784\u5316\u63d0\u793a\u548c\u7a7a\u95f4\u5b9a\u4f4d\u65b9\u6cd5\uff0c\u65e8\u5728\u63d0\u9ad8\u6a21\u578b\u5728\u5b89\u5168\u5173\u952e\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5305\u542b\u56db\u4e2a\u6838\u5fc3\u7ec4\u4ef6\u7684\u65b9\u6cd5\uff0c\u5176\u4e2d\u5305\u62ec\uff1a\u4efb\u52a1\u5206\u7c7b\u4e0e\u8c03\u5ea6\u3001\u4efb\u52a1\u7279\u5b9a\u63d0\u793a\u3001\u89c6\u89c9\u7ec4\u88c5\u6a21\u5757\uff0c\u4ee5\u53ca\u57fa\u4e8e\u4efb\u52a1\u7684\u6a21\u578b\u63a8\u7406\u53c2\u6570\u914d\u7f6e\u3002\u6846\u67b6\u5b9e\u65bd\u5728Qwen2.5-VL-72B\u6a21\u578b\u4e0a\uff0c\u901a\u8fc7\u8fd9\u4e9b\u7ec4\u4ef6\u7684\u8bbe\u8ba1\uff0c\u5b9e\u73b0\u4e86\u5bf9\u5404\u79cd\u573a\u666f\u7684\u7406\u89e3\u548c\u5904\u7406\u3002", "result": "\u5b9e\u65bd\u7684\u65b9\u6cd5\u5728\u5e72\u51c0\u6570\u636e\u96c6\u4e0a\u8fbe\u5230\u4e8670.87\uff05\u7684\u5e73\u5747\u51c6\u786e\u7387\uff0c\u800c\u5728\u635f\u574f\u6570\u636e\u96c6\u4e0a\u8fbe\u5230\u4e8672.85\uff05\u7684\u51c6\u786e\u7387\uff0c\u5c55\u793a\u4e86\u7ed3\u6784\u5316\u63d0\u793a\u548c\u7a7a\u95f4\u5b9a\u4f4d\u5728\u5b89\u5168\u5173\u952e\u4efb\u52a1\u4e2d\u63d0\u5347VLM\u6027\u80fd\u7684\u91cd\u8981\u4f5c\u7528\u3002", "conclusion": "\u6240\u63d0\u7684\u6846\u67b6\u53ca\u65b9\u6cd5\u8bc1\u660e\u4e86\u7ed3\u6784\u5316\u63d0\u793a\u548c\u7a7a\u95f4\u5b9a\u4f4d\u663e\u8457\u63d0\u9ad8\u4e86VLM\u5728\u5b89\u5168\u5173\u952e\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\uff0c\u7279\u522b\u662f\u5728\u53d7\u635f\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u7a81\u51fa\uff0c\u8868\u73b0\u51fa\u8f83\u9ad8\u7684\u9c81\u68d2\u6027\u548c\u51c6\u786e\u6027\u3002\u8fd9\u79cd\u65b9\u6cd5\u4e3a\u81ea\u4e3b\u9a7e\u9a76\u573a\u666f\u7406\u89e3\u4efb\u52a1\u63d0\u4f9b\u4e86\u65b0\u7684\u7814\u7a76\u65b9\u5411\u3002"}}
{"id": "2510.24339", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24339", "abs": "https://arxiv.org/abs/2510.24339", "authors": ["Yunxuan Jiang", "Silan Hu", "Xiaoning Wang", "Yuanyuan Zhang", "Xiangyu Chang"], "title": "VDSAgents: A PCS-Guided Multi-Agent System for Veridical Data Science Automation", "comment": "29 pages, 6 figures. Yunxuan Jiang and Silan Hu contributed equally.\n  Code available at https://github.com/fengzer/VDSAgents", "summary": "Large language models (LLMs) become increasingly integrated into data science\nworkflows for automated system design. However, these LLM-driven data science\nsystems rely solely on the internal reasoning of LLMs, lacking guidance from\nscientific and theoretical principles. This limits their trustworthiness and\nrobustness, especially when dealing with noisy and complex real-world datasets.\nThis paper provides VDSAgents, a multi-agent system grounded in the\nPredictability-Computability-Stability (PCS) principles proposed in the\nVeridical Data Science (VDS) framework. Guided by PCS principles, the system\nimplements a modular workflow for data cleaning, feature engineering, modeling,\nand evaluation. Each phase is handled by an elegant agent, incorporating\nperturbation analysis, unit testing, and model validation to ensure both\nfunctionality and scientific auditability. We evaluate VDSAgents on nine\ndatasets with diverse characteristics, comparing it with state-of-the-art\nend-to-end data science systems, such as AutoKaggle and DataInterpreter, using\nDeepSeek-V3 and GPT-4o as backends. VDSAgents consistently outperforms the\nresults of AutoKaggle and DataInterpreter, which validates the feasibility of\nembedding PCS principles into LLM-driven data science automation.", "AI": {"tldr": "LLMs are widely used in data science workflows but lack scientific guidance. VDSAgents, a multi-agent system based on Predictability-Computability-Stability (PCS) principles, integrates modular workflow for data science tasks with enhanced trustworthiness and robustness, showing superior performance compared to other automated systems.", "motivation": "The paper aims to address the limitation of LLM-driven data science systems by grounding them in scientific and theoretical principles, enhancing trustworthiness and robustness.", "method": "The method introduces VDSAgents, a multi-agent system that follows PCS principles and implements a modular workflow for data science tasks with built-in validation mechanisms.", "result": "VDSAgents outperforms state-of-the-art data science automation tools in various datasets, proving the effectiveness of its methodology.", "conclusion": "Embedding PCS principles into LLM-driven data science systems is feasible and improves the trustworthiness and robustness of the systems."}}
{"id": "2510.23681", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.23681", "abs": "https://arxiv.org/abs/2510.23681", "authors": ["Carl Hvarfner", "David Eriksson", "Eytan Bakshy", "Max Balandat"], "title": "Informed Initialization for Bayesian Optimization and Active Learning", "comment": "28 pages", "summary": "Bayesian Optimization is a widely used method for optimizing expensive\nblack-box functions, relying on probabilistic surrogate models such as Gaussian\nProcesses. The quality of the surrogate model is crucial for good optimization\nperformance, especially in the few-shot setting where only a small number of\nbatches of points can be evaluated. In this setting, the initialization plays a\ncritical role in shaping the surrogate's predictive quality and guiding\nsubsequent optimization. Despite this, practitioners typically rely on\n(quasi-)random designs to cover the input space. However, such approaches\nneglect two key factors: (a) space-filling designs may not be desirable to\nreduce predictive uncertainty, and (b) efficient hyperparameter learning during\ninitialization is essential for high-quality prediction, which may conflict\nwith space-filling designs. To address these limitations, we propose\nHyperparameter-Informed Predictive Exploration (HIPE), a novel acquisition\nstrategy that balances predictive uncertainty reduction with hyperparameter\nlearning using information-theoretic principles. We derive a closed-form\nexpression for HIPE in the Gaussian Process setting and demonstrate its\neffectiveness through extensive experiments in active learning and few-shot BO.\nOur results show that HIPE outperforms standard initialization strategies in\nterms of predictive accuracy, hyperparameter identification, and subsequent\noptimization performance, particularly in large-batch, few-shot settings\nrelevant to many real-world Bayesian Optimization applications.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5148\u9a8c\u77e5\u8bc6\u5f15\u5bfc\u7684\u9884\u6d4b\u63a2\u7d22(HIPE)\u7b56\u7565\uff0c\u6539\u8fdb\u4e86\u8d1d\u53f6\u65af\u4f18\u5316\u4e2d\u521d\u59cb\u5316\u9636\u6bb5\u7684\u6027\u80fd\uff0c\u7279\u522b\u662f\u5728\u5927\u6279\u6b21\u3001\u5c11\u91cf\u6837\u672c\u7684\u60c5\u51b5\u4e0b\uff0c\u80fd\u591f\u63d0\u4f9b\u66f4\u9ad8\u7684\u9884\u6d4b\u51c6\u786e\u6027\u3001\u66f4\u597d\u7684\u8d85\u53c2\u6570\u8bc6\u522b\uff0c\u8fdb\u800c\u63d0\u5347\u540e\u7eed\u4f18\u5316\u7684\u8868\u73b0\u3002", "motivation": "\u5f53\u524d\u7684\u8d1d\u53f6\u65af\u4f18\u5316\u65b9\u6cd5\u591a\u91c7\u7528\u7a7a\u95f4\u586b\u5145\u8bbe\u8ba1\u521d\u59cb\u5316\uff0c\u4f46\u8fd9\u79cd\u65b9\u6cd5\u5ffd\u89c6\u4e86\u4e24\u65b9\u9762\uff1a\u51cf\u5c11\u9884\u6d4b\u4e0d\u786e\u5b9a\u6027\u4e0e\u9ad8\u6548\u5b66\u4e60\u8d85\u53c2\u6570\u4e4b\u95f4\u7684\u51b2\u7a81\uff1b\u4ee5\u53ca\u7a7a\u95f4\u586b\u5145\u8bbe\u8ba1\u53ef\u80fd\u4e0d\u9002\u7528\u4e8e\u6240\u6709\u573a\u666f\u3002\u56e0\u6b64\uff0c\u672c\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u9650\u5236\uff0c\u63d0\u51fa\u4e00\u79cd\u65b0\u7684\u521d\u59cb\u5316\u7b56\u7565\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u83b7\u53d6\u7b56\u7565\uff0c\u79f0\u4e3aHyperparameter-Informed Predictive Exploration (HIPE)\uff0c\u8be5\u7b56\u7565\u5229\u7528\u4fe1\u606f\u8bba\u539f\u7406\u6765\u5e73\u8861\u51cf\u5c11\u9884\u6d4b\u4e0d\u786e\u5b9a\u6027\u4e0e\u8d85\u53c2\u6570\u5b66\u4e60\uff0c\u4ece\u800c\u4f18\u5316\u9ad8\u6210\u672c\u9ed1\u7bb1\u51fd\u6570\u3002\u6b64\u5916\uff0c\u8fd8\u63d0\u4f9b\u4e86Gaussian Process\u8bbe\u7f6e\u4e0b\u7684HIPE\u7684\u95ed\u5f0f\u8868\u8fbe\u5f0f\u3002", "result": "\u901a\u8fc7\u5e7f\u6cdb\u7684\u5b9e\u9a8c\uff0c\u8bc1\u660e\u4e86HIPE\u5728\u4e3b\u52a8\u5b66\u4e60\u548c\u5c11\u91cf\u6837\u672c\u8d1d\u53f6\u65af\u4f18\u5316\u4e2d\u7684\u6709\u6548\u6027\uff0c\u7279\u522b\u662f\u5728\u5927\u6279\u6b21\u3001\u5c11\u91cf\u6837\u672c\u7684\u573a\u666f\u4e0b\uff0c\u8be5\u65b9\u6cd5\u8868\u73b0\u660e\u663e\u4f18\u4e8e\u4f20\u7edf\u7684\u521d\u59cb\u5316\u7b56\u7565\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u7684\u65b0\u7b56\u7565HIPE\uff0c\u901a\u8fc7\u5e73\u8861\u51cf\u5c11\u9884\u6d4b\u4e0d\u786e\u5b9a\u6027\u4e0e\u5b66\u4e60\u8d85\u53c2\u6570\u4e4b\u95f4\u7684\u5173\u7cfb\uff0c\u5728\u5e7f\u6cdb\u7684\u5b9e\u9a8c\u9a8c\u8bc1\u4e2d\u663e\u793a\u51fa\u4e86\u5353\u8d8a\u7684\u6027\u80fd\u3002"}}
{"id": "2510.24195", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.24195", "abs": "https://arxiv.org/abs/2510.24195", "authors": ["Ziqi Zhou", "Yifan Hu", "Yufei Song", "Zijing Li", "Shengshan Hu", "Leo Yu Zhang", "Dezhong Yao", "Long Zheng", "Hai Jin"], "title": "Vanish into Thin Air: Cross-prompt Universal Adversarial Attacks for SAM2", "comment": "Accepted by NeurIPS 2025", "summary": "Recent studies reveal the vulnerability of the image segmentation foundation\nmodel SAM to adversarial examples. Its successor, SAM2, has attracted\nsignificant attention due to its strong generalization capability in video\nsegmentation. However, its robustness remains unexplored, and it is unclear\nwhether existing attacks on SAM can be directly transferred to SAM2. In this\npaper, we first analyze the performance gap of existing attacks between SAM and\nSAM2 and highlight two key challenges arising from their architectural\ndifferences: directional guidance from the prompt and semantic entanglement\nacross consecutive frames. To address these issues, we propose UAP-SAM2, the\nfirst cross-prompt universal adversarial attack against SAM2 driven by dual\nsemantic deviation. For cross-prompt transferability, we begin by designing a\ntarget-scanning strategy that divides each frame into k regions, each randomly\nassigned a prompt, to reduce prompt dependency during optimization. For\neffectiveness, we design a dual semantic deviation framework that optimizes a\nUAP by distorting the semantics within the current frame and disrupting the\nsemantic consistency across consecutive frames. Extensive experiments on six\ndatasets across two segmentation tasks demonstrate the effectiveness of the\nproposed method for SAM2. The comparative results show that UAP-SAM2\nsignificantly outperforms state-of-the-art (SOTA) attacks by a large margin.", "AI": {"tldr": "\u7814\u7a76\u9996\u6b21\u5206\u6790\u4e86SAM2\u5728\u89c6\u9891\u5206\u5272\u4e2d\u7684\u6027\u80fd\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9SAM2\u7684\u8de8\u63d0\u793a\u901a\u7528\u5bf9\u6297\u653b\u51fb\u65b9\u6cd5UAP-SAM2\uff0c\u8be5\u65b9\u6cd5\u5229\u7528\u53cc\u91cd\u8bed\u4e49\u504f\u5dee\u4f18\u5316\u901a\u7528\u5bf9\u6297\u6837\u672c\uff0c\u5b9e\u9a8c\u663e\u793a\u5176\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6700\u4f73\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u5bf9SAM\u7684\u653b\u51fb\u662f\u5426\u53ef\u4ee5\u76f4\u63a5\u5e94\u7528\u4e8e\u5176\u540e\u7ee7\u8005SAM2\u5c1a\u672a\u660e\u786e\uff0c\u672c\u6b21\u7814\u7a76\u65e8\u5728\u63a2\u7d22SAM2\u7684\u8106\u5f31\u6027\uff0c\u5e76\u63d0\u51fa\u4e00\u79cd\u65b0\u7684\u5bf9\u6297\u653b\u51fb\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u53cc\u91cd\u8bed\u4e49\u504f\u5dee\u7684UAP-SAM2\u8de8\u63d0\u793a\u901a\u7528\u5bf9\u6297\u653b\u51fb\u65b9\u6cd5\uff0c\u901a\u8fc7\u8bbe\u8ba1\u76ee\u6807\u626b\u63cf\u7b56\u7565\u51cf\u5c11\u63d0\u793a\u4f9d\u8d56\uff0c\u540c\u65f6\u4f18\u5316\u901a\u7528\u5bf9\u6297\u6837\u672c\u4ee5\u6253\u7834\u5e27\u95f4\u8bed\u4e49\u4e00\u81f4\u6027\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cUAP-SAM2\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u8868\u73b0\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u7684\u6700\u4f73\u653b\u51fb\u65b9\u6cd5\u3002", "conclusion": "\u8bc1\u660e\u4e86\u73b0\u6709\u9488\u5bf9SAM\u7684\u653b\u51fb\u65e0\u6cd5\u76f4\u63a5\u5e94\u7528\u4e8eSAM2\uff0c\u901a\u8fc7UAP-SAM2\u80fd\u591f\u6709\u6548\u653b\u51fbSAM2\uff0c\u8fd9\u7a81\u663e\u4e86\u5176\u8106\u5f31\u6027\u3002"}}
{"id": "2510.24211", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.24211", "abs": "https://arxiv.org/abs/2510.24211", "authors": ["Junhyuk So", "Hyunho Kook", "Chaeyeon Jang", "Eunhyeok Park"], "title": "MC-SJD : Maximal Coupling Speculative Jacobi Decoding for Autoregressive Visual Generation Acceleration", "comment": null, "summary": "While autoregressive (AR) modeling has recently emerged as a new paradigm in\nvisual generation, its practical adoption is severely constrained by the slow\ninference speed of per-token generation, which often requires thousands of\nsteps to produce a single sample. To address this challenge, we propose MC-SJD,\na training-free, lossless parallel decoding framework designed to accelerate AR\nvisual generation by extending the recently introduced Speculative Jacobi\nDecoding (SJD). Although SJD shows strong potential for accelerating AR\ngeneration, we demonstrate that token instability across iterations\nsignificantly reduces the acceptance rate, a limitation that primarily arises\nfrom the independent sampling process used during draft token generation. To\novercome this, we introduce MC-SJD, an information-theoretic approach based on\ncoupling, which substantially accelerates standard SJD by maximizing the\nprobability of sampling identical draft tokens across consecutive iterations,\nall while preserving its lossless property. Remarkably, this method requires\nonly a single-line modification to the existing algorithm, yet achieves\nsubstantial performance gains, delivering up to a ~4.2x acceleration in image\ngeneration and ~13.3x acceleration in video generation compared to standard AR\ndecoding, without any degradation in output quality.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aMC-SJD\u7684\u8bad\u7ec3\u514d\u8d39\u4e14\u65e0\u635f\u7684\u5e76\u884c\u89e3\u7801\u6846\u67b6\uff0c\u7528\u4e8e\u52a0\u901fAR\u89c6\u89c9\u751f\u6210\u3002\u901a\u8fc7\u6700\u5927\u5316\u7684\u6982\u7387\u4f7f\u5f97\u8fde\u7eed\u8fed\u4ee3\u4e2d\u7684\u4e34\u65f6\u6807\u8bb0\u91c7\u6837\u76f8\u540c\uff0c\u8be5\u65b9\u6cd5\u5b9e\u73b0\u4e86\u5bf9\u5f53\u524dAR\u751f\u6210\u7ea64.2\u500d\u56fe\u50cf\u751f\u6210\u52a0\u901f\u548c13.3\u500d\u89c6\u9891\u751f\u6210\u52a0\u901f\uff0c\u4e14\u4e0d\u964d\u4f4e\u8f93\u51fa\u8d28\u91cf", "motivation": "\u5f53\u524dAR\u6a21\u578b\u7684\u63a8\u7406\u901f\u5ea6\u6162\uff0c\u6bcf\u751f\u6210\u4e00\u4e2a\u7269\u54c1\u9700\u8981\u6570\u5343\u6b65\uff0c\u9650\u5236\u4e86\u5176\u5b9e\u9645\u5e94\u7528\u3002SJD\u867d\u6709\u6f5c\u529b\u52a0\u901fAR\u751f\u6210\uff0c\u4f46\u56e0\u72ec\u7acb\u91c7\u6837\u5bfc\u81f4\u7684\u8fed\u4ee3\u95f4\u4e34\u65f6\u6807\u8bb0\u4e0d\u7a33\u5b9a\u4e25\u91cd\u964d\u4f4e\u4e86\u5176\u6548\u7387\u3002\u4e3a\u89e3\u51b3\u6b64\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e86MC-SJD", "method": "MC-SJD\u662f\u4e00\u79cd\u57fa\u4e8e\u8026\u5408\u7684\u4fe1\u606f\u7406\u8bba\u65b9\u6cd5\uff0c\u901a\u8fc7\u6700\u5927\u5316\u8fde\u7eed\u8fed\u4ee3\u4e2d\u4e34\u65f6\u6807\u8bb0\u76f8\u540c\u91c7\u6837\u7684\u6982\u7387\uff0c\u4ee5\u6d88\u9664\u8fed\u4ee3\u95f4\u4e34\u65f6\u6807\u8bb0\u4e0d\u7a33\u5b9a\u7684\u5f71\u54cd\uff0c\u6b64\u65b9\u6cd5\u4ec5\u9700\u5bf9\u73b0\u6709\u7b97\u6cd5\u8fdb\u884c\u5355\u4e00\u4fee\u6539\u5c31\u80fd\u5b9e\u73b0\u52a0\u901f", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cMC-SJD\u80fd\u591f\u5b9e\u73b0\u56fe\u50cf\u751f\u6210\u7ea64.2\u500d\u548c\u89c6\u9891\u751f\u6210\u7ea613.3\u500d\u7684\u52a0\u901f\uff0c\u4e14\u4fdd\u6301\u8f93\u51fa\u8d28\u91cf\u4e0d\u53d8", "conclusion": "MC-SJD\u4e3aAR\u89c6\u89c9\u751f\u6210\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u4e14\u7b80\u5355\u7684\u52a0\u901f\u65b9\u6848\uff0c\u8bc1\u660e\u4e86\u8be5\u65b9\u6cd5\u5728\u56fe\u50cf\u548c\u89c6\u9891\u751f\u6210\u4e2d\u7684\u4f18\u8d8a\u6027\u80fd"}}
{"id": "2510.23693", "categories": ["cs.LG", "cs.AI", "cs.CY"], "pdf": "https://arxiv.org/pdf/2510.23693", "abs": "https://arxiv.org/abs/2510.23693", "authors": ["Joachim Baumann"], "title": "On the Societal Impact of Machine Learning", "comment": "PhD thesis", "summary": "This PhD thesis investigates the societal impact of machine learning (ML). ML\nincreasingly informs consequential decisions and recommendations, significantly\naffecting many aspects of our lives. As these data-driven systems are often\ndeveloped without explicit fairness considerations, they carry the risk of\ndiscriminatory effects. The contributions in this thesis enable more\nappropriate measurement of fairness in ML systems, systematic decomposition of\nML systems to anticipate bias dynamics, and effective interventions that reduce\nalgorithmic discrimination while maintaining system utility. I conclude by\ndiscussing ongoing challenges and future research directions as ML systems,\nincluding generative artificial intelligence, become increasingly integrated\ninto society. This work offers a foundation for ensuring that ML's societal\nimpact aligns with broader social values.", "AI": {"tldr": "\u8be5\u535a\u58eb\u8bba\u6587\u63a2\u8ba8\u4e86\u673a\u5668\u5b66\u4e60\uff08ML\uff09\u7684\u793e\u4f1a\u5f71\u54cd\uff0c\u6307\u51fa\u8fd9\u4e9b\u6570\u636e\u9a71\u52a8\u7cfb\u7edf\u5728\u6ca1\u6709\u660e\u786e\u516c\u5e73\u6027\u8003\u91cf\u7684\u60c5\u51b5\u4e0b\u5f00\u53d1\uff0c\u53ef\u80fd\u5e26\u6765\u6b67\u89c6\u6027\u540e\u679c\u3002\u8bba\u6587\u63d0\u51fa\u4e86\u66f4\u5408\u9002\u7684\u6d4b\u91cf\u65b9\u6cd5\uff0c\u7cfb\u7edf\u5206\u89e3\u65b9\u6cd5\u4ee5\u9884\u9632\u504f\u89c1\u52a8\u6001\uff0c\u5e76\u63d0\u51fa\u4e86\u51cf\u5c11\u7b97\u6cd5\u6b67\u89c6\u7684\u6709\u6548\u5e72\u9884\u63aa\u65bd\uff0c\u540c\u65f6\u4fdd\u6301\u7cfb\u7edf\u6548\u7528\u3002\u8bba\u6587\u8fd8\u8ba8\u8bba\u4e86ML\u7cfb\u7edf\uff08\u5305\u62ec\u751f\u6210\u6027\u4eba\u5de5\u667a\u80fd\uff09\u5728\u672a\u6765\u793e\u4f1a\u96c6\u6210\u8fc7\u7a0b\u4e2d\u9762\u4e34\u7684\u6311\u6218\u548c\u7814\u7a76\u65b9\u5411\uff0c\u5f3a\u8c03\u4e86\u786e\u4fddML\u793e\u4f1a\u5f71\u54cd\u4e0e\u5e7f\u6cdb\u793e\u4f1a\u4ef7\u503c\u4e00\u81f4\u7684\u91cd\u8981\u6027\u3002", "motivation": "\u8bba\u6587\u6307\u51fa\u968f\u7740\u673a\u5668\u5b66\u4e60\u7cfb\u7edf\u5728\u51b3\u7b56\u4e2d\u7684\u5e94\u7528\u8d8a\u6765\u8d8a\u5e7f\u6cdb\uff0c\u5176\u7f3a\u4e4f\u516c\u5e73\u8003\u8651\u53ef\u80fd\u5bfc\u81f4\u6b67\u89c6\u6027\u540e\u679c\uff0c\u56e0\u6b64\u8bba\u6587\u65e8\u5728\u63d0\u51fa\u66f4\u5408\u9002\u7684\u516c\u5e73\u6027\u6d4b\u91cf\u65b9\u6cd5\u548c\u6709\u6548\u7684\u51cf\u5c11\u6b67\u89c6\u7684\u5e72\u9884\u63aa\u65bd\uff0c\u4ee5\u786e\u4fdd\u673a\u5668\u5b66\u4e60\u7684\u793e\u4f1a\u5f71\u54cd\u4e0e\u793e\u4f1a\u4ef7\u503c\u4e00\u81f4\u3002", "method": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u66f4\u4e3a\u5408\u9002\u6d4b\u91cfML\u7cfb\u7edf\u516c\u5e73\u6027\u7684\u65b9\u6cd5\uff0c\u8fdb\u884c\u4e86\u7cfb\u7edf\u7684ML\u7cfb\u7edf\u5206\u89e3\u6765\u9884\u6d4b\u504f\u89c1\u52a8\u6001\uff0c\u63d0\u51fa\u4e86\u6709\u6548\u7684\u51cf\u5c11\u7b97\u6cd5\u6b67\u89c6\u7684\u5e72\u9884\u63aa\u65bd\uff0c\u540c\u65f6\u4fdd\u6301\u7cfb\u7edf\u6548\u7528\u3002", "result": "\u8bba\u6587\u7684\u8d21\u732e\u5728\u4e8e\u63d0\u51fa\u4e86\u66f4\u5408\u9002\u7684\u6d4b\u91cfML\u516c\u5e73\u6027\u7684\u65b9\u6cd5\uff0c\u7cfb\u7edf\u6027\u5730\u5206\u89e3\u4e86ML\u7cfb\u7edf\u7684\u504f\u89c1\u52a8\u6001\uff0c\u5e76\u63d0\u51fa\u4e86\u51cf\u5c11\u7b97\u6cd5\u6b67\u89c6\u7684\u6709\u6548\u63aa\u65bd\uff0c\u540c\u65f6\u4fdd\u6301\u7cfb\u7edf\u7684\u6548\u7528\u3002", "conclusion": "\u672c\u6587\u5f3a\u8c03\u4e86\u5728ML\u7cfb\u7edf\uff08\u5305\u62ec\u751f\u6210\u6027AI\uff09\u6108\u53d1\u878d\u5165\u793e\u4f1a\u7684\u540c\u65f6\uff0c\u9762\u4e34\u7684\u6311\u6218\u548c\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002\u4e5f\u4e3a\u786e\u4fdd\u673a\u5668\u5b66\u4e60\u7684\u793e\u4f1a\u5f71\u54cd\u4e0e\u793e\u4f1a\u4ef7\u503c\u4e00\u81f4\u63d0\u4f9b\u4e86\u57fa\u7840\u3002"}}
{"id": "2510.24213", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.24213", "abs": "https://arxiv.org/abs/2510.24213", "authors": ["Haoxin Yang", "Yihong Lin", "Jingdan Kang", "Xuemiao Xu", "Yue Li", "Cheng Xu", "Shengfeng He"], "title": "Beyond Inference Intervention: Identity-Decoupled Diffusion for Face Anonymization", "comment": null, "summary": "Face anonymization aims to conceal identity information while preserving\nnon-identity attributes. Mainstream diffusion models rely on inference-time\ninterventions such as negative guidance or energy-based optimization, which are\napplied post-training to suppress identity features. These interventions often\nintroduce distribution shifts and entangle identity with non-identity\nattributes, degrading visual fidelity and data utility. To address this, we\npropose \\textbf{ID\\textsuperscript{2}Face}, a training-centric anonymization\nframework that removes the need for inference-time optimization. The rationale\nof our method is to learn a structured latent space where identity and\nnon-identity information are explicitly disentangled, enabling direct and\ncontrollable anonymization at inference. To this end, we design a conditional\ndiffusion model with an identity-masked learning scheme. An Identity-Decoupled\nLatent Recomposer uses an Identity Variational Autoencoder to model identity\nfeatures, while non-identity attributes are extracted from same-identity pairs\nand aligned through bidirectional latent alignment. An Identity-Guided Latent\nHarmonizer then fuses these representations via soft-gating conditioned on\nnoisy feature prediction. The model is trained with a recomposition-based\nreconstruction loss to enforce disentanglement. At inference, anonymization is\nachieved by sampling a random identity vector from the learned identity space.\nTo further suppress identity leakage, we introduce an Orthogonal Identity\nMapping strategy that enforces orthogonality between sampled and source\nidentity vectors. Experiments demonstrate that ID\\textsuperscript{2}Face\noutperforms existing methods in visual quality, identity suppression, and\nutility preservation.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u7528\u4e8e\u4eba\u8138\u533f\u540d\u5316\u7684\u6846\u67b6ID\textsuperscript{2}Face\uff0c\u8be5\u6846\u67b6\u5728\u8bad\u7ec3\u9636\u6bb5\u5c31\u89e3\u8026\u4e86\u8eab\u4efd\u548c\u975e\u8eab\u4efd\u5c5e\u6027\uff0c\u4e0d\u9700\u8981\u63a8\u7406\u65f6\u7684\u989d\u5916\u4f18\u5316\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u533f\u540d\u5316\u540e\u7684\u56fe\u50cf\u8d28\u91cf\u548c\u6570\u636e\u5229\u7528\u6027\u3002", "motivation": "\u4e3b\u6d41\u7684\u6269\u6563\u6a21\u578b\u5728\u63a8\u7406\u65f6\u9700\u8981\u989d\u5916\u7684\u5e72\u9884\u6765\u6291\u5236\u8eab\u4efd\u4fe1\u606f\uff0c\u8fd9\u53ef\u80fd\u4f1a\u5f15\u53d1\u6570\u636e\u5206\u5e03\u7684\u53d8\u5316\u548c\u8eab\u4efd\u4e0e\u975e\u8eab\u4efd\u4fe1\u606f\u7684\u7ea0\u7f20\uff0c\u964d\u4f4e\u4e86\u56fe\u50cf\u8d28\u91cf\u548c\u6570\u636e\u7684\u5b9e\u7528\u6027\u3002\u672c\u6587\u7684\u52a8\u673a\u662f\u4e3a\u4e86\u627e\u5230\u4e00\u79cd\u4e0d\u9700\u8981\u540e\u5904\u7406\u5c31\u80fd\u6709\u6548\u6291\u5236\u8eab\u4efd\u4fe1\u606f\u7684\u65b9\u6cd5\uff0c\u540c\u65f6\u4fdd\u6301\u56fe\u50cf\u8d28\u91cf\u548c\u6570\u636e\u7684\u9ad8\u5229\u7528\u6027\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u8bad\u7ec3\u4e3a\u4e2d\u5fc3\u7684\u4eba\u8138\u533f\u540d\u5316\u6846\u67b6ID\textsuperscript{2}Face\uff0c\u8be5\u65b9\u6cd5\u5f15\u5165\u4e86\u4e00\u4e2a\u6761\u4ef6\u6269\u6563\u6a21\u578b\u548c\u8eab\u4efd\u89e3\u8026\u7684\u6f5c\u5728\u7a7a\u95f4\u91cd\u7ec4\u5668\uff0c\u901a\u8fc7\u5b66\u4e60\u7ed3\u6784\u5316\u7684\u6f5c\u5728\u7a7a\u95f4\u6765\u663e\u5f0f\u5730\u89e3\u8026\u8eab\u4efd\u548c\u975e\u8eab\u4efd\u4fe1\u606f\u3002\u901a\u8fc7\u8eab\u4efd\u53d8\u5206\u81ea\u7f16\u7801\u5668\u6765\u6a21\u62df\u8eab\u4efd\u7279\u5f81\uff0c\u540c\u65f6\u4ece\u5177\u6709\u76f8\u540c\u8eab\u4efd\u7684\u6210\u5bf9\u6570\u636e\u4e2d\u63d0\u53d6\u975e\u8eab\u4efd\u5c5e\u6027\u5e76\u901a\u8fc7\u53cc\u5411\u6f5c\u5728\u5bf9\u9f50\u8fdb\u884c\u6821\u51c6\uff0c\u518d\u901a\u8fc7\u6761\u4ef6\u7684\u8f6f\u95e8\u63a7\u878d\u5408\u8868\u793a\u4ee5\u6291\u5236\u8eab\u4efd\u6cc4\u6f0f\u3002\u6a21\u578b\u901a\u8fc7\u4e00\u4e2a\u57fa\u4e8e\u91cd\u7ec4\u7684\u91cd\u6784\u635f\u5931\u8fdb\u884c\u8bad\u7ec3\u4ee5\u5f3a\u5236\u6267\u884c\u89e3\u8026\u3002\u63a8\u7406\u65f6\uff0c\u901a\u8fc7\u4ece\u5b66\u4e60\u5230\u7684\u8eab\u4efd\u7a7a\u95f4\u4e2d\u91c7\u6837\u968f\u673a\u7684\u8eab\u4efd\u5411\u91cf\u6765\u5b9e\u73b0\u533f\u540d\u5316\u3002\u4e3a\u4e86\u8fdb\u4e00\u6b65\u6291\u5236\u8eab\u4efd\u6cc4\u9732\uff0c\u5f15\u5165\u4e86\u6b63\u4ea4\u8eab\u4efd\u6620\u5c04\u7b56\u7565\u6765\u5f3a\u5236\u6837\u672c\u8eab\u4efd\u5411\u91cf\u4e0e\u6e90\u8eab\u4efd\u5411\u91cf\u4e4b\u95f4\u7684\u6b63\u4ea4\u6027\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cID\textsuperscript{2}Face\u5728\u89c6\u89c9\u8d28\u91cf\u548c\u8eab\u4efd\u6291\u5236\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5e76\u4e14\u80fd\u591f\u66f4\u597d\u5730\u4fdd\u6301\u6570\u636e\u7684\u5b9e\u7528\u6027\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u4e14\u9ad8\u6548\u7684\u8bad\u7ec3\u4e2d\u5fc3\u4eba\u8138\u533f\u540d\u5316\u6846\u67b6ID\textsuperscript{2}Face\uff0c\u901a\u8fc7\u5b66\u4e60\u7ed3\u6784\u5316\u7684\u6f5c\u5728\u7a7a\u95f4\u4ee5\u660e\u786e\u5730\u89e3\u8026\u8eab\u4efd\u548c\u975e\u8eab\u4efd\u4fe1\u606f\uff0c\u6709\u6548\u63d0\u9ad8\u4e86\u533f\u540d\u540e\u7684\u56fe\u50cf\u8d28\u91cf\u548c\u6570\u636e\u7684\u5b9e\u7528\u6027\uff0c\u907f\u514d\u4e86\u4e3b\u6d41\u6269\u6563\u6a21\u578b\u4e2d\u5e38\u89c1\u7684\u63a8\u7406\u65f6\u9700\u989d\u5916\u4f18\u5316\u7684\u95ee\u9898\u3002"}}
{"id": "2510.24390", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24390", "abs": "https://arxiv.org/abs/2510.24390", "authors": ["Xianjun Gao", "Jianchun Liu", "Hongli Xu", "Liusheng Huang"], "title": "Improving LLM Reasoning via Dependency-Aware Query Decomposition and Logic-Parallel Content Expansion", "comment": null, "summary": "The integration of Large Language Models (LLMs) into real-time Web\napplications, such as AI-powered search and conversational agents, presents a\nfundamental Web infrastructure challenge: reconciling the demand for\nhigh-quality, complex reasoning with the stringent low-latency and\nhigh-throughput requirements of interactive services. Current LLM reasoning,\nhindered by computationally inefficient sequential generation and rigid\nreasoning strategies, creates a critical bottleneck for the Web services.\nExisting approaches typically optimize the LLM reasoning for either efficiency\nor quality but struggle to achieve both, and thus fail to meet the dual\nrequirements of modern Web platforms. To overcome these limitations, we propose\nOrion, a novel and efficient reasoning framework that enables dependency-aware\nquery decomposition and logic-parallel content expansion. Concretely, Orion\ndecomposes a single query reasoning process into two synergistic phases: (1)\n\\textit{key point generation}, which distills logically structured key points\nthrough retrieval-augmented few-shot prompting, and (2) \\textit{content\nparallel expansion}, which concurrently elaborates on these points based on a\ndependency graph to ensure logical consistency. Furthermore, Orion introduces a\npipeline scheduling mechanism that exploits the complementary computational\ncharacteristics of the two phases (generation imposes pressure on GPU computing\nand expansion stresses on GPU memory) across multiple queries, enabling\ncross-query parallelism and dramatically improving reasoning performance (\\ie,\nefficiency and quality). Experiments on diverse benchmarks show that Orion not\nonly delivers up to 4.33x higher token generation speed and 3.42x lower answer\nlatency over the baselines but also improves reasoning quality by up to 18.75%\nthrough explicitly modeling inter-point dependencies.", "AI": {"tldr": "Orion\u662f\u4e00\u4e2a\u65b0\u7684\u9ad8\u6548\u63a8\u7406\u6846\u67b6\uff0c\u5b83\u901a\u8fc7\u4f9d\u8d56\u611f\u77e5\u67e5\u8be2\u5206\u89e3\u548c\u903b\u8f91\u5e76\u884c\u5185\u5bb9\u6269\u5c55\u6765\u89e3\u51b3LLM\u5728Web\u5e94\u7528\u4e2d\u7684\u74f6\u9888\u95ee\u9898\u3002\u8be5\u6846\u67b6\u5c06\u67e5\u8be2\u63a8\u7406\u8fc7\u7a0b\u5206\u4e3a\u4e24\u4e2a\u534f\u540c\u9636\u6bb5\uff1a\u5173\u952e\u70b9\u751f\u6210\u548c\u5185\u5bb9\u5e76\u884c\u6269\u5c55\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u4e0e\u57fa\u51c6\u76f8\u6bd4\uff0cOrion\u53ef\u4ee5\u5c06\u6807\u8bb0\u751f\u6210\u901f\u5ea6\u63d0\u9ad84.33\u500d\uff0c\u5c06\u7b54\u6848\u5ef6\u8fdf\u964d\u4f4e3.42\u500d\uff0c\u63a8\u7406\u8d28\u91cf\u63d0\u9ad818.75%\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u5b9e\u65f6Web\u5e94\u7528\u4e2d\u7684\u96c6\u6210\u9762\u4e34\u4e00\u4e2a\u57fa\u672c\u7684Web\u57fa\u7840\u8bbe\u65bd\u6311\u6218\uff0c\u5373\u5728\u6ee1\u8db3\u9ad8\u8d28\u91cf\u590d\u6742\u63a8\u7406\u9700\u6c42\u7684\u540c\u65f6\uff0c\u8fd8\u8981\u9002\u5e94\u4ea4\u4e92\u5f0f\u670d\u52a1\u7684\u4e25\u683c\u4f4e\u5ef6\u8fdf\u548c\u9ad8\u541e\u5410\u91cf\u8981\u6c42\u3002\u73b0\u6709\u7684LLM\u63a8\u7406\u65b9\u6cd5\u5728\u6548\u7387\u548c\u8d28\u91cf\u4e0a\u96be\u4ee5\u517c\u5f97\uff0c\u65e0\u6cd5\u6ee1\u8db3\u73b0\u4ee3Web\u5e73\u53f0\u7684\u9700\u6c42\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u63a8\u7406\u6846\u67b6Orion\uff0c\u5b83\u5305\u542b\u4f9d\u8d56\u611f\u77e5\u67e5\u8be2\u5206\u89e3\u548c\u903b\u8f91\u5e76\u884c\u5185\u5bb9\u6269\u5c55\u4e24\u4e2a\u9636\u6bb5\uff0c\u540c\u65f6\u5f15\u5165\u4e86\u4e00\u79cd\u6d41\u6c34\u7ebf\u8c03\u5ea6\u673a\u5236\uff0c\u5229\u7528\u4e24\u4e2a\u9636\u6bb5\u7684\u4e92\u8865\u8ba1\u7b97\u7279\u6027\u6765\u8de8\u591a\u4e2a\u67e5\u8be2\u5b9e\u73b0\u8de8\u67e5\u8be2\u5e76\u884c\uff0c\u4ece\u800c\u663e\u8457\u63d0\u9ad8\u63a8\u7406\u6027\u80fd\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0cOrion\u5728\u591a\u6837\u5316\u7684\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u4e8e\u57fa\u7ebf\uff0c\u4e0d\u4ec5\u5c06\u4ee4\u724c\u751f\u6210\u901f\u5ea6\u63d0\u9ad8\u4e864.33\u500d\uff0c\u8fd8\u5c06\u7b54\u6848\u5ef6\u8fdf\u964d\u4f4e\u4e863.42\u500d\uff0c\u800c\u4e14\u901a\u8fc7\u663e\u5f0f\u5efa\u6a21\u70b9\u95f4\u4f9d\u8d56\u6027\uff0c\u63a8\u7406\u8d28\u91cf\u63d0\u9ad8\u4e8618.75%\u3002", "conclusion": "Orion\u6709\u6548\u5730\u89e3\u51b3\u4e86LLM\u63a8\u7406\u5728Web\u5e94\u7528\u4e2d\u7684\u74f6\u9888\u95ee\u9898\uff0c\u5b83\u80fd\u591f\u5728\u4fdd\u8bc1\u63a8\u7406\u8d28\u91cf\u7684\u540c\u65f6\u5b9e\u73b0\u9ad8\u6548\u7684\u63a8\u7406\u6027\u80fd\uff0c\u5bf9\u4e8e\u73b0\u4ee3Web\u5e73\u53f0\u6765\u8bf4\u662f\u4e00\u4e2a\u91cd\u8981\u7684\u8fdb\u5c55\u3002"}}
{"id": "2510.23727", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.23727", "abs": "https://arxiv.org/abs/2510.23727", "authors": ["Anisha Saha", "Varsha Suresh", "Timothy Hospedales", "Vera Demberg"], "title": "MUStReason: A Benchmark for Diagnosing Pragmatic Reasoning in Video-LMs for Multimodal Sarcasm Detection", "comment": null, "summary": "Sarcasm is a specific type of irony which involves discerning what is said\nfrom what is meant. Detecting sarcasm depends not only on the literal content\nof an utterance but also on non-verbal cues such as speaker's tonality, facial\nexpressions and conversational context. However, current multimodal models\nstruggle with complex tasks like sarcasm detection, which require identifying\nrelevant cues across modalities and pragmatically reasoning over them to infer\nthe speaker's intention. To explore these limitations in VideoLMs, we introduce\nMUStReason, a diagnostic benchmark enriched with annotations of\nmodality-specific relevant cues and underlying reasoning steps to identify\nsarcastic intent. In addition to benchmarking sarcasm classification\nperformance in VideoLMs, using MUStReason we quantitatively and qualitatively\nevaluate the generated reasoning by disentangling the problem into perception\nand reasoning, we propose PragCoT, a framework that steers VideoLMs to focus on\nimplied intentions over literal meaning, a property core to detecting sarcasm.", "AI": {"tldr": "\u4ecb\u7ecd\u4e86\u4e00\u79cd\u65b0\u7684\u591a\u6a21\u6001\u6a21\u578b Benchmark MUStReason \u548c\u6846\u67b6 PragCoT\uff0c\u7528\u4e8e\u5206\u6790\u68c0\u6d4b\u89c6\u9891\u4e2d\u7684\u8bbd\u523a\u5185\u5bb9\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u6a21\u578b\u5728\u591a\u6a21\u6001\u4f9d\u8d56\u548c\u610f\u56fe\u63a8\u7406\u4e2d\u7684\u4e0d\u8db3", "motivation": "\u5f53\u524d\u7684\u591a\u6a21\u6001\u6a21\u578b\u5728\u5904\u7406\u9700\u8981\u8de8\u6a21\u6001\u8bc6\u522b\u76f8\u5173\u7ebf\u7d22\u5e76\u8fdb\u884c\u610f\u56fe\u63a8\u7406\u7684\u590d\u6742\u4efb\u52a1\u65f6\u5b58\u5728\u56f0\u96be\uff0c\u56e0\u6b64\u5f15\u5165\u4e86\u65b0\u7684 Benchmark \u548c\u6846\u67b6\u6765\u6539\u5584\u8fd9\u4e00\u95ee\u9898", "method": "\u63d0\u51fa\u4e86 MUStReason Benchmark\uff0c\u5176\u5305\u542b\u9488\u5bf9\u4e0d\u540c\u6a21\u6001\u7684\u76f8\u5173\u7ebf\u7d22\u6ce8\u91ca\u548c\u63a8\u7406\u6b65\u9aa4\uff0c\u4ee5\u53ca PragCoT \u6846\u67b6\uff0c\u8be5\u6846\u67b6\u4f7f\u5f97 VideoLMs \u66f4\u5173\u6ce8\u9690\u542b\u610f\u56fe\u800c\u4e0d\u662f\u5b57\u9762\u610f\u601d", "result": "\u65b0\u7684 Benchmark \u548c\u6846\u67b6\u80fd\u591f\u6539\u5584\u89c6\u9891\u4e2d\u8bbd\u523a\u5185\u5bb9\u7684\u68c0\u6d4b\uff0c\u6539\u8fdb\u4e86\u73b0\u6709\u6a21\u578b\u5728\u591a\u6a21\u6001\u4f9d\u8d56\u548c\u610f\u56fe\u63a8\u7406\u4e0a\u7684\u4e0d\u8db3", "conclusion": "\u901a\u8fc7\u5f15\u5165\u65b0\u7684 Benchmark \u548c\u6846\u67b6\uff0c\u7814\u7a76\u8005\u53ef\u4ee5\u66f4\u597d\u5730\u8bca\u65ad\u548c\u63d0\u5347\u73b0\u6709\u6a21\u578b\u5728\u5904\u7406\u8bbd\u523a\u8bed\u8a00\u4efb\u52a1\u7684\u80fd\u529b"}}
{"id": "2510.24214", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.24214", "abs": "https://arxiv.org/abs/2510.24214", "authors": ["Jinhong Deng", "Wen Li", "Joey Tianyi Zhou", "Yang He"], "title": "SCOPE: Saliency-Coverage Oriented Token Pruning for Efficient Multimodel LLMs", "comment": "NeurIPS 2025", "summary": "Multimodal Large Language Models (MLLMs) typically process a large number of\nvisual tokens, leading to considerable computational overhead, even though many\nof these tokens are redundant. Existing visual token pruning methods primarily\nfocus on selecting the most salient tokens based on attention scores, resulting\nin the semantic incompleteness of the selected tokens. In this paper, we\npropose a novel visual token pruning strategy, called\n\\textbf{S}aliency-\\textbf{C}overage \\textbf{O}riented token \\textbf{P}runing\nfor \\textbf{E}fficient MLLMs (SCOPE), to jointly model both the saliency and\ncoverage of the selected visual tokens to better preserve semantic\ncompleteness. Specifically, we introduce a set-coverage for a given set of\nselected tokens, computed based on the token relationships. We then define a\ntoken-coverage gain for each unselected token, quantifying how much additional\ncoverage would be obtained by including it. By integrating the saliency score\ninto the token-coverage gain, we propose our SCOPE score and iteratively select\nthe token with the highest SCOPE score. We conduct extensive experiments on\nmultiple vision-language understanding benchmarks using the LLaVA-1.5 and\nLLaVA-Next models. Experimental results demonstrate that our method\nconsistently outperforms prior approaches. Our code is available at\n\\href{https://github.com/kinredon/SCOPE}{https://github.com/kinredon/SCOPE}.", "AI": {"tldr": "\u63d0\u51fa\u4e86SCOPE\u7b56\u7565\uff0c\u65e8\u5728\u540c\u65f6\u8003\u8651\u89c6\u89c9\u4ee4\u724c\u7684\u663e\u8457\u6027\u548c\u8986\u76d6\u7387\uff0c\u4ece\u800c\u66f4\u5b8c\u6574\u5730\u4fdd\u7559\u8bed\u4e49\u3002\u5728\u591a\u4e2a\u89c6\u89c9-\u8bed\u8a00\u7406\u89e3\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u8be5\u65b9\u6cd5\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002\u4ee3\u7801\u5df2\u5f00\u6e90\u3002", "motivation": "\u73b0\u6709\u89c6\u89c9\u4ee4\u724c\u88c1\u526a\u65b9\u6cd5\u4fa7\u91cd\u9009\u62e9\u6700\u663e\u8457\u7684\u4ee4\u724c\uff0c\u5bfc\u81f4\u8bed\u4e49\u4e0d\u5b8c\u6574\u3002\u4e3a\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u63d0\u51fa\u4e86SCOPE\u7b56\u7565\uff0c\u8be5\u7b56\u7565\u8054\u5408\u5efa\u6a21\u6240\u9009\u89c6\u89c9\u4ee4\u724c\u7684\u663e\u8457\u6027\u548c\u8986\u76d6\u7387\u4ee5\u66f4\u597d\u5730\u4fdd\u7559\u8bed\u4e49\u5b8c\u6574\u6027\u3002", "method": "\u5f15\u5165\u57fa\u4e8e\u4ee4\u724c\u5173\u7cfb\u8ba1\u7b97\u7684\u96c6\u5408\u8986\u76d6\u5ea6\uff0c\u5e76\u5b9a\u4e49\u672a\u9009\u4ee4\u724c\u7684\u4ee4\u724c\u8986\u76d6\u5ea6\u589e\u76ca\uff0c\u91cf\u5316\u4e86\u5305\u542b\u8be5\u4ee4\u724c\u6240\u80fd\u83b7\u5f97\u7684\u989d\u5916\u8986\u76d6\u5ea6\u3002\u901a\u8fc7\u5c06\u663e\u8457\u5ea6\u8bc4\u5206\u6574\u5408\u5230\u4ee4\u724c\u8986\u76d6\u5ea6\u589e\u76ca\u4e2d\u6765\u63d0\u51faSCOPE\u8bc4\u5206\uff0c\u5e76\u4e14\u8fed\u4ee3\u9009\u62e9\u5177\u6709\u6700\u9ad8SCOPE\u8bc4\u5206\u7684\u4ee4\u724c\u3002\u5b9e\u9a8c\u5728LLaVA-1.5\u548cLLaVA-Next\u6a21\u578b\u4e0a\u7684\u591a\u4e2a\u89c6\u89c9-\u8bed\u8a00\u7406\u89e3\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fdb\u884c\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u63d0\u51fa\u7684SCOPE\u7b56\u7565\u5728\u591a\u4e2a\u89c6\u89c9-\u8bed\u8a00\u7406\u89e3\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4e00\u8d2f\u5730\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u4ee3\u7801\u5df2\u901a\u8fc7https://github.com/kinredon/SCOPE\u5f00\u6e90\u3002", "conclusion": "\u901a\u8fc7\u8054\u5408\u5efa\u6a21\u89c6\u89c9\u4ee4\u724c\u7684\u663e\u8457\u6027\u548c\u8986\u76d6\u7387\uff0c\u4f18\u5148\u9009\u62e9\u80fd\u5e26\u6765\u6709\u6548\u4fe1\u606f\u548c\u8986\u76d6\u5ea6\u7684\u4ee4\u724c\uff0c\u964d\u4f4e\u5197\u4f59\u5ea6\u548c\u8ba1\u7b97\u5f00\u9500\uff0c\u63d0\u9ad8\u4e86\u8bed\u4e49\u5b8c\u6574\u6027\u548c\u6a21\u578b\u6548\u7387\u3002"}}
{"id": "2510.23751", "categories": ["cs.LG", "cs.AI", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.23751", "abs": "https://arxiv.org/abs/2510.23751", "authors": ["Ignavier Ng", "Patrick Bl\u00f6baum", "Siddharth Bhandari", "Kun Zhang", "Shiva Kasiviswanathan"], "title": "Debiasing Reward Models by Representation Learning with Guarantees", "comment": null, "summary": "Recent alignment techniques, such as reinforcement learning from human\nfeedback, have been widely adopted to align large language models with human\npreferences by learning and leveraging reward models. In practice, these models\noften exploit spurious correlations, involving, e.g., response length,\ndiscrimination, sycophancy, and conceptual bias, which is a problem that has\nreceived increasing attention. In this work, we propose a principled framework\nthat mitigates these biases in reward models while preserving the underlying\nfactors that reflect intended preferences. We first provide a formulation of\nthe data-generating process, assuming that the observed data (e.g., text) is\ngenerated from both spurious and non-spurious latent variables. We show that,\ninterestingly, these non-spurious latent variables can be theoretically\nidentified from data, regardless of whether a surrogate for the spurious latent\nvariables is available. This further inspires a practical method that uses\nvariational inference to recover these variables and leverages them to train\nreward models. Experiments on synthetic and real-world datasets demonstrate\nthat our method effectively mitigates spurious correlation issues and yields\nmore robust reward models.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6846\u67b6\uff0c\u65e8\u5728\u51cf\u5c11\u5956\u52b1\u6a21\u578b\u4e2d\u7684\u504f\u5dee\uff0c\u540c\u65f6\u4fdd\u6301\u53cd\u6620\u9884\u671f\u504f\u597d\u7684\u6f5c\u5728\u56e0\u7d20\u3002\u901a\u8fc7\u53d8\u5206\u63a8\u7406\u6765\u6062\u590d\u8fd9\u4e9b\u6f5c\u5728\u53d8\u91cf\uff0c\u5e76\u5229\u7528\u5b83\u4eec\u6765\u8bad\u7ec3\u5956\u52b1\u6a21\u578b\uff0c\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u6709\u6548\u5730\u7f13\u89e3\u4e86\u865a\u5047\u76f8\u5173\u6027\u95ee\u9898\uff0c\u5e76\u4ea7\u751f\u4e86\u66f4\u7a33\u5065\u7684\u5956\u52b1\u6a21\u578b\u3002", "motivation": "\u73b0\u6709\u7684\u5bf9\u9f50\u6280\u672f\uff0c\u5982\u4ece\u4eba\u7c7b\u53cd\u9988\u4e2d\u5b66\u4e60\u7684\u5f3a\u5316\u5b66\u4e60\uff0c\u5b58\u5728\u5229\u7528\u865a\u5047\u76f8\u5173\u6027\u7684\u95ee\u9898\uff0c\u4f8b\u5982\u54cd\u5e94\u957f\u5ea6\u3001\u6b67\u89c6\u3001\u5949\u627f\u548c\u6982\u5ff5\u504f\u5dee\u3002\u672c\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u504f\u89c1\u95ee\u9898\u3002", "method": "\u9996\u5148\uff0c\u7ed9\u51fa\u4e86\u4e00\u4e2a\u6570\u636e\u751f\u6210\u8fc7\u7a0b\u7684\u516c\u5f0f\u5316\uff0c\u5047\u8bbe\u89c2\u5bdf\u5230\u7684\u6570\u636e\u662f\u7531\u865a\u5047\u548c\u975e\u865a\u5047\u6f5c\u5728\u53d8\u91cf\u751f\u6210\u7684\u3002\u8bbe\u8ba1\u4e86\u4f7f\u7528\u53d8\u5206\u63a8\u7406\u6765\u6062\u590d\u8fd9\u4e9b\u53d8\u91cf\uff0c\u4ece\u800c\u8bad\u7ec3\u5956\u52b1\u6a21\u578b\u7684\u65b9\u6cd5\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u7f13\u89e3\u865a\u5047\u76f8\u5173\u6027\u95ee\u9898\uff0c\u5e76\u63d0\u9ad8\u5956\u52b1\u6a21\u578b\u7684\u9c81\u68d2\u6027\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u80fd\u591f\u901a\u8fc7\u7406\u8bba\u624b\u6bb5\u8bc6\u522b\u975e\u865a\u5047\u6f5c\u5728\u53d8\u91cf\uff0c\u5e76\u901a\u8fc7\u53d8\u5206\u63a8\u7406\u6062\u590d\u8fd9\u4e9b\u53d8\u91cf\u6765\u8bad\u7ec3\u5956\u52b1\u6a21\u578b\uff0c\u6700\u7ec8\u63d0\u9ad8\u4e86\u5956\u52b1\u6a21\u578b\u7684\u9c81\u68d2\u6027\u548c\u51c6\u786e\u6027\u3002"}}
{"id": "2510.24435", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24435", "abs": "https://arxiv.org/abs/2510.24435", "authors": ["Benjamin Grando Moreira"], "title": "Human-Level Reasoning: A Comparative Study of Large Language Models on Logical and Abstract Reasoning", "comment": "12 pages", "summary": "Evaluating reasoning ability in Large Language Models (LLMs) is important for\nadvancing artificial intelligence, as it transcends mere linguistic task\nperformance. It involves understanding whether these models truly understand\ninformation, perform inferences, and are able to draw conclusions in a logical\nand valid way. This study compare logical and abstract reasoning skills of\nseveral LLMs - including GPT, Claude, DeepSeek, Gemini, Grok, Llama, Mistral,\nPerplexity, and Sabi\\'a - using a set of eight custom-designed reasoning\nquestions. The LLM results are benchmarked against human performance on the\nsame tasks, revealing significant differences and indicating areas where LLMs\nstruggle with deduction.", "AI": {"tldr": "\u672c\u6587\u8bc4\u4f30\u4e86\u591a\u4e2a\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\uff0c\u5e76\u5c06\u5176\u4e0e\u4eba\u7c7b\u8868\u73b0\u8fdb\u884c\u6bd4\u8f83\uff0c\u7ed3\u679c\u663e\u793a\u6a21\u578b\u5728\u6f14\u7ece\u63a8\u7406\u4e0a\u5b58\u5728\u4e0d\u8db3", "motivation": "\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u63a8\u7406\u80fd\u529b\u5bf9\u4e8e\u63a8\u52a8\u4eba\u5de5\u667a\u80fd\u7684\u53d1\u5c55\u81f3\u5173\u91cd\u8981\u3002\u8fd9\u6d89\u53ca\u5230\u7406\u89e3\u8fd9\u4e9b\u6a21\u578b\u662f\u5426\u771f\u6b63\u7406\u89e3\u4fe1\u606f\uff0c\u8fdb\u884c\u63a8\u7406\u5e76\u4ee5\u903b\u8f91\u548c\u6709\u6548\u7684\u65b9\u5f0f\u5f97\u51fa\u7ed3\u8bba", "method": "\u901a\u8fc7\u4e00\u7ec4\u516b\u4e2a\u81ea\u5b9a\u4e49\u8bbe\u8ba1\u7684\u63a8\u7406\u95ee\u9898\u5bf9\u51e0\u79cdLLM\uff0c\u5305\u62ecGPT\uff0cClaude\uff0cDeepSeek\uff0cGemini\uff0cGrok\uff0cLlama\uff0cMistral\uff0cPerplexity\u548cSabi'a\u7b49\u7684\u903b\u8f91\u548c\u62bd\u8c61\u63a8\u7406\u6280\u80fd\u8fdb\u884c\u6bd4\u8f83", "result": "LLM\u7684\u8868\u73b0\u4e0e\u4eba\u7c7b\u5728\u76f8\u540c\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\u8fdb\u884c\u4e86\u57fa\u51c6\u6d4b\u8bd5\uff0c\u63ed\u793a\u4e86\u663e\u8457\u7684\u5dee\u5f02\uff0c\u8868\u660eLLM\u5728\u6f14\u7ece\u63a8\u7406\u65b9\u9762\u5b58\u5728\u5dee\u8ddd", "conclusion": "\u7814\u7a76\u8868\u660e\uff0c\u867d\u7136LLM\u5728\u591a\u79cd\u4efb\u52a1\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5728\u63a8\u7406\u80fd\u529b\uff0c\u7279\u522b\u662f\u5728\u6f14\u7ece\u63a8\u7406\u65b9\u9762\uff0c\u4ecd\u8868\u73b0\u51fa\u5bf9\u4fe1\u606f\u7684\u7406\u89e3\u548c\u6709\u6548\u63a8\u7406\u7684\u5c40\u9650\u6027"}}
{"id": "2510.24260", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.24260", "abs": "https://arxiv.org/abs/2510.24260", "authors": ["Zhaotong Yang", "Yi Chen", "Yanying Li", "Shengfeng He", "Yangyang Xu", "Junyu Dong", "Jian Yang", "Yong Du"], "title": "DeshadowMamba: Deshadowing as 1D Sequential Similarity", "comment": null, "summary": "Recent deep models for image shadow removal often rely on attention-based\narchitectures to capture long-range dependencies. However, their fixed\nattention patterns tend to mix illumination cues from irrelevant regions,\nleading to distorted structures and inconsistent colors. In this work, we\nrevisit shadow removal from a sequence modeling perspective and explore the use\nof Mamba, a selective state space model that propagates global context through\ndirectional state transitions. These transitions yield an efficient global\nreceptive field while preserving positional continuity. Despite its potential,\ndirectly applying Mamba to image data is suboptimal, since it lacks awareness\nof shadow-non-shadow semantics and remains susceptible to color interference\nfrom nearby regions. To address these limitations, we propose CrossGate, a\ndirectional modulation mechanism that injects shadow-aware similarity into\nMamba's input gate, allowing selective integration of relevant context along\ntransition axes. To further ensure appearance fidelity, we introduce ColorShift\nregularization, a contrastive learning objective driven by global color\nstatistics. By synthesizing structured informative negatives, it guides the\nmodel to suppress color contamination and achieve robust color restoration.\nTogether, these components adapt sequence modeling to the structural integrity\nand chromatic consistency required for shadow removal. Extensive experiments on\npublic benchmarks demonstrate that DeshadowMamba achieves state-of-the-art\nvisual quality and strong quantitative performance.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5e8f\u5217\u5efa\u6a21\u65b9\u6cd5DeshadowMamba\uff0c\u7ed3\u5408Mamba\u6a21\u578b\u548cCrossGate\u673a\u5236\uff0c\u4ee5\u53caColorShift\u6b63\u5219\u5316\uff0c\u6709\u6548\u63d0\u5347\u4e86\u56fe\u50cf\u53bb\u9634\u5f71\u7684\u6548\u679c\uff0c\u8fbe\u5230\u4e86\u76ee\u524d\u6700\u5148\u8fdb\u7684\u89c6\u89c9\u8d28\u91cf\u548c\u91cf\u5316\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684\u57fa\u4e8e\u6ce8\u610f\u529b\u673a\u5236\u7684\u6df1\u5ea6\u6a21\u578b\u5728\u56fe\u50cf\u53bb\u9634\u5f71\u5904\u7406\u4e2d\u5b58\u5728\u56fa\u5b9a\u6ce8\u610f\u529b\u6a21\u5f0f\u5bfc\u81f4\u7684\u5149\u7167\u7ebf\u7d22\u6df7\u6742\u95ee\u9898\uff0c\u672c\u6587\u65e8\u5728\u901a\u8fc7\u5e8f\u5217\u5efa\u6a21\u7684\u65b9\u5f0f\u6765\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "\u9996\u5148\uff0c\u4f7f\u7528Mamba\u6a21\u578b\u8fdb\u884c\u5168\u5c40\u4e0a\u4e0b\u6587\u4f20\u64ad\uff1b\u7136\u540e\uff0c\u901a\u8fc7CrossGate\u673a\u5236\u5b9e\u73b0\u9634\u5f71\u610f\u8bc6\u76f8\u4f3c\u6027\u7684\u8f93\u5165\u95e8\u6ce8\u5165\uff0c\u9009\u62e9\u6027\u5730\u6574\u5408\u76f8\u5173\u4e0a\u4e0b\u6587\uff1b\u6700\u540e\uff0c\u5f15\u5165ColorShift\u6b63\u5219\u5316\u6765\u786e\u4fdd\u5916\u89c2\u771f\u5b9e\u3002", "result": "\u5728\u516c\u5171\u57fa\u51c6\u6d4b\u8bd5\u4e0a\uff0cDeshadowMamba\u5c55\u793a\u4e86\u76ee\u524d\u6700\u597d\u7684\u89c6\u89c9\u8d28\u91cf\u548c\u5f3a\u5927\u7684\u91cf\u5316\u6027\u80fd\u3002", "conclusion": "\u901a\u8fc7\u5e8f\u5217\u5efa\u6a21\u548c\u521b\u65b0\u7684\u65b9\u6cd5\uff0c\u6709\u6548\u63d0\u9ad8\u4e86\u56fe\u50cf\u53bb\u9634\u5f71\u7684\u51c6\u786e\u6027\u548c\u89c6\u89c9\u8d28\u91cf\uff0c\u4e3a\u56fe\u50cf\u5904\u7406\u9886\u57df\u63d0\u4f9b\u4e86\u4e00\u4e2a\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.23794", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.23794", "abs": "https://arxiv.org/abs/2510.23794", "authors": ["Jun Liu", "Tao Zhou", "Jiarui Li", "Xiaohui Zhong", "Peng Zhang", "Jie Feng", "Lei Chen", "Hao Li"], "title": "Revealing the Potential of Learnable Perturbation Ensemble Forecast Model for Tropical Cyclone Prediction", "comment": "30 pages, 21 figures, 1 table", "summary": "Tropical cyclones (TCs) are highly destructive and inherently uncertain\nweather systems. Ensemble forecasting helps quantify these uncertainties, yet\ntraditional systems are constrained by high computational costs and limited\ncapability to fully represent atmospheric nonlinearity. FuXi-ENS introduces a\nlearnable perturbation scheme for ensemble generation, representing a novel\nAI-based forecasting paradigm. Here, we systematically compare FuXi-ENS with\nECMWF-ENS using all 90 global TCs in 2018, examining their performance in\nTC-related physical variables, track and intensity forecasts, and the\nassociated dynamical and thermodynamical fields. FuXi-ENS demonstrates clear\nadvantages in predicting TC-related physical variables, and achieves more\naccurate track forecasts with reduced ensemble spread, though it still\nunderestimates intensity relative to observations. Further dynamical and\nthermodynamical analyses reveal that FuXi-ENS better captures large-scale\ncirculation, with moisture turbulent energy more tightly concentrated around\nthe TC warm core, whereas ECMWF-ENS exhibits a more dispersed distribution.\nThese findings highlight the potential of learnable perturbations to improve TC\nforecasting skill and provide valuable insights for advancing AI-based ensemble\nprediction of extreme weather events that have significant societal impacts.", "AI": {"tldr": "FuXi-ENS, a learnable perturbation scheme for ensemble generation, shows clear advantages in predicting tropical cyclone-related physical variables and track forecasts compared to ECMWF-ENS, while also better capturing large-scale circulation and moisture turbulent energy distribution.", "motivation": "\u5f53\u524d\u7684\u96c6\u5408\u9884\u62a5\u7cfb\u7edf\u53d7\u5236\u4e8e\u9ad8\u8ba1\u7b97\u6210\u672c\u548c\u6709\u9650\u7684\u80fd\u529b\u6765\u5145\u5206\u8868\u793a\u5927\u6c14\u975e\u7ebf\u6027\u3002FuXi-ENS\u4f5c\u4e3a\u4e00\u79cd\u65b0\u578b\u7684AI\u57fa\u4e8e\u9884\u62a5\u8303\u5f0f\uff0c\u65e8\u5728\u901a\u8fc7\u53ef\u5b66\u4e60\u7684\u6270\u52a8\u65b9\u6848\u6765\u6539\u8fdb\u70ed\u5e26\u6c14\u65cb\u7684\u9884\u62a5\u6027\u80fd, \u5b83\u7684\u5e94\u7528\u6709\u52a9\u4e8e\u63d0\u9ad8\u6781\u7aef\u5929\u6c14\u4e8b\u4ef6\u7684\u793e\u4f1a\u5f71\u54cd\u9884\u62a5\u6280\u5de7.", "method": "\u8be5\u7814\u7a76\u7cfb\u7edf\u5730\u6bd4\u8f83\u4e86FuXi-ENS\u548cECMWF-ENS\u57282018\u5e74\u5168\u740390\u4e2a\u70ed\u5e26\u6c14\u65cb\u4e2d\u7684\u8868\u73b0\uff0c\u7279\u522b\u662f\u5728\u70ed\u5e26\u6c14\u65cb\u76f8\u5173\u7684\u7269\u7406\u53d8\u91cf\u3001\u8def\u5f84\u548c\u5f3a\u5ea6\u9884\u6d4b\uff0c\u4ee5\u53ca\u76f8\u5173\u7684\u52a8\u529b\u548c\u70ed\u529b\u573a.", "result": "FuXi-ENS\u5728\u9884\u6d4b\u70ed\u5e26\u6c14\u65cb\u76f8\u5173\u7269\u7406\u53d8\u91cf\u548c\u8def\u5f84\u9884\u6d4b\u65b9\u9762\u6709\u660e\u663e\u7684\u4f18\u52bf\uff0c\u5c3d\u7ba1\u5728\u5f3a\u5ea6\u9884\u6d4b\u65b9\u9762\u4ecd\u4f4e\u4e8e\u89c2\u6d4b\u503c\u3002\u52a8\u529b\u548c\u70ed\u529b\u5206\u6790\u8fdb\u4e00\u6b65\u63ed\u793a\uff0cFuXi-ENS\u66f4\u597d\u5730\u6355\u6349\u4e86\u5927\u5c3a\u5ea6\u73af\u6d41\uff0c\u5e76\u4e14\u5c06\u4e0e\u70ed\u5e26\u6c14\u65cb\u6696\u6838\u5fc3\u76f8\u5173\u7684\u6e7f\u6da6\u6e4d\u6d41\u80fd\u91cf\u66f4\u96c6\u4e2d\u5730\u805a\u96c6\u5728\u4e00\u8d77\uff0c\u800cECMWF-ENS\u5219\u663e\u793a\u51fa\u66f4\u5206\u6563\u7684\u5206\u5e03.", "conclusion": "\u8fd9\u9879\u7814\u7a76\u5f3a\u8c03\u4e86\u53ef\u5b66\u4e60\u6270\u52a8\u65b9\u6848\u5728\u63d0\u9ad8\u70ed\u5e26\u6c14\u65cb\u9884\u62a5\u6280\u5de7\u65b9\u9762\u7684\u6f5c\u5728\u4f5c\u7528\uff0c\u5e76\u4e3a\u901a\u8fc7AI\u65b9\u6cd5\u63d0\u9ad8\u6781\u7aef\u5929\u6c14\u4e8b\u4ef6\u9884\u6d4b\u7684\u793e\u4f1a\u5f71\u54cd\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u89c1\u89e3."}}
{"id": "2510.23802", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.23802", "abs": "https://arxiv.org/abs/2510.23802", "authors": ["Nathan Paek", "Yongyi Zang", "Qihui Yang", "Randal Leistikow"], "title": "Learning Interpretable Features in Audio Latent Spaces via Sparse Autoencoders", "comment": "Accepted to NeurIPS 2025 Mechanistic Interpretability Workshop", "summary": "While sparse autoencoders (SAEs) successfully extract interpretable features\nfrom language models, applying them to audio generation faces unique\nchallenges: audio's dense nature requires compression that obscures semantic\nmeaning, and automatic feature characterization remains limited. We propose a\nframework for interpreting audio generative models by mapping their latent\nrepresentations to human-interpretable acoustic concepts. We train SAEs on\naudio autoencoder latents, then learn linear mappings from SAE features to\ndiscretized acoustic properties (pitch, amplitude, and timbre). This enables\nboth controllable manipulation and analysis of the AI music generation process,\nrevealing how acoustic properties emerge during synthesis. We validate our\napproach on continuous (DiffRhythm-VAE) and discrete (EnCodec, WavTokenizer)\naudio latent spaces, and analyze DiffRhythm, a state-of-the-art text-to-music\nmodel, to demonstrate how pitch, timbre, and loudness evolve throughout\ngeneration. While our work is only done on audio modality, our framework can be\nextended to interpretable analysis of visual latent space generation models.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u97f3\u9891\u751f\u6210\u6a21\u578b\u7684\u6f5c\u5728\u8868\u793a\u6620\u5c04\u5230\u4eba\u7c7b\u53ef\u7406\u89e3\u7684\u58f0\u5b66\u6982\u5ff5\u6765\u89e3\u91ca\u8fd9\u4e9b\u6a21\u578b\u3002\u8be5\u65b9\u6cd5\u4f9d\u8d56\u4e8e\u7a00\u758f\u81ea\u7f16\u7801\u5668\uff0c\u65e8\u5728\u4f7f\u97f3\u9891\u751f\u6210\u8fc7\u7a0b\u4e2d\u7684\u58f0\u5b66\u5c5e\u6027\uff08\u5982\u97f3\u9ad8\u3001\u54cd\u5ea6\u548c\u97f3\u8272\uff09\u66f4\u5bb9\u6613\u88ab\u63a7\u5236\u548c\u5206\u6790\u3002\u8be5\u6846\u67b6\u88ab\u9a8c\u8bc1\u5728\u8fde\u7eed\u548c\u79bb\u6563\u7684\u97f3\u9891\u6f5c\u5728\u7a7a\u95f4\u4e0a\u90fd\u6709\u6548\u679c\uff0c\u5e76\u4e14\u53ef\u4ee5\u6269\u5c55\u5230\u53ef\u89c6\u5316\u7684\u6f5c\u5728\u7a7a\u95f4\u751f\u6210\u6a21\u578b\u7684\u89e3\u91ca\u6027\u5206\u6790\u4e2d\u3002", "motivation": "\u97f3\u9891\u7684\u5bc6\u96c6\u6027\u8d28\u4f7f\u5f97\u81ea\u52a8\u7279\u5f81\u523b\u753b\u53d8\u5f97\u56f0\u96be\uff0c\u5e76\u4e14\u5e94\u7528\u7a00\u758f\u81ea\u7f16\u7801\u5668\u5230\u97f3\u9891\u751f\u6210\u65f6\u4f1a\u9762\u4e34\u6311\u6218\u3002\u8be5\u7814\u7a76\u76ee\u7684\u662f\u5c06\u97f3\u9891\u751f\u6210\u6a21\u578b\u7684\u6f5c\u5728\u8868\u793a\u6620\u5c04\u5230\u4eba\u7c7b\u53ef\u7406\u89e3\u7684\u58f0\u5b66\u6982\u5ff5\u4e0a\uff0c\u4ece\u800c\u4f7f\u5f97\u97f3\u9891\u751f\u6210\u8fd9\u4e00\u8fc7\u7a0b\u4e2d\u7684\u58f0\u5b66\u5c5e\u6027\u66f4\u5bb9\u6613\u88ab\u7406\u89e3\u548c\u63a7\u5236\u3002", "method": "\u8bad\u7ec3\u7a00\u758f\u81ea\u7f16\u7801\u5668\u6765\u5904\u7406\u97f3\u9891\u81ea\u52a8\u7f16\u7801\u5668\u7684\u6f5c\u5728\u8868\u793a\uff0c\u7136\u540e\u5b66\u4e60\u4ece\u7a00\u758f\u81ea\u7f16\u7801\u5668\u7279\u5f81\u5230\u79bb\u6563\u5316\u58f0\u5b66\u5c5e\u6027\uff08\u97f3\u9ad8\u3001\u632f\u5e45\u3001\u97f3\u8272\uff09\u7684\u7ebf\u6027\u6620\u5c04\uff0c\u4ece\u800c\u5b9e\u73b0\u53ef\u63a7\u7684\u64cd\u63a7\u53ca\u5206\u6790\u8fc7\u7a0b\u3002\u8fd9\u4e2a\u65b9\u6cd5\u9a8c\u8bc1\u4e86\u5728\u8fde\u7eed\u548c\u79bb\u6563\u97f3\u9891\u6f5c\u5728\u7a7a\u95f4\u4e2d\u7684\u6709\u6548\u6027\uff0c\u5e76\u4e14\u5c55\u793a\u4e86\u5982\u4f55\u5206\u6790\u5176\u4e2d\u58f0\u5b66\u5c5e\u6027\u7684\u751f\u6210\u8fc7\u7a0b\u3002\u6b64\u5916\uff0c\u8fd9\u4e2a\u6846\u67b6\u8fd8\u53ef\u4ee5\u88ab\u6269\u5c55\u5230\u53ef\u89c6\u5316\u6f5c\u5728\u7a7a\u95f4\u751f\u6210\u6a21\u578b\u7684\u89e3\u91ca\u6027\u5206\u6790\u5f53\u4e2d\u3002", "result": "\u8be5\u7814\u7a76\u6210\u529f\u5b9e\u73b0\u4e86\u5c06\u97f3\u9891\u81ea\u7f16\u7801\u5668\u6f5c\u5728\u8868\u793a\u6620\u5c04\u5230\u4eba\u7c7b\u53ef\u7406\u89e3\u7684\u58f0\u5b66\u6982\u5ff5\u4e0a\u7684\u76ee\u6807\uff0c\u63ed\u793a\u4e86\u97f3\u9ad8\u7b49\u5c5e\u6027\u5728\u97f3\u751f\u6210\u8fc7\u7a0b\u4e2d\u7684\u6f14\u5316\uff0c\u4ece\u800c\u63d0\u9ad8\u4e86\u97f3\u9891\u751f\u6210\u6a21\u578b\u7684\u53ef\u89e3\u91ca\u6027\u548c\u53ef\u63a7\u6027\u3002", "conclusion": "\u8fd9\u9879\u7814\u7a76\u63d0\u51fa\u7684\u4e00\u4e2a\u6846\u67b6\uff0c\u80fd\u591f\u4f7f\u97f3\u9891\u751f\u6210\u6a21\u578b\u4e2d\u7684\u58f0\u5b66\u5c5e\u6027\u53d8\u5f97\u6613\u4e8e\u4eba\u7406\u89e3\u3002\u8fd9\u4e00\u65b9\u6cd5\u9a8c\u8bc1\u4e86\u5728\u8fde\u7eed\u548c\u79bb\u6563\u97f3\u9891\u6f5c\u5728\u7a7a\u95f4\u4e0a\u7684\u6709\u6548\u6027\uff0c\u5e76\u4e14\u663e\u793a\u51fa\u53ef\u4ee5\u5728\u53ef\u89c6\u5316\u7684\u6f5c\u5728\u7a7a\u95f4\u751f\u6210\u6a21\u578b\u7684\u89e3\u91ca\u6027\u5206\u6790\u4e2d\u5e94\u7528\u3002"}}
{"id": "2510.24278", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24278", "abs": "https://arxiv.org/abs/2510.24278", "authors": ["Pietro Bongini", "Valentina Molinari", "Andrea Costanzo", "Benedetta Tondi", "Mauro Barni"], "title": "Training-free Source Attribution of AI-generated Images via Resynthesis", "comment": "14 pages, 4 figures, 1 table, accepted at \"The 17th IEEE\n  INTERNATIONAL WORKSHOP ON INFORMATION FORENSICS AND SECURITY (WIFS2025)\",\n  Perth, Australia", "summary": "Synthetic image source attribution is a challenging task, especially in data\nscarcity conditions requiring few-shot or zero-shot classification\ncapabilities. We present a new training-free one-shot attribution method based\non image resynthesis. A prompt describing the image under analysis is\ngenerated, then it is used to resynthesize the image with all the candidate\nsources. The image is attributed to the model which produced the resynthesis\nclosest to the original image in a proper feature space. We also introduce a\nnew dataset for synthetic image attribution consisting of face images from\ncommercial and open-source text-to-image generators. The dataset provides a\nchallenging attribution framework, useful for developing new attribution models\nand testing their capabilities on different generative architectures. The\ndataset structure allows to test approaches based on resynthesis and to compare\nthem to few-shot methods. Results from state-of-the-art few-shot approaches and\nother baselines show that the proposed resynthesis method outperforms existing\ntechniques when only a few samples are available for training or fine-tuning.\nThe experiments also demonstrate that the new dataset is a challenging one and\nrepresents a valuable benchmark for developing and evaluating future few-shot\nand zero-shot methods.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u4e00\u6b21\u6027\u5f52\u56e0\u65b9\u6cd5\uff0c\u5229\u7528\u56fe\u50cf\u518d\u5408\u6210\u6280\u672f\u5bf9\u5408\u6210\u56fe\u50cf\u6765\u6e90\u8fdb\u884c\u5f52\u56e0\u3002\u8be5\u65b9\u6cd5\u5bf9\u56fe\u50cf\u63cf\u8ff0\u751f\u6210\u63d0\u793a\uff0c\u7136\u540e\u4f7f\u7528\u8be5\u63d0\u793a\u91cd\u65b0\u5408\u6210\u56fe\u50cf\uff0c\u5e76\u5c06\u56fe\u50cf\u5f52\u56e0\u4e8e\u751f\u6210\u7684\u91cd\u6784\u56fe\u50cf\u4e0e\u539f\u59cb\u56fe\u50cf\u6700\u63a5\u8fd1\u7684\u6a21\u578b\u3002", "motivation": "\u5728\u6570\u636e\u7a00\u7f3a\u6761\u4ef6\u4e0b\uff0c\u5b9e\u73b0\u5408\u6210\u56fe\u50cf\u6e90\u7684\u4e00\u6b21\u6027\u5f52\u56e0\u662f\u4e00\u4e2a\u6311\u6218\u3002\u76ee\u524d\u7684\u65b9\u6cd5\u5f80\u5f80\u9700\u8981\u5927\u91cf\u7684\u8bad\u7ec3\u6837\u672c\uff0c\u8fd9\u5728\u6570\u636e\u7a00\u7f3a\u7684\u6761\u4ef6\u4e0b\u96be\u4ee5\u5b9e\u73b0\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u56fe\u50cf\u518d\u5408\u6210\u7684\u8bad\u7ec3\u81ea\u7531\u7684\u4e00\u6b21\u6027\u5f52\u56e0\u65b9\u6cd5\u3002\u751f\u6210\u56fe\u50cf\u63cf\u8ff0\u7684\u63d0\u793a\uff0c\u7136\u540e\u4f7f\u7528\u8be5\u63d0\u793a\u5bf9\u6240\u6709\u5019\u9009\u6e90\u91cd\u65b0\u5408\u6210\u56fe\u50cf\uff0c\u6700\u540e\u5f52\u56e0\u4e8e\u91cd\u6784\u7ed3\u679c\u4e0e\u539f\u59cb\u56fe\u50cf\u6700\u63a5\u8fd1\u7684\u6a21\u578b\u3002\u540c\u65f6\u5f15\u5165\u4e86\u5305\u542b\u5546\u4e1a\u548c\u5f00\u6e90\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u5668\u7684\u5408\u6210\u56fe\u50cf\u6570\u636e\u96c6\u3002\u8be5\u6570\u636e\u96c6\u4e3a\u5f00\u53d1\u65b0\u7684\u5f52\u56e0\u6a21\u578b\u548c\u6d4b\u8bd5\u5176\u5728\u4e0d\u540c\u751f\u6210\u67b6\u6784\u4e0b\u7684\u80fd\u529b\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6311\u6218\u6027\u7684\u6846\u67b6", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u53ea\u6709\u5c11\u91cf\u6837\u672c\u7528\u4e8e\u8bad\u7ec3\u6216\u5fae\u8c03\u7684\u60c5\u51b5\u4e0b\u4f18\u4e8e\u73b0\u6709\u7684\u5f52\u56e0\u6280\u672f\u3002\u6b64\u5916\uff0c\u65b0\u6570\u636e\u96c6\u662f\u4e00\u4e2a\u5177\u6709\u6311\u6218\u6027\u7684\u57fa\u51c6\uff0c\u53ef\u7528\u4e8e\u5f00\u53d1\u548c\u8bc4\u4ef7\u672a\u6765\u7684\u5c11\u6837\u672c\u548c\u96f6\u6837\u672c\u65b9\u6cd5", "conclusion": "\u8be5\u65b9\u6cd5\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u4e14\u65e0\u9700\u8bad\u7ec3\u7684\u4e00\u6b21\u6027\u5f52\u56e0\u624b\u6bb5\uff0c\u9488\u5bf9\u6570\u636e\u7a00\u7f3a\u548c\u6a21\u578b\u6cdb\u5316\u80fd\u529b\u7684\u6311\u6218\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6709\u4ef7\u503c\u7684\u89e3\u51b3\u65b9\u6848\u3002\u540c\u65f6\u65b0\u6570\u636e\u96c6\u4e3a\u672a\u6765\u7684\u7814\u7a76\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6709\u4ef7\u503c\u7684\u57fa\u51c6\u3002"}}
{"id": "2510.24461", "categories": ["cs.AI", "cs.RO"], "pdf": "https://arxiv.org/pdf/2510.24461", "abs": "https://arxiv.org/abs/2510.24461", "authors": ["Korneel Van den Berghe", "Stein Stroobants", "Vijay Janapa Reddi", "G. C. H. E. de Croon"], "title": "Adaptive Surrogate Gradients for Sequential Reinforcement Learning in Spiking Neural Networks", "comment": null, "summary": "Neuromorphic computing systems are set to revolutionize energy-constrained\nrobotics by achieving orders-of-magnitude efficiency gains, while enabling\nnative temporal processing. Spiking Neural Networks (SNNs) represent a\npromising algorithmic approach for these systems, yet their application to\ncomplex control tasks faces two critical challenges: (1) the non-differentiable\nnature of spiking neurons necessitates surrogate gradients with unclear\noptimization properties, and (2) the stateful dynamics of SNNs require training\non sequences, which in reinforcement learning (RL) is hindered by limited\nsequence lengths during early training, preventing the network from bridging\nits warm-up period.\n  We address these challenges by systematically analyzing surrogate gradient\nslope settings, showing that shallower slopes increase gradient magnitude in\ndeeper layers but reduce alignment with true gradients. In supervised learning,\nwe find no clear preference for fixed or scheduled slopes. The effect is much\nmore pronounced in RL settings, where shallower slopes or scheduled slopes lead\nto a 2.1x improvement in both training and final deployed performance. Next, we\npropose a novel training approach that leverages a privileged guiding policy to\nbootstrap the learning process, while still exploiting online environment\ninteractions with the spiking policy. Combining our method with an adaptive\nslope schedule for a real-world drone position control task, we achieve an\naverage return of 400 points, substantially outperforming prior techniques,\nincluding Behavioral Cloning and TD3BC, which achieve at most --200 points\nunder the same conditions. This work advances both the theoretical\nunderstanding of surrogate gradient learning in SNNs and practical training\nmethodologies for neuromorphic controllers demonstrated in real-world robotic\nsystems.", "AI": {"tldr": "\u672c\u6587\u89e3\u51b3\u4e86SNN\u5728\u5f3a\u5316\u5b66\u4e60\u5e94\u7528\u4e2d\u7684\u4e24\u4e2a\u5173\u952e\u6311\u6218\uff1a\u4f18\u5316\u6548\u679c\u4e0d\u4f73\u7684\u95ee\u9898\u548c\u5e8f\u5217\u8bad\u7ec3\u65e9\u671f\u957f\u5ea6\u53d7\u9650\u7684\u95ee\u9898\u3002\u901a\u8fc7\u5206\u6790\u66ff\u4ee3\u68af\u5ea6\u659c\u7387\u8bbe\u7f6e\uff0c\u53d1\u73b0\u8f83\u6d45\u7684\u659c\u7387\u4f1a\u5728\u5f3a\u5316\u5b66\u4e60\u4e2d\u663e\u8457\u63d0\u9ad8\u8bad\u7ec3\u548c\u6700\u7ec8\u90e8\u7f72\u6027\u80fd\u3002\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u8bad\u7ec3\u65b9\u6cd5\uff0c\u5229\u7528\u7279\u6743\u7b56\u7565\u5f15\u5bfc\u5b66\u4e60\u8fc7\u7a0b\uff0c\u7ed3\u5408\u81ea\u9002\u5e94\u659c\u7387\u8c03\u5ea6\uff0c\u5728\u73b0\u5b9e\u4e16\u754c\u4e2d\u7684\u65e0\u4eba\u673a\u4f4d\u7f6e\u63a7\u5236\u4efb\u52a1\u4e2d\u8fbe\u5230\u4e86400\u7684\u5e73\u5747\u56de\u62a5\uff0c\u5927\u5e45\u4f18\u4e8e\u524d\u4eba\u6280\u672f\u3002", "motivation": "\u672c\u6587\u7684\u7814\u7a76\u52a8\u673a\u662f\u89e3\u51b3SNN\u5728\u6267\u884c\u590d\u6742\u63a7\u5236\u4efb\u52a1\u65f6\u9762\u4e34\u7684\u4e24\u4e2a\u4e3b\u8981\u95ee\u9898\uff1a\u975e\u53ef\u5fae\u7684\u7279\u6027\u8981\u6c42\u4f7f\u7528\u66ff\u4ee3\u68af\u5ea6\uff0c\u4f46\u5176\u4f18\u5316\u6027\u80fd\u5c1a\u672a\u6e05\u6670\uff1bSNN\u7684\u72b6\u6001\u52a8\u6001\u9700\u8981\u5e8f\u5217\u8bad\u7ec3\uff0c\u4f46\u5728\u5f3a\u5316\u5b66\u4e60\u4e2d\u65e9\u671f\u8bad\u7ec3\u65f6\u5176\u5e8f\u5217\u957f\u5ea6\u53d7\u9650\uff0c\u8fd9\u4e24\u70b9\u963b\u788d\u4e86SNN\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u6027\u80fd\u8868\u73b0\u548c\u53d1\u5c55\u3002", "method": "\u672c\u6587\u7684\u65b9\u6cd5\u5305\u62ec\uff1a1.\u7cfb\u7edf\u5730\u5206\u6790\u4e86\u66ff\u4ee3\u68af\u5ea6\u659c\u7387\u8bbe\u7f6e\u5bf9\u68af\u5ea6\u5927\u5c0f\u548c\u4e0e\u771f\u5b9e\u68af\u5ea6\u7684\u5bf9\u9f50\u5ea6\u7684\u5f71\u54cd\u30022. \u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u8bad\u7ec3\u65b9\u5f0f\uff0c\u8be5\u65b9\u5f0f\u5229\u7528\u4e00\u4e2a\u7279\u6743\u6307\u5bfc\u653f\u7b56\u6765\u542f\u52a8\u5b66\u4e60\u8fc7\u7a0b\uff0c\u540c\u65f6\u5229\u7528\u5728\u7ebf\u73af\u5883\u4ea4\u4e92\u8fdb\u884c\u5b66\u4e60\u30023.\u91c7\u7528\u4e86\u81ea\u9002\u5e94\u659c\u7387\u8c03\u5ea6\u7b56\u7565\u3002\u8fd9\u4e9b\u65b9\u6cd5\u88ab\u5e94\u7528\u4e8e\u89e3\u51b3\u65e0\u4eba\u673a\u4f4d\u7f6e\u63a7\u5236\u95ee\u9898\uff0c\u53d6\u5f97\u4e86\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u5728RTS\u65e0\u4eba\u673a\u4f4d\u7f6e\u63a7\u5236\u4efb\u52a1\u4e2d\uff0c\u672c\u6587\u7684\u65b9\u6cd5\u8fbe\u5230\u4e86400\u7684\u5e73\u5747\u56de\u62a5\uff0c\u8fd9\u4e2a\u7ed3\u679c\u5927\u5e45\u8d85\u8d8a\u4e86\u5148\u524d\u7684\u65b9\u6cd5\uff0c\u5305\u62ec\u884c\u4e3a\u514b\u9686\u548cTD3BC\uff0c\u8fd9\u4e9b\u65b9\u6cd5\u5728\u8fd9\u79cd\u6761\u4ef6\u4e0b\u6700\u591a\u53ea\u80fd\u8fbe\u5230-200\u7684\u56de\u62a5\u3002", "conclusion": "\u672c\u6587\u5de5\u4f5c\u6df1\u5316\u4e86\u5bf9SNN\u4e2d\u66ff\u4ee3\u68af\u5ea6\u5b66\u4e60\u7684\u7406\u8bba\u7406\u89e3\uff0c\u5e76\u5c55\u793a\u4e86\u5728\u5b9e\u9645\u673a\u5668\u4eba\u7cfb\u7edf\u4e2d\u7528\u4e8e\u795e\u7ecf\u5f62\u6001\u63a7\u5236\u5668\u7684\u5b9e\u7528\u8bad\u7ec3\u65b9\u6cd5\u3002\u65b0\u6280\u672f\u4e0d\u4ec5\u5728\u7406\u8bba\u4e0a\u63a8\u8fdb\u4e86SNN\u7814\u7a76\u7684\u8fdb\u6b65\uff0c\u4e5f\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u8bc1\u660e\u4e86\u5176\u4f18\u8d8a\u7684\u6027\u80fd\u3002"}}
{"id": "2510.23804", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.23804", "abs": "https://arxiv.org/abs/2510.23804", "authors": ["Adela DePavia", "Vasileios Charisopoulos", "Rebecca Willett"], "title": "How do simple rotations affect the implicit bias of Adam?", "comment": null, "summary": "Adaptive gradient methods such as Adam and Adagrad are widely used in machine\nlearning, yet their effect on the generalization of learned models -- relative\nto methods like gradient descent -- remains poorly understood. Prior work on\nbinary classification suggests that Adam exhibits a ``richness bias,'' which\ncan help it learn nonlinear decision boundaries closer to the Bayes-optimal\ndecision boundary relative to gradient descent. However, the coordinate-wise\npreconditioning scheme employed by Adam renders the overall method sensitive to\northogonal transformations of feature space. We show that this sensitivity can\nmanifest as a reversal of Adam's competitive advantage: even small rotations of\nthe underlying data distribution can make Adam forfeit its richness bias and\nconverge to a linear decision boundary that is farther from the Bayes-optimal\ndecision boundary than the one learned by gradient descent. To alleviate this\nissue, we show that a recently proposed reparameterization method -- which\napplies an orthogonal transformation to the optimization objective -- endows\nany first-order method with equivariance to data rotations, and we empirically\ndemonstrate its ability to restore Adam's bias towards rich decision\nboundaries.", "AI": {"tldr": "\u81ea\u9002\u5e94\u68af\u5ea6\u6cd5\u5728\u673a\u5668\u5b66\u4e60\u4e2d\u5e7f\u6cdb\u5e94\u7528\uff0c\u4f46\u5176\u5bf9\u6240\u5b66\u4e60\u6a21\u578b\u6cdb\u5316\u6027\u80fd\u7684\u5f71\u54cd\u4ecd\u4e0d\u6e05\u695a\u3002\u4e0e\u68af\u5ea6\u4e0b\u964d\u76f8\u6bd4\uff0c\u81ea\u9002\u5e94\u68af\u5ea6\u6cd5\u5b58\u5728\u4e30\u5bcc\u7684\u504f\u5dee\uff0c\u53ef\u4ee5\u5e2e\u52a9\u6a21\u578b\u5b66\u4e60\u66f4\u63a5\u8fd1\u8d1d\u53f6\u65af\u6700\u4f18\u51b3\u7b56\u8fb9\u754c\u7684\u975e\u7ebf\u6027\u51b3\u7b56\u8fb9\u754c\u3002\u4f46\u8fd9\u79cd\u65b9\u6cd5\u5bf9\u7279\u5f81\u7a7a\u95f4\u7684\u6b63\u4ea4\u53d8\u6362\u654f\u611f\uff0c\u53ef\u4ee5\u901a\u8fc7\u6b63\u4ea4\u53d8\u6362\u6765\u6062\u590d\u8be5\u65b9\u6cd5\u5bf9\u4e30\u5bcc\u51b3\u7b56\u8fb9\u754c\u504f\u5411\u3002", "motivation": "\u7814\u7a76\u81ea\u9002\u5e94\u68af\u5ea6\u6cd5\uff08\u5982Adam\u548cAdagrad\uff09\u5bf9\u5b66\u4e60\u5230\u7684\u6a21\u578b\u6cdb\u5316\u6027\u80fd\u7684\u5f71\u54cd\uff0c\u5bf9\u6bd4\u5176\u4e0e\u5e38\u89c4\u68af\u5ea6\u4e0b\u964d\u65b9\u6cd5\u7684\u8868\u73b0\u5dee\u5f02\uff0c\u63a2\u7d22\u5982\u4f55\u7f13\u89e3Adam\u65b9\u6cd5\u5bf9\u7279\u5f81\u7a7a\u95f4\u6b63\u4ea4\u53d8\u6362\u654f\u611f\u7684\u95ee\u9898\u53ca\u7ed9\u51fa\u89e3\u51b3\u65b9\u6848\u4ee5\u4fdd\u7559\u5176\u4f18\u70b9\u3002", "method": "\u9488\u5bf9\u81ea\u9002\u5e94\u68af\u5ea6\u6cd5\u5b58\u5728\u7684\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u53c2\u6570\u5316\u65b9\u6cd5\uff0c\u5bf9\u4f18\u5316\u76ee\u6807\u5e94\u7528\u6b63\u4ea4\u53d8\u6362\uff0c\u4f7f\u5176\u5bf9\u6570\u636e\u65cb\u8f6c\u5177\u6709\u7b49\u53d8\u6027\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u63d0\u51fa\u7684\u53c2\u6570\u5316\u65b9\u6cd5\u80fd\u591f\u6062\u590dAdam\u65b9\u6cd5\u5bf9\u4e30\u5bcc\u51b3\u7b56\u8fb9\u754c\u7684\u504f\u5411\uff0c\u6539\u5584\u5176\u5b66\u4e60\u66f2\u7ebf\u53ca\u6700\u7ec8\u6a21\u578b\u7684\u51b3\u7b56\u8fb9\u754c\u3002", "conclusion": "\u81ea\u9002\u5e94\u68af\u5ea6\u65b9\u6cd5\u5728\u5b66\u4e60\u8fc7\u7a0b\u4e2d\u5c55\u73b0\u51fa\u503e\u5411\u4e8e\u590d\u6742\u5ea6\u8f83\u9ad8\u7684\u51b3\u7b56\u8fb9\u754c\u7684\u7279\u70b9\uff0c\u4f46\u5176\u5bf9\u7279\u5f81\u7a7a\u95f4\u7684\u65cb\u8f6c\u662f\u8106\u5f31\u7684\u3002\u901a\u8fc7\u5f15\u5165\u6b63\u4ea4\u53d8\u6362\u53ef\u4ee5\u589e\u5f3a\u5176\u9c81\u68d2\u6027\u540c\u65f6\u4fdd\u7559\u5176\u539f\u672c\u7684\u4f18\u70b9\u3002"}}
{"id": "2510.24285", "categories": ["cs.CV", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.24285", "abs": "https://arxiv.org/abs/2510.24285", "authors": ["Juntian Zhang", "Song Jin", "Chuanqi Cheng", "Yuhan Liu", "Yankai Lin", "Xun Zhang", "Yufei Zhang", "Fei Jiang", "Guojun Yin", "Wei Lin", "Rui Yan"], "title": "ViPER: Empowering the Self-Evolution of Visual Perception Abilities in Vision-Language Model", "comment": null, "summary": "The limited capacity for fine-grained visual perception presents a critical\nbottleneck for Vision-Language Models (VLMs) in real-world applications.\nAddressing this is challenging due to the scarcity of high-quality data and the\nlimitations of existing methods: supervised fine-tuning (SFT) often compromises\ngeneral capabilities, while reinforcement fine-tuning (RFT) prioritizes textual\nreasoning over visual perception. To bridge this gap, we propose a novel\ntwo-stage task that structures visual perception learning as a coarse-to-fine\nprogressive process. Based on this task formulation, we develop ViPER, a\nself-bootstrapping framework specifically designed to enable iterative\nevolution through self-critiquing and self-prediction. By synergistically\nintegrating image-level and instance-level reconstruction with a two-stage\nreinforcement learning strategy, ViPER establishes a closed-loop training\nparadigm, where internally synthesized data directly fuel the enhancement of\nperceptual ability. Applied to the Qwen2.5-VL family, ViPER produces the\nQwen-Viper series. With an average gain of 1.7% on seven comprehensive\nbenchmarks spanning various tasks and up to 6.0% on fine-grained perception,\nQwen-Viper consistently demonstrates superior performance across different\nvision-language scenarios while maintaining generalizability. Beyond enabling\nself-improvement in perceptual capabilities, ViPER provides concrete evidence\nfor the reciprocal relationship between generation and understanding, a\nbreakthrough to developing more autonomous and capable VLMs.", "AI": {"tldr": "ViPER\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u4efb\u52a1\u7ed3\u6784\uff0c\u7528\u4e8e\u589e\u5f3a\u89c6\u89c9\u611f\u77e5\u5b66\u4e60\uff0c\u5e76\u901a\u8fc7\u81ea\u4e3e\u6846\u67b6\u548c\u5faa\u73af\u8bad\u7ec3\u7b56\u7565\u6765\u63d0\u5347\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u80fd\u529b\uff0c\u63d0\u9ad8\u4e86\u591a\u4e2a\u4efb\u52a1\u7684\u6027\u80fd\uff0c\u5c24\u5176\u662f\u7cbe\u7ec6\u89c6\u89c9\u611f\u77e5\u4efb\u52a1\u4e0a\u8868\u73b0\u7a81\u51fa\u3002", "motivation": "\u73b0\u6709\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u771f\u5b9e\u5e94\u7528\u573a\u666f\u4e2d\u9762\u4e34\u89c6\u89c9\u611f\u77e5\u6027\u80fd\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u4e3b\u8981\u662f\u56e0\u4e3a\u9ad8\u8d28\u91cf\u6570\u636e\u7a00\u7f3a\u548c\u73b0\u6709\u65b9\u6cd5\uff08\u76d1\u7763\u5fae\u8c03\u548c\u5f3a\u5316\u5b66\u4e60\u5fae\u8c03\uff09\u5728\u89c6\u89c9\u611f\u77e5\u548c\u6587\u5b57\u63a8\u7406\u4e4b\u95f4\u96be\u4ee5\u5e73\u8861\u3002\u4e3a\u4e86\u5e94\u5bf9\u8fd9\u4e00\u6311\u6218\uff0c\u63d0\u51fa\u4e86\u89e3\u51b3\u65b9\u6848\u3002", "method": "ViPER\u901a\u8fc7\u4e24\u7ea7\u5f3a\u5316\u5b66\u4e60\u6574\u5408\u56fe\u50cf\u7ea7\u522b\u548c\u5b9e\u4f8b\u7ea7\u522b\u7684\u91cd\u5efa\uff0c\u5e76\u5efa\u7acb\u95ed\u73af\u8bad\u7ec3\u6a21\u5f0f\uff0c\u5229\u7528\u5185\u90e8\u5408\u6210\u6570\u636e\u6765\u63d0\u5347\u611f\u77e5\u80fd\u529b\u3002\u8be5\u65b9\u6cd5\u9002\u7528\u4e8eQwen2.5-VL\u5bb6\u65cf\uff0c\u751f\u6210\u4e86Qwen-Viper\u7cfb\u5217\u3002", "result": "\u901a\u8fc7Qwen-Viper\u6a21\u578b\u5728\u4e03\u4e2a\u7efc\u5408\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u8868\u73b0\uff0c\u663e\u793a\u4e86\u5e73\u57471.7\uff05\u7684\u6027\u80fd\u63d0\u5347\uff0c\u7279\u522b\u662f\u5728\u7cbe\u7ec6\u89c6\u89c9\u611f\u77e5\u4efb\u52a1\u4e0a\u67096.0%\u7684\u63d0\u5347\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u89c6\u89c9\u8bed\u8a00\u573a\u666f\u4e2d\u7684\u4f18\u8d8a\u6027\u3002\u6b64\u5916\uff0cViPER\u8fd8\u5c55\u793a\u4e86\u751f\u6210\u4e0e\u7406\u89e3\u4e4b\u95f4\u7684\u76f8\u4e92\u4fc3\u8fdb\u5173\u7cfb\uff0c\u8fd9\u4e3a\u5f00\u53d1\u66f4\u52a0\u81ea\u4e3b\u548c\u6709\u80fd\u529b\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u63d0\u4f9b\u4e86\u4f9d\u636e\u3002", "conclusion": "ViPER\u901a\u8fc7\u81ea\u76d1\u7763\u548c\u81ea\u6211\u9884\u6d4b\u7684\u95ed\u73af\u8bad\u7ec3\u6846\u67b6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u611f\u77e5\u80fd\u529b\u3002\u8be5\u65b9\u6cd5\u4e0d\u4ec5\u63d0\u5347\u4e86\u6a21\u578b\u7684\u6027\u80fd\uff0c\u8fd8\u52a0\u6df1\u4e86\u5bf9\u751f\u6210\u4e0e\u7406\u89e3\u4e4b\u95f4\u76f8\u4e92\u5173\u7cfb\u7684\u7406\u89e3\uff0c\u4fc3\u8fdb\u4e86VLM\u6a21\u578b\u7684\u53d1\u5c55\u3002"}}
{"id": "2510.24528", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24528", "abs": "https://arxiv.org/abs/2510.24528", "authors": ["Zihan Chen", "Song Wang", "Xingbo Fu", "Chengshuai Shi", "Zhenyu Lei", "Cong Shen", "Jundong Li"], "title": "From Cross-Task Examples to In-Task Prompts: A Graph-Based Pseudo-Labeling Framework for In-context Learning", "comment": null, "summary": "The capability of in-context learning (ICL) enables large language models\n(LLMs) to perform novel tasks without parameter updates by conditioning on a\nfew input-output examples. However, collecting high-quality examples for new or\nchallenging tasks can be costly and labor-intensive. In this work, we propose a\ncost-efficient two-stage pipeline that reduces reliance on LLMs for data\nlabeling. Our approach first leverages readily available cross-task examples to\nprompt an LLM and pseudo-label a small set of target task instances. We then\nintroduce a graph-based label propagation method that spreads label information\nto the remaining target examples without additional LLM queries. The resulting\nfully pseudo-labeled dataset is used to construct in-task demonstrations for\nICL. This pipeline combines the flexibility of cross-task supervision with the\nscalability of LLM-free propagation. Experiments across five tasks demonstrate\nthat our method achieves strong performance while lowering labeling costs.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u964d\u4f4e\u6210\u672c\u7684\u4e24\u7ea7\u7ba1\u9053\uff0c\u7528\u4e8e\u51cf\u5c11\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u6570\u636e\u6807\u6ce8\u4f9d\u8d56\u6027\u3002\u8be5\u65b9\u6cd5\u9996\u5148\u5229\u7528\u8de8\u4efb\u52a1\u793a\u4f8b\u6807\u6ce8\u5c11\u91cf\u76ee\u6807\u4efb\u52a1\u5b9e\u4f8b\uff0c\u7136\u540e\u5f15\u5165\u56fe\u6807\u7b7e\u4f20\u64ad\u65b9\u6cd5\uff0c\u5b9e\u73b0\u65e0\u9700\u989d\u5916LLM\u67e5\u8be2\u7684\u6570\u636e\u6807\u6ce8\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u6b64\u65b9\u6cd5\u5728\u964d\u4f4e\u6807\u6ce8\u6210\u672c\u7684\u540c\u65f6\uff0c\u4fdd\u6301\u4e86\u826f\u597d\u7684\u6027\u80fd\u3002", "motivation": "\u6536\u96c6\u9ad8\u8d28\u91cf\u7684\u65b0\u4efb\u52a1\u6216\u6311\u6218\u6027\u4efb\u52a1\u793a\u4f8b\u65e2\u6602\u8d35\u53c8\u8017\u65f6\uff0c\u56e0\u6b64\u63d0\u51fa\u4e86\u51cf\u5c11LLM\u4f9d\u8d56\u6027\u7684\u9ad8\u6548\u6570\u636e\u6807\u6ce8\u65b9\u6cd5\uff0c\u4ee5\u964d\u4f4e\u6210\u672c\u3002", "method": "\u672c\u65b9\u6cd5\u91c7\u7528\u4e86\u4e24\u4e2a\u9636\u6bb5\uff1a\u7b2c\u4e00\u9636\u6bb5\u5229\u7528\u73b0\u6709\u8de8\u4efb\u52a1\u793a\u4f8b\u6807\u6ce8\u5c11\u91cf\u76ee\u6807\u4efb\u52a1\u5b9e\u4f8b\uff0c\u7b2c\u4e8c\u9636\u6bb5\u91c7\u7528\u56fe\u6807\u7b7e\u4f20\u64ad\u65b9\u6cd5\uff0c\u5c06\u6807\u6ce8\u4fe1\u606f\u6563\u5e03\u7ed9\u5269\u4f59\u7684\u76ee\u6807\u793a\u4f8b\uff0c\u65e0\u9700\u989d\u5916LLM\u67e5\u8be2\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u8be5\u65b9\u6cd5\u80fd\u591f\u964d\u4f4e\u6570\u636e\u6807\u6ce8\u6210\u672c\uff0c\u5e76\u4fdd\u6301\u4f18\u826f\u7684\u6027\u80fd\u3002\u5728\u4e94\u4e2a\u4efb\u52a1\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u591f\u5728\u964d\u4f4e\u6807\u6ce8\u6210\u672c\u7684\u540c\u65f6\u4fdd\u6301\u5f88\u597d\u7684\u8868\u73b0\u3002", "conclusion": "\u8be5\u7ba1\u9053\u7ed3\u5408\u4e86\u8de8\u4efb\u52a1\u76d1\u7763\u7684\u7075\u6d3b\u6027\u548c\u56fe\u6807\u7b7e\u4f20\u64ad\u7684\u5927\u89c4\u6a21\u53ef\u6269\u5c55\u6027\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u964d\u4f4e\u6210\u672c\u7684\u6807\u6ce8\u7b56\u7565\u3002"}}
{"id": "2510.23810", "categories": ["cs.LG", "math.AP", "physics.comp-ph", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.23810", "abs": "https://arxiv.org/abs/2510.23810", "authors": ["Sumanta Roy", "Bahador Bahmani", "Ioannis G. Kevrekidis", "Michael D. Shields"], "title": "A Physics-informed Multi-resolution Neural Operator", "comment": "26 pages, 14 figures, 4 tables", "summary": "The predictive accuracy of operator learning frameworks depends on the\nquality and quantity of available training data (input-output function pairs),\noften requiring substantial amounts of high-fidelity data, which can be\nchallenging to obtain in some real-world engineering applications. These\ndatasets may be unevenly discretized from one realization to another, with the\ngrid resolution varying across samples. In this study, we introduce a\nphysics-informed operator learning approach by extending the Resolution\nIndependent Neural Operator (RINO) framework to a fully data-free setup,\naddressing both challenges simultaneously. Here, the arbitrarily (but\nsufficiently finely) discretized input functions are projected onto a latent\nembedding space (i.e., a vector space of finite dimensions), using pre-trained\nbasis functions. The operator associated with the underlying partial\ndifferential equations (PDEs) is then approximated by a simple multi-layer\nperceptron (MLP), which takes as input a latent code along with spatiotemporal\ncoordinates to produce the solution in the physical space. The PDEs are\nenforced via a finite difference solver in the physical space. The validation\nand performance of the proposed method are benchmarked on several numerical\nexamples with multi-resolution data, where input functions are sampled at\nvarying resolutions, including both coarse and fine discretizations.", "AI": {"tldr": "\u4ecb\u7ecd\u4e86\u4e00\u79cd\u57fa\u4e8e\u7269\u7406\u4fe1\u606f\u7684\u7b97\u5b50\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u6269\u5c55\u5206\u8fa8\u7387\u65e0\u5173\u795e\u7ecf\u7b97\u5b50\u6846\u67b6\uff0c\u4f7f\u5176\u5728\u5b8c\u5168\u4e0d\u9700\u8981\u6570\u636e\u7684\u60c5\u51b5\u4e0b\u5de5\u4f5c\uff0c\u8be5\u65b9\u6cd5\u80fd\u591f\u5904\u7406\u4e0d\u540c\u5206\u8fa8\u7387\u7684\u6570\u636e\uff0c\u5e76\u4f7f\u7528\u591a\u5c42\u611f\u77e5\u5668\u8fdb\u884c\u7b97\u5b50\u903c\u8fd1\u3002", "motivation": "\u5728\u5b9e\u9645\u5de5\u7a0b\u5e94\u7528\u4e2d\uff0c\u9ad8\u7cbe\u5ea6\u8bad\u7ec3\u6570\u636e\u96be\u4ee5\u83b7\u53d6\uff0c\u4e14\u8bad\u7ec3\u6570\u636e\u7684\u7f51\u683c\u5206\u8fa8\u7387\u53ef\u80fd\u5728\u4e0d\u540c\u6837\u672c\u4e4b\u95f4\u5b58\u5728\u5dee\u5f02\u3002\u56e0\u6b64\uff0c\u5f00\u53d1\u65e0\u9700\u8bad\u7ec3\u6570\u636e\u5e76\u4e14\u80fd\u591f\u5904\u7406\u4e0d\u540c\u5206\u8fa8\u7387\u6570\u636e\u7684\u7b97\u5b50\u5b66\u4e60\u65b9\u6cd5\u662f\u5fc5\u8981\u7684\u3002", "method": "\u5229\u7528\u9884\u8bad\u7ec3\u7684\u57fa\u51fd\u6570\u5c06\u8f93\u5165\u51fd\u6570\u6295\u5f71\u5230\u4e00\u4e2a\u9690\u542b\u5d4c\u5165\u7a7a\u95f4\u4e2d\uff0c\u7136\u540e\u4f7f\u7528\u591a\u5c42\u611f\u77e5\u5668\u57fa\u4e8e\u9690\u542b\u7801\u548c\u65f6\u7a7a\u5750\u6807\u751f\u6210\u7269\u7406\u7a7a\u95f4\u4e2d\u7684\u89e3\u3002\u90e8\u5206\u5fae\u5206\u65b9\u7a0b\u5728\u7269\u7406\u7a7a\u95f4\u4e2d\u901a\u8fc7\u6709\u9650\u5dee\u5206\u6c42\u89e3\u5668\u5f3a\u5236\u6267\u884c\u3002\u6b64\u5916\uff0c\u8fd9\u79cd\u65b9\u6cd5\u4e0d\u9700\u8981\u4efb\u4f55\u8bad\u7ec3\u6570\u636e\uff0c\u4ec5\u4f9d\u8d56\u4e8e\u7269\u7406\u77e5\u8bc6\u3002", "result": "\u5728\u591a\u5206\u8fa8\u7387\u6570\u636e\u7684\u6570\u503c\u793a\u4f8b\u4e2d\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u7684\u6027\u80fd\uff0c\u8f93\u5165\u51fd\u6570\u5728\u4e0d\u540c\u5206\u8fa8\u7387\u4e0a\u91c7\u6837\uff0c\u5305\u62ec\u7c97\u7565\u548c\u7cbe\u7ec6\u79bb\u6563\u5316\u3002\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\u8be5\u65b9\u6cd5\u6709\u6548\u4e14\u5177\u6709\u6f5c\u529b\u3002", "conclusion": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u7b97\u5b50\u5b66\u4e60\u65b9\u6cd5\uff0c\u5373\u6269\u5c55\u7684\u5206\u8fa8\u7387\u65e0\u5173\u795e\u7ecf\u7b97\u5b50\u6846\u67b6\uff0c\u80fd\u591f\u5728\u5b8c\u5168\u6570\u636e\u81ea\u7531\u7684\u60c5\u51b5\u4e0b\u5de5\u4f5c\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u4e0d\u540c\u5206\u8fa8\u7387\u6570\u636e\u7684\u6311\u6218\u3002"}}
{"id": "2510.24321", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24321", "abs": "https://arxiv.org/abs/2510.24321", "authors": ["Ivica Dimitrovski", "Vlatko Spasev", "Ivan Kitanovski"], "title": "Few-Shot Remote Sensing Image Scene Classification with CLIP and Prompt Learning", "comment": null, "summary": "Remote sensing applications increasingly rely on deep learning for scene\nclassification. However, their performance is often constrained by the scarcity\nof labeled data and the high cost of annotation across diverse geographic and\nsensor domains. While recent vision-language models like CLIP have shown\npromise by learning transferable representations at scale by aligning visual\nand textual modalities, their direct application to remote sensing remains\nsuboptimal due to significant domain gaps and the need for task-specific\nsemantic adaptation. To address this critical challenge, we systematically\nexplore prompt learning as a lightweight and efficient adaptation strategy for\nfew-shot remote sensing image scene classification. We evaluate several\nrepresentative methods, including Context Optimization, Conditional Context\nOptimization, Multi-modal Prompt Learning, and Prompting with Self-Regulating\nConstraints. These approaches reflect complementary design philosophies: from\nstatic context optimization to conditional prompts for enhanced generalization,\nmulti-modal prompts for joint vision-language adaptation, and semantically\nregularized prompts for stable learning without forgetting. We benchmark these\nprompt-learning methods against two standard baselines: zero-shot CLIP with\nhand-crafted prompts and a linear probe trained on frozen CLIP features.\nThrough extensive experiments on multiple benchmark remote sensing datasets,\nincluding cross-dataset generalization tests, we demonstrate that prompt\nlearning consistently outperforms both baselines in few-shot scenarios.\nNotably, Prompting with Self-Regulating Constraints achieves the most robust\ncross-domain performance. Our findings underscore prompt learning as a scalable\nand efficient solution for bridging the domain gap in satellite and aerial\nimagery, providing a strong foundation for future research in this field.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u5728\u8fdc\u7a0b\u9065\u611f\u56fe\u50cf\u573a\u666f\u5206\u7c7b\u4e2d\u4f7f\u7528\u63d0\u793a\u5b66\u4e60\u4f5c\u4e3a\u4e00\u79cd\u8f7b\u91cf\u7ea7\u4e14\u9ad8\u6548\u7684\u9002\u5e94\u7b56\u7565\u6765\u89e3\u51b3\u6807\u6ce8\u6570\u636e\u7a00\u5c11\u548c\u6210\u672c\u95ee\u9898\u3002\u901a\u8fc7\u5b9e\u9a8c\uff0c\u7814\u7a76\u5c55\u793a\u4e86\u63d0\u793a\u5b66\u4e60\u5728\u5c11\u6837\u672c\u573a\u666f\u4e0b\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5c24\u5176\u5728\u8de8\u57df\u6027\u80fd\u65b9\u9762\u3002\u8fd9\u8868\u660e\u63d0\u793a\u5b66\u4e60\u53ef\u4ee5\u4f5c\u4e3a\u8fde\u63a5\u9065\u611f\u9886\u57df\u7684\u53ef\u6269\u5c55\u548c\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002", "motivation": "\u76ee\u524d\uff0c\u8fdc\u7a0b\u9065\u611f\u5e94\u7528\u4e2d\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u53d7\u9650\u4e8e\u6807\u8bb0\u6570\u636e\u7a00\u7f3a\u548c\u6807\u6ce8\u6210\u672c\u9ad8\u6602\u7684\u95ee\u9898\u3002\u5c3d\u7ba1\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u5982CLIP\u901a\u8fc7\u89c6\u89c9\u548c\u6587\u672c\u6a21\u6001\u7684\u5b66\u4e60\u63d0\u4f9b\u4e86\u6709\u5e0c\u671b\u7684\u65b9\u6cd5\uff0c\u4f46\u5b83\u4eec\u76f4\u63a5\u5e94\u7528\u4e8e\u8fdc\u7a0b\u9065\u611f\u8868\u73b0\u4e0d\u4f73\u3002\u672c\u6587\u610f\u56fe\u901a\u8fc7\u63a2\u7d22\u63d0\u793a\u5b66\u4e60\u4f5c\u4e3a\u89e3\u51b3\u8fd9\u7c7b\u95ee\u9898\u7684\u6709\u6548\u7b56\u7565\u3002", "method": "\u672c\u6587\u7cfb\u7edf\u6027\u5730\u63a2\u8ba8\u4e86\u56db\u79cd\u57fa\u4e8e\u63d0\u793a\u5b66\u4e60\u7684\u65b9\u6cd5\uff1a\u4e0a\u4e0b\u6587\u4f18\u5316\u3001\u6761\u4ef6\u4e0a\u4e0b\u6587\u4f18\u5316\u3001\u591a\u6a21\u6001\u63d0\u793a\u5b66\u4e60\u548c\u5e26\u6709\u81ea\u8c03\u8282\u7ea6\u675f\u7684\u63d0\u793a\u3002\u8fd9\u4e9b\u65b9\u6cd5\u6db5\u76d6\u4e86\u9759\u6001\u4e0a\u4e0b\u6587\u4f18\u5316\u5230\u6761\u4ef6\u63d0\u793a\u589e\u5f3a\u6cdb\u5316\u3001\u591a\u6a21\u6001\u63d0\u793a\u548c\u8bed\u4e49\u6b63\u5219\u5316\u7684\u63d0\u793a\u3002", "result": "\u901a\u8fc7\u5728\u591a\u4e2a\u57fa\u51c6\u9065\u611f\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u6bd4\u8f83\uff0c\u7814\u7a76\u5c55\u793a\u4e86\u63d0\u793a\u5b66\u4e60\u65b9\u6cd5\u5728\u5c11\u6837\u672c\u573a\u666f\u4e0b\u4f18\u4e8e\u57fa\u7ebf\u3002\u7279\u522b\u5730\uff0c\u5e26\u6709\u81ea\u8c03\u8282\u7ea6\u675f\u7684\u63d0\u793a\u65b9\u6cd5\u8fbe\u5230\u4e86\u6700\u7a33\u5065\u7684\u8de8\u57df\u8868\u73b0\u3002", "conclusion": "\u672c\u6587\u7684\u7814\u7a76\u7ed3\u679c\u5f3a\u8c03\u4e86\u63d0\u793a\u5b66\u4e60\u4f5c\u4e3a\u8fde\u63a5\u9065\u611f\u9886\u57df\u5dee\u8ddd\u7684\u53ef\u6269\u5c55\u548c\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u575a\u5b9e\u7684\u57fa\u7840\u3002"}}
{"id": "2510.24551", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24551", "abs": "https://arxiv.org/abs/2510.24551", "authors": ["Gang Chen", "Changshuo Liu", "Gene Anne Ooi", "Marcus Tan", "Zhongle Xie", "Jianwei Yin", "James Wei Luen Yip", "Wenqiao Zhang", "Jiaqi Zhu", "Beng Chin Ooi"], "title": "Generative AI for Healthcare: Fundamentals, Challenges, and Perspectives", "comment": null, "summary": "Generative Artificial Intelligence (GenAI) is taking the world by storm. It\npromises transformative opportunities for advancing and disrupting existing\npractices, including healthcare. From large language models (LLMs) for clinical\nnote synthesis and conversational assistance to multimodal systems that\nintegrate medical imaging, electronic health records, and genomic data for\ndecision support, GenAI is transforming the practice of medicine and the\ndelivery of healthcare, such as diagnosis and personalized treatments, with\ngreat potential in reducing the cognitive burden on clinicians, thereby\nimproving overall healthcare delivery. However, GenAI deployment in healthcare\nrequires an in-depth understanding of healthcare tasks and what can and cannot\nbe achieved. In this paper, we propose a data-centric paradigm in the design\nand deployment of GenAI systems for healthcare. Specifically, we reposition the\ndata life cycle by making the medical data ecosystem as the foundational\nsubstrate for generative healthcare systems. This ecosystem is designed to\nsustainably support the integration, representation, and retrieval of diverse\nmedical data and knowledge. With effective and efficient data processing\npipelines, such as semantic vector search and contextual querying, it enables\nGenAI-powered operations for upstream model components and downstream clinical\napplications. Ultimately, it not only supplies foundation models with\nhigh-quality, multimodal data for large-scale pretraining and domain-specific\nfine-tuning, but also serves as a knowledge retrieval backend to support\ntask-specific inference via the agentic layer. The ecosystem enables the\ndeployment of GenAI for high-quality and effective healthcare delivery.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u6570\u636e\u4e3a\u4e2d\u5fc3\u7684\u8303\u5f0f\uff0c\u7528\u4e8e\u8bbe\u8ba1\u548c\u90e8\u7f72\u9762\u5411\u533b\u7597\u4fdd\u5065\u7684\u751f\u6210\u5f0f\u4eba\u5de5\u667a\u80fd\uff08GenAI\uff09\u7cfb\u7edf\u3002\u901a\u8fc7\u91cd\u65b0\u5b9a\u4f4d\u533b\u7597\u6570\u636e\u751f\u547d\u5468\u671f\uff0c\u5efa\u7acb\u4e00\u4e2a\u751f\u6001\u7cfb\u7edf\u4ee5\u652f\u6301\u5404\u79cd\u533b\u7597\u6570\u636e\u548c\u77e5\u8bc6\u7684\u96c6\u6210\u3001\u8868\u793a\u548c\u68c0\u7d22\uff0c\u4ece\u800c\u4f18\u5316\u4e0a\u6e38\u6a21\u578b\u7ec4\u4ef6\u548c\u4e0b\u6e38\u4e34\u5e8a\u5e94\u7528\u3002", "motivation": "\u5f53\u524d\u751f\u6210\u5f0f\u4eba\u5de5\u667a\u80fd\u6b63\u5728\u9769\u65b0\u533b\u7597\u5b9e\u8df5\u548c\u533b\u7597\u4ea4\u4ed8\uff0c\u7279\u522b\u662f\u5728\u4e34\u5e8a\u6ce8\u91ca\u3001\u5bf9\u8bdd\u652f\u6301\u4ee5\u53ca\u51b3\u7b56\u652f\u6301\u7b49\u9886\u57df\u3002\u7136\u800c\uff0c\u8981\u6709\u6548\u90e8\u7f72GenAI\uff0c\u6211\u4eec\u9700\u8981\u6df1\u523b\u7406\u89e3\u533b\u7597\u4efb\u52a1\uff0c\u5e76\u660e\u786e\u5176\u5e94\u7528\u573a\u666f\u548c\u9650\u5236\u3002\u56e0\u6b64\uff0c\u672c\u6587\u5e0c\u671b\u901a\u8fc7\u6570\u636e\u4e3a\u4e2d\u5fc3\u7684\u8303\u5f0f\uff0c\u63a8\u52a8GenAI\u66f4\u52a0\u6709\u6548\u5730\u652f\u6301\u533b\u7597\u4fdd\u5065\u3002", "method": "\u901a\u8fc7\u91cd\u65b0\u5b9a\u4f4d\u6570\u636e\u751f\u547d\u5468\u671f\uff0c\u4f7f\u533b\u7597\u6570\u636e\u751f\u6001\u7cfb\u7edf\u6210\u4e3a\u751f\u6210\u533b\u7597\u7cfb\u7edf\u7684\u6839\u57fa\u3002\u751f\u6001\u7cfb\u7edf\u80fd\u591f\u53ef\u6301\u7eed\u5730\u652f\u6301\u5404\u5f0f\u533b\u7597\u6570\u636e\u548c\u77e5\u8bc6\u7684\u96c6\u6210\u3001\u8868\u793a\u548c\u68c0\u7d22\uff0c\u5e76\u62e5\u6709\u9ad8\u6548\u7684\u8bed\u4e49\u5411\u91cf\u641c\u7d22\u548c\u4e0a\u4e0b\u6587\u67e5\u8be2\u7b49\u6570\u636e\u5904\u7406\u6d41\u7a0b\uff0c\u4ee5\u652f\u6301\u57fa\u4e8e\u6570\u636e\u7684\u751f\u6210\u5f0f\u64cd\u4f5c\u3002\u8fd9\u79cd\u751f\u6001\u7cfb\u7edf\u4e0d\u4ec5\u4e3a\u5927\u89c4\u6a21\u9884\u8bad\u7ec3\u548c\u7279\u5b9a\u9886\u57df\u7684\u5fae\u8c03\u63d0\u4f9b\u9ad8\u8d28\u91cf\u7684\u591a\u6a21\u6001\u6570\u636e\uff0c\u8fd8\u4f5c\u4e3a\u77e5\u8bc6\u68c0\u7d22\u540e\u7aef\uff0c\u652f\u6301\u4efb\u52a1\u7279\u5b9a\u7684\u63a8\u7406\u8fc7\u7a0b\u3002", "result": "\u6587\u7ae0\u63d0\u51fa\u4e86\u4e00\u4e2a\u4ee5\u6570\u636e\u4e3a\u4e2d\u5fc3\u7684\u67b6\u6784\uff0c\u65e8\u5728\u6539\u8fdbGenAI\u5728\u533b\u7597\u9886\u57df\u4e2d\u7684\u5e94\u7528\uff0c\u901a\u8fc7\u4e00\u4e2a\u5f3a\u5065\u7684\u6570\u636e\u751f\u6001\u7cfb\u7edf\u6765\u4f18\u5316GenAI\u7684\u90e8\u7f72\u548c\u5229\u7528\u3002\u8fd9\u79cd\u67b6\u6784\u63d0\u5347\u4e86\u533b\u7597\u4ea4\u4ed8\u7684\u9ad8\u8d28\u91cf\u548c\u6709\u6548\u6027\u3002", "conclusion": "\u901a\u8fc7\u5021\u5bfc\u6570\u636e\u4e3a\u4e2d\u5fc3\u7684\u521b\u65b0\u65b9\u6cd5\uff0c\u53ef\u4ee5\u66f4\u52a0\u6709\u6548\u5730\u5229\u7528\u751f\u6210\u5f0f\u4eba\u5de5\u667a\u80fd\u6765\u4f18\u5316\u533b\u7597\u4fdd\u5065\u7684\u4ea4\u4ed8\u3002\u672a\u6765\u7684\u7814\u7a76\u5c06\u66f4\u8fdb\u4e00\u6b65\u63a2\u7d22\u5176\u5b9e\u9645\u5e94\u7528\u548c\u6539\u8fdb\u3002"}}
{"id": "2510.23817", "categories": ["cs.LG", "stat.ME"], "pdf": "https://arxiv.org/pdf/2510.23817", "abs": "https://arxiv.org/abs/2510.23817", "authors": ["Pedro Cortes dos Santos", "Matheus Becali Rocha", "Renato A Krohling"], "title": "Combining SHAP and Causal Analysis for Interpretable Fault Detection in Industrial Processes", "comment": null, "summary": "Industrial processes generate complex data that challenge fault detection\nsystems, often yielding opaque or underwhelming results despite advanced\nmachine learning techniques. This study tackles such difficulties using the\nTennessee Eastman Process, a well-established benchmark known for its intricate\ndynamics, to develop an innovative fault detection framework. Initial attempts\nwith standard models revealed limitations in both performance and\ninterpretability, prompting a shift toward a more tractable approach. By\nemploying SHAP (SHapley Additive exPlanations), we transform the problem into a\nmore manageable and transparent form, pinpointing the most critical process\nfeatures driving fault predictions. This reduction in complexity unlocks the\nability to apply causal analysis through Directed Acyclic Graphs, generated by\nmultiple algorithms, to uncover the underlying mechanisms of fault propagation.\nThe resulting causal structures align strikingly with SHAP findings,\nconsistently highlighting key process elements-like cooling and separation\nsystems-as pivotal to fault development. Together, these methods not only\nenhance detection accuracy but also provide operators with clear, actionable\ninsights into fault origins, a synergy that, to our knowledge, has not been\npreviously explored in this context. This dual approach bridges predictive\npower with causal understanding, offering a robust tool for monitoring complex\nmanufacturing environments and paving the way for smarter, more interpretable\nfault detection in industrial systems.", "AI": {"tldr": "\u7814\u7a76\u5f00\u53d1\u4e86\u4e00\u4e2a\u65b0\u7684\u6545\u969c\u68c0\u6d4b\u6846\u67b6\uff0c\u7ed3\u5408SHAP\u548c\u56e0\u679c\u5206\u6790\uff0c\u6765\u6539\u5584\u5de5\u4e1a\u8fc7\u7a0b\u4e2d\u7684\u6545\u969c\u68c0\u6d4b\u6027\u80fd\u548c\u53ef\u89e3\u91ca\u6027\uff0c\u7279\u522b\u9488\u5bf9\u590d\u6742\u7684\u6570\u636e\u73af\u5883\u548cTE\u8fc7\u7a0b\u7684\u6311\u6218\u3002\u751f\u6210\u7684\u56e0\u679c\u7ed3\u6784\u4e0eSHAP\u7684\u53d1\u73b0\u76f8\u543b\u5408\uff0c\u63d0\u9ad8\u4e86\u6545\u969c\u68c0\u6d4b\u7684\u51c6\u786e\u6027\uff0c\u5e76\u4e3a\u64cd\u4f5c\u4eba\u5458\u63d0\u4f9b\u4e86\u6e05\u6670\u7684\u89c1\u89e3\u3002", "motivation": "\u5de5\u4e1a\u8fc7\u7a0b\u4ea7\u751f\u4e86\u590d\u6742\u7684\u6570\u636e\uff0c\u4f20\u7edf\u7684\u6545\u969c\u68c0\u6d4b\u7cfb\u7edf\u9762\u4e34\u6027\u80fd\u548c\u89e3\u91ca\u6027\u7684\u6311\u6218\u3002\u4e3a\u4e86\u66f4\u597d\u5730\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u7814\u7a76\u4eba\u5458\u8bd5\u56fe\u901a\u8fc7\u5f15\u5165SHAP\u548c\u56e0\u679c\u56fe\u7684\u65b9\u6cd5\u6765\u63d0\u9ad8\u7cbe\u5ea6\u548c\u53ef\u89e3\u91ca\u6027\uff0c\u7279\u522b\u662f\u5728TE\u8fc7\u7a0b\u8fd9\u79cd\u5177\u6709\u590d\u6742\u52a8\u6001\u7279\u6027\u7684\u5178\u578b\u573a\u666f\u4e2d\u3002", "method": "\u8be5\u7814\u7a76\u9996\u5148\u4f7f\u7528\u6807\u51c6\u6a21\u578b\u5c1d\u8bd5\u6545\u969c\u68c0\u6d4b\uff0c\u4f46\u53d1\u73b0\u6027\u80fd\u548c\u89e3\u91ca\u6027\u6709\u9650\u3002\u7136\u540e\u4f7f\u7528SHAP\u5c06\u95ee\u9898\u8f6c\u5316\u4e3a\u66f4\u6613\u7ba1\u7406\u548c\u900f\u660e\u7684\u5f62\u5f0f\uff0c\u5e76\u5229\u7528\u591a\u79cd\u7b97\u6cd5\u751f\u6210\u7684\u6709\u5411\u65e0\u73af\u56fe\u8fdb\u884c\u56e0\u679c\u5206\u6790\uff0c\u63ed\u793a\u6545\u969c\u4f20\u64ad\u7684\u673a\u5236\u3002", "result": "\u901a\u8fc7\u8fd9\u79cd\u65b9\u6cd5\uff0c\u7814\u7a76\u6210\u529f\u5730\u63d0\u9ad8\u4e86\u6545\u969c\u68c0\u6d4b\u7684\u51c6\u786e\u6027\u548c\u89e3\u91ca\u6027\u3002\u751f\u6210\u7684\u56e0\u679c\u56fe\u4e0eSHAP\u7684\u7ed3\u679c\u4e00\u81f4\uff0c\u90fd\u5f3a\u8c03\u4e86\u51b7\u5374\u7cfb\u7edf\u548c\u5206\u79bb\u7cfb\u7edf\u7b49\u5173\u952e\u8fc7\u7a0b\u56e0\u7d20\u5bf9\u6545\u969c\u53d1\u5c55\u7684\u91cd\u8981\u6027\u3002\u8fd9\u4e9b\u65b9\u6cd5\u4e3a\u64cd\u4f5c\u4eba\u5458\u63d0\u4f9b\u4e86\u66f4\u6e05\u6670\u3001\u53ef\u64cd\u4f5c\u7684\u5173\u4e8e\u6545\u969c\u6765\u6e90\u7684\u6d1e\u5bdf\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408SHAP\u548c\u56e0\u679c\u56fe\u7684\u65b9\u6cd5\u6846\u67b6\uff0c\u4ee5\u63d0\u9ad8\u6545\u969c\u68c0\u6d4b\u7684\u51c6\u786e\u6027\u548c\u53ef\u89e3\u91ca\u6027\u3002\u8fd9\u79cd\u65b9\u6cd5\u4e0d\u4ec5\u589e\u5f3a\u4e86\u9884\u6d4b\u80fd\u529b\uff0c\u8fd8\u63d0\u4f9b\u4e86\u5bf9\u6f5c\u5728\u6545\u969c\u673a\u5236\u7684\u7406\u89e3\uff0c\u6709\u52a9\u4e8e\u6539\u8fdb\u590d\u6742\u5236\u9020\u73af\u5883\u4e0b\u7684\u76d1\u6d4b\u548c\u6545\u969c\u9884\u9632\u3002"}}
{"id": "2510.24645", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24645", "abs": "https://arxiv.org/abs/2510.24645", "authors": ["Zengzhuang Xu", "Bingguang Hao", "Zechuan Wang", "Yuntao Wen", "Maolin Wang", "Yang Liu", "Long Chen", "Dong Wang", "Yicheng Chen", "Cunyin Peng", "Chenyi Zhuang", "Jinjie Gu", "Leilei Gan", "Xiangyu Zhao", "Shi Gu"], "title": "FunReason-MT Technical Report: Overcoming the Complexity Barrier in Multi-Turn Function Calling", "comment": null, "summary": "Function calling (FC) empowers large language models (LLMs) and autonomous\nagents to interface with external tools, a critical capability for solving\ncomplex, real-world problems. As this ability becomes increasingly central to\nadvanced AI systems, the need for high-quality, multi-turn training data to\ndevelop and refine it cannot be overstated. Existing data synthesis methods,\nsuch as random environment sampling or multi-agent role-playing, are not\npowerful enough to generate high-quality data in real-world environments.\nPractical challenges come in three folds: targeted model training, isolation of\ntool architecture, and multi-turn logical dependency. To address these\nstructural deficiencies, we present FunReason-MT, a novel data synthesis\nframework for real-world multi-turn tool use. FunReason-MT resolves the\ncomplexity barrier in multi-turn FC data by employing 1) Environment-API Graph\nInteractions to gather varied high-quality trajectories, 2) Advanced Tool-Query\nSynthesis to simplify hard query construction, and 3) Guided Iterative Chain\nfor sophisticated CoT generation. Evaluations on Berkeley Function-Calling\nLeaderboard (BFCLv3) demonstrate the power of our framework: a 4B model built\nupon FunReason-MT generated data achieves state-of-the-art performance among\ncomparable-sized models, outperforming most close-source models. Further\nperformance improvements on BFCLv4 confirm that FunReason-MT provides a\nreliable and robust source for agentic learning.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86FunReason-MT\u6846\u67b6\uff0c\u7528\u4e8e\u751f\u6210\u7528\u4e8e\u73b0\u5b9e\u4e16\u754c\u4e2d\u591a\u8f6e\u6b21\u5de5\u5177\u4f7f\u7528\u7684\u9ad8\u8d28\u91cf\u8bad\u7ec3\u6570\u636e\uff0c\u8be5\u6846\u67b6\u5728Berkeley Function-Calling Leaderboard\u4e0a\u7684\u8bc4\u4f30\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u8d85\u8fc7\u4e86\u5927\u591a\u6570\u5c01\u95ed\u6e90\u6a21\u578b\u7684\u6027\u80fd", "motivation": "\u73b0\u6709\u7684\u6570\u636e\u5408\u6210\u65b9\u6cd5\u5728\u751f\u6210\u771f\u5b9e\u4e16\u754c\u7684\u9ad8\u8d28\u91cf\u8bad\u7ec3\u6570\u636e\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u56e0\u6b64\u672c\u6587\u63d0\u51fa\u4e86FunReason-MT\u6846\u67b6\uff0c\u4ee5\u6539\u8fdb\u8fd9\u4e00\u95ee\u9898", "method": "\u63d0\u51fa\u4e86\u73af\u5883API\u56fe\u4ea4\u4e92\u3001\u9ad8\u7ea7\u5de5\u5177\u67e5\u8be2\u5408\u6210\u548c\u5f15\u5bfc\u5f0f\u8fed\u4ee3\u94fe\u8fd9\u4e09\u4e2a\u7ec4\u4ef6\u6765\u751f\u6210\u9ad8\u8d28\u91cf\u7684\u591a\u8f6e\u6b21\u5de5\u5177\u4f7f\u7528\u6570\u636e", "result": "\u57fa\u4e8eFunReason-MT\u751f\u6210\u7684\u6570\u636e\u8bad\u7ec3\u76844B\u6a21\u578b\u5728Berkeley Function-Calling Leaderboard\u4e0a\u7684\u8bc4\u4f30\u4e2d\uff0c\u76f8\u8f83\u4e8e\u540c\u89c4\u6a21\u6a21\u578b\u548c\u5176\u4ed6\u5c01\u95ed\u6e90\u6a21\u578b\u663e\u793a\u51fa\u4f18\u8d8a\u7684\u6027\u80fd", "conclusion": "FunReason-MT\u6846\u67b6\u63d0\u4f9b\u4e86\u9ad8\u8d28\u91cf\u7684\u6570\u636e\u5408\u6210\u80fd\u529b\uff0c\u5bf9\u89e3\u51b3\u73b0\u5b9e\u4e16\u754c\u4e2d\u7684\u590d\u6742\u95ee\u9898\u5177\u6709\u91cd\u8981\u610f\u4e49"}}
{"id": "2510.23818", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.23818", "abs": "https://arxiv.org/abs/2510.23818", "authors": ["Yilang Zhang", "Xiaodong Yang", "Yiwei Cai", "Georgios B. Giannakis"], "title": "ScaLoRA: Optimally Scaled Low-Rank Adaptation for Efficient High-Rank Fine-Tuning", "comment": null, "summary": "As large language models (LLMs) continue to scale in size, the computational\noverhead has become a major bottleneck for task-specific fine-tuning. While\nlow-rank adaptation (LoRA) effectively curtails this cost by confining the\nweight updates to a low-dimensional subspace, such a restriction can hinder\neffectiveness and slow convergence. This contribution deals with these\nlimitations by accumulating progressively a high-rank weight update from\nconsecutive low-rank increments. Specifically, the per update optimal low-rank\nmatrix is identified to minimize the loss function and closely approximate full\nfine-tuning. To endow efficient and seamless optimization without restarting,\nthis optimal choice is formed by appropriately scaling the columns of the\noriginal low-rank matrix. Rigorous performance guarantees reveal that the\noptimal scaling can be found analytically. Extensive numerical tests with\npopular LLMs scaling up to 12 billion parameters demonstrate a consistent\nperformance gain and fast convergence relative to state-of-the-art LoRA\nvariants on diverse tasks including natural language understanding, commonsense\nreasoning, and mathematical problem solving.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u7d2f\u79ef\u8fde\u7eed\u7684\u4f4e\u79e9\u589e\u91cf\u6765\u6784\u5efa\u9ad8\u79e9\u6743\u91cd\u66f4\u65b0\uff0c\u4ece\u800c\u5728\u4e0d\u8fc7\u5ea6\u589e\u52a0\u8ba1\u7b97\u6210\u672c\u7684\u524d\u63d0\u4e0b\u6539\u8fdbLLM\u7684\u5fae\u8c03\u8fc7\u7a0b\uff0c\u5b9e\u9a8c\u8bc1\u660e\u8fd9\u79cd\u65b9\u6cd5\u5728\u591a\u79cd\u4efb\u52a1\u4e0a\u90fd\u53d6\u5f97\u4e86\u66f4\u5feb\u7684\u6536\u655b\u901f\u5ea6\u548c\u66f4\u597d\u7684\u6027\u80fd\u3002", "motivation": "\u63d0\u51fa\u8fd9\u79cd\u65b9\u6cd5\u662f\u4e3a\u4e86\u51cf\u5c11\u5927\u89c4\u6a21\u8bed\u8a00\u6a21\u578b\u5728\u7279\u5b9a\u4efb\u52a1\u4e0a\u8fdb\u884c\u5fae\u8c03\u65f6\u7684\u8ba1\u7b97\u6210\u672c\uff0c\u540c\u65f6\u907f\u514d\u4f20\u7edf\u4f4e\u79e9\u9002\u914d\u65b9\u6cd5\u5728\u6709\u6548\u6027\u4e0a\u53ef\u80fd\u5b58\u5728\u7684\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u7d2f\u79ef\u8fde\u7eed\u7684\u4f4e\u79e9\u589e\u91cf\u6765\u6784\u5efa\u9ad8\u79e9\u6743\u91cd\u66f4\u65b0\uff0c\u6bcf\u4e2a\u66f4\u65b0\u8fed\u4ee3\u4e2d\u5bfb\u627e\u6700\u4f18\u7684\u4f4e\u79e9\u77e9\u9635\u6765\u6700\u5c0f\u5316\u635f\u5931\u51fd\u6570\u5e76\u63a5\u8fd1\u5168\u5fae\u8c03\u6548\u679c\u3002\u540c\u65f6\uff0c\u901a\u8fc7\u9002\u5f53\u5730\u8c03\u6574\u4f4e\u79e9\u77e9\u9635\u7684\u5217\u6765\u5b9e\u73b0\u9ad8\u6548\u7684\u4f18\u5316\u5e76\u907f\u514d\u91cd\u65b0\u5f00\u59cb\u8fc7\u7a0b\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\u8be5\u65b9\u6cd5\u5728\u591a\u4e2a\u4e0d\u540c\u7684\u4efb\u52a1\u4e0a\u7684\u6027\u80fd\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u7684\u4f4e\u79e9\u9002\u914d\u65b9\u6cd5\uff0c\u540c\u65f6\u5177\u6709\u66f4\u5feb\u7684\u6536\u655b\u901f\u5ea6\u3002", "conclusion": "\u8be5\u63d0\u51fa\u7684\u65b9\u6cd5\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u65b9\u6cd5\uff0c\u5728\u964d\u4f4e\u5927\u89c4\u6a21\u8bed\u8a00\u6a21\u578b\u8ba1\u7b97\u6210\u672c\u7684\u540c\u65f6\uff0c\u4fdd\u6301\u4e86\u8f83\u597d\u7684\u5fae\u8c03\u6548\u679c\u548c\u5feb\u901f\u7684\u6536\u655b\u901f\u5ea6\u3002"}}
{"id": "2510.24650", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24650", "abs": "https://arxiv.org/abs/2510.24650", "authors": ["Nitin Rai", "Daeun", "Choi", "Nathan S. Boyd", "Arnold W. Schumann"], "title": "Advancing site-specific disease and pest management in precision agriculture: From reasoning-driven foundation models to adaptive, feedback-based learning", "comment": "26 pages, 8 figures, and 2 tables", "summary": "Site-specific disease management (SSDM) in crops has advanced rapidly through\nmachine and deep learning (ML and DL) for real-time computer vision. Research\nevolved from handcrafted feature extraction to large-scale automated feature\nlearning. With foundation models (FMs), crop disease datasets are now processed\nin fundamentally new ways. Unlike traditional neural networks, FMs integrate\nvisual and textual data, interpret symptoms in text, reason about\nsymptom-management relationships, and support interactive QA for growers and\neducators. Adaptive and imitation learning in robotics further enables\nfield-based disease management. This review screened approx. 40 articles on FM\napplications for SSDM, focusing on large-language models (LLMs) and\nvision-language models (VLMs), and discussing their role in adaptive learning\n(AL), reinforcement learning (RL), and digital twin frameworks for targeted\nspraying. Key findings: (a) FMs are gaining traction with surging literature in\n2023-24; (b) VLMs outpace LLMs, with a 5-10x increase in publications; (c) RL\nand AL are still nascent for smart spraying; (d) digital twins with RL can\nsimulate targeted spraying virtually; (e) addressing the sim-to-real gap is\ncritical for real-world deployment; (f) human-robot collaboration remains\nlimited, especially in human-in-the-loop approaches where robots detect early\nsymptoms and humans validate uncertain cases; (g) multi-modal FMs with\nreal-time feedback will drive next-gen SSDM. For updates, resources, and\ncontributions, visit, https://github.com/nitin-dominic/AgriPathogenDatabase, to\nsubmit papers, code, or datasets.", "AI": {"tldr": "\u8be5\u7814\u7a76\u7efc\u8ff0\u4e86\u7ea640\u7bc7\u5173\u4e8e\u57fa\u7840\u6a21\u578b\uff08FMs\uff09\u5728\u4f5c\u7269\u7279\u5b9a\u75be\u75c5\u7ba1\u7406\uff08SSDM\uff09\u4e2d\u7684\u5e94\u7528\u6587\u7ae0\uff0c\u91cd\u70b9\u8ba8\u8bba\u4e86\u5927\u89c4\u6a21\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u548c\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u5728\u81ea\u9002\u5e94\u5b66\u4e60\uff08AL\uff09\u3001\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u548c\u6570\u5b57\u5b6a\u751f\u6846\u67b6\u4e2d\u5bf9\u7cbe\u51c6\u55b7\u6d12\u7684\u652f\u6301\uff0c\u5e76\u63a2\u8ba8\u4e86\u673a\u5668\u4eba\u5728SSDM\u4e2d\u7684\u89d2\u8272\u4ee5\u53ca\u4eba\u673a\u534f\u4f5c\u7684\u6311\u6218\u548c\u672a\u6765\u65b9\u5411\u3002\u6838\u5fc3\u53d1\u73b0\u5305\u62ec\u57fa\u7840\u6a21\u578b\u7684\u6587\u732e\u5448\u4e0a\u5347\u8d8b\u52bf\u3001\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u589e\u957f\u901f\u5ea6\u9ad8\u4e8e\u5927\u89c4\u6a21\u8bed\u8a00\u6a21\u578b\u7b49\u3002", "motivation": "\u7814\u7a76\u7684\u4e3b\u8981\u52a8\u673a\u662f\u63a2\u8ba8\u57fa\u7840\u6a21\u578b\uff08FMs\uff09\u5728\u4f5c\u7269\u7279\u5b9a\u75be\u75c5\u7ba1\u7406\u4e2d\u7684\u5e94\u7528\u548c\u53d1\u5c55\uff0c\u5c24\u5176\u662f\u5728\u81ea\u9002\u5e94\u5b66\u4e60\u3001\u5f3a\u5316\u5b66\u4e60\u548c\u6570\u5b57\u5b6a\u751f\u6846\u67b6\u4e2d\u7684\u4f5c\u7528\uff0c\u63d0\u9ad8\u7cbe\u51c6\u55b7\u6d12\u7684\u6548\u7387\u548c\u51c6\u786e\u6027\u3002", "method": "\u7efc\u8ff0\u6587\u7ae0\u7b5b\u9009\u4e8640\u7bc7\u5173\u4e8e\u57fa\u7840\u6a21\u578b\u5728\u4f5c\u7269\u75be\u75c5\u7ba1\u7406\u4e2d\u5e94\u7528\u7684\u6587\u7ae0\uff0c\u96c6\u4e2d\u5728\u5927\u89c4\u6a21\u8bed\u8a00\u6a21\u578b\u548c\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4e0a\uff0c\u5e76\u8ba8\u8bba\u4e86\u5b83\u4eec\u5728\u81ea\u9002\u5e94\u5b66\u4e60\u3001\u5f3a\u5316\u5b66\u4e60\u548c\u6570\u5b57\u5b6a\u751f\u6846\u67b6\u4e2d\u7684\u5e94\u7528\u3002", "result": "\u53d1\u73b0\u57fa\u7840\u6a21\u578b\u7684\u5e94\u7528\u6587\u732e\u57282023-2024\u5e74\u5448\u73b0\u4e0a\u5347\u8d8b\u52bf\uff0c\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u589e\u957f\u901f\u5ea6\u5feb\u4e8e\u5927\u89c4\u6a21\u8bed\u8a00\u6a21\u578b\u3002\u81ea\u9002\u5e94\u5b66\u4e60\u548c\u5f3a\u5316\u5b66\u4e60\u5728\u667a\u80fd\u55b7\u6d12\u4e0a\u4ecd\u5904\u4e8e\u521d\u7ea7\u9636\u6bb5\uff0c\u800c\u6570\u5b57\u5b6a\u751f\u7ed3\u5408\u5f3a\u5316\u5b66\u4e60\u53ef\u4ee5\u865a\u62df\u6a21\u62df\u7cbe\u51c6\u55b7\u6d12\u3002\u89e3\u51b3\u4eff\u771f\u5230\u73b0\u5b9e\u7684\u8f6c\u6362\u5dee\u8ddd\u662f\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u5173\u952e\u95ee\u9898\u3002\u4eba\u673a\u534f\u4f5c\u5c24\u5176\u662f\u5728\u4eba\u5de5\u5728\u73af\u8def\u65b9\u6cd5\u4e2d\u4ecd\u7136\u5177\u6709\u6311\u6218\u6027\uff0c\u673a\u5668\u4eba\u68c0\u6d4b\u65e9\u671f\u75c7\u72b6\u800c\u4eba\u7c7b\u9a8c\u8bc1\u4e0d\u786e\u5b9a\u60c5\u51b5\u3002\u591a\u6a21\u6001\u57fa\u7840\u6a21\u578b\u7ed3\u5408\u5b9e\u65f6\u53cd\u9988\u5c06\u9a71\u52a8\u4e0b\u4e00\u4ee3\u7279\u5b9a\u75be\u75c5\u7ba1\u7406\u7684\u53d1\u5c55\u3002", "conclusion": "\u7efc\u8ff0\u5f3a\u8c03\u4e86\u57fa\u7840\u6a21\u578b\u5728\u4f5c\u7269\u7279\u5b9a\u75be\u75c5\u7ba1\u7406\u4e2d\u7684\u91cd\u8981\u5e94\u7528\u548c\u672a\u6765\u53d1\u5c55\u65b9\u5411\uff0c\u6307\u51fa\u591a\u6a21\u6001\u57fa\u7840\u6a21\u578b\u548c\u5b9e\u65f6\u53cd\u9988\u7684\u7ed3\u5408\u5c06\u63a8\u52a8\u4e0b\u4e00\u4ee3SSDM\u6280\u672f\u7684\u53d1\u5c55\uff0c\u4ee5\u5b9e\u73b0\u66f4\u9ad8\u6548\u3001\u66f4\u51c6\u786e\u7684\u4f5c\u7269\u75be\u75c5\u7ba1\u7406\u3002"}}
{"id": "2510.23866", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.23866", "abs": "https://arxiv.org/abs/2510.23866", "authors": ["Paul Rosu", "Muchang Bahng", "Erick Jiang", "Rico Zhu", "Vahid Tarokh"], "title": "A PDE-Informed Latent Diffusion Model for 2-m Temperature Downscaling", "comment": null, "summary": "This work presents a physics-conditioned latent diffusion model tailored for\ndynamical downscaling of atmospheric data, with a focus on reconstructing\nhigh-resolution 2-m temperature fields. Building upon a pre-existing diffusion\narchitecture and employing a residual formulation against a reference UNet, we\nintegrate a partial differential equation (PDE) loss term into the model's\ntraining objective. The PDE loss is computed in the full resolution (pixel)\nspace by decoding the latent representation and is designed to enforce physical\nconsistency through a finite-difference approximation of an effective\nadvection-diffusion balance. Empirical observations indicate that conventional\ndiffusion training already yields low PDE residuals, and we investigate how\nfine-tuning with this additional loss further regularizes the model and\nenhances the physical plausibility of the generated fields. The entirety of our\ncodebase is available on Github, for future reference and development.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7269\u7406\u6761\u4ef6\u7684\u6f5c\u5728\u6269\u6563\u6a21\u578b\uff0c\u7528\u4e8e\u5927\u6c14\u6570\u636e\u7684\u52a8\u529b\u4e0b\u91c7\u6837\uff0c\u91cd\u70b9\u91cd\u67842\u7c73\u6e29\u5ea6\u573a\u3002\u901a\u8fc7\u5728\u6a21\u578b\u4e2d\u52a0\u5165PDE\u635f\u5931\u9879\u6765\u589e\u5f3a\u7269\u7406\u4e00\u81f4\u6027\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u8fd9\u79cd\u989d\u5916\u7684\u635f\u5931\u9879\u53ef\u4ee5\u8fdb\u4e00\u6b65\u6b63\u5219\u5316\u6a21\u578b\uff0c\u63d0\u9ad8\u751f\u6210\u573a\u7684\u7269\u7406\u5408\u7406\u6027\u3002\u4ee3\u7801\u5f00\u6e90\u5728Github\u4e0a\u3002", "motivation": "\u4e3a\u4e86\u63d0\u9ad8\u5927\u6c14\u6570\u636e\u52a8\u529b\u4e0b\u91c7\u6837\u7684\u7269\u7406\u4e00\u81f4\u6027\uff0c\u7279\u522b\u662f\u57282\u7c73\u6e29\u5ea6\u573a\u7684\u9ad8\u5206\u8fa8\u7387\u91cd\u5efa\u4e2d\uff0c\u672c\u6587\u5c1d\u8bd5\u5728\u5df2\u6709\u7684\u6269\u6563\u6a21\u578b\u57fa\u7840\u4e0a\uff0c\u5f15\u5165\u7269\u7406\u6761\u4ef6\u7684\u7ea6\u675f\u3002\u7269\u7406\u8fc7\u7a0b\u7684\u5f15\u5165\u6709\u52a9\u4e8e\u63d0\u5347\u6a21\u578b\u751f\u6210\u6570\u636e\u7684\u7269\u7406\u5408\u7406\u6027\u3002", "method": "\u57fa\u4e8e\u73b0\u6709\u7684\u6269\u6563\u67b6\u6784\uff0c\u5e76\u7ed3\u5408\u53c2\u8003UNet\u7684\u6b8b\u5dee\u5f62\u5f0f\uff0c\u5c06PDE\u635f\u5931\u9879\u5f15\u5165\u5230\u6a21\u578b\u8bad\u7ec3\u76ee\u6807\u4e2d\uff0cPDE\u635f\u5931\u901a\u8fc7\u6709\u9650\u5dee\u5206\u6cd5\u8ba1\u7b97\u5168\u5206\u8fa8\u7387\uff08\u50cf\u7d20\u7ea7\u522b\uff09\u7684\u7a7a\u95f4\u6b8b\u5dee\uff0c\u8fdb\u800c\u7528\u6765\u5f3a\u5236\u6a21\u578b\u8f93\u51fa\u6570\u636e\u7684\u7269\u7406\u4e00\u81f4\u6027\u3002", "result": "\u5b9e\u9a8c\u53d1\u73b0\uff0c\u5373\u4f7f\u6ca1\u6709\u5f15\u5165PDE\u635f\u5931\u9879\uff0c\u5e38\u89c4\u7684\u6269\u6563\u8bad\u7ec3\u4e5f\u80fd\u8fbe\u5230\u5f88\u4f4e\u7684PDE\u6b8b\u5dee\u3002\u8fdb\u4e00\u6b65\u7684\u5fae\u8c03\u5b9e\u9a8c\u8868\u660e\uff0c\u5f15\u5165PDE\u635f\u5931\u9879\u80fd\u8fdb\u4e00\u6b65\u6b63\u5219\u5316\u6a21\u578b\uff0c\u5e76\u63d0\u9ad8\u751f\u6210\u573a\u7684\u7269\u7406\u53ef\u4fe1\u5ea6\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u7269\u7406\u6761\u4ef6\u4e0b\uff0c\u5229\u7528\u6269\u6563\u6a21\u578b\u548cPDE\u635f\u5931\u9879\u6765\u6539\u8fdb\u5927\u6c14\u6570\u636e\u7684\u52a8\u529b\u4e0b\u91c7\u6837\uff0c\u8fd9\u79cd\u65b9\u6cd5\u4e0d\u4ec5\u4fdd\u7559\u4e86\u539f\u6709\u7684\u6570\u636e\u751f\u6210\u80fd\u529b\uff0c\u8fd8\u63d0\u9ad8\u4e86\u6570\u636e\u7684\u7269\u7406\u4e00\u81f4\u6027\u3002"}}
{"id": "2510.24378", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.24378", "abs": "https://arxiv.org/abs/2510.24378", "authors": ["Yann Kerverdo", "Florent Leray", "Youwan Mah\u00e9", "St\u00e9phanie Leplaideur", "Francesca Galassi"], "title": "Stroke Lesion Segmentation in Clinical Workflows: A Modular, Lightweight, and Deployment-Ready Tool", "comment": null, "summary": "Deep learning frameworks such as nnU-Net achieve state-of-the-art performance\nin brain lesion segmentation but remain difficult to deploy clinically due to\nheavy dependencies and monolithic design. We introduce \\textit{StrokeSeg}, a\nmodular and lightweight framework that translates research-grade stroke lesion\nsegmentation models into deployable applications. Preprocessing, inference, and\npostprocessing are decoupled: preprocessing relies on the Anima toolbox with\nBIDS-compliant outputs, and inference uses ONNX Runtime with \\texttt{Float16}\nquantisation, reducing model size by about 50\\%. \\textit{StrokeSeg} provides\nboth graphical and command-line interfaces and is distributed as Python scripts\nand as a standalone Windows executable. On a held-out set of 300 sub-acute and\nchronic stroke subjects, segmentation performance was equivalent to the\noriginal PyTorch pipeline (Dice difference $<10^{-3}$), demonstrating that\nhigh-performing research pipelines can be transformed into portable, clinically\nusable tools.", "AI": {"tldr": "\u5f15\u5165\u4e86StrokeSeg\u6846\u67b6\uff0c\u7528\u4e8e\u5c06\u7814\u7a76\u7ea7\u522b\u7684\u4e2d\u98ce\u75c5\u53d8\u5206\u5272\u6a21\u578b\u8f6c\u5316\u4e3a\u4e34\u5e8a\u53ef\u7528\u7684\u5e94\u7528\u7a0b\u5e8f\uff0c\u8be5\u6846\u67b6\u8f7b\u91cf\u5316\u4e14\u6a21\u5757\u5316\uff0c\u63d0\u9ad8\u4e86\u4fbf\u643a\u6027\u548c\u4e34\u5e8a\u9002\u7528\u6027\u3002\u4e0e\u5176\u4ed6\u6846\u67b6\u76f8\u6bd4\uff0c\u5b83\u5728\u6027\u80fd\u4e0a\u51e0\u4e4e\u6ca1\u6709\u635f\u5931\uff0c\u800c\u6a21\u578b\u5927\u5c0f\u51cf\u5c0f\u4e86\u7ea650%\u3002", "motivation": "\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\u5982nnU-Net\u867d\u7136\u5728\u8111\u75c5\u53d8\u5206\u5272\u4e2d\u7684\u8868\u73b0\u975e\u5e38\u51fa\u8272\uff0c\u4f46\u5374\u56e0\u4e3a\u590d\u6742\u7684\u4f9d\u8d56\u5173\u7cfb\u548c\u5355\u4f53\u8bbe\u8ba1\u800c\u96be\u4ee5\u5728\u4e34\u5e8a\u5e94\u7528\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u52a0\u6a21\u5757\u5316\u548c\u8f7b\u91cf\u5316\u7684\u6846\u67b6\uff0c\u4ee5\u4fbf\u4e8e\u5c06\u7814\u7a76\u7ea7\u7684\u6a21\u578b\u8f6c\u5316\u4e3a\u9002\u5408\u4e34\u5e8a\u4f7f\u7528\u7684\u5de5\u5177\u3002", "method": "\u5c06\u9884\u5904\u7406\uff0c\u63a8\u7406\u548c\u540e\u5904\u7406\u5206\u5f00\uff0c\u9884\u5904\u7406\u4f7f\u7528Anima\u5de5\u5177\u7bb1\u9644\u5e26\u7684BIDS\u517c\u5bb9\u8f93\u51fa\u3002\u63a8\u7406\u90e8\u5206\u4f7f\u7528ONNX Runtime\uff0c\u5e76\u91c7\u7528Float16\u91cf\u5316\u6765\u51cf\u5c0f\u6a21\u578b\u5927\u5c0f\uff0c\u5c06\u6a21\u578b\u5927\u5c0f\u51cf\u5c11\u4e86\u7ea650%\u3002\u5f00\u53d1\u4e86\u56fe\u5f62\u548c\u547d\u4ee4\u884c\u754c\u9762\uff0c\u5e76\u53d1\u5e03\u4e86Python\u811a\u672c\u548c\u72ec\u7acb\u7684Windows\u5b89\u88c5\u7a0b\u5e8f\u3002", "result": "\u5f53\u4f7f\u7528300\u540d\u6162\u6027\u548c\u6025\u6027\u4e2d\u98ce\u60a3\u8005\u7684\u62bd\u6837\u6570\u636e\u96c6\u8fdb\u884c\u6d4b\u8bd5\u65f6\uff0c\u4e0e\u5176\u4ed6\u539f\u751f\u7684PyTorch\u7ba1\u9053\u76f8\u6bd4\uff0cSegmentation\u6027\u80fd\u51e0\u4e4e\u6ca1\u6709\u4e0b\u964d\uff0c\u5dee\u5f02\u5c0f\u4e8e10^-3\uff0c\u663e\u793a\u4e86\u7814\u7a76\u7ea7\u7ba1\u9053\u53ef\u4ee5\u5b8c\u5168\u8f6c\u5316\u4e3a\u9ad8\u6548\u3001\u4fbf\u4e8e\u643a\u5e26\u4e14\u9002\u5408\u7528\u4e8e\u4e34\u5e8a\u7684\u5de5\u5177\u3002", "conclusion": "\u901a\u8fc7\u5c06\u6df1\u5ea6\u5b66\u4e60\u7684\u75c5\u53d8\u5206\u5272\u6a21\u578b\u8f6c\u5316\u4e3a\u66f4\u4fbf\u643a\u548c\u6709\u6548\u7684\u5de5\u5177\uff0cStrokeSeg\u63d0\u4f9b\u4e86\u663e\u8457\u7684\u6539\u8fdb\uff0c\u4f7f\u5f97\u8fc7\u53bb\u53ea\u80fd\u7528\u4e8e\u7814\u7a76\u7684\u9ad8\u6027\u80fd\u5de5\u5177\u4e5f\u80fd\u7528\u4e8e\u4e34\u5e8a\u3002"}}
{"id": "2510.24663", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24663", "abs": "https://arxiv.org/abs/2510.24663", "authors": ["Yifu Lu", "Shengjie Liu", "Li Dong"], "title": "OrchDAG: Complex Tool Orchestration in Multi-Turn Interactions with Plan DAGs", "comment": "9 pages, 4 figures", "summary": "Agentic tool use has gained traction with the rise of agentic tool calling,\nyet most existing work overlooks the complexity of multi-turn tool\ninteractions. We introduce OrchDAG, a synthetic data generation pipeline that\nmodels tool execution as directed acyclic graphs (DAGs) with controllable\ncomplexity. Using this dataset, we benchmark model performance and propose a\ngraph-based reward to enhance RLVR training. Experiments show that the dataset\npresents a challenging but solvable benchmark, and the proposed reward is\neffective when combined with GRPO-style algorithms, highlighting the importance\nof leveraging topological structure and data complexity in multi-turn tool use.", "AI": {"tldr": "\u4ecb\u7ecd\u4e86OrchDAG\uff0c\u4e00\u79cd\u7528\u4e8e\u751f\u6210\u5408\u6210\u6570\u636e\u7684\u6570\u636e\u751f\u6210\u7ba1\u7ebf\uff0c\u8be5\u7ba1\u7ebf\u5c06\u5de5\u5177\u6267\u884c\u5efa\u6a21\u4e3a\u6709\u5411\u65e0\u73af\u56fe\uff08DAG\uff09\uff0c\u5177\u6709\u53ef\u63a7\u7684\u590d\u6742\u6027\u3002\u4f7f\u7528\u6b64\u6570\u636e\u96c6\uff0c\u5bf9\u6a21\u578b\u6027\u80fd\u8fdb\u884c\u4e86\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5e76\u63d0\u51fa\u4e86\u57fa\u4e8e\u56fe\u7684\u5956\u52b1\u4ee5\u589e\u5f3aRLVR\u8bad\u7ec3\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u6570\u636e\u96c6\u63d0\u4f9b\u4e86\u5177\u6709\u6311\u6218\u6027\u7684\u57fa\u51c6\uff0c\u5e76\u4e14\u6240\u63d0\u51fa\u7684\u5956\u52b1\u5728\u4e0eGRPO\u6837\u5f0f\u7684\u7b97\u6cd5\u7ed3\u5408\u4f7f\u7528\u65f6\u662f\u6709\u6548\u7684\uff0c\u5f3a\u8c03\u4e86\u5728\u591a\u8f6e\u5de5\u5177\u4f7f\u7528\u4e2d\u5229\u7528\u62d3\u6251\u7ed3\u6784\u548c\u6570\u636e\u590d\u6742\u6027\u7684\u91cd\u8981\u6027\u3002", "motivation": "\u8bb8\u591a\u73b0\u6709\u7684\u5de5\u4f5c\u5ffd\u7565\u4e86\u591a\u8f6e\u5de5\u5177\u4ea4\u4e92\u7684\u590d\u6742\u6027\uff0c\u672c\u6587\u65e8\u5728\u901a\u8fc7\u5f15\u5165OrchDAG\u6765\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002OrchDAG\u751f\u6210\u7684\u5408\u6210\u6570\u636e\u53ef\u4ee5\u6a21\u62df\u5177\u6709\u53ef\u63a7\u590d\u6742\u6027\u7684\u5de5\u5177\u6267\u884c\uff0c\u53ef\u4ee5\u4f5c\u4e3a\u7814\u7a76\u5de5\u5177\u4ea4\u4e92\u590d\u6742\u6027\u7684\u57fa\u51c6\u3002\u540c\u65f6\uff0c\u8bba\u6587\u8fd8\u65e8\u5728\u901a\u8fc7\u5f15\u5165\u57fa\u4e8e\u56fe\u7684\u5956\u52b1\u6765\u63d0\u9ad8RLVR\u8bad\u7ec3\u7684\u6548\u679c\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u5408\u6210\u6570\u636e\u751f\u6210\u7ba1\u7ebfOrchDAG\uff0c\u5c06\u5de5\u5177\u6267\u884c\u5efa\u6a21\u4e3aDAG\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u5bf9\u8be5\u6570\u636e\u96c6\u8fdb\u884c\u4e86\u57fa\u51c6\u6d4b\u8bd5\u3002\u5f15\u5165\u4e86\u4e00\u79cd\u57fa\u4e8e\u56fe\u7684\u5956\u52b1\uff0c\u7528\u4ee5\u589e\u5f3aRLVR\u8bad\u7ec3\uff0c\u5e76\u7ed3\u5408GRPO\u98ce\u683c\u7684\u7b97\u6cd5\u8fdb\u884c\u4e86\u5b9e\u9a8c\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u5c55\u793a\u4e86\u8be5\u6570\u636e\u96c6\u63d0\u4f9b\u4e86\u591a\u8f6e\u5de5\u5177\u4f7f\u7528\u4e2d\u7684\u590d\u6742\u6311\u6218\uff0c\u800c\u63d0\u51fa\u7684\u57fa\u4e8e\u56fe\u7684\u5956\u52b1\u5728\u4e0eGRPO\u98ce\u683c\u7b97\u6cd5\u7ed3\u5408\u4f7f\u7528\u65f6\u80fd\u591f\u663e\u8457\u63d0\u5347\u6a21\u578b\u6027\u80fd\u3002", "conclusion": "\u672c\u7814\u7a76\u5f3a\u8c03\u4e86\u5728\u591a\u8f6e\u5de5\u5177\u4f7f\u7528\u4e2d\u5229\u7528\u6570\u636e\u751f\u6210\u65b9\u6cd5\u548c\u57fa\u4e8e\u56fe\u7684\u5956\u52b1\u673a\u5236\u7684\u91cd\u8981\u6027\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6709\u6548\u7684\u6570\u636e\u751f\u6210\u548c\u8bc4\u4f30\u5de5\u5177\u4ea4\u4e92\u590d\u6742\u6027\u7684\u6846\u67b6\u3002"}}
{"id": "2510.23868", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.23868", "abs": "https://arxiv.org/abs/2510.23868", "authors": ["Zhichao Wang"], "title": "GIFT: Group-relative Implicit Fine Tuning Integrates GRPO with DPO and UNA", "comment": null, "summary": "I propose \\textbf{G}roup-relative \\textbf{I}mplicit \\textbf{F}ine\n\\textbf{T}uning (GIFT), a novel reinforcement learning framework for aligning\nLLMs. Instead of directly maximizing cumulative rewards like PPO or GRPO, GIFT\nminimizes the discrepancy between implicit and explicit reward models. It\ncombines three key ideas: (1) the online multi-response generation and\nnormalization of GRPO, (2) the implicit reward formulation of DPO, and (3) the\nimplicit-explicit reward alignment principle of UNA. By jointly normalizing the\nimplicit and explicit rewards, GIFT eliminates an otherwise intractable term\nthat prevents effective use of implicit rewards. This normalization transforms\nthe complex reward maximization objective into a simple mean squared error\n(MSE) loss between the normalized reward functions, converting a non-convex\noptimization problem into a convex, stable, and analytically differentiable\nformulation. Unlike offline methods such as DPO and UNA, GIFT remains on-policy\nand thus retains exploration capability. Compared to GRPO, it requires fewer\nhyperparameters, converges faster, and generalizes better with significantly\nreduced training overfitting. Empirically, GIFT achieves superior reasoning and\nalignment performance on mathematical benchmarks while remaining\ncomputationally efficient.", "AI": {"tldr": "\u63d0\u51fa\u4e86GIFT\u6846\u67b6\uff0c\u4e00\u79cd\u65b0\u7684\u5bf9\u9f50LLM\u7684\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u6700\u5c0f\u5316\u9690\u5f0f\u5956\u52b1\u6a21\u578b\u548c\u663e\u5f0f\u5956\u52b1\u6a21\u578b\u4e4b\u95f4\u7684\u5dee\u5f02\u6765\u5bf9\u9f50\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u800c\u4e0d\u662f\u76f4\u63a5\u6700\u5927\u5316\u7d2f\u79ef\u5956\u52b1\u3002GIFT\u5c06\u4e09\u79cd\u5173\u952e\u601d\u60f3\u76f8\u7ed3\u5408\uff0c\u89e3\u51b3\u4e86\u4ee5\u5f80\u65b9\u6cd5\u4e2d\u7684\u67d0\u4e9b\u95ee\u9898\uff0c\u5e76\u4e14\u5728\u6570\u5b66\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5982PPO\u3001GRPO\u7b49\u76f4\u63a5\u6700\u5927\u5316\u7d2f\u79ef\u5956\u52b1\u6216\u5b58\u5728\u8fc7\u5ea6\u62df\u5408\u3001\u975e\u51f8\u4f18\u5316\u95ee\u9898\uff0c\u9700\u8981\u66f4\u591a\u8d85\u53c2\u6570\u3002\u4e3a\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u5e76\u63d0\u9ad8\u6536\u655b\u901f\u5ea6\u548c\u6cdb\u5316\u80fd\u529b\uff0c\u63d0\u51fa\u4e86\u57fa\u4e8e\u6700\u5c0f\u5316\u5956\u52b1\u6a21\u578b\u5dee\u5f02\u7684\u65b0\u578b\u5bf9\u9f50\u6846\u67b6GIFT\uff0c\u540c\u65f6\u4fdd\u6301\u6548\u7387\u548c\u7a33\u5b9a\u6027\u3002", "method": "GIFT\u7ed3\u5408\u4e86GRPO\u7684\u5728\u7ebf\u591a\u54cd\u5e94\u751f\u6210\u548c\u5f52\u4e00\u5316\u3001DPO\u7684\u9690\u5f0f\u5956\u52b1\u516c\u5f0f\u5316\u4ee5\u53caUNA\u7684\u9690\u5f0f-\u663e\u5f0f\u5956\u52b1\u6821\u51c6\u539f\u5219\u3002\u901a\u8fc7\u540c\u65f6\u5f52\u4e00\u5316\u9690\u5f0f\u548c\u663e\u5f0f\u5956\u52b1\uff0c\u5c06\u590d\u6742\u7684\u5956\u52b1\u6700\u5927\u5316\u76ee\u6807\u8f6c\u6362\u4e3a\u7b80\u5316\u7684\u5747\u65b9\u8bef\u5dee\u635f\u5931\u51fd\u6570\uff0c\u4ece\u800c\u5c06\u975e\u51f8\u4f18\u5316\u95ee\u9898\u8f6c\u5316\u4e3a\u51f8\u3001\u7a33\u5b9a\u3001\u53ef\u5206\u6790\u7684\u5fae\u5206\u5f62\u5f0f\u3002\u8be5\u65b9\u6cd5\u662f\u975e\u79bb\u7ebf\u65b9\u6cd5\uff0c\u56e0\u6b64\u4fdd\u7559\u4e86\u63a2\u7d22\u80fd\u529b\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cGIFT\u5728\u6570\u5b66\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u7684\u63a8\u7406\u548c\u5bf9\u9f50\u6027\u80fd\u66f4\u4f18\uff0c\u4e14\u8ba1\u7b97\u6548\u7387\u66f4\u9ad8\u3002\u4e0eGRPO\u76f8\u6bd4\uff0c\u5b83\u9700\u8981\u66f4\u5c11\u7684\u8d85\u53c2\u6570\uff0c\u6536\u655b\u901f\u5ea6\u66f4\u5feb\uff0c\u4e14\u6cdb\u5316\u80fd\u529b\u66f4\u5f3a\uff0c\u663e\u8457\u51cf\u5c11\u8bad\u7ec3\u8fc7\u62df\u5408\u3002", "conclusion": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5bf9\u9f50\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u4f18\u5316\u6846\u67b6GIFT\uff0c\u901a\u8fc7\u7ed3\u5408\u4e09\u79cd\u5173\u952e\u601d\u60f3\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5b58\u5728\u7684\u95ee\u9898\uff0c\u8fbe\u5230\u4e86\u66f4\u597d\u7684\u6027\u80fd\u3002"}}
{"id": "2510.24690", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24690", "abs": "https://arxiv.org/abs/2510.24690", "authors": ["Shengjie Liu", "Li Dong", "Zhenyu Zhang"], "title": "Bridging Tool Dependencies and Domain Knowledge: A Graph-Based Framework for In-Context Planning", "comment": "4 pages, 2 figures, short paper, NeurIPS 2025 workshop on Bridging\n  Language, Agent, and World Models for Reasoning and Planning", "summary": "We present a framework for uncovering and exploiting dependencies among tools\nand documents to enhance exemplar artifact generation. Our method begins by\nconstructing a tool knowledge graph from tool schemas,including descriptions,\narguments, and output payloads, using a DeepResearch-inspired analysis. In\nparallel, we derive a complementary knowledge graph from internal documents and\nSOPs, which is then fused with the tool graph. To generate exemplar plans, we\nadopt a deep-sparse integration strategy that aligns structural tool\ndependencies with procedural knowledge. Experiments demonstrate that this\nunified framework effectively models tool interactions and improves plan\ngeneration, underscoring the benefits of linking tool graphs with domain\nknowledge graphs for tool-augmented reasoning and planning.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6846\u67b6\uff0c\u901a\u8fc7\u6316\u6398\u548c\u5229\u7528\u5de5\u5177\u53ca\u6587\u6863\u4e4b\u95f4\u7684\u4f9d\u8d56\u5173\u7cfb\uff0c\u6765\u589e\u5f3a\u793a\u4f8b\u751f\u6210\u3002\u901a\u8fc7DeepResearch\u542f\u53d1\u7684\u5206\u6790\u65b9\u6cd5\u6784\u5efa\u5de5\u5177\u77e5\u8bc6\u56fe\u8c31\uff0c\u5e76\u7ed3\u5408\u5185\u90e8\u6587\u6863\u77e5\u8bc6\u56fe\u8c31\uff0c\u91c7\u7528\u6df1\u7a00\u758f\u96c6\u6210\u7b56\u7565\u751f\u6210\u8303\u4f8b\u8ba1\u5212\u3002\u5b9e\u9a8c\u8868\u660e\u8be5\u6846\u67b6\u80fd\u591f\u6709\u6548\u5efa\u6a21\u5de5\u5177\u4ea4\u4e92\u5e76\u63d0\u5347\u8ba1\u5212\u751f\u6210\u8d28\u91cf\u3002", "motivation": "\u4e3a\u4e86\u66f4\u597d\u5730\u7406\u89e3\u548c\u5229\u7528\u5de5\u5177\u4e4b\u95f4\u7684\u4f9d\u8d56\u5173\u7cfb\uff0c\u4ee5\u589e\u5f3a\u8303\u4f8b\u751f\u6210\u7684\u8d28\u91cf\u548c\u6548\u7387\u3002\u63d0\u9ad8\u5de5\u5177\u548c\u6587\u6863\u4e4b\u95f4\u7684\u8054\u7cfb\u5728\u751f\u6210\u8ba1\u5212\u4e2d\u7684\u4f5c\u7528\u3002", "method": "\u4ece\u5de5\u5177\u63cf\u8ff0\u3001\u53c2\u6570\u548c\u8f93\u51fa\u8d1f\u8f7d\u6784\u5efa\u5de5\u5177\u77e5\u8bc6\u56fe\u8c31\uff1b\u4ece\u5185\u90e8\u6587\u6863\u548c\u64cd\u4f5c\u624b\u518c\u6784\u5efa\u77e5\u8bc6\u56fe\u8c31\uff0c\u5e76\u5c06\u5176\u4e0e\u5de5\u5177\u56fe\u8c31\u878d\u5408\uff1b\u91c7\u7528\u6df1\u7a00\u758f\u96c6\u6210\u7b56\u7565\u751f\u6210\u8303\u4f8b\u8ba1\u5212\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\u8be5\u6846\u67b6\u80fd\u6709\u6548\u5efa\u6a21\u5de5\u5177\u4ea4\u4e92\u5e76\u63d0\u5347\u8ba1\u5212\u751f\u6210\u8d28\u91cf\uff0c\u7279\u522b\u5f3a\u8c03\u4e86\u5c06\u5de5\u5177\u56fe\u8c31\u4e0e\u9886\u57df\u77e5\u8bc6\u56fe\u8c31\u8fde\u63a5\u7684\u91cd\u8981\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u901a\u8fc7\u6846\u67b6\u7684\u6709\u6548\u7406\u8bba\u548c\u5b9e\u9a8c\u7ed3\u679c\u9a8c\u8bc1\u4e86\u5c06\u5de5\u5177\u56fe\u8c31\u4e0e\u9886\u57df\u77e5\u8bc6\u56fe\u8c31\u76f8\u7ed3\u5408\u7684\u5408\u7406\u6027\uff0c\u672a\u6765\u7684\u5de5\u4f5c\u53ef\u4ee5\u5728\u6b64\u57fa\u7840\u4e0a\u8fdb\u4e00\u6b65\u4f18\u5316\u548c\u6269\u5c55\u3002"}}
{"id": "2510.24385", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.24385", "abs": "https://arxiv.org/abs/2510.24385", "authors": ["Herman Bergstr\u00f6m", "Zhongqi Yue", "Fredrik D. Johansson"], "title": "When are radiology reports useful for training medical image classifiers?", "comment": null, "summary": "Medical images used to train machine learning models are often accompanied by\nradiology reports containing rich expert annotations. However, relying on these\nreports as inputs for clinical prediction requires the timely manual work of a\ntrained radiologist. This raises a natural question: when can radiology reports\nbe leveraged during training to improve image-only classification? Prior works\nare limited to evaluating pre-trained image representations by fine-tuning them\nto predict diagnostic labels, often extracted from reports, ignoring tasks with\nlabels that are weakly associated with the text. To address this gap, we\nconduct a systematic study of how radiology reports can be used during both\npre-training and fine-tuning, across diagnostic and prognostic tasks (e.g.,\n12-month readmission), and under varying training set sizes. Our findings\nreveal that: (1) Leveraging reports during pre-training is beneficial for\ndownstream classification tasks where the label is well-represented in the\ntext; however, pre-training through explicit image-text alignment can be\ndetrimental in settings where it's not; (2) Fine-tuning with reports can lead\nto significant improvements and even have a larger impact than the pre-training\nmethod in certain settings. These results provide actionable insights into when\nand how to leverage privileged text data to train medical image classifiers\nwhile highlighting gaps in current research.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86\u5728\u533b\u5b66\u56fe\u50cf\u5206\u7c7b\u4e2d\u5229\u7528\u653e\u5c04\u79d1\u62a5\u544a\u7684\u65f6\u673a\u548c\u65b9\u6cd5\uff0c\u4ece\u800c\u5728\u9884\u8bad\u7ec3\u548c\u5fae\u8c03\u9636\u6bb5\u6539\u8fdb\u5206\u7c7b\u5668\u6027\u80fd\u3002\u53d1\u73b0\u653e\u5c04\u79d1\u62a5\u544a\u5bf9\u6587\u672c\u4e2d\u826f\u597d\u8868\u793a\u7684\u6807\u7b7e\u6709\u5229\uff0c\u4f46\u5728\u67d0\u4e9b\u573a\u666f\u4e0b\u901a\u8fc7\u663e\u5f0f\u56fe\u50cf-\u6587\u672c\u5bf9\u9f50\u8fdb\u884c\u9884\u8bad\u7ec3\u53ef\u80fd\u4f1a\u6709\u5bb3\u3002\u540c\u65f6\uff0c\u5728\u67d0\u4e9b\u60c5\u51b5\u4e0b\uff0c\u5229\u7528\u62a5\u544a\u8fdb\u884c\u5fae\u8c03\u751a\u81f3\u80fd\u6bd4\u9884\u8bad\u7ec3\u65b9\u6cd5\u5e26\u6765\u66f4\u5927\u7684\u6539\u8fdb\u3002", "motivation": "\u4f20\u7edf\u7684\u673a\u5668\u5b66\u4e60\u6a21\u578b\u8bad\u7ec3\u901a\u5e38\u4f9d\u8d56\u653e\u5c04\u79d1\u62a5\u544a\u4e2d\u7684\u4e13\u5bb6\u6ce8\u91ca\uff0c\u4f46\u8fd9\u4e9b\u62a5\u544a\u9700\u8981\u7ecf\u8fc7\u8bad\u7ec3\u7684\u653e\u5c04\u79d1\u533b\u751f\u624b\u52a8\u8f93\u5165\uff0c\u9650\u5236\u4e86\u5176\u5728\u4e34\u5e8a\u9884\u6d4b\u4e2d\u7684\u5373\u65f6\u5e94\u7528\u3002\u56e0\u6b64\uff0c\u7814\u7a76\u63a2\u8ba8\u4e86\u5728\u4f55\u79cd\u60c5\u51b5\u4e0b\u53ef\u4ee5\u5229\u7528\u653e\u5c04\u79d1\u62a5\u544a\u52a0\u5f3a\u57fa\u4e8e\u56fe\u50cf\u7684\u5206\u7c7b\u6027\u80fd\uff0c\u586b\u8865\u4e86\u5f53\u524d\u7814\u7a76\u7684\u7a7a\u767d\u3002", "method": "\u7814\u7a76\u7cfb\u7edf\u6027\u5730\u8003\u5bdf\u4e86\u653e\u5c04\u79d1\u62a5\u544a\u5728\u9884\u8bad\u7ec3\u548c\u5fae\u8c03\u9636\u6bb5\u7684\u4f5c\u7528\uff0c\u7814\u7a76\u4e86\u8bca\u65ad\u548c\u9884\u540e\u4efb\u52a1\u4e2d\u7684\u6548\u679c\uff0c\u5e76\u6d4b\u8bd5\u4e86\u4e0d\u540c\u8bad\u7ec3\u96c6\u5927\u5c0f\u7684\u60c5\u51b5\u3002\u62a5\u544a\u662f\u5426\u4e0e\u6587\u672c\u6587\u672c\u5f3a\u76f8\u5173\u662f\u7814\u7a76\u4e2d\u8003\u8651\u7684\u5173\u952e\u56e0\u7d20\u4e4b\u4e00\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c\u5728\u6807\u7b7e\u5728\u6587\u672c\u4e2d\u6709\u826f\u597d\u8868\u793a\u7684\u60c5\u51b5\u4e0b\uff0c\u5229\u7528\u62a5\u544a\u8fdb\u884c\u9884\u8bad\u7ec3\u662f\u6709\u76ca\u7684\uff0c\u4f46\u5728\u4e0e\u6587\u672c\u5f31\u76f8\u5173\u7684\u573a\u666f\u4e2d\uff0c\u663e\u5f0f\u7684\u56fe\u50cf-\u6587\u672c\u5bf9\u9f50\u9884\u8bad\u7ec3\u53cd\u800c\u53ef\u80fd\u6548\u679c\u4e0d\u4f73\u3002\u6b64\u5916\uff0c\u5229\u7528\u62a5\u544a\u8fdb\u884c\u5fae\u8c03\u5728\u67d0\u4e9b\u60c5\u51b5\u4e0b\u53ef\u4ee5\u5e26\u6765\u663e\u8457\u6539\u8fdb\uff0c\u4e14\u6539\u8fdb\u5e45\u5ea6\u751a\u81f3\u8d85\u8fc7\u9884\u8bad\u7ec3\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u4f9b\u4e86\u6709\u5173\u4f55\u65f6\u4ee5\u53ca\u5982\u4f55\u5229\u7528\u7279\u6743\u6587\u672c\u6570\u636e\u6765\u8bad\u7ec3\u533b\u5b66\u56fe\u50cf\u5206\u7c7b\u5668\u7684\u5b9e\u9645\u89c1\u89e3\uff0c\u5e76\u6307\u51fa\u4e86\u5f53\u524d\u7814\u7a76\u7684\u4e0d\u8db3\u4e4b\u5904\u3002"}}
{"id": "2505.22820", "categories": ["cs.LG", "cs.AI", "econ.TH", "stat.ML"], "pdf": "https://arxiv.org/pdf/2505.22820", "abs": "https://arxiv.org/abs/2505.22820", "authors": ["Ayush Sawarni", "Sahasrajit Sarmasarkar", "Vasilis Syrgkanis"], "title": "Preference Learning with Response Time: Robust Losses and Guarantees", "comment": "Accepted at NeurIPS 2025", "summary": "This paper investigates the integration of response time data into human\npreference learning frameworks for more effective reward model elicitation.\nWhile binary preference data has become fundamental in fine-tuning foundation\nmodels, generative AI systems, and other large-scale models, the valuable\ntemporal information inherent in user decision-making remains largely\nunexploited. We propose novel methodologies to incorporate response time\ninformation alongside binary choice data, leveraging the Evidence Accumulation\nDrift Diffusion (EZ) model, under which response time is informative of the\npreference strength. We develop Neyman-orthogonal loss functions that achieve\noracle convergence rates for reward model learning, matching the theoretical\noptimal rates that would be attained if the expected response times for each\nquery were known a priori. Our theoretical analysis demonstrates that for\nlinear reward functions, conventional preference learning suffers from error\nrates that scale exponentially with reward magnitude. In contrast, our response\ntime-augmented approach reduces this to polynomial scaling, representing a\nsignificant improvement in sample efficiency. We extend these guarantees to\nnon-parametric reward function spaces, establishing convergence properties for\nmore complex, realistic reward models. Our extensive experiments validate our\ntheoretical findings in the context of preference learning over images.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u5c06\u54cd\u5e94\u65f6\u95f4\u6570\u636e\u6574\u5408\u5230\u4eba\u7c7b\u504f\u597d\u5b66\u4e60\u6846\u67b6\u4e2d\u7684\u65b9\u6cd5\uff0c\u4ee5\u66f4\u6709\u6548\u5730\u63d0\u53d6\u5956\u52b1\u6a21\u578b\u3002\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ed3\u5408\u4e8c\u5143\u9009\u62e9\u6570\u636e\u548c\u54cd\u5e94\u65f6\u95f4\u4fe1\u606f\u6765\u6539\u5584\u5956\u52b1\u6a21\u578b\u5b66\u4e60\uff0c\u8fd9\u79cd\u65b0\u65b9\u6cd5\u63d0\u9ad8\u4e86\u6837\u672c\u6548\u7387\uff0c\u5c24\u5176\u5728\u5904\u7406\u7ebf\u6027\u5956\u52b1\u51fd\u6570\u65f6\uff0c\u5176\u8bef\u5dee\u7387\u4ec5\u4e3a\u6307\u6570\u7ea7\u589e\u957f\u7684\u4e00\u5c0f\u90e8\u5206\uff0c\u5728\u975e\u53c2\u6570\u5956\u52b1\u51fd\u6570\u7a7a\u95f4\u4e2d\u4e5f\u5f97\u5230\u4e86\u6536\u655b\u6027\u4fdd\u8bc1\u3002\u5b9e\u9a8c\u8868\u660e\u8be5\u65b9\u6cd5\u5728\u5904\u7406\u56fe\u50cf\u504f\u597d\u5b66\u4e60\u65f6\u7684\u6709\u6548\u6027\u5f97\u5230\u4e86\u9a8c\u8bc1\u3002", "motivation": "\u4f20\u7edf\u7684\u4eba\u7c7b\u504f\u597d\u5b66\u4e60\u6846\u67b6\u4e3b\u8981\u4f9d\u8d56\u4e8c\u5143\u504f\u597d\u6570\u636e\uff0c\u4f46\u5ffd\u7565\u4e86\u7528\u6237\u51b3\u7b56\u8fc7\u7a0b\u4e2d\u6709\u4ef7\u503c\u7684\u65f6\u95f4\u4fe1\u606f\u3002\u672c\u6587\u5e0c\u671b\u901a\u8fc7\u5f15\u5165\u54cd\u5e94\u65f6\u95f4\u6570\u636e\uff0c\u4ee5\u671f\u63d0\u9ad8\u5956\u52b1\u6a21\u578b\u7684\u63d0\u53d6\u6548\u7387\u548c\u51c6\u786e\u6027\uff0c\u5c24\u5176\u662f\u5728\u5904\u7406\u7ebf\u6027\u5956\u52b1\u51fd\u6570\u65f6\u514b\u670d\u6307\u6570\u7ea7\u8bef\u5dee\u7387\u7684\u77ed\u677f\u3002", "method": "\u8be5\u7814\u7a76\u57fa\u4e8e\u8bc1\u636e\u79ef\u7d2f\u6f02\u79fb\u6269\u6563(EZ)\u6a21\u578b\u7684\u539f\u7406\uff0c\u63d0\u51fa\u4e86\u5229\u7528\u54cd\u5e94\u65f6\u95f4\u6570\u636e\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u6784\u5efaNeyman\u6b63\u4ea4\u635f\u5931\u51fd\u6570\u5b9e\u73b0\u4e86\u5956\u52b1\u6a21\u578b\u5b66\u4e60\u7684\u6700\u4f18\u7406\u8bba\u6536\u655b\u7387\uff0c\u540c\u65f6\u5ef6\u4f38\u5230\u4e86\u975e\u53c2\u6570\u5956\u52b1\u51fd\u6570\u7a7a\u95f4\uff0c\u4fdd\u8bc1\u4e86\u5728\u66f4\u591a\u73b0\u5b9e\u5956\u52b1\u6a21\u578b\u4e2d\u7684\u6536\u655b\u6027\u3002\u5b9e\u9a8c\u5728\u56fe\u50cf\u504f\u597d\u5b66\u4e60\u4e2d\u8fdb\u884c\u4e86\u9a8c\u8bc1\u3002", "result": "\u901a\u8fc7\u5c06\u54cd\u5e94\u65f6\u95f4\u4fe1\u606f\u878d\u5165\u504f\u597d\u5b66\u4e60\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u7ebf\u6027\u5956\u52b1\u51fd\u6570\u7684\u5b66\u4e60\u6548\u7387\uff0c\u4ece\u6307\u6570\u7ea7\u7684\u8bef\u5dee\u7387\u4e0b\u964d\u5230\u4e86\u591a\u9636\u591a\u9879\u5f0f\u7ea7\uff0c\u5176\u5728\u975e\u53c2\u6570\u5956\u52b1\u51fd\u6570\u7a7a\u95f4\u4e2d\u7684\u6536\u655b\u6027\u4e5f\u5f97\u5230\u4e86\u4fdd\u8bc1\uff0c\u5b9e\u9a8c\u7ed3\u679c\u8bc1\u660e\u4e86\u6240\u63d0\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684\u65b0\u65b9\u6cd5\u8bbe\u6cd5\u901a\u8fc7\u5f15\u5165\u54cd\u5e94\u65f6\u95f4\u6570\u636e\u63d0\u9ad8\u504f\u597d\u5b66\u4e60\u7684\u6548\u7387\uff0c\u6709\u6548\u964d\u4f4e\u4e86\u8bef\u5dee\u7387\uff0c\u5e76\u5728\u591a\u79cd\u5956\u52b1\u51fd\u6570\u4e2d\u90fd\u8868\u73b0\u51fa\u8f83\u597d\u7684\u6536\u655b\u6027\uff0c\u4e3a\u66f4\u590d\u6742\u3001\u73b0\u5b9e\u7684\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\u548c\u5b9e\u9a8c\u9a8c\u8bc1\u3002"}}
{"id": "2510.23901", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.23901", "abs": "https://arxiv.org/abs/2510.23901", "authors": ["Cristobal Heredia", "Pedro Chumpitaz-Flores", "Kaixun Hua"], "title": "RS-ORT: A Reduced-Space Branch-and-Bound Algorithm for Optimal Regression Trees", "comment": "20 pages, 1 figure, uses ICLR 2026 LaTeX style. Submitted to arXiv as\n  a preprint version", "summary": "Mixed-integer programming (MIP) has emerged as a powerful framework for\nlearning optimal decision trees. Yet, existing MIP approaches for regression\ntasks are either limited to purely binary features or become computationally\nintractable when continuous, large-scale data are involved. Naively binarizing\ncontinuous features sacrifices global optimality and often yields needlessly\ndeep trees. We recast the optimal regression-tree training as a two-stage\noptimization problem and propose Reduced-Space Optimal Regression Trees\n(RS-ORT) - a specialized branch-and-bound (BB) algorithm that branches\nexclusively on tree-structural variables. This design guarantees the\nalgorithm's convergence and its independence from the number of training\nsamples. Leveraging the model's structure, we introduce several bound\ntightening techniques - closed-form leaf prediction, empirical threshold\ndiscretization, and exact depth-1 subtree parsing - that combine with\ndecomposable upper and lower bounding strategies to accelerate the training.\nThe BB node-wise decomposition enables trivial parallel execution, further\nalleviating the computational intractability even for million-size datasets.\nBased on the empirical studies on several regression benchmarks containing both\nbinary and continuous features, RS-ORT also delivers superior training and\ntesting performance than state-of-the-art methods. Notably, on datasets with up\nto 2,000,000 samples with continuous features, RS-ORT can obtain guaranteed\ntraining performance with a simpler tree structure and a better generalization\nability in four hours.", "AI": {"tldr": "\u6587\u7ae0\u63d0\u51fa\u4e86Reduced-Space Optimal Regression Trees (RS-ORT)\uff0c\u8fd9\u662f\u4e00\u79cd\u4e13\u95e8\u9488\u5bf9\u56de\u5f52\u4efb\u52a1\u8bbe\u8ba1\u7684\u5206\u652f\u5b9a\u754c\u7b97\u6cd5\uff0c\u80fd\u591f\u5728\u5904\u7406\u5927\u89c4\u6a21\u8fde\u7eed\u6570\u636e\u65f6\u4fdd\u8bc1\u5168\u5c40\u6700\u4f18\u6027\uff0c\u540c\u65f6\u4fdd\u6301\u8f83\u4f4e\u7684\u6811\u6df1\u5ea6\u548c\u826f\u597d\u7684\u6cdb\u5316\u80fd\u529b\u3002\u6b64\u7b97\u6cd5\u901a\u8fc7\u8282\u70b9\u7ea7\u522b\u7684\u5206\u89e3\u548c\u591a\u79cd\u8fb9\u754c\u6536\u7d27\u6280\u672f\u52a0\u901f\u8bad\u7ec3\u8fc7\u7a0b\uff0c\u5e76\u53ef\u4ee5\u5b9e\u73b0\u5e76\u884c\u6267\u884c\uff0c\u4ece\u800c\u63d0\u5347\u8ba1\u7b97\u6548\u7387\uff0c\u9002\u7528\u4e8e\u5305\u542b\u767e\u4e07\u6837\u672c\u7684\u6570\u636e\u96c6\u3002", "motivation": "\u73b0\u6709\u7684MIP\u56de\u5f52\u4efb\u52a1\u65b9\u6cd5\u8981\u4e48\u4ec5\u9650\u4e8e\u4e8c\u8fdb\u5236\u7279\u5f81\uff0c\u8981\u4e48\u5728\u5904\u7406\u8fde\u7eed\u5927\u89c4\u6a21\u6570\u636e\u65f6\u53d8\u5f97\u8ba1\u7b97\u4e0a\u65e0\u6cd5\u89e3\u51b3\u3002\u5355\u7eaf\u5c06\u8fde\u7eed\u7279\u5f81\u4e8c\u503c\u5316\u4f1a\u635f\u5931\u5168\u5c40\u6700\u4f18\u6027\u3002\u6587\u7ae0\u610f\u5728\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u63d0\u51fa\u4e00\u79cd\u65b0\u7684\u65b9\u6cd5\u6765\u5b66\u4e60\u6700\u4f18\u51b3\u7b56\u6811\u3002", "method": "\u6587\u7ae0\u5c06\u6700\u4f18\u56de\u5f52\u6811\u8bad\u7ec3\u91cd\u65b0\u5b9a\u4e49\u4e3a\u4e00\u4e2a\u4e24\u9636\u6bb5\u4f18\u5316\u95ee\u9898\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u4e13\u95e8\u7684\u5206\u652f\u5b9a\u754c\u7b97\u6cd5RS-ORT\uff0c\u8be5\u7b97\u6cd5\u53ea\u5728\u6811\u7ed3\u6784\u53d8\u91cf\u4e0a\u5206\u652f\u3002\u901a\u8fc7\u5229\u7528\u6a21\u578b\u7ed3\u6784\uff0c\u5f15\u5165\u4e86\u591a\u79cd\u8fb9\u754c\u6536\u7d27\u6280\u672f\u6765\u52a0\u901f\u8bad\u7ec3\u8fc7\u7a0b\uff0c\u540c\u65f6\u8be5\u65b9\u6cd5\u8fd8\u80fd\u5b9e\u73b0\u5e76\u884c\u6267\u884c\uff0c\u63d0\u9ad8\u8ba1\u7b97\u6548\u7387\u3002", "result": "\u5728\u591a\u4e2a\u5305\u542b\u4e8c\u8fdb\u5236\u548c\u8fde\u7eed\u7279\u5f81\u7684\u56de\u5f52\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8bc1\u660e\uff0cRS-ORT\u5728\u8bad\u7ec3\u6027\u80fd\u548c\u6d4b\u8bd5\u6027\u80fd\u65b9\u9762\u5747\u8d85\u8fc7\u4e86\u73b0\u6709\u65b9\u6cd5\uff0c\u5e76\u4e14\u5728\u5177\u6709\u8fde\u7eed\u7279\u5f81\u7684\u6570\u636e\u96c6\u4e0a\uff0c\u80fd\u591f\u57284\u5c0f\u65f6\u5185\uff0c\u83b7\u5f97\u4fdd\u8bc1\u7684\u8bad\u7ec3\u6027\u80fd\uff0c\u5177\u6709\u66f4\u7b80\u5355\u7684\u6811\u7ed3\u6784\u548c\u66f4\u5f3a\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "RS-ORT\u901a\u8fc7\u4e13\u95e8\u7684\u5206\u652f\u5b9a\u754c\u7b97\u6cd5\u8bbe\u8ba1\u548c\u4e00\u7cfb\u5217\u4f18\u5316\u6280\u672f\uff0c\u4fdd\u8bc1\u4e86\u7b97\u6cd5\u5728\u5904\u7406\u5927\u89c4\u6a21\u8fde\u7eed\u6570\u636e\u96c6\u65f6\u7684\u5168\u5c40\u6700\u4f18\u6027\u3001\u8ba1\u7b97\u6548\u7387\u548c\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2510.24398", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.24398", "abs": "https://arxiv.org/abs/2510.24398", "authors": ["Youwan Mah\u00e9", "Elise Bannier", "St\u00e9phanie Leplaideur", "Elisa Fromont", "Francesca Galassi"], "title": "Unsupervised Detection of Post-Stroke Brain Abnormalities", "comment": null, "summary": "Post-stroke MRI not only delineates focal lesions but also reveals secondary\nstructural changes, such as atrophy and ventricular enlargement. These\nabnormalities, increasingly recognised as imaging biomarkers of recovery and\noutcome, remain poorly captured by supervised segmentation methods. We evaluate\nREFLECT, a flow-based generative model, for unsupervised detection of both\nfocal and non-lesional abnormalities in post-stroke patients. Using dual-expert\ncentral-slice annotations on ATLAS data, performance was assessed at the object\nlevel with Free-Response ROC analysis for anomaly maps. Two models were trained\non lesion-free slices from stroke patients (ATLAS) and on healthy controls\n(IXI) to test the effect of training data. On ATLAS test subjects, the\nIXI-trained model achieved higher lesion segmentation (Dice = 0.37 vs 0.27) and\nimproved sensitivity to non-lesional abnormalities (FROC = 0.62 vs 0.43).\nTraining on fully healthy anatomy improves the modelling of normal variability,\nenabling broader and more reliable detection of structural abnormalities.", "AI": {"tldr": "REFLECT, a flow-based generative model, is evaluated for unsupervised detection of abnormalities in post-stroke patients, showing improvements in lesion segmentation and sensitivity to non-lesional abnormalities when trained on healthy controls.", "motivation": "Supervised segmentation methods are not good at capturing secondary structural changes after a stroke, such as atrophy and ventricular enlargement. REFLECT aims to address this issue with unsupervised detection of both focal and non-lesional abnormalities.", "method": "The study uses REFLECT, a flow-based generative model, to detect abnormalities in post-stroke patients' MRI scans. It compares the model's performance when trained on lesion-free slices from stroke patients versus healthy controls.", "result": "The model trained on healthy controls achieved better performance in lesion segmentation and showed higher sensitivity to non-lesional abnormalities compared to the model trained on lesion-free slices from stroke patients.", "conclusion": "Training on fully healthy anatomy helps the model better capture normal variability, leading to more reliable detection of structural abnormalities in post-stroke patients."}}
{"id": "2510.16620", "categories": ["cs.IT", "cs.AI", "cs.CR", "cs.LG", "eess.SP", "math.IT"], "pdf": "https://arxiv.org/pdf/2510.16620", "abs": "https://arxiv.org/abs/2510.16620", "authors": ["Yingyao Zhou", "Natasha Devroye", "Onur G\u00fcnl\u00fc"], "title": "Feedback Lunch: Deep Feedback Codes for Wiretap Channels", "comment": null, "summary": "We consider reversely-degraded wiretap channels, for which the secrecy\ncapacity is zero if there is no channel feedback. This work focuses on a seeded\nmodular code design for the Gaussian wiretap channel with channel output\nfeedback, combining universal hash functions for security and learned\nfeedback-based codes for reliability to achieve positive secrecy rates. We\nstudy the trade-off between communication reliability and information leakage,\nillustrating that feedback enables agreeing on a secret key shared between\nlegitimate parties, overcoming the security advantage of the wiretapper. Our\nfindings also motivate code designs for sensing-assisted secure communication,\nto be used in next-generation integrated sensing and communication methods.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u5177\u6709\u8f93\u51fa\u53cd\u9988\u7684\u9ad8\u65af\u7a83\u542c\u4fe1\u9053\uff0c\u901a\u8fc7\u7ed3\u5408\u901a\u7528\u54c8\u5e0c\u51fd\u6570\u548c\u57fa\u4e8e\u53cd\u9988\u5b66\u4e60\u7684\u7f16\u7801\uff0c\u5728\u4fdd\u8bc1\u4fe1\u606f\u53ef\u9760\u4f20\u8f93\u7684\u540c\u65f6\u5b9e\u73b0\u79d8\u5bc6\u901f\u7387\u3002\u53d1\u73b0\u53cd\u9988\u4f7f\u5408\u6cd5\u7528\u6237\u80fd\u591f\u534f\u5546\u5171\u4eab\u7684\u79d8\u5bc6\u5bc6\u94a5\uff0c\u514b\u670d\u4e86\u7a83\u542c\u8005\u7684\u5b89\u5168\u4f18\u52bf\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4f20\u611f\u8f85\u52a9\u7684\u5b89\u5168\u901a\u4fe1\u4ee3\u7801\u8bbe\u8ba1\u65b9\u6cd5\u3002", "motivation": "\u8003\u8651\u4fdd\u5bc6\u5bb9\u91cf\u4e3a\u96f6\u7684\u53cd\u5411\u9000\u5316\u7a83\u542c\u4fe1\u9053\uff0c\u5728\u6ca1\u6709\u4fe1\u9053\u53cd\u9988\u7684\u60c5\u51b5\u4e0b\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u5e26\u6709\u4fe1\u9053\u8f93\u51fa\u53cd\u9988\u7684\u79cd\u5b50\u6a21\u5757\u5316\u4ee3\u7801\u8bbe\u8ba1\uff0c\u4ee5\u5b9e\u73b0\u6b63\u7684\u79d8\u5bc6\u901f\u7387\u3002\u4e3b\u8981\u7814\u7a76\u4e86\u901a\u4fe1\u53ef\u9760\u6027\u548c\u4fe1\u606f\u6cc4\u9732\u4e4b\u95f4\u7684\u6743\u8861\uff0c\u4ee5\u514b\u670d\u7a83\u542c\u8005\u7684\u5b89\u5168\u4f18\u52bf\uff0c\u5e76\u4e3a\u4e0b\u4e00\u4ee3\u96c6\u6210\u4f20\u611f\u548c\u901a\u4fe1\u63d0\u4f9b\u4ee3\u7801\u8bbe\u8ba1\u65b9\u6cd5\u3002", "method": "\u7ed3\u5408\u901a\u7528\u54c8\u5e0c\u51fd\u6570\u548c\u57fa\u4e8e\u53cd\u9988\u5b66\u4e60\u7684\u7f16\u7801\u8bbe\u8ba1\uff0c\u5229\u7528\u53cd\u9988\u4f7f\u5408\u6cd5\u7528\u6237\u4e4b\u95f4\u53ef\u4ee5\u5efa\u7acb\u5171\u4eab\u7684\u79d8\u5bc6\u5bc6\u94a5\uff0c\u4ee5\u63d0\u9ad8\u79d8\u5bc6\u901a\u4fe1\u7684\u5b89\u5168\u6027\u3002", "result": "\u7814\u7a76\u8868\u660e\uff0c\u901a\u8fc7\u8fd9\u79cd\u65b9\u6cd5\u80fd\u591f\u5b9e\u73b0\u6b63\u7684\u79d8\u5bc6\u901f\u7387\uff0c\u540c\u65f6\u5177\u6709\u8f83\u9ad8\u7684\u901a\u4fe1\u53ef\u9760\u6027\uff0c\u514b\u670d\u4e86\u7a83\u542c\u8005\u7684\u5b89\u5168\u4f18\u52bf\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u5c55\u793a\u4e86\u5728\u7a83\u542c\u4fe1\u9053\u4e2d\u5229\u7528\u53cd\u9988\u5b9e\u73b0\u6b63\u79d8\u5bc6\u901f\u7387\u7684\u6f5c\u529b\uff0c\u5e76\u901a\u8fc7\u5f15\u5165\u4f20\u611f\u8f85\u52a9\u7684\u5b89\u5168\u901a\u4fe1\u4ee3\u7801\u8bbe\u8ba1\u65b9\u6cd5\uff0c\u4e3a\u672a\u6765\u7684\u96c6\u6210\u4f20\u611f\u548c\u901a\u4fe1\u6280\u672f\u94fa\u5e73\u4e86\u9053\u8def\u3002"}}
{"id": "2510.23906", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.23906", "abs": "https://arxiv.org/abs/2510.23906", "authors": ["Wasim Ahmad", "Maha Shadaydeh", "Joachim Denzler"], "title": "Group Interventions on Deep Networks for Causal Discovery in Subsystems", "comment": "Submitted to IEEE Access. We are working on the revised version", "summary": "Causal discovery uncovers complex relationships between variables, enhancing\npredictions, decision-making, and insights into real-world systems, especially\nin nonlinear multivariate time series. However, most existing methods primarily\nfocus on pairwise cause-effect relationships, overlooking interactions among\ngroups of variables, i.e., subsystems and their collective causal influence. In\nthis study, we introduce gCDMI, a novel multi-group causal discovery method\nthat leverages group-level interventions on trained deep neural networks and\nemploys model invariance testing to infer causal relationships. Our approach\ninvolves three key steps. First, we use deep learning to jointly model the\nstructural relationships among groups of all time series. Second, we apply\ngroup-wise interventions to the trained model. Finally, we conduct model\ninvariance testing to determine the presence of causal links among variable\ngroups. We evaluate our method on simulated datasets, demonstrating its\nsuperior performance in identifying group-level causal relationships compared\nto existing methods. Additionally, we validate our approach on real-world\ndatasets, including brain networks and climate ecosystems. Our results\nhighlight that applying group-level interventions to deep learning models,\ncombined with invariance testing, can effectively reveal complex causal\nstructures, offering valuable insights for domains such as neuroscience and\nclimate science.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u7684\u591a\u7ec4\u56e0\u679c\u53d1\u73b0\u65b9\u6cd5gCDMI\uff0c\u5229\u7528\u6df1\u5ea6\u5b66\u4e60\u5bf9\u65f6\u95f4\u5e8f\u5217\u7ec4\u7684\u7ed3\u6784\u5173\u7cfb\u8fdb\u884c\u5efa\u6a21\uff0c\u5e76\u901a\u8fc7\u7fa4\u7ec4\u5e72\u9884\u548c\u6a21\u578b\u4e0d\u53d8\u6027\u6d4b\u8bd5\u6765\u63a8\u65ad\u56e0\u679c\u5173\u7cfb\uff0c\u8be5\u65b9\u6cd5\u5728\u8bc6\u522b\u7fa4\u7ec4\u56e0\u679c\u5173\u7cfb\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5e76\u5728\u5b9e\u9645\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u5927\u591a\u6570\u73b0\u6709\u7684\u56e0\u679c\u53d1\u73b0\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u53d8\u91cf\u5bf9\u4e4b\u95f4\u7684\u56e0\u679c\u5173\u7cfb\uff0c\u5ffd\u7565\u4e86\u53d8\u91cf\u7ec4\u4e4b\u95f4\u7684\u96c6\u4f53\u56e0\u679c\u5f71\u54cd\u3002\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u65b9\u6cd5\u6765\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u4ece\u800c\u66f4\u597d\u5730\u7406\u89e3\u590d\u6742\u7cfb\u7edf\u7684\u56e0\u679c\u7ed3\u6784\u3002", "method": "\u8be5\u65b9\u6cd5\u5305\u62ec\u4e09\u4e2a\u6b65\u9aa4\uff1a\u7b2c\u4e00\uff0c\u4f7f\u7528\u6df1\u5ea6\u5b66\u4e60\u6280\u672f\u5bf9\u6240\u6709\u65f6\u95f4\u5e8f\u5217\u7ec4\u7684\u7ed3\u6784\u5173\u7cfb\u8fdb\u884c\u8054\u5408\u5efa\u6a21\u3002\u7b2c\u4e8c\uff0c\u5bf9\u8bad\u7ec3\u540e\u7684\u6a21\u578b\u8fdb\u884c\u7fa4\u7ec4\u5e72\u9884\u3002\u7b2c\u4e09\uff0c\u901a\u8fc7\u6a21\u578b\u4e0d\u53d8\u6027\u6d4b\u8bd5\u6765\u786e\u5b9a\u53d8\u91cf\u7ec4\u4e4b\u95f4\u7684\u56e0\u679c\u5173\u7cfb\u7684\u5b58\u5728\u6027\u3002", "result": "\u6211\u4eec\u5728\u6a21\u62df\u548c\u5b9e\u9645\u6570\u636e\u96c6\u4e0a\u5bf9\u65b9\u6cd5\u8fdb\u884c\u4e86\u8bc4\u4f30\uff0c\u8bc1\u660e\u4e86\u672c\u65b9\u6cd5\u5728\u8bc6\u522b\u7fa4\u7ec4\u56e0\u679c\u5173\u7cfb\u65b9\u9762\u7684\u4f18\u8d8a\u6027\uff0c\u7279\u522b\u662f\u5728\u795e\u7ecf\u79d1\u5b66\u548c\u6c14\u5019\u79d1\u5b66\u7b49\u9886\u57df\u7684\u590d\u6742\u56e0\u679c\u7ed3\u6784\u65b9\u9762\u3002", "conclusion": "\u672c\u7814\u7a76\u5c55\u793a\u4e86gCDMI\u5728\u5904\u7406\u591a\u53d8\u91cf\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u4e2d\u7684\u590d\u6742\u56e0\u679c\u7ed3\u6784\u65b9\u9762\u7684\u6709\u6548\u6027\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u4e2d\u7684\u7fa4\u7ec4\u5e72\u9884\u548c\u6a21\u578b\u4e0d\u53d8\u6027\u6d4b\u8bd5\uff0c\u63d0\u4f9b\u4e86\u4e00\u79cd\u65b0\u9896\u800c\u5f3a\u5927\u7684\u65b9\u6cd5\u6765\u8bc6\u522b\u56e0\u679c\u5173\u7cfb\u3002"}}
{"id": "2510.24399", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2510.24399", "abs": "https://arxiv.org/abs/2510.24399", "authors": ["Toan Van Nguyen", "Rasmus G. K. Christiansen", "Dirk Kraft", "Leon Bodenhagen"], "title": "GenTrack: A New Generation of Multi-Object Tracking", "comment": "This work has been submitted to the IEEE for possible publication", "summary": "This paper introduces a novel multi-object tracking (MOT) method, dubbed\nGenTrack, whose main contributions include: a hybrid tracking approach\nemploying both stochastic and deterministic manners to robustly handle unknown\nand time-varying numbers of targets, particularly in maintaining target\nidentity (ID) consistency and managing nonlinear dynamics, leveraging particle\nswarm optimization (PSO) with some proposed fitness measures to guide\nstochastic particles toward their target distribution modes, enabling effective\ntracking even with weak and noisy object detectors, integration of social\ninteractions among targets to enhance PSO-guided particles as well as improve\ncontinuous updates of both strong (matched) and weak (unmatched) tracks,\nthereby reducing ID switches and track loss, especially during occlusions, a\nGenTrack-based redefined visual MOT baseline incorporating a comprehensive\nstate and observation model based on space consistency, appearance, detection\nconfidence, track penalties, and social scores for systematic and efficient\ntarget updates, and the first-ever publicly available source-code reference\nimplementation with minimal dependencies, featuring three variants, including\nGenTrack Basic, PSO, and PSO-Social, facilitating flexible reimplementation.\nExperimental results have shown that GenTrack provides superior performance on\nstandard benchmarks and real-world scenarios compared to state-of-the-art\ntrackers, with integrated implementations of baselines for fair comparison.\nPotential directions for future work are also discussed. The source-code\nreference implementations of both the proposed method and compared-trackers are\nprovided on GitHub: https://github.com/SDU-VelKoTek/GenTrack", "AI": {"tldr": "GenTrack\u662f\u4e00\u79cd\u65b0\u7684\u591a\u76ee\u6807\u8ddf\u8e2a(MOT)\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ed3\u5408\u968f\u673a\u548c\u786e\u5b9a\u6027\u65b9\u6cd5\u6765\u5904\u7406\u672a\u77e5\u548c\u968f\u65f6\u95f4\u53d8\u5316\u7684\u76ee\u6807\u6570\u91cf\uff0c\u7279\u522b\u662f\u5728\u7ef4\u62a4\u76ee\u6807\u8eab\u4efd\u4e00\u81f4\u6027\u65b9\u9762\u8868\u73b0\u4f18\u5f02\u3002GenTrack\u5229\u7528\u7c92\u5b50\u7fa4\u4f18\u5316(PSO)\u6765\u5e2e\u52a9\u8ddf\u8e2a\uff0c\u5373\u4fbf\u9762\u5bf9\u5f31\u548c\u6709\u566a\u58f0\u7684\u5bf9\u8c61\u68c0\u6d4b\u5668\uff0c\u4f9d\u7136\u80fd\u591f\u4fdd\u6301\u6709\u6548\u7684\u8ddf\u8e2a\u3002\u540c\u65f6\uff0c\u8be5\u65b9\u6cd5\u8fd8\u8003\u8651\u4e86\u76ee\u6807\u4e4b\u95f4\u7684\u793e\u4f1a\u4ea4\u4e92\uff0c\u63d0\u9ad8\u4e86\u8ddf\u8e2a\u7684\u6548\u7387\u548c\u51c6\u786e\u6027\uff0c\u751a\u81f3\u5728\u906e\u6321\u60c5\u51b5\u4e0b\u7684\u8868\u73b0\u4e5f\u5341\u5206\u51fa\u8272\u3002\u5b9e\u9a8c\u8868\u660e\uff0cGenTrack\u5728\u6807\u51c6\u57fa\u51c6\u548c\u771f\u5b9e\u573a\u666f\u4e0b\u6027\u80fd\u51fa\u8272\uff0c\u4f18\u4e8e\u73b0\u6709\u7684\u8ddf\u968f\u8005\u3002\u5e76\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5305\u542b\u6807\u51c6\u6bd4\u8f83\u65b9\u6cd5\u7684\u5f00\u6e90\u4ee3\u7801\u5b9e\u73b0\u3002", "motivation": "\u5f53\u5904\u7406\u76ee\u6807\u6570\u91cf\u672a\u77e5\u4e14\u968f\u65f6\u95f4\u53d8\u5316\u7684\u60c5\u51b5\u65f6\uff0c\u4e8b\u5b9e\u8bc1\u660e\u4f20\u7edf\u7684\u591a\u76ee\u6807\u8ddf\u8e2a\uff08MOT\uff09\u65b9\u6cd5\u5728\u7ef4\u62a4\u8eab\u4efd\u4e00\u81f4\u6027\u4ee5\u53ca\u5904\u7406\u975e\u7ebf\u6027\u52a8\u529b\u5b66\u4e0a\u5b58\u5728\u6311\u6218\u3002\u56e0\u6b64\uff0c\u6211\u4eec\u5e0c\u671b\u63d0\u51fa\u4e00\u79cd\u65b0\u7684MOT\u65b9\u6cd5\uff0c\u53ef\u4ee5\u5728\u8fd9\u4e9b\u56f0\u96be\u573a\u666f\u4e0b\u8868\u73b0\u51fa\u66f4\u597d\u7684\u6027\u80fd\u548c\u9c81\u68d2\u6027\uff0c\u7279\u522b\u662f\u5f53\u76ee\u6807\u906e\u6321\u65f6\uff0c\u9700\u8981\u4fdd\u6301\u6b63\u786e\u7684\u8eab\u4efd\u5e76\u51cf\u5c11\u8eab\u4efd\u5207\u6362\u548c\u8ddf\u8e2a\u4e22\u5931\u3002\u540c\u65f6\uff0c\u8be5\u65b9\u6cd5\u8fd8\u9700\u8981\u80fd\u591f\u9002\u5e94\u5f31\u548c\u6709\u566a\u58f0\u7684\u5bf9\u8c61\u68c0\u6d4b\u5668\u3002", "method": "\u63d0\u51fa\u7684\u65b9\u6cd5\u7ed3\u5408\u4e86\u968f\u673a\u548c\u786e\u5b9a\u6027\u7684\u624b\u6bb5\u4ee5\u5e94\u5bf9\u591a\u4e2a\u76ee\u6807\u968f\u65f6\u95f4\u53d8\u5316\u7684\u95ee\u9898\u3002\u5229\u7528\u7c92\u5b50\u7fa4\u4f53\u4f18\u5316\uff08PSO\uff09\u5f15\u5bfc\u7c92\u5b50\u627e\u5230\u5b83\u4eec\u7684\u76ee\u6807\u5206\u5e03\u6a21\u5f0f\uff0c\u8fd9\u6709\u52a9\u4e8e\u5f31\u76ee\u6807\u68c0\u6d4b\u5668\u7684\u6709\u6548\u8ddf\u8e2a\u3002\u540c\u65f6\u8003\u8651\u4e86\u76ee\u6807\u4e4b\u95f4\u7684\u793e\u4f1a\u5173\u7cfb\uff0c\u4ee5\u63d0\u9ad8\u8ddf\u8e2a\u7cbe\u5ea6\u548c\u66f4\u65b0\u6548\u7387\uff0c\u5c24\u5176\u662f\u5f53\u76ee\u6807\u906e\u6321\u65f6\u3002\u63d0\u51fa\u4e86\u4e00\u4e2a\u5305\u62ec\u7a7a\u95f4\u4e00\u81f4\u6027\uff0c\u5916\u89c2\u68c0\u6d4b\u7f6e\u4fe1\u5ea6\uff0c\u8ddf\u8e2a\u60e9\u7f5a\u548c\u793e\u4ea4\u8bc4\u5206\u7684\u7684\u5168\u9762\u72b6\u6001\u548c\u89c2\u6d4b\u6a21\u578b\uff0c\u7528\u4e8e\u7cfb\u7edf\u6027\u548c\u9ad8\u6548\u7684\u76ee\u6807\u66f4\u65b0\u3002\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5305\u542b\u4e09\u79cd\u53d8\u4f53\uff08\u57fa\u7840\uff0cPSO\u548cPSO\u793e\u4ea4\uff09\u7684\u6e90\u4ee3\u7801\u5b9e\u73b0\uff0c\u4ee5\u4fbf\u4e8e\u7075\u6d3b\u91cd\u65b0\u5b9e\u65bd\u3002\u6b64\u5b9e\u65bd\u5177\u6709\u6700\u5c0f\u7684\u4f9d\u8d56\u6027\uff0c\u786e\u4fdd\u4e86\u6613\u4e8e\u4f7f\u7528\u6027\u3002\u63d0\u4f9b\u53ef\u6bd4\u8f83\u7684\u8ffd\u8e2a\u5668\u7684\u6e90\u4ee3\u7801\u5b9e\u73b0\uff0c\u4ee5\u8fdb\u884c\u516c\u5e73\u6bd4\u8f83\u3002\u9664\u6b64\u4e4b\u5916\uff0c\u63d0\u4f9b\u4e86\u5728GitHub\u4e0a\u7684\u5f00\u6e90\u4ee3\u7801\u5b9e\u73b0\uff1ahttps://github.com/SDU-VelKoTek/GenTrack", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u76f8\u6bd4\u5176\u4ed6\u6700\u5148\u8fdb\u7684\u8ffd\u8e2a\u5668\uff0cGenTrack\u5728\u6807\u51c6\u57fa\u51c6\u6d4b\u8bd5\u548c\u73b0\u5b9e\u751f\u6d3b\u573a\u666f\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u5e76\u4e14\u5728\u5904\u7406\u5404\u79cd\u6311\u6218\u6027\u573a\u666f\u65f6\u8868\u73b0\u7a81\u51fa\uff0c\u5982\u672a\u77e5\u6570\u91cf\u7684\u76ee\u6807\uff0c\u975e\u7ebf\u6027\u52a8\u529b\u5b66\uff0c\u5f31\u548c\u6709\u566a\u58f0\u7684\u5bf9\u8c61\u68c0\u6d4b\u5668\uff0c\u4ee5\u53ca\u5c24\u5176\u662f\u5728\u5904\u7406\u906e\u6321\u60c5\u51b5\u4e0b\u7684\u8ddf\u8e2a\u6027\u80fd\u3002\u8fd9\u4e9b\u5b9e\u9a8c\u7ed3\u679c\u4ee4\u4eba\u4fe1\u670d\u5730\u5c55\u793a\u4e86GenTrack\u5728\u5904\u7406\u591a\u76ee\u6807\u8ddf\u8e2a\u4efb\u52a1\u65b9\u9762\u7684\u4f18\u8d8a\u6027\u80fd\uff0c\u63d0\u4f9b\u4e86\u8ddf\u8e2a\u4efb\u52a1\u7684\u4f18\u826f\u6027\u80fd\u4fdd\u8bc1\u3002", "conclusion": "\u901a\u8fc7\u4f7f\u7528PSO\u548c\u8003\u8651\u793e\u4f1a\u4ea4\u4e92\u7684\u65b0\u65b9\u6cd5\uff0cGenTrack\u5728\u5404\u79cd\u591a\u76ee\u6807\u8ddf\u8e2a\u573a\u666f\u4e2d\u5c55\u73b0\u4e86\u4f18\u79c0\u7684\u6027\u80fd\uff0c\u7279\u522b\u662f\u5728\u5904\u7406\u590d\u6742\u7684\u906e\u6321\u548c\u73af\u5883\u53d8\u5316\u65f6\u8868\u73b0\u51fa\u8272\u3002\u8be5\u65b9\u6cd5\u4e0d\u4ec5\u7406\u8bba\u4e0a\u6709\u663e\u8457\u521b\u65b0\uff0c\u4e14\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u8868\u73b0\u4e5f\u4ee4\u4eba\u6ee1\u610f\uff0c\u8bc1\u660e\u4e86\u5176\u5728MOT\u9886\u57df\u7684\u5b9e\u7528\u6027\u3002\u5f00\u6e90\u4ee3\u7801\u7684\u63d0\u4f9b\u8fdb\u4e00\u6b65\u5f3a\u5316\u4e86\u5176\u4ed6\u7684\u79d1\u7814\u4eba\u5458\uff0c\u7814\u7a76\u8005\u548c\u5f00\u53d1\u8005\u5bf9\u8be5\u65b9\u6cd5\u7684\u4f7f\u7528\u5b9e\u8df5\uff0c\u5f00\u542f\u4e86\u672a\u6765\u591a\u76ee\u6807\u8ddf\u8e2a\u95ee\u9898\u7684\u65b0\u89c6\u89d2\u3002\u5bf9\u4e8e\u672a\u6765\u7684\u7814\u7a76\uff0c\u6587\u4e2d\u4e5f\u63a2\u8ba8\u4e86\u4e00\u4e9b\u6f5c\u5728\u7684\u65b9\u5411\u3002"}}
{"id": "2510.23912", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.23912", "abs": "https://arxiv.org/abs/2510.23912", "authors": ["Marko Karbevski", "Antonij Mijoski"], "title": "Key and Value Weights Are Probably All You Need: On the Necessity of the Query, Key, Value weight Triplet in Decoder-Only Transformers", "comment": null, "summary": "The Query, Key, Value weight triplet is a building block of current attention\nmechanisms in state-of-the-art LLMs. We theoretically investigate whether this\ntriplet can be reduced, proving under simplifying assumptions that the Query\nweights are redundant, thereby reducing the number of non-embedding/lm-head\nparameters by over 8%. We validate the theory on full-complexity GPT-3 small\narchitectures (with layer normalization, skip connections, and weight decay)\ntrained from scratch, demonstrating that the reduced model achieves comparable\nvalidation loss to standard baselines. These findings motivate the\ninvestigation of the Query weight redundancy at scale.", "AI": {"tldr": "\u7406\u8bba\u4e0a\u8bc1\u660e\u4e86\u67e5\u8be2\u6743\u91cd\u5728\u6ce8\u610f\u529b\u673a\u5236\u4e2d\u7684\u5197\u4f59\u6027\uff0c\u5e76\u5728\u7b80\u5316\u5047\u8bbe\u4e0b\u9a8c\u8bc1\u4e86\u51cf\u5c11\u67e5\u8be2\u6743\u91cd\u53ef\u4ee5\u964d\u4f4e\u6a21\u578b\u53c2\u6570\u6570\u91cf\u800c\u4e0d\u4f1a\u5f71\u54cd\u6027\u80fd\uff0c\u4e3a\u5927\u89c4\u6a21\u6a21\u578b\u4e2d\u7684\u67e5\u8be2\u6743\u91cd\u5197\u4f59\u6027\u7814\u7a76\u63d0\u4f9b\u4e86\u7406\u8bba\u4f9d\u636e\u548c\u5b9e\u8bc1\u652f\u6301\u3002", "motivation": "\u63a2\u8ba8\u67e5\u8be2\u3001\u952e\u503c\u4e09\u5143\u7ec4\u5728\u6ce8\u610f\u529b\u673a\u5236\u4e2d\u7684\u5fc5\u8981\u6027\uff0c\u7279\u522b\u662f\u67e5\u8be2\u6743\u91cd\u662f\u5426\u662f\u5fc5\u9700\u7684\uff0c\u8fd9\u4e00\u7406\u8bba\u7814\u7a76\u4e3a\u5927\u6a21\u578b\u53c2\u6570\u4f18\u5316\u63d0\u4f9b\u4e86\u53ef\u80fd\u7684\u65b9\u5411\u3002", "method": "\u5728\u7b80\u5316\u5047\u8bbe\u4e0b\u7406\u8bba\u8bc1\u660e\u67e5\u8be2\u6743\u91cd\u7684\u5197\u4f59\u6027\uff0c\u5e76\u901a\u8fc7\u5b9e\u9645\u8bad\u7ec3\u9a8c\u8bc1\u4e86\u51cf\u5c11\u67e5\u8be2\u6743\u91cd\u540e\u7684\u6a21\u578b\u5728GPT-3\u5c0f\u6a21\u578b\u67b6\u6784\u4e2d\u7684\u6027\u80fd\u3002", "result": "\u51cf\u5c11\u4e86\u8d85\u8fc78%\u7684\u975e\u5d4c\u5165\u5c42/lm\u5934\u53c2\u6570\uff0c\u4e14\u9a8c\u8bc1\u635f\u5931\u4e0e\u6807\u51c6\u57fa\u51c6\u76f8\u4f3c\uff0c\u8868\u660e\u51cf\u5c11\u67e5\u8be2\u6743\u91cd\u4e0d\u4f1a\u663e\u8457\u5f71\u54cd\u6a21\u578b\u6027\u80fd\u3002", "conclusion": "\u51cf\u5c11\u4e86\u6a21\u578b\u4e2d\u7684\u53c2\u6570\u6570\u91cf\uff0c\u540c\u65f6\u6ca1\u6709\u727a\u7272\u6a21\u578b\u7684\u6027\u80fd\uff0c\u4e3a\u5927\u89c4\u6a21\u8bed\u8a00\u6a21\u578b\u7684\u53c2\u6570\u4f18\u5316\u63d0\u4f9b\u4e86\u65b0\u7684\u601d\u8def\u3002"}}
{"id": "2510.24410", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2510.24410", "abs": "https://arxiv.org/abs/2510.24410", "authors": ["Toan Van Nguyen", "Rasmus G. K. Christiansen", "Dirk Kraft", "Leon Bodenhagen"], "title": "A Hybrid Approach for Visual Multi-Object Tracking", "comment": "This work has been submitted to the IEEE for possible publication", "summary": "This paper proposes a visual multi-object tracking method that jointly\nemploys stochastic and deterministic mechanisms to ensure identifier\nconsistency for unknown and time-varying target numbers under nonlinear\ndynamics. A stochastic particle filter addresses nonlinear dynamics and\nnon-Gaussian noise, with support from particle swarm optimization (PSO) to\nguide particles toward state distribution modes and mitigate divergence through\nproposed fitness measures incorporating motion consistency, appearance\nsimilarity, and social-interaction cues with neighboring targets. Deterministic\nassociation further enforces identifier consistency via a proposed cost matrix\nincorporating spatial consistency between particles and current detections,\ndetection confidences, and track penalties. Subsequently, a novel scheme is\nproposed for the smooth updating of target states while preserving their\nidentities, particularly for weak tracks during interactions with other targets\nand prolonged occlusions. Moreover, velocity regression over past states\nprovides trend-seed velocities, enhancing particle sampling and state updates.\nThe proposed tracker is designed to operate flexibly for both pre-recorded\nvideos and camera live streams, where future frames are unavailable.\nExperimental results confirm superior performance compared to state-of-the-art\ntrackers. The source-code reference implementations of both the proposed method\nand compared-trackers are provided on GitHub:\nhttps://github.com/SDU-VelKoTek/GenTrack2", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u968f\u673a\u548c\u786e\u5b9a\u673a\u5236\u7684\u89c6\u89c9\u591a\u5bf9\u8c61\u8ddf\u8e2a\u65b9\u6cd5\uff0c\u4ee5\u786e\u4fdd\u5728\u975e\u7ebf\u6027\u52a8\u529b\u5b66\u4e0b\u7684\u672a\u77e5\u548c\u65f6\u53d8\u76ee\u6807\u6570\u91cf\u7684\u6807\u8bc6\u4e00\u81f4\u3002\u968f\u673a\u7c92\u5b50\u6ee4\u6ce2\u5668\u4f7f\u7528PSO\u7c92\u5b50\u7fa4\u4f18\u5316\u652f\u6301\uff0c\u5904\u7406\u975e\u7ebf\u6027\u52a8\u529b\u5b66\u548c\u975e\u9ad8\u65af\u566a\u58f0\uff0c\u901a\u8fc7\u5f15\u5165\u79fb\u52a8\u4e00\u81f4\u6027\u3001\u5916\u89c2\u76f8\u4f3c\u6027\u548c\u793e\u4f1a\u4e92\u52a8\u7ebf\u7d22\u6765\u7f13\u89e3\u5206\u6b67\u3002\u786e\u5b9a\u6027\u5173\u8054\u8fdb\u4e00\u6b65\u901a\u8fc7\u6210\u672c\u77e9\u9635\u5f3a\u5236\u6807\u8bc6\u4e00\u81f4\u6027\uff0c\u8be5\u77e9\u9635\u878d\u5165\u4e86\u7c92\u5b50\u548c\u5f53\u524d\u68c0\u6d4b\u4e4b\u95f4\u7684\u7a7a\u95f4\u4e00\u81f4\u6027\u3001\u68c0\u6d4b\u7f6e\u4fe1\u5ea6\u548c\u8ddf\u8e2a\u60e9\u7f5a\u3002\u4e00\u79cd\u65b0\u9896\u7684\u65b9\u6848\u88ab\u63d0\u51fa\uff0c\u53ef\u4ee5\u5e73\u6ed1\u5730\u66f4\u65b0\u76ee\u6807\u72b6\u6001\uff0c\u7279\u522b\u662f\u5728\u4e0e\u5176\u5b83\u76ee\u6807\u4ea4\u4e92\u548c\u957f\u65f6\u95f4\u906e\u6321\u65f6\u4fdd\u6301\u5176\u8eab\u4efd\u3002\u6b64\u5916\uff0c\u901f\u5ea6\u56de\u5f52\u63d0\u9ad8\u4e86\u7c92\u5b50\u91c7\u6837\u548c\u72b6\u6001\u66f4\u65b0\u7684\u8d8b\u52bf\u3002\u8be5\u8ddf\u8e2a\u5668\u9002\u7528\u4e8e\u9884\u5f55\u89c6\u9891\u548c\u76f8\u673a\u5b9e\u65f6\u6d41\uff0c\u53ef\u4ee5\u63d0\u4f9b\u4f18\u4e8e\u5f53\u524d\u6700\u4f18\u8ddf\u8e2a\u5668\u7684\u6027\u80fd\u3002\u8be5\u7814\u7a76\u7684\u4ee3\u7801\u53ef\u901a\u8fc7GitHub\u83b7\u53d6\uff1ahttps://github.com/SDU-VelKoTek/GenTrack2", "motivation": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u968f\u673a\u548c\u786e\u5b9a\u673a\u5236\u7684\u591a\u5bf9\u8c61\u8ddf\u8e2a\u7684\u65b9\u6cd5\uff0c\u76ee\u6807\u662f\u786e\u4fdd\u975e\u7ebf\u6027\u52a8\u529b\u5b66\u672a\u77e5\u548c\u65f6\u53d8\u76ee\u6807\u6570\u91cf\u4e0b\u7684\u6807\u8bc6\u4e00\u81f4\u6027\uff0c\u540c\u65f6\u4f18\u5316\u7c92\u5b50\u6ee4\u6ce2\u5668\u5728\u5904\u7406\u975e\u7ebf\u6027\u52a8\u529b\u5b66\u548c\u975e\u9ad8\u65af\u566a\u58f0\u7684\u80fd\u529b\uff0c\u901a\u8fc7\u4e0e\u786e\u5b9a\u6027\u5173\u8054\u76f8\u7ed3\u5408\u4ee5\u4fdd\u6301\u8ddf\u8e2a\u4e00\u81f4\u6027\u548c\u5e73\u6ed1\u66f4\u65b0\u76ee\u6807\u72b6\u6001", "method": "\u65b9\u6cd5\u6d89\u53ca\u5c06\u968f\u673a\u7c92\u5b50\u6ee4\u6ce2\u5668\u4e0e\u7c92\u5b50\u7fa4\u4f18\u5316\u7ed3\u5408\uff0c\u6765\u5904\u7406\u975e\u9ad8\u65af\u566a\u58f0\u548c\u975e\u7ebf\u6027\u52a8\u529b\u5b66\uff0c\u540c\u65f6\u8fd8\u901a\u8fc7\u4e00\u79cd\u6210\u672c\u77e9\u9635\u5b9e\u73b0\u4e86\u4e00\u79cd\u65b0\u7684\u786e\u5b9a\u6027\u5173\u8054\u3002\u4e00\u79cd\u65b0\u9896\u7684\u65b9\u6848\u88ab\u63d0\u51fa\uff0c\u76ee\u7684\u5728\u4e8e\u5e73\u6ed1\u5730\u66f4\u65b0\u76ee\u6807\u72b6\u6001\uff0c\u7279\u522b\u662f\u5728\u4e0e\u5176\u5b83\u76ee\u6807\u4ea4\u4e92\u548c\u957f\u65f6\u95f4\u906e\u6321\u65f6\u4fdd\u6301\u5176\u6807\u8bc6\u4e00\u81f4\u6027\u3002\u9664\u6b64\u4e4b\u5916\uff0c\u4f7f\u7528\u901f\u5ea6\u56de\u5f52\u6765\u6539\u5584\u7c92\u5b50\u91c7\u6837\u548c\u72b6\u6001\u66f4\u65b0\u7684\u8d8b\u52bf", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u8ffd\u8e2a\u5668\u76f8\u8f83\u4e8e\u5f53\u524d\u6700\u4f18\u65b9\u6cd5\uff0c\u5177\u6709\u66f4\u52a0\u4f18\u8d8a\u7684\u6027\u80fd", "conclusion": "\u8fd9\u79cd\u7ed3\u5408\u968f\u673a\u4e0e\u786e\u5b9a\u65b9\u6cd5\u7684\u591a\u76ee\u6807\u8ddf\u8e2a\u624b\u6cd5\uff0c\u5728\u975e\u7ebf\u6027\u52a8\u529b\u5b66\u4e2d\u80fd\u591f\u9ad8\u6548\u5730\u8ffd\u8e2a\u4e0d\u65ad\u53d8\u5316\u7684\u52a8\u6001\uff0c\u7279\u522b\u662f\u5728\u89c6\u9891\u6216\u6444\u50cf\u673a\u5b9e\u65f6\u6d41\u4e2d\u5904\u7406\u672a\u77e5\u6570\u91cf\u7684\u5bf9\u8c61\u65f6\u3002\u8fd9\u79cd\u65b9\u6cd5\u4e0d\u4ec5\u5904\u7406\u597d\u4e86\u7c92\u5b50\u6ee4\u6ce2\u5728\u975e\u9ad8\u65af\u566a\u58f0\u548c\u975e\u7ebf\u6027\u52a8\u529b\u5b66\u4e2d\u7684\u5e94\u7528\u5c40\u9650\uff0c\u800c\u4e14\u8fd8\u901a\u8fc7\u5f15\u5165\u786e\u5b9a\u6027\u5173\u8054\u6539\u5584\u4e86\u8ddf\u8e2a\u4e00\u81f4\u6027\u548c\u76ee\u6807\u72b6\u6001\u7684\u66f4\u65b0\uff0c\u5c55\u793a\u4e86\u4f18\u4e8e\u5f53\u524d\u6700\u4f18\u65b9\u6cd5\u7684\u5b9e\u8df5\u6548\u679c\u3002"}}
{"id": "2510.23914", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.23914", "abs": "https://arxiv.org/abs/2510.23914", "authors": ["Arsenii Mustafin", "Xinyi Sheng", "Dominik Baumann"], "title": "Geometry-Inspired Unified Framework for Discounted and Average Reward MDPs", "comment": "12 pages, 1 figure", "summary": "The theoretical analysis of Markov Decision Processes (MDPs) is commonly\nsplit into two cases - the average-reward case and the discounted-reward case -\nwhich, while sharing similarities, are typically analyzed separately. In this\nwork, we extend a recently introduced geometric interpretation of MDPs for the\ndiscounted-reward case to the average-reward case, thereby unifying both. This\nallows us to extend a major result known for the discounted-reward case to the\naverage-reward case: under a unique and ergodic optimal policy, the Value\nIteration algorithm achieves a geometric convergence rate.", "AI": {"tldr": "\u672c\u6587\u5c06\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\uff08MDP\uff09\u7684\u51e0\u4f55\u89e3\u91ca\u4ece\u6298\u6263\u5956\u52b1\u60c5\u51b5\u6269\u5c55\u5230\u5e73\u5747\u5956\u52b1\u60c5\u51b5\uff0c\u5e76\u8bc1\u660e\u4e86\u5728\u552f\u4e00\u7684\u3001\u904d\u5386\u7684\u6700\u4f18\u7b56\u7565\u4e0b\uff0c\u4ef7\u503c\u8fed\u4ee3\u7b97\u6cd5\u8fbe\u5230\u4e86\u51e0\u4f55\u6536\u655b\u901f\u7387.", "motivation": "\u4f20\u7edf\u4e0a\uff0c\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\uff08MDP\uff09\u7684\u7406\u8bba\u5206\u6790\u5206\u4e3a\u6298\u6263\u5956\u52b1\u60c5\u51b5\u548c\u5e73\u5747\u5956\u52b1\u60c5\u51b5\uff0c\u5c3d\u7ba1\u8fd9\u4e24\u8005\u6709\u76f8\u4f3c\u4e4b\u5904\uff0c\u4f46\u901a\u5e38\u5206\u522b\u8fdb\u884c\u5206\u6790\u3002\u901a\u8fc7\u5c06\u51e0\u4f55\u89e3\u91ca\u4ece\u6298\u6263\u5956\u52b1\u60c5\u51b5\u6269\u5c55\u5230\u5e73\u5747\u5956\u52b1\u60c5\u51b5\uff0c\u7edf\u4e00\u4e86\u8fd9\u4e24\u8005\u7684\u5206\u6790\u65b9\u6cd5\uff0c\u4ece\u800c\u53ef\u4ee5\u5c06\u6298\u6263\u5956\u52b1\u60c5\u51b5\u4e0b\u7684\u4e00\u4e2a\u91cd\u8981\u7ed3\u679c\u5ef6\u4f38\u5230\u5e73\u5747\u5956\u52b1\u60c5\u51b5.", "method": "\u901a\u8fc7\u5f15\u5165\u51e0\u4f55\u65b9\u6cd5\u89e3\u91ca\u5e73\u5747\u5956\u52b1\u60c5\u51b5\u4e0b\u7684\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\u95ee\u9898\uff0c\u8fdb\u800c\u5206\u6790\u4ef7\u503c\u8fed\u4ee3\u7b97\u6cd5\u5728\u552f\u4e00\u4e14\u904d\u5386\u7684\u6700\u4f18\u7b56\u7565\u4e0b\u7684\u6536\u655b\u6027\u8d28.", "result": "\u5728\u552f\u4e00\u7684\u3001\u904d\u5386\u7684\u6700\u4f18\u7b56\u7565\u4e0b\uff0c\u4ef7\u503c\u8fed\u4ee3\u7b97\u6cd5\u5728\u5e73\u5747\u5956\u52b1\u60c5\u51b5\u4e0b\u7684\u6536\u655b\u901f\u7387\u662f\u51e0\u4f55\u7684\uff0c\u8fd9\u4e0e\u6298\u6263\u5956\u52b1\u60c5\u51b5\u4e0b\u7684\u7ed3\u679c\u4e00\u81f4.", "conclusion": "\u901a\u8fc7\u4e3a\u5e73\u5747\u5956\u52b1\u60c5\u51b5\u4e0b\u7684\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\u5f15\u5165\u51e0\u4f55\u89e3\u91ca\uff0c\u5f97\u51fa\u4e86\u4e00\u4e2a\u4e0e\u6298\u6263\u5956\u52b1\u60c5\u51b5\u76f8\u5e94\u7684\u7ed3\u679c\uff0c\u5373\u5728\u552f\u4e00\u4e14\u904d\u5386\u7684\u6700\u4f18\u7b56\u7565\u4e0b\uff0c\u4ef7\u503c\u8fed\u4ee3\u7b97\u6cd5\u8fbe\u5230\u51e0\u4f55\u6536\u655b\u901f\u7387."}}
{"id": "2510.24413", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.24413", "abs": "https://arxiv.org/abs/2510.24413", "authors": ["Ali Ahmad Faour", "Nabil Amacha", "Ali J. Ghandour"], "title": "50 Years of Water Body Monitoring: The Case of Qaraaoun Reservoir, Lebanon", "comment": null, "summary": "The sustainable management of the Qaraaoun Reservoir, the largest surface\nwater body in Lebanon located in the Bekaa Plain, depends on reliable\nmonitoring of its storage volume despite frequent sensor malfunctions and\nlimited maintenance capacity. This study introduces a sensor-free approach that\nintegrates open-source satellite imagery, advanced water-extent segmentation,\nand machine learning to estimate the reservoir surface area and volume in near\nreal time. Sentinel-2 and Landsat images are processed, where surface water is\ndelineated using a newly proposed water segmentation index. A machine learning\nmodel based on Support Vector Regression (SVR) is trained on a curated dataset\nthat includes water surface area, water level, and water volume calculations\nusing a reservoir bathymetry survey. The model is then able to estimate\nreservoir volume relying solely on surface area extracted from satellite\nimagery, without the need for ground measurements. Water segmentation using the\nproposed index aligns with ground truth for more than 95 percent of the\nshoreline. Hyperparameter tuning with GridSearchCV yields an optimized SVR\nperformance with error under 1.5 percent of full reservoir capacity and\ncoefficients of determination exceeding 0.98. These results demonstrate the\nrobustness and cost-effectiveness of the method, offering a practical solution\nfor continuous, sensor-independent monitoring of reservoir storage. The\nproposed methodology can be replicated for other water bodies, and the\nresulting 50 years of time-series data is valuable for research on climate\nchange and environmental patterns.", "AI": {"tldr": "\u8fd9\u9879\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u4f20\u611f\u5668\u76d1\u6d4b\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u5bf9\u536b\u661f\u5f71\u50cf\u8fdb\u884c\u6c34\u4f53\u5206\u5272\u548c\u673a\u5668\u5b66\u4e60\uff0c\u5b9e\u65f6\u4f30\u8ba1Qaraaoun\u6c34\u5e93\u7684\u6c34\u9762\u9762\u79ef\u548c\u4f53\u79ef\u3002\u8fd9\u79cd\u65b9\u6cd5\u7cbe\u5ea6\u9ad8\u4e14\u6210\u672c\u4f4e\uff0c\u4e3a\u6c34\u5e93\u5b58\u50a8\u76d1\u6d4b\u63d0\u4f9b\u4e86\u4e00\u79cd\u65b0\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u540c\u65f6\u4e5f\u4e3a\u6c14\u5019\u53d8\u5316\u548c\u73af\u5883\u6a21\u5f0f\u7684\u7814\u7a76\u63d0\u4f9b\u4e86\u591a\u5e74\u7684\u6570\u636e\u652f\u6301\u3002", "motivation": "\u6301\u7eed\u6027\u7ba1\u7406\u9ece\u5df4\u5ae9\u8d1d\u5361\u5e73\u539f\u7684\u6700\u5927\u5730\u8868\u6c34\u4f53Qaraaoun\u6c34\u5e93\u5728\u4f20\u611f\u5668\u9891\u7e41\u6545\u969c\u548c\u7ef4\u62a4\u80fd\u529b\u6709\u9650\u7684\u60c5\u51b5\u4e0b\u4f9d\u8d56\u4e8e\u53ef\u9760\u7684\u76d1\u63a7\u3002\u7136\u800c\uff0c\u4f20\u7edf\u4f9d\u8d56\u4e8e\u4f20\u611f\u5668\u7684\u65b9\u6cd5\u96be\u4ee5\u5728\u8d44\u6e90\u6709\u9650\u7684\u60c5\u51b5\u4e0b\u53ef\u9760\u8fd0\u884c\u3002\u56e0\u6b64\uff0c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u4f20\u611f\u5668\u72ec\u7acb\u7684\u76d1\u63a7\u65b0\u65b9\u6cd5\u3002", "method": "\u7814\u7a76\u4f7f\u7528Sentinel-2\u548cLandsat\u536b\u661f\u63d0\u4f9b\u7684\u5f71\u50cf\u6570\u636e\uff0c\u901a\u8fc7\u65b0\u7684\u6c34\u5206\u5272\u6307\u6570\u8fdb\u884c\u6c34\u4f53\u8fb9\u7f18\u786e\u5b9a\u3002\u518d\u901a\u8fc7\u652f\u6301\u5411\u91cf\u56de\u5f52\u6a21\u578b\uff0c\u4f7f\u7528\u5305\u542b\u536b\u661f\u5f71\u50cf\u63d0\u53d6\u7684\u6c34\u9762\u9762\u79ef\u3001\u6c34\u4f4d\u4ee5\u53ca\u901a\u8fc7\u6c34\u5e93\u5730\u5f62\u6d4b\u91cf\u8ba1\u7b97\u7684\u6c34\u4f53\u79ef\u7684\u8bad\u7ec3\u6570\u636e\u96c6\u8fdb\u884c\u8bad\u7ec3\uff0c\u4ece\u800c\u5728\u6ca1\u6709\u5730\u9762\u6d4b\u91cf\u7684\u60c5\u51b5\u4e0b\u4f30\u8ba1\u6c34\u5e93\u4f53\u79ef\u3002", "result": "\u7814\u7a76\u5f97\u51fa\u7684\u65b9\u6cd5\u572895%\u4ee5\u4e0a\u7684\u6c34\u5cb8\u7ebf\u5904\u4e0e\u5b9e\u5730\u8c03\u67e5\u76f8\u7b26\uff0c\u7ecf\u8fc7Hyperparameter\u8c03\u4f18\u540e\uff0c\u652f\u6301\u5411\u91cf\u56de\u5f52\u6a21\u578b\u7684\u9884\u6d4b\u8bef\u5dee\u4f4e\u4e8e\u6c34\u5e93\u6ee1\u5bb9\u91cf\u76841.5%\uff0c\u51b3\u5b9a\u7cfb\u6570\u8d85\u8fc70.98\uff0c\u8bc1\u660e\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u65b0\u65b9\u6cd5\u6709\u6548\u3001\u7ecf\u6d4e\u4e14\u65e0\u9700\u4f20\u611f\u5668\uff0c\u4e3a\u6d41\u57df\u7ba1\u7406\u63d0\u4f9b\u4e86\u65b0\u7684\u5de5\u5177\uff0c\u540c\u65f6\u5bf9\u6c14\u5019\u53d8\u5316\u548c\u73af\u5883\u6a21\u5f0f\u7684\u7814\u7a76\u5177\u6709\u957f\u671f\u7684\u6570\u636e\u652f\u6301\u3002"}}
{"id": "2510.24414", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.24414", "abs": "https://arxiv.org/abs/2510.24414", "authors": ["Reem Hammoud", "Abdul karim Gizzini", "Ali J. Ghandour"], "title": "XAI Evaluation Framework for Semantic Segmentation", "comment": null, "summary": "Ensuring transparency and trust in artificial intelligence (AI) models is\nessential, particularly as they are increasingly applied in safety-critical and\nhigh-stakes domains. Explainable AI (XAI) has emerged as a promising approach\nto address this challenge, yet the rigorous evaluation of XAI methods remains\ncrucial for optimizing the trade-offs between model complexity, predictive\nperformance, and interpretability. While extensive progress has been achieved\nin evaluating XAI techniques for classification tasks, evaluation strategies\ntailored to semantic segmentation remain relatively underexplored. This work\nintroduces a comprehensive and systematic evaluation framework specifically\ndesigned for assessing XAI in semantic segmentation, explicitly accounting for\nboth spatial and contextual task complexities. The framework employs\npixel-level evaluation strategies and carefully designed metrics to provide\nfine-grained interpretability insights. Simulation results using recently\nadapted class activation mapping (CAM)-based XAI schemes demonstrate the\nefficiency, robustness, and reliability of the proposed methodology. These\nfindings contribute to advancing transparent, trustworthy, and accountable\nsemantic segmentation models.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8bc4\u4f30\u53ef\u89e3\u91ca\u6027AI\u5728\u8bed\u4e49\u5206\u5272\u4e2d\u5e94\u7528\u7684\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u901a\u8fc7\u50cf\u7d20\u7ea7\u8bc4\u4f30\u7b56\u7565\u548c\u8bbe\u8ba1\u6307\u6807\u6765\u8bc4\u4f30\u6a21\u578b\u7684\u900f\u660e\u5ea6\u548c\u53ef\u4fe1\u5ea6\uff0c\u7279\u522b\u662f\u5bf9\u4e8e\u7a7a\u95f4\u548c\u4e0a\u4e0b\u6587\u4efb\u52a1\u7684\u590d\u6742\u6027\u3002\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3001\u9c81\u68d2\u6027\u548c\u53ef\u9760\u6027\uff0c\u6709\u52a9\u4e8e\u63d0\u5347\u8bed\u4e49\u5206\u5272\u6a21\u578b\u7684\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "\u968f\u7740\u4eba\u5de5\u667a\u80fd\u6a21\u578b\u5728\u5b89\u5168\u5173\u952e\u548c\u9ad8\u98ce\u9669\u9886\u57df\u4e2d\u7684\u5e94\u7528\u589e\u52a0\uff0c\u786e\u4fdd\u8fd9\u4e9b\u6a21\u578b\u7684\u900f\u660e\u6027\u548c\u4fe1\u4efb\u5ea6\u81f3\u5173\u91cd\u8981\u3002\u73b0\u6709\u7684\u8bc4\u4f30\u65b9\u6cd5\u666e\u904d\u5b58\u5728\u5bf9\u4e8e\u5206\u7c7b\u4efb\u52a1\u8bc4\u4f30\u8f83\u591a\uff0c\u800c\u5bf9\u4e8e\u8bed\u4e49\u5206\u5272\u4efb\u52a1\u7684\u8bc4\u4f30\u8f83\u5c11\u3002\u56e0\u6b64\uff0c\u8be5\u5de5\u4f5c\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u4e0d\u8db3\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u80fd\u591f\u5168\u9762\u8bc4\u4f30\u8bed\u4e49\u5206\u5272\u4efb\u52a1\u4e2dXAI\u65b9\u6cd5\u7684\u6846\u67b6\u3002", "method": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u8bc4\u4f30\u6846\u67b6\uff0c\u5305\u62ec\u50cf\u7d20\u7ea7\u522b\u7684\u8bc4\u4f30\u7b56\u7565\u548c\u4e13\u95e8\u8bbe\u8ba1\u7684\u8bc4\u4ef7\u6307\u6807\uff0c\u4ee5\u9002\u5e94\u8bed\u4e49\u5206\u5272\u4efb\u52a1\u7684\u7279\u5b9a\u9700\u6c42\uff0c\u5c24\u5176\u662f\u5bf9\u7a7a\u95f4\u548c\u4e0a\u4e0b\u6587\u7684\u590d\u6742\u6027\u7684\u8003\u91cf\u3002\u6846\u67b6\u4e2d\u4f7f\u7528\u4e86\u57fa\u4e8e\u7c7b\u6fc0\u6d3b\u6620\u5c04\u7684XAI\u65b9\u6cd5\u6a21\u62df\u5b9e\u9a8c\uff0c\u4ee5\u9a8c\u8bc1\u65b0\u63d0\u51fa\u7684\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "result": "\u7814\u7a76\u901a\u8fc7\u57fa\u4e8e\u7c7b\u6fc0\u6d3b\u6620\u5c04\u7684XAI\u65b9\u6cd5\u7684\u6a21\u62df\u5b9e\u9a8c\uff0c\u5c55\u793a\u4e86\u65b0\u63d0\u51fa\u7684\u8bc4\u4f30\u6846\u67b6\u5728\u6548\u7387\u3001\u9c81\u68d2\u6027\u548c\u53ef\u9760\u6027\u65b9\u9762\u7684\u4f18\u52bf\uff0c\u8bc1\u660e\u4e86\u8be5\u6846\u67b6\u5728\u8bed\u4e49\u5206\u5272\u4efb\u52a1\u4e2d\u8bc4\u4f30XAI\u65b9\u6cd5\u7684\u9002\u7528\u6027\u3002", "conclusion": "\u8be5\u8bba\u6587\u63d0\u51fa\u7684\u6846\u67b6\u6709\u52a9\u4e8e\u63d0\u9ad8\u8bed\u4e49\u5206\u5272\u6a21\u578b\u7684\u89e3\u91ca\u6027\u548c\u900f\u660e\u5ea6\uff0c\u4f7f\u6a21\u578b\u66f4\u52a0\u53ef\u9760\u548c\u53ef\u4fe1\u3002\u8fd9\u5c06\u5bf9\u5f00\u53d1\u66f4\u900f\u660e\u3001\u53ef\u4fe1\u8d56\u548c\u8d1f\u8d23\u7684\u8bed\u4e49\u5206\u5272\u6a21\u578b\u4ea7\u751f\u91cd\u8981\u7684\u8d21\u732e\u3002"}}
{"id": "2510.23931", "categories": ["cs.LG", "cs.CR", "cs.DC", "68T07 (Primary) 68M14, 68P27, 68Q32, 94A16, 62H35 (Secondary)", "I.2.11; I.2.6; C.2.4; D.4.6; K.4.1"], "pdf": "https://arxiv.org/pdf/2510.23931", "abs": "https://arxiv.org/abs/2510.23931", "authors": ["Miguel Fernandez-de-Retana", "Unai Zulaika", "Rub\u00e9n S\u00e1nchez-Corcuera", "Aitor Almeida"], "title": "Differential Privacy: Gradient Leakage Attacks in Federated Learning Environments", "comment": "17 pages, 12 figures", "summary": "Federated Learning (FL) allows for the training of Machine Learning models in\na collaborative manner without the need to share sensitive data. However, it\nremains vulnerable to Gradient Leakage Attacks (GLAs), which can reveal private\ninformation from the shared model updates. In this work, we investigate the\neffectiveness of Differential Privacy (DP) mechanisms - specifically, DP-SGD\nand a variant based on explicit regularization (PDP-SGD) - as defenses against\nGLAs. To this end, we evaluate the performance of several computer vision\nmodels trained under varying privacy levels on a simple classification task,\nand then analyze the quality of private data reconstructions obtained from the\nintercepted gradients in a simulated FL environment. Our results demonstrate\nthat DP-SGD significantly mitigates the risk of gradient leakage attacks,\nalbeit with a moderate trade-off in model utility. In contrast, PDP-SGD\nmaintains strong classification performance but proves ineffective as a\npractical defense against reconstruction attacks. These findings highlight the\nimportance of empirically evaluating privacy mechanisms beyond their\ntheoretical guarantees, particularly in distributed learning scenarios where\ninformation leakage may represent an unassumable critical threat to data\nsecurity and privacy.", "AI": {"tldr": "\u7814\u7a76\u4e86\u5dee\u5206\u9690\u79c1\u673a\u5236\uff08DP-SGD\u548cPDP-SGD\uff09\u5728\u8054\u90a6\u5b66\u4e60\u4e2d\u7684\u6709\u6548\u6027\uff0c\u4ee5\u9632\u5fa1\u68af\u5ea6\u6cc4\u9732\u653b\u51fb\u3002\u7ed3\u679c\u8868\u660e\uff0cDP-SGD\u663e\u8457\u51cf\u5c11\u653b\u51fb\u98ce\u9669\uff0c\u4f46\u6709\u8f7b\u5fae\u7684\u6a21\u578b\u6027\u80fd\u635f\u5931\u3002\u800cPDP-SGD\u867d\u7136\u4fdd\u6301\u4e86\u5206\u7c7b\u6027\u80fd\uff0c\u4f46\u5bf9\u8fd8\u539f\u653b\u51fb\u65e0\u6548\u3002\u8fd9\u5f3a\u8c03\u4e86\u5728\u5206\u5e03\u5f0f\u5b66\u4e60\u4e2d\u5b9e\u8bc1\u8bc4\u4f30\u9690\u79c1\u673a\u5236\u7684\u91cd\u8981\u6027\uff0c\u800c\u4e0d\u4ec5\u4ec5\u662f\u7406\u8bba\u4fdd\u8bc1\u3002", "motivation": "\u7814\u7a76\u5dee\u5206\u9690\u79c1\u673a\u5236\u4f5c\u4e3a\u9632\u5fa1\u8054\u90a6\u5b66\u4e60\u4e2d\u7684\u68af\u5ea6\u6cc4\u9732\u653b\u51fb\u7684\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "method": "\u8bc4\u4f30\u4e86\u51e0\u4e2a\u8ba1\u7b97\u673a\u89c6\u89c9\u6a21\u578b\u5728\u4e0d\u540c\u9690\u79c1\u6761\u4ef6\u4e0b\u4f7f\u7528DP-SGD\u548cPDP-SGD\u8bad\u7ec3\u7684\u7ed3\u679c\uff0c\u5e76\u5206\u6790\u4e86\u5728\u6a21\u62df\u7684\u8054\u90a6\u5b66\u4e60\u73af\u5883\u4e2d\u901a\u8fc7\u622a\u83b7\u7684\u68af\u5ea6\u6062\u590d\u7684\u79c1\u4eba\u6570\u636e\u8d28\u91cf\u3002", "result": "DP-SGD\u663e\u8457\u51cf\u8f7b\u4e86\u68af\u5ea6\u6cc4\u9732\u653b\u51fb\u7684\u98ce\u9669\uff0c\u5c3d\u7ba1\u4f1a\u5bfc\u81f4\u4e00\u5b9a\u7684\u6a21\u578b\u6027\u80fd\u635f\u5931\u3002PDP-SGD\u5728\u5206\u7c7b\u6027\u80fd\u4e0a\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5bf9\u8fd8\u539f\u653b\u51fb\u65e0\u6548\u3002", "conclusion": "\u5b9e\u8bc1\u8bc4\u4f30\u9690\u79c1\u673a\u5236\u5728\u9632\u5fa1\u68af\u5ea6\u6cc4\u9732\u653b\u51fb\u4e2d\u7684\u6709\u6548\u6027\u81f3\u5173\u91cd\u8981\uff0c\u7279\u522b\u662f\u5728\u8054\u90a6\u5b66\u4e60\u7b49\u5206\u5e03\u5f0f\u5b66\u4e60\u573a\u666f\u4e2d\u3002"}}
{"id": "2510.24437", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.24437", "abs": "https://arxiv.org/abs/2510.24437", "authors": ["Zhineng Zhao", "Zhihai He", "Zikun Zhou", "Siwei Ma", "Yaowei Wang"], "title": "Deeply-Conditioned Image Compression via Self-Generated Priors", "comment": null, "summary": "Learned image compression (LIC) has shown great promise for achieving high\nrate-distortion performance. However, current LIC methods are often limited in\ntheir capability to model the complex correlation structures inherent in\nnatural images, particularly the entanglement of invariant global structures\nwith transient local textures within a single monolithic representation. This\nlimitation precipitates severe geometric deformation at low bitrates. To\naddress this, we introduce a framework predicated on functional decomposition,\nwhich we term Deeply-Conditioned Image Compression via self-generated priors\n(DCIC-sgp). Our central idea is to first encode a potent, self-generated prior\nto encapsulate the image's structural backbone. This prior is subsequently\nutilized not as mere side-information, but to holistically modulate the entire\ncompression pipeline. This deep conditioning, most critically of the analysis\ntransform, liberates it to dedicate its representational capacity to the\nresidual, high-entropy details. This hierarchical, dependency-driven approach\nachieves an effective disentanglement of information streams. Our extensive\nexperiments validate this assertion; visual analysis demonstrates that our\nmethod substantially mitigates the geometric deformation artifacts that plague\nconventional codecs at low bitrates. Quantitatively, our framework establishes\nhighly competitive performance, achieving significant BD-rate reductions of\n14.4%, 15.7%, and 15.1% against the VVC test model VTM-12.1 on the Kodak, CLIC,\nand Tecnick datasets.", "AI": {"tldr": "\u901a\u8fc7\u5f15\u5165\u6df1\u5ea6\u6761\u4ef6\u5316\u56fe\u50cf\u538b\u7f29\u6846\u67b6\uff08DCIC-sgp\uff09\uff0c\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u65b9\u6cd5\u6765\u89e3\u51b3\u73b0\u6709\u5b66\u4e60\u56fe\u50cf\u538b\u7f29\uff08LIC\uff09\u5728\u4f4e\u6bd4\u7279\u7387\u65f6\u51fa\u73b0\u7684\u4e25\u91cd\u51e0\u4f55\u5931\u771f\u95ee\u9898\u3002\u8be5\u6846\u67b6\u5229\u7528\u81ea\u751f\u6210\u7684\u5148\u9a8c\u6765\u6574\u4f53\u8c03\u5236\u538b\u7f29\u6d41\u6c34\u7ebf\uff0c\u663e\u8457\u6539\u5584\u4e86\u56fe\u50cf\u7684\u538b\u7f29\u6027\u80fd\u3002\u5728Kodak\u3001CLIC\u548cTecnick\u6570\u636e\u96c6\u4e0a\uff0c\u4e0eVVC\u6d4b\u8bd5\u6a21\u578bVTM-12.1\u76f8\u6bd4\uff0c\u672c\u65b9\u6cd5\u53d6\u5f97\u4e8614.4%\uff0c15.7%\uff0c\u548c15.1%\u7684BD\u7387\u51cf\u5c11\u3002", "motivation": "\u5f53\u524d\u7684LIC\u65b9\u6cd5\u96be\u4ee5\u5904\u7406\u81ea\u7136\u56fe\u50cf\u4e2d\u7684\u590d\u6742\u76f8\u5173\u7ed3\u6784\uff0c\u7279\u522b\u662f\u5728\u5355\u4e00\u6574\u4f53\u8868\u793a\u4e2d\u540c\u65f6\u5b58\u5728\u4e0d\u53d8\u7684\u5168\u5c40\u7ed3\u6784\u548c\u77ac\u65f6\u7684\u5c40\u90e8\u7eb9\u7406\u7684\u60c5\u51b5\u4e0b\u3002\u8fd9\u79cd\u9650\u5236\u5bfc\u81f4\u5728\u4f4e\u6bd4\u7279\u7387\u65f6\u51fa\u73b0\u4e25\u91cd\u7684\u51e0\u4f55\u5931\u771f\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cd\u65b0\u7684\u65b9\u6cd5\u6765\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8e\u529f\u80fd\u5206\u89e3\u7684\u6846\u67b6\uff0c\u79f0\u4e3a\u6df1\u5ea6\u6761\u4ef6\u5316\u56fe\u50cf\u538b\u7f29\uff08DCIC-sgp\uff09\u3002\u9996\u5148\uff0c\u7f16\u7801\u4e00\u4e2a\u5f3a\u5927\u7684\u81ea\u751f\u6210\u5148\u9a8c\u6765\u6355\u6349\u56fe\u50cf\u7684\u7ed3\u6784\u57fa\u7840\u3002\u7136\u540e\uff0c\u5229\u7528\u8fd9\u4e2a\u5148\u9a8c\u6765\u6574\u4f53\u8c03\u5236\u6574\u4e2a\u538b\u7f29\u6d41\u6c34\u7ebf\uff0c\u7279\u522b\u5f3a\u8c03\u6df1\u5ea6\u6761\u4ef6\u5316\u5206\u6790\u53d8\u6362\uff0c\u4ee5\u4fbf\u5b83\u53ef\u4ee5\u4e13\u6ce8\u4e8e\u9ad8\u71b5\u6b8b\u5dee\u3002\u8fd9\u79cd\u65b9\u6cd5\u5b9e\u73b0\u4e86\u4fe1\u606f\u6d41\u7684\u6709\u6548\u89e3\u8026\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u4e0e\u4f20\u7edf\u7684\u538b\u7f29\u7f16\u89e3\u7801\u5668\u76f8\u6bd4\uff0c\u8be5\u65b9\u6cd5\u5728\u4f4e\u6bd4\u7279\u7387\u65f6\u663e\u8457\u51cf\u5c11\u4e86\u51e0\u4f55\u5931\u771f\u3002\u5728Kodak\u3001CLIC\u548cTecnick\u6570\u636e\u96c6\u4e0a\uff0c\u8be5\u6846\u67b6\u76f8\u5bf9\u4e8eVVC\u6d4b\u8bd5\u6a21\u578bVTM-12.1\u53d6\u5f97\u4e86\u663e\u8457\u7684BD\u7387\u51cf\u5c11\u3002\u5177\u4f53\u800c\u8a00\uff0c\u76f8\u5bf9\u4e8eVTM-12.1\uff0c\u8be5\u65b9\u6cd5\u5206\u522b\u5728\u4e09\u4e2a\u6570\u636e\u96c6\u4e2d\u53d6\u5f97\u4e8614.4%\u300115.7%\u548c15.1%\u7684BD\u7387\u51cf\u5c11\u3002", "conclusion": "\u901a\u8fc7\u5f15\u5165\u6df1\u5ea6\u6761\u4ef6\u5316\u56fe\u50cf\u538b\u7f29\u6846\u67b6\uff08DCIC-sgp\uff09\uff0c\u6709\u6548\u5730\u89e3\u51b3\u4e86\u73b0\u6709LIC\u65b9\u6cd5\u5728\u4f4e\u6bd4\u7279\u7387\u65f6\u51fa\u73b0\u7684\u51e0\u4f55\u5931\u771f\u95ee\u9898\uff0c\u5e76\u5728\u51e0\u79cd\u5e38\u89c1\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\u3002"}}
{"id": "2510.24448", "categories": ["cs.CV", "cs.AI", "68T07, 68T45, 68T20", "I.2.10; I.4.8; I.5.1; I.2.6"], "pdf": "https://arxiv.org/pdf/2510.24448", "abs": "https://arxiv.org/abs/2510.24448", "authors": ["Pablo Acuaviva", "Aram Davtyan", "Mariam Hassan", "Sebastian Stapf", "Ahmad Rahimi", "Alexandre Alahi", "Paolo Favaro"], "title": "Rethinking Visual Intelligence: Insights from Video Pretraining", "comment": "Updated version from preprint arXiv:2506.07280 (Gen2Gen) focused on\n  visual intelligence. This work can be considered as v2", "summary": "Large language models (LLMs) have demonstrated that large-scale pretraining\nenables systems to adapt rapidly to new problems with little supervision in the\nlanguage domain. This success, however, has not translated as effectively to\nthe visual domain, where models, including LLMs, continue to struggle with\ncompositional understanding, sample efficiency, and general-purpose\nproblem-solving. We investigate Video Diffusion Models (VDMs) as a promising\ndirection for bridging this gap. Pretraining on spatiotemporal data endows\nthese models with strong inductive biases for structure and dynamics, which we\nhypothesize can support broad task adaptability. To test this, we design a\ncontrolled evaluation in which both a pretrained LLM and a pretrained VDM are\nequipped with lightweight adapters and presented with tasks in their natural\nmodalities. Across benchmarks including ARC-AGI, ConceptARC, visual games,\nroute planning, and cellular automata, VDMs demonstrate higher data efficiency\nthan their language counterparts. Taken together, our results indicate that\nvideo pretraining offers inductive biases that support progress toward visual\nfoundation models.", "AI": {"tldr": "\u7814\u7a76\u4eba\u5458\u63a2\u8ba8\u4e86\u89c6\u9891\u6269\u6563\u6a21\u578b\uff08VDM\uff09\u4f5c\u4e3a\u5f25\u5408\u8bed\u8a00\u6a21\u578b\u5728\u89c6\u89c9\u7406\u89e3\u65b9\u9762\u4e0d\u8db3\u7684\u6f5c\u5728\u65b9\u5411\u3002\u901a\u8fc7\u5728\u65f6\u7a7a\u6570\u636e\u4e0a\u8fdb\u884c\u9884\u8bad\u7ec3\uff0cVDM\u88ab\u8d4b\u4e88\u4e86\u7ed3\u6784\u548c\u52a8\u6001\u7684\u5f3a\u5927\u5f52\u7eb3\u504f\u5dee\uff0c\u8fd9\u5728\u4e00\u7cfb\u5217\u4efb\u52a1\u4e2d\u663e\u793a\u51fa\u4e86\u6bd4\u8bed\u8a00\u6a21\u578b\u66f4\u9ad8\u7684\u6570\u636e\u6548\u7387\uff0c\u8bf4\u660e\u89c6\u9891\u9884\u8bad\u7ec3\u652f\u6301\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u7684\u53d1\u5c55\u3002", "motivation": "\u76ee\u524d\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u5904\u7406\u8bed\u8a00\u4efb\u52a1\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5728\u89c6\u89c9\u4efb\u52a1\u4e2d\u5374\u9762\u4e34\u6311\u6218\uff0c\u5982\u7ec4\u6210\u7406\u89e3\u3001\u91c7\u6837\u6548\u7387\u548c\u901a\u7528\u95ee\u9898\u89e3\u51b3\u7b49\u3002\u4e3a\u4e86\u63a2\u7d22\u89c6\u9891\u6269\u6563\u6a21\u578b\u4f5c\u4e3a\u6865\u6881\u7684\u4f5c\u7528\uff0c\u7814\u7a76\u8005\u4eec\u8fdb\u884c\u4e86\u8fd9\u9879\u7814\u7a76\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e00\u79cd\u63a7\u5236\u6027\u8bc4\u4f30\uff0c\u5176\u4e2d\u65e2\u5305\u62ec\u4e86\u7ecf\u8fc7\u9884\u8bad\u7ec3\u7684\u8bed\u8a00\u6a21\u578b\uff0c\u4e5f\u5305\u62ec\u4e86\u7ecf\u8fc7\u9884\u8bad\u7ec3\u7684\u89c6\u9891\u6269\u6563\u6a21\u578b\uff0c\u8fd9\u4e24\u8005\u90fd\u914d\u5907\u4e86\u8f7b\u91cf\u7ea7\u7684\u9002\u914d\u5668\uff0c\u5e76\u4ee5\u5404\u81ea\u81ea\u7136\u6a21\u5f0f\u7684\u4efb\u52a1\u5448\u73b0\u7ed9\u6a21\u578b\u3002", "result": "\u5728ARCGI\u3001ConceptARC\u3001\u89c6\u89c9\u6e38\u620f\u3001\u8def\u7ebf\u89c4\u5212\u548c\u7ec6\u80de\u81ea\u52a8\u673a\u7b49\u57fa\u51c6\u6d4b\u8bd5\u4e0b\uff0cVDM\u8868\u73b0\u51fa\u66f4\u9ad8\u7684\u6570\u636e\u6548\u7387\uff0c\u8bf4\u660e\u89c6\u9891\u9884\u8bad\u7ec3\u8d4b\u4e88\u4e86\u6a21\u578b\u66f4\u5f3a\u7684\u80fd\u529b\u6765\u9002\u5e94\u591a\u6837\u5316\u7684\u4efb\u52a1\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u89c6\u9891\u9884\u8bad\u7ec3\u6709\u52a9\u4e8e\u652f\u6491\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u7684\u53d1\u5c55\uff0c\u63d0\u4f9b\u4e86\u91cd\u8981\u7684\u5f52\u7eb3\u504f\u5dee\uff0c\u4ee5\u89e3\u51b3\u89c6\u89c9\u4efb\u52a1\u4e2d\u7684\u6311\u6218\u3002"}}
{"id": "2510.24456", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.24456", "abs": "https://arxiv.org/abs/2510.24456", "authors": ["Vivek Chetia", "Abdul Taher Khan", "Rahish Gogoi", "David Kapsian Khual", "Purnendu Bikash", "Sajal Saha"], "title": "A Critical Study towards the Detection of Parkinsons Disease using ML Technologies", "comment": null, "summary": "The proposed solution is Deep Learning Technique that will be able classify\nthree types of tea leaves diseases from which two diseases are caused by the\npests and one due to pathogens (infectious organisms) and environmental\nconditions and also show the area damaged by a disease in leaves. Namely Red\nRust, Helopeltis and Red spider mite respectively. In this paper we have\nevaluated two models namely SSD MobileNet V2 and Faster R-CNN ResNet50 V1 for\nthe object detection. The SSD MobileNet V2 gave precision of 0.209 for IOU\nrange of 0.50:0.95 with recall of 0.02 on IOU 0.50:0.95 and final mAP of 20.9%.\nWhile Faster R-CNN ResNet50 V1 has precision of 0.252 on IOU range of 0.50:0.95\nand recall of 0.044 on IOU of 0.50:0.95 with a mAP of 25%, which is better than\nSSD. Also used Mask R-CNN for Object Instance Segmentation where we have\nimplemented our custom method to calculate the damaged diseased portion of\nleaves. Keywords: Tea Leaf Disease, Deep Learning, Red Rust, Helopeltis and Red\nSpider Mite, SSD MobileNet V2, Faster R-CNN ResNet50 V1 and Mask RCNN.", "AI": {"tldr": "\u5728\u672c\u6587\u4e2d\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u8336\u6811\u53f6\u75c5\u5bb3\u5206\u7c7b\u4e0e\u5b9a\u4f4d\u6280\u672f\uff0c\u53ef\u4ee5\u8bc6\u522b\u548c\u5b9a\u4f4d\u4e09\u79cd\u8336\u6811\u53f6\u75c5\u5bb3\uff08\u5373\u7ea2\u9508\u75c5\u3001Helopeltis \u548c\u7ea2\u8718\u86db\u5bb3\uff09\uff0c\u5e76\u8ba1\u7b97\u75c5\u5bb3\u9762\u79ef\u3002\u8bc4\u4f30\u4e86 SSD MobileNet V2 \u548c Faster R-CNN ResNet50 V1 \u7684\u6027\u80fd\uff0cFaster R-CNN ResNet50 V1 \u5728 mAP \u4e0a\u7565\u4f18\u4e8e SSD MobileNet V2\u3002\u8fd8\u4f7f\u7528\u4e86 Mask R-CNN \u8fdb\u884c\u5b9e\u4f8b\u5206\u5272\uff0c\u4ee5\u8ba1\u7b97\u75c5\u5bb3\u533a\u57df\u3002", "motivation": "\u8336\u53f6\u75c5\u5bb3\u4f1a\u5f71\u54cd\u8336\u53f6\u7684\u54c1\u8d28\u548c\u4ea7\u91cf\uff0c\u56e0\u6b64\u901a\u8fc7\u6df1\u5ea6\u5b66\u4e60\u6280\u672f\u5bf9\u8336\u53f6\u75c5\u5bb3\u8fdb\u884c\u51c6\u786e\u3001\u5feb\u901f\u7684\u8bc6\u522b\u4e0e\u8bc4\u4f30\uff0c\u4ee5\u8f85\u52a9\u8336\u53f6\u751f\u4ea7\u4e2d\u7684\u75c5\u5bb3\u7ba1\u7406\u3002", "method": "\u672c\u7814\u7a76\u8bc4\u4f30\u4e86 SSD MobileNet V2 \u548c Faster R-CNN ResNet50 V1 \u6a21\u578b\u7528\u4e8e\u8336\u6811\u53f6\u75c5\u5bb3\u8bc6\u522b\u7684\u6548\u679c\uff0c\u5e76\u4f7f\u7528 Mask R-CNN \u8fdb\u884c\u5b9e\u4f8b\u5206\u5272\uff0c\u8ba1\u7b97\u75c5\u5bb3\u7684\u9762\u79ef\u3002\u4e3a\u4e86\u6267\u884c\u5b9e\u4f8b\u5206\u5272\uff0c\u5b9e\u65bd\u4e86\u4e00\u4e2a\u81ea\u5b9a\u4e49\u65b9\u6cd5\u6765\u5224\u5b9a\u8bc6\u522b\u7684\u75c5\u5bb3\u533a\u57df\u3002", "result": "SSD MobileNet V2 \u5728 mAP \u4e0a\u7684\u5206\u6570\u4e3a 20.9%\uff1bFaster R-CNN ResNet50 V1 \u7684 mAP \u4e3a 25%\uff0c\u63d0\u9ad8\u4e86\u8bc6\u522b\u7684\u7cbe\u5ea6\u548c\u53ec\u56de\u7387\uff0c\u5728\u89e3\u51b3\u8336\u53f6\u75c5\u5bb3\u8bc6\u522b\u7684\u4efb\u52a1\u4e0a\u8868\u73b0\u7a0d\u597d\u3002Mask R-CNN \u5b9e\u73b0\u4e86\u66f4\u7cbe\u786e\u7684\u5b9e\u4f8b\u5206\u5272\uff0c\u80fd\u591f\u8ba1\u7b97\u75c5\u5bb3\u9762\u79ef\u3002", "conclusion": "\u5229\u7528\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u53ef\u4ee5\u6709\u6548\u5730\u8bc6\u522b\u548c\u91cf\u5316\u8336\u6811\u53f6\u75be\u75c5\uff0c\u4ece\u800c\u652f\u6301\u4f5c\u7269\u5065\u5eb7\u76d1\u6d4b\u548c\u75c5\u5bb3\u7ba1\u7406\u3002Faster R-CNN ResNet50 V1 \u548c Mask R-CNN \u662f\u8f83\u4e3a\u7406\u60f3\u7684\u7b97\u6cd5\u9009\u62e9\u3002"}}
{"id": "2510.23948", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.23948", "abs": "https://arxiv.org/abs/2510.23948", "authors": ["Qianfeng Wen", "Zhenwei Tang", "Ashton Anderson"], "title": "ChessQA: Evaluating Large Language Models for Chess Understanding", "comment": "33 pages,8 figures", "summary": "Chess provides an ideal testbed for evaluating the reasoning, modeling, and\nabstraction capabilities of large language models (LLMs), as it has\nwell-defined structure and objective ground truth while admitting a wide\nspectrum of skill levels. However, existing evaluations of LLM ability in chess\nare ad hoc and narrow in scope, making it difficult to accurately measure LLM\nchess understanding and how it varies with scale, post-training methodologies,\nor architecture choices. We present ChessQA, a comprehensive benchmark that\nassesses LLM chess understanding across five task categories (Structural,\nMotifs, Short Tactics, Position Judgment, and Semantic), which approximately\ncorrespond to the ascending abstractions that players master as they accumulate\nchess knowledge, from understanding basic rules and learning tactical motifs to\ncorrectly calculating tactics, evaluating positions, and semantically\ndescribing high-level concepts. In this way, ChessQA captures a more\ncomprehensive picture of chess ability and understanding, going significantly\nbeyond the simple move quality evaluations done previously, and offers a\ncontrolled, consistent setting for diagnosis and comparison. Furthermore,\nChessQA is inherently dynamic, with prompts, answer keys, and construction\nscripts that can evolve as models improve. Evaluating a range of contemporary\nLLMs, we find persistent weaknesses across all five categories and provide\nresults and error analyses by category. We will release the code, periodically\nrefreshed datasets, and a public leaderboard to support further research.", "AI": {"tldr": "ChessQA \u662f\u4e00\u4e2a\u7528\u4e8e\u8bc4\u4f30 LLM \u5728\u56fd\u9645\u8c61\u68cb\u7406\u89e3\u65b9\u9762\u7684\u57fa\u51c6\uff0c\u6db5\u76d6\u4e86\u4ece\u57fa\u672c\u89c4\u5219\u5230\u9ad8\u9636\u6982\u5ff5\u7b49\u591a\u4e2a\u62bd\u8c61\u7ea7\u522b\u7684\u4efb\u52a1\u3002\u8fd9\u9879\u7814\u7a76\u8fd8\u53d1\u73b0\u73b0\u4ee3 LLM \u5728\u8fd9\u4e94\u4e2a\u7c7b\u522b\u4e2d\u5b58\u5728\u6301\u7eed\u5b58\u5728\u7684\u5f31\u70b9\uff0c\u5e76\u5c06\u5728\u672a\u6765\u53d1\u5e03\u4ee3\u7801\u3001\u6570\u636e\u96c6\u548c\u6392\u884c\u699c\u4ee5\u652f\u6301\u8fdb\u4e00\u6b65\u7684\u7814\u7a76\u3002", "motivation": "\u5f53\u524d\u5bf9LLM\u5728\u56fd\u9645\u8c61\u68cb\u4e2d\u7406\u89e3\u80fd\u529b\u7684\u8bc4\u4f30\u662f\u4e34\u65f6\u7684\uff0c\u8303\u56f4\u72ed\u7a84\u3002\u6709\u5fc5\u8981\u901a\u8fc7\u4e00\u4e2a\u5168\u9762\u7684\u57fa\u51c6\u6765\u8fdb\u884c\u66f4\u51c6\u786e\u7684\u8bc4\u4f30\uff0c\u4ee5\u4fbf\u66f4\u51c6\u786e\u5730\u6d4b\u91cfLLM\u5bf9\u56fd\u9645\u8c61\u68cb\u7684\u7406\u89e3\u4ee5\u53ca\u5b83\u4eec\u968f\u89c4\u6a21\u3001\u8bad\u7ec3\u540e\u65b9\u6cd5\u6216\u67b6\u6784\u9009\u62e9\u7684\u53d8\u5316\u60c5\u51b5\u3002", "method": "\u63d0\u51fa\u4e86ChessQA\uff0c\u8fd9\u662f\u4e00\u4e2a\u5728\u4e94\u4e2a\u4efb\u52a1\u7c7b\u522b\uff08\u7ed3\u6784\u5316\u3001\u6a21\u5f0f\u3001\u77ed\u6218\u672f\u3001\u4f4d\u7f6e\u5224\u65ad\u548c\u8bed\u4e49\uff09\u4e0b\u8861\u91cfLLM\u56fd\u9645\u8c61\u68cb\u7406\u89e3\u80fd\u529b\u7684\u7efc\u5408\u6027\u57fa\u51c6\u3002\u8fd9\u4e9b\u4efb\u52a1\u7c7b\u522b\u6309\u7167\u73a9\u5bb6\u968f\u7740\u68cb\u827a\u77e5\u8bc6\u79ef\u7d2f\u800c\u638c\u63e1\u7684\u9010\u6b65\u4e0a\u5347\u7684\u62bd\u8c61\u5c42\u6b21\u6765\u5927\u81f4\u5bf9\u5e94\u3002\u6b64\u5916\uff0cChessQA\u5177\u6709\u52a8\u6001\u6027\u8d28\uff0c\u5176\u63d0\u793a\u3001\u7b54\u6848\u952e\u548c\u6784\u5efa\u811a\u672c\u53ef\u6839\u636e\u6a21\u578b\u6539\u8fdb\u800c\u4e0d\u65ad\u53d1\u5c55\u3002\u672c\u7814\u7a76\u8bc4\u4f30\u4e86\u4e00\u7cfb\u5217\u73b0\u6210\u7684LLM\uff0c\u63d0\u4f9b\u4e86\u6bcf\u4e2a\u7c7b\u522b\u7684\u7ed3\u679c\u548c\u9519\u8bef\u5206\u6790\u3002\u8bbe\u7acb\u4e86\u4e00\u4e2a\u516c\u5171\u6392\u884c\u699c\u6765\u652f\u6301\u672a\u6765\u7684\u7814\u7a76\u3002", "result": "\u672c\u7814\u7a76\u53d1\u73b0\uff0c\u5c3d\u7ba1\u5404LLM\u5728\u67d0\u4e9b\u7279\u5b9a\u4efb\u52a1\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5728\u4e94\u4e2a\u7c7b\u522b\u4e2d\u5747\u5b58\u5728\u6301\u7eed\u5b58\u5728\u7684\u5f31\u70b9\u3002\u6bcf\u4e2a\u7c7b\u522b\u7684\u8bc4\u4f30\u7ed3\u679c\u548c\u9519\u8bef\u5206\u6790\u90fd\u8bf4\u660e\u4e86\u8fd9\u4e9b\u6a21\u578b\u7684\u5c40\u9650\u6027\u3002", "conclusion": "\u672c\u7814\u7a76\u5f15\u5165\u4e86ChessQA\u4f5c\u4e3a\u56fd\u9645\u8c61\u68cb\u7406\u89e3\u548c\u80fd\u529b\u66f4\u5168\u9762\u8bc4\u4f30\u7684\u57fa\u51c6\uff0c\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5728LLM\u56fd\u9645\u8c61\u68cb\u7406\u89e3\u80fd\u529b\u8bca\u65ad\u548c\u6bd4\u8f83\u4e2d\u7684\u63a7\u5236\u548c\u4e00\u81f4\u73af\u5883\u3002LLM\u5728\u5176\u6280\u80fd\u7075\u6d3b\u6027\u548c\u5e7f\u6cdb\u6027\u4e0a\u8868\u73b0\u51fa\u4e00\u5b9a\u7684\u5c40\u9650\u6027\uff0c\u8fd9\u4e3a\u672a\u6765\u7684\u6a21\u578b\u6539\u8fdb\u548c\u7814\u7a76\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u89c1\u89e3\u3002"}}
{"id": "2510.24464", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.24464", "abs": "https://arxiv.org/abs/2510.24464", "authors": ["Charles Javerliat", "Pierre Raimbaud", "Guillaume Lavou\u00e9"], "title": "Kineo: Calibration-Free Metric Motion Capture From Sparse RGB Cameras", "comment": null, "summary": "Markerless multiview motion capture is often constrained by the need for\nprecise camera calibration, limiting accessibility for non-experts and\nin-the-wild captures. Existing calibration-free approaches mitigate this\nrequirement but suffer from high computational cost and reduced reconstruction\naccuracy.\n  We present Kineo, a fully automatic, calibration-free pipeline for markerless\nmotion capture from videos captured by unsynchronized, uncalibrated,\nconsumer-grade RGB cameras. Kineo leverages 2D keypoints from off-the-shelf\ndetectors to simultaneously calibrate cameras, including Brown-Conrady\ndistortion coefficients, and reconstruct 3D keypoints and dense scene point\nmaps at metric scale. A confidence-driven spatio-temporal keypoint sampling\nstrategy, combined with graph-based global optimization, ensures robust\ncalibration at a fixed computational cost independent of sequence length. We\nfurther introduce a pairwise reprojection consensus score to quantify 3D\nreconstruction reliability for downstream tasks.\n  Evaluations on EgoHumans and Human3.6M demonstrate substantial improvements\nover prior calibration-free methods. Compared to previous state-of-the-art\napproaches, Kineo reduces camera translation error by approximately 83-85%,\ncamera angular error by 86-92%, and world mean-per-joint error (W-MPJPE) by\n83-91%.\n  Kineo is also efficient in real-world scenarios, processing multi-view\nsequences faster than their duration in specific configuration (e.g., 36min to\nprocess 1h20min of footage). The full pipeline and evaluation code are openly\nreleased to promote reproducibility and practical adoption at\nhttps://liris-xr.github.io/kineo/.", "AI": {"tldr": "Kineo\u662f\u4e00\u5957\u9488\u5bf9\u65e0\u6807\u8bb0\u8fd0\u52a8\u6355\u6349\u7684\u5168\u81ea\u52a8\u3001\u65e0\u9700\u6821\u51c6\u7684\u6d41\u6c34\u7ebf\uff0c\u53ef\u4ee5\u5728\u65e0\u540c\u6b65\u548c\u672a\u7ecf\u6821\u51c6\u7684\u6d88\u8d39\u7ea7RGB\u6444\u50cf\u673a\u5f55\u5236\u7684\u89c6\u9891\u4e2d\u5e94\u7528\u3002\u5b83\u53ef\u4ee5\u901a\u8fc7\u4e8c\u7ef4\u5173\u952e\u70b9\u540c\u65f6\u6821\u51c6\u6444\u50cf\u673a\u5e76\u91cd\u5efa\u4e09\u7ef4\u5173\u952e\u70b9\u548c\u5bc6\u96c6\u573a\u666f\u70b9\u5730\u56fe\uff0c\u8868\u73b0\u51fa\u4f18\u5f02\u7684\u6027\u80fd\uff0c\u5f97\u5230\u6bd4\u524d\u4eba\u6821\u51c6\u65b9\u6cd5\u66f4\u9ad8\u7684\u7cbe\u786e\u5ea6\uff0c\u540c\u65f6\u8ba1\u7b97\u6210\u672c\u8f83\u4f4e\u3002\u8be5\u6d41\u6c34\u7ebf\u5df2\u7ecf\u5728EgoHumans\u548cHuman3.6M\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u8bc4\u4f30\uff0c\u8bc1\u5b9e\u5176\u4f18\u8d8a\u6027\u3002\u5b8c\u6574\u6e90\u4ee3\u7801\u5df2\u7ecf\u516c\u5f00\u4ee5\u52a9\u529b\u524d\u6cbf\u7814\u7a76\u548c\u5b9e\u9645\u5e94\u7528\u3002", "motivation": "\u73b0\u6709\u65e0\u9700\u6821\u51c6\u7684\u65e0\u6807\u8bb0\u52a8\u4f5c\u6355\u6349\u65b9\u6cd5\u8ba1\u7b97\u6210\u672c\u9ad8\u4e14\u91cd\u5efa\u7cbe\u5ea6\u4f4e\uff0c\u9650\u5236\u4e86\u5176\u5e94\u7528\u3002\u56e0\u6b64\uff0c\u7814\u7a76\u5f00\u53d1\u4e86\u4e00\u5957\u5168\u81ea\u52a8\u7684\u65e0\u9700\u6821\u51c6\u7684\u6d41\u6c34\u7ebf\uff0c\u53ef\u4ee5\u7cbe\u786e\u540c\u65f6\u6821\u51c6\u6444\u50cf\u673a\u548c\u91cd\u5efa\u4e09\u7ef4\u5173\u952e\u70b9\uff0c\u9002\u7528\u6027\u5e7f\uff0c\u9002\u5e94\u6027\u5f3a\u3002", "method": "Kineo\u91c7\u7528\u4e8c\u7ef4\u5173\u952e\u70b9\u68c0\u6d4b\u6765\u81ea\u6444\u50cf\u673a\u7684\u89c6\u9891\u5e27\uff0c\u5229\u7528\u7a7a\u95f4-\u65f6\u95f4\u81ea\u9002\u5e94\u7684\u5173\u952e\u70b9\u91c7\u6837\u7b56\u7565\u548c\u5168\u5c40\u4e09\u7ef4\u91cd\u5efa\u65b9\u6cd5\uff0c\u91cd\u5efa\u51fa\u5927\u91cf\u7684\u573a\u666f\u70b9\uff0c\u8fd9\u4e9b\u70b9\u5f62\u6210\u573a\u666f\u7684\u4e09\u7ef4\u7ed3\u6784\uff0c\u540c\u65f6\u8be5\u65b9\u6cd5\u7684\u8ba1\u7b97\u590d\u6742\u5ea6\u4e0e\u89c6\u9891\u5e8f\u5217\u957f\u5ea6\u65e0\u5173\u3002", "result": "\u4e0e\u524d\u4eba\u65b9\u6cd5\u5bf9\u6bd4\uff0cKineo\u663e\u8457\u964d\u4f4e\u4e86\u76f8\u673a\u95f4\u7684\u9519\u8bef\uff0c\u4ee5\u53ca\u4e16\u754c\u4e0a\u7684\u5e73\u5747\u5173\u8282\u70b9\u8bef\u5dee\uff0c\u4e5f\u8bc1\u660e\u4e86\u5728\u5b9e\u9645\u573a\u666f\u5e94\u7528\u4e2d\u7684\u9ad8\u6548\u6027\u3002\u5176\u6d41\u6c34\u7ebf\u5728\u4e24\u4e2a\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u5f97\u5230\u4e86\u51c6\u786e\u9a8c\u8bc1\u3002\u8bc4\u4ef7\u4ee3\u7801\u548c\u6e90\u7801\u5747\u5df2\u516c\u5f00\uff0c\u4fbf\u4e8e\u5728\u7814\u7a76\u548c\u5b9e\u8df5\u4e2d\u4f7f\u7528\u3002 ", "conclusion": "Kineo\u63d0\u4f9b\u4e86\u4e00\u79cd\u5168\u65b0\u7684\u65e0\u9700\u6821\u51c6\u7684\u591a\u89c6\u56fe\u65e0\u6807\u8bb0\u52a8\u4f5c\u6355\u6349\u7684\u65b9\u6cd5\uff0c\u80fd\u591f\u5728\u4f4e\u6210\u672c\u9ad8\u6548\u7684\u73af\u5883\u91cc\u53d6\u5f97\u5353\u8d8a\u7684\u91cd\u5efa\u7cbe\u5ea6\uff0c\u6709\u52a9\u4e8e\u63a8\u52a8\u65e0\u6807\u8bb0\u52a8\u4f5c\u6355\u6349\u6280\u672f\u7684\u53d1\u5c55\u3002"}}
{"id": "2510.23966", "categories": ["cs.LG", "cs.SE"], "pdf": "https://arxiv.org/pdf/2510.23966", "abs": "https://arxiv.org/abs/2510.23966", "authors": ["Scott Emmons", "Roland S. Zimmermann", "David K. Elson", "Rohin Shah"], "title": "A Pragmatic Way to Measure Chain-of-Thought Monitorability", "comment": "The first two authors contributed equally", "summary": "While Chain-of-Thought (CoT) monitoring offers a unique opportunity for AI\nsafety, this opportunity could be lost through shifts in training practices or\nmodel architecture. To help preserve monitorability, we propose a pragmatic way\nto measure two components of it: legibility (whether the reasoning can be\nfollowed by a human) and coverage (whether the CoT contains all the reasoning\nneeded for a human to also produce the final output). We implement these\nmetrics with an autorater prompt that enables any capable LLM to compute the\nlegibility and coverage of existing CoTs. After sanity-checking our prompted\nautorater with synthetic CoT degradations, we apply it to several frontier\nmodels on challenging benchmarks, finding that they exhibit high\nmonitorability. We present these metrics, including our complete autorater\nprompt, as a tool for developers to track how design decisions impact\nmonitorability. While the exact prompt we share is still a preliminary version\nunder ongoing development, we are sharing it now in the hopes that others in\nthe community will find it useful. Our method helps measure the default\nmonitorability of CoT - it should be seen as a complement, not a replacement,\nfor the adversarial stress-testing needed to test robustness against\ndeliberately evasive models.", "AI": {"tldr": "\u4e3a\u4e86\u4fdd\u6301AI\u5b89\u5168\u4e2d\u7684Chain-of-Thought (CoT)\u7684\u53ef\u76d1\u63a7\u6027\uff0c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u5b9e\u7528\u7684\u65b9\u6cd5\u6765\u8861\u91cf\u5176\u53ef\u8bfb\u6027\u548c\u8986\u76d6\u7387\uff0c\u4ee5\u6b64\u4f5c\u4e3a\u5de5\u5177\u6765\u76d1\u63a7\u8bbe\u8ba1\u51b3\u7b56\u7684\u5f71\u54cd\u3002\u8fd9\u79cd\u65b9\u6cd5\u53ef\u4ee5\u901a\u8fc7\u5bf9\u524d\u6cbf\u6a21\u578b\u7684\u5e94\u7528\uff0c\u53d1\u73b0\u8fd9\u4e9b\u6a21\u578b\u5728\u5177\u6709\u6311\u6218\u6027\u7684\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5177\u6709\u9ad8\u53ef\u76d1\u63a7\u6027\uff0c\u4f46\u9700\u8981\u4e0e\u5bf9\u6297\u538b\u529b\u6d4b\u8bd5\u7ed3\u5408\u4ee5\u5168\u9762\u8bc4\u4f30\u6a21\u578b\u7684\u7a33\u5065\u6027\u3002", "motivation": "\u7531\u4e8e\u8bad\u7ec3\u5b9e\u8df5\u6216\u6a21\u578b\u67b6\u6784\u7684\u53d8\u5316\u53ef\u80fd\u4f1a\u5bfc\u81f4Chain-of-Thought (CoT)\u76d1\u63a7\u7684\u673a\u4f1a\u4e27\u5931\uff0c\u56e0\u6b64\u7814\u7a76\u65e8\u5728\u63d0\u51fa\u4e00\u79cd\u5b9e\u7528\u7684\u65b9\u6cd5\uff0c\u6765\u8861\u91cf\u548c\u4fdd\u6301CoT\u7684\u53ef\u76d1\u63a7\u6027\uff0c\u4ee5\u4fdd\u969cAI\u7684\u5b89\u5168\u6027\u3002", "method": "\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u6d4b\u91cfCoT\u53ef\u8bfb\u6027\u548c\u8986\u76d6\u7387\u7684\u65b9\u6cd5\uff1a\u901a\u8fc7\u4e00\u4e2a\u81ea\u52a8\u8bc4\u5206\u63d0\u793a\uff0c\u4f7f\u5f97\u4efb\u4f55\u6709\u80fd\u529b\u7684LLM\u53ef\u4ee5\u8ba1\u7b97CoT\u7684\u53ef\u8bfb\u6027\u548c\u8986\u76d6\u7387\u3002\u7136\u540e\uff0c\u7814\u7a76\u901a\u8fc7\u4f7f\u7528\u5408\u6210\u7684CoT\u9000\u5316\u5bf9\u81ea\u52a8\u8bc4\u5206\u5668\u8fdb\u884c\u4e86\u51c6\u786e\u6027\u68c0\u67e5\uff0c\u5e76\u5c06\u5176\u5e94\u7528\u4e8e\u51e0\u6b3e\u524d\u6cbf\u6a21\u578b\uff0c\u4ee5\u9a8c\u8bc1\u5176\u5728\u5177\u6709\u6311\u6218\u6027\u7684\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u7684\u6027\u80fd\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c\u524d\u6cbf\u6a21\u578b\u5728\u5177\u6709\u6311\u6218\u6027\u7684\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5177\u6709\u9ad8\u53ef\u76d1\u63a7\u6027\u3002\u540c\u65f6\uff0c\u7814\u7a76\u4e5f\u63d0\u51fa\uff0c\u8fd9\u4e9b\u81ea\u52a8\u8bc4\u5206\u7684\u63d0\u793a\u6280\u672f\uff0c\u5e94\u4f5c\u4e3a\u5bf9\u6297\u538b\u529b\u6d4b\u8bd5\u7684\u8865\u5145\uff0c\u800c\u4e0d\u662f\u66ff\u4ee3\u54c1\uff0c\u4ee5\u5168\u9762\u8bc4\u4f30\u6a21\u578b\u7684\u7a33\u5065\u6027\u3002", "conclusion": "\u7814\u7a76\u63d0\u4f9b\u4e86\u4e00\u5957\u7528\u4e8e\u6d4b\u91cfCoT\u53ef\u76d1\u63a7\u6027\u7684\u5de5\u5177\uff0c\u8fd9\u662f\u5bf9AI\u5b89\u5168\u7ef4\u62a4\u7684\u4e00\u79cd\u91cd\u8981\u8d21\u732e\uff0c\u4f46\u540c\u65f6\u63d0\u51fa\uff0c\u8be5\u65b9\u6cd5\u9700\u8981\u4e0e\u5bf9\u6297\u538b\u529b\u6d4b\u8bd5\u76f8\u7ed3\u5408\uff0c\u624d\u80fd\u786e\u4fdd\u6a21\u578b\u7684\u5b89\u5168\u6027\u548c\u7a33\u5065\u6027\u3002"}}
{"id": "2510.24474", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.24474", "abs": "https://arxiv.org/abs/2510.24474", "authors": ["Kyungmin Lee", "Sihyun Yu", "Jinwoo Shin"], "title": "Decoupled MeanFlow: Turning Flow Models into Flow Maps for Accelerated Sampling", "comment": null, "summary": "Denoising generative models, such as diffusion and flow-based models, produce\nhigh-quality samples but require many denoising steps due to discretization\nerror. Flow maps, which estimate the average velocity between timesteps,\nmitigate this error and enable faster sampling. However, their training\ntypically demands architectural changes that limit compatibility with\npretrained flow models. We introduce Decoupled MeanFlow, a simple decoding\nstrategy that converts flow models into flow map models without architectural\nmodifications. Our method conditions the final blocks of diffusion transformers\non the subsequent timestep, allowing pretrained flow models to be directly\nrepurposed as flow maps. Combined with enhanced training techniques, this\ndesign enables high-quality generation in as few as 1 to 4 steps. Notably, we\nfind that training flow models and subsequently converting them is more\nefficient and effective than training flow maps from scratch. On ImageNet\n256x256 and 512x512, our models attain 1-step FID of 2.16 and 2.12,\nrespectively, surpassing prior art by a large margin. Furthermore, we achieve\nFID of 1.51 and 1.68 when increasing the steps to 4, which nearly matches the\nperformance of flow models while delivering over 100x faster inference.", "AI": {"tldr": "\u672c\u6587\u5f15\u5165\u4e86\u4e00\u79cd\u7b80\u5355\u7684\u65b9\u6cd5Decoupled MeanFlow\uff0c\u53ef\u4ee5\u5c06\u73b0\u6709\u7684\u6d41\u6a21\u578b\u8f6c\u6362\u4e3a\u6d41\u573a\u6a21\u578b\uff0c\u800c\u65e0\u9700\u4efb\u4f55\u67b6\u6784\u4fee\u6539\u3002\u8fd9\u79cd\u65b9\u6cd5\u4f7f\u5f97\u9884\u8bad\u7ec3\u7684\u6d41\u6a21\u578b\u53ef\u4ee5\u88ab\u76f4\u63a5\u8f6c\u6362\u4e3a\u6d41\u573a\u6a21\u578b\uff0c\u4ece\u800c\u5b9e\u73b0\u9ad8\u8d28\u91cf\u7684\u6837\u672c\u751f\u6210\uff0c\u901f\u5ea6\u4e5f\u663e\u8457\u63d0\u9ad8\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u8fd9\u79cd\u65b9\u6cd5\u5728ImageNet\u6570\u636e\u96c6\u4e0a\u7684\u6027\u80fd\u8d85\u8fc7\u4e86\u73b0\u6709\u6280\u672f\uff0c\u540c\u65f6\u63a8\u7406\u901f\u5ea6\u63d0\u5347\u4e86\u7ea6100\u500d\u3002", "motivation": "\u73b0\u6709\u7684\u53bb\u566a\u751f\u6210\u6a21\u578b\uff08\u5982\u6269\u6563\u548c\u6d41\u6a21\u578b\uff09\u867d\u7136\u80fd\u591f\u751f\u6210\u9ad8\u8d28\u91cf\u7684\u6837\u672c\uff0c\u4f46\u56e0\u4e3a\u79bb\u6563\u5316\u8bef\u5dee\u9700\u8981\u591a\u4e2a\u53bb\u566a\u6b65\u9aa4\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u5f15\u5165Decoupled MeanFlow\u65b9\u6cd5\u6765\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u540c\u65f6\u4fdd\u6301\u6d41\u6a21\u578b\u7684\u517c\u5bb9\u6027\u3002", "method": "\u672c\u6587\u63d0\u51fa\u7684\u65b9\u6cd5Decoupled MeanFlow\uff0c\u901a\u8fc7\u5728\u6269\u6563\u53d8\u6362\u5668\u7684\u6700\u540e\u51e0\u4e2a\u5757\u4e2d\u9488\u5bf9\u540e\u7eed\u65f6\u95f4\u6b65\u957f\u8fdb\u884c\u6761\u4ef6\u5316\uff0c\u5c06\u9884\u8bad\u7ec3\u7684\u6d41\u6a21\u578b\u8f6c\u6362\u4e3a\u6d41\u573a\u6a21\u578b\u3002\u8fd9\u79cd\u65b9\u6cd5\u65e0\u9700\u5bf9\u67b6\u6784\u8fdb\u884c\u4fee\u6539\u5373\u53ef\u5b9e\u73b0\u8fd9\u4e00\u8f6c\u6362\u3002\u7ed3\u5408\u589e\u5f3a\u7684\u8bad\u7ec3\u6280\u672f\uff0c\u8be5\u65b9\u6cd5\u80fd\u591f\u5728\u8f83\u5c11\u7684\u6b65\u9aa4\u5185\u5b9e\u73b0\u9ad8\u8d28\u91cf\u7684\u6837\u672c\u751f\u6210\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u901a\u8fc7\u672c\u6587\u63d0\u51fa\u7684\u65b9\u6cd5\u5c06\u6d41\u6a21\u578b\u8f6c\u6362\u540e\uff0c\u80fd\u591f\u5728ImageNet 256x256\u548c512x512\u6570\u636e\u96c6\u4e0a\u5206\u522b\u8fbe\u52301\u6b65FID 2.16\u548c2.12\uff0c\u8d85\u8fc7\u4e86\u73b0\u6709\u6280\u672f\u3002\u5f53\u589e\u52a0\u52304\u6b65\u65f6\uff0c\u5206\u522b\u8fbe\u5230FID 1.51\u548c1.68\uff0c\u63a5\u8fd1\u6d41\u6a21\u578b\u7684\u6027\u80fd\uff0c\u4f46\u63a8\u7406\u901f\u5ea6\u63d0\u5347\u4e86100\u591a\u500d\u3002", "conclusion": "\u901a\u8fc7\u5f15\u5165Decoupled MeanFlow\u65b9\u6cd5\uff0c\u53ef\u4ee5\u5c06\u9884\u8bad\u7ec3\u7684\u6d41\u6a21\u578b\u76f4\u63a5\u8f6c\u6362\u4e3a\u6d41\u573a\u6a21\u578b\uff0c\u5b9e\u73b0\u9ad8\u8d28\u91cf\u751f\u6210\u7684\u540c\u65f6\u6781\u5927\u5730\u63d0\u5347\u4e86\u6548\u7387\u3002"}}
{"id": "2510.23972", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.23972", "abs": "https://arxiv.org/abs/2510.23972", "authors": ["Andra\u017e Jelin\u010di\u010d", "Owen Lockwood", "Akhil Garlapati", "Guillaume Verdon", "Trevor McCourt"], "title": "An efficient probabilistic hardware architecture for diffusion-like models", "comment": "9 pages, 6 figures", "summary": "The proliferation of probabilistic AI has promoted proposals for specialized\nstochastic computers. Despite promising efficiency gains, these proposals have\nfailed to gain traction because they rely on fundamentally limited modeling\ntechniques and exotic, unscalable hardware. In this work, we address these\nshortcomings by proposing an all-transistor probabilistic computer that\nimplements powerful denoising models at the hardware level. A system-level\nanalysis indicates that devices based on our architecture could achieve\nperformance parity with GPUs on a simple image benchmark using approximately\n10,000 times less energy.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5168\u6676\u4f53\u7ba1\u7684\u6982\u7387\u8ba1\u7b97\u673a\uff0c\u8fd9\u79cd\u8ba1\u7b97\u673a\u53ef\u4ee5\u5728\u786c\u4ef6\u7ea7\u522b\u5b9e\u73b0\u5f3a\u5927\u7684\u53bb\u566a\u6a21\u578b\u3002\u7cfb\u7edf\u7ea7\u5206\u6790\u8868\u660e\uff0c\u57fa\u4e8e\u8fd9\u79cd\u67b6\u6784\u7684\u8bbe\u5907\u5728\u4f7f\u7528\u5c11\u91cf\u80fd\u91cf\u7684\u60c5\u51b5\u4e0b\uff0c\u53ef\u4ee5\u5728\u7b80\u5355\u7684\u56fe\u50cf\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4e0eGPU\u8fbe\u5230\u6027\u80fd\u6301\u5e73\u3002", "motivation": "\u73b0\u6709\u7684\u4e13\u95e8\u5316\u968f\u673a\u8ba1\u7b97\u673a\u7684\u63d0\u6848\u7531\u4e8e\u5176\u6709\u9650\u7684\u5efa\u6a21\u6280\u672f\u548c\u4e0d\u9002\u7528\u7684\u786c\u4ef6\u800c\u672a\u80fd\u5f97\u5230\u5e7f\u6cdb\u5e94\u7528\u3002\u56e0\u6b64\uff0c\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8e\u5168\u6676\u4f53\u7ba1\u7684\u6982\u7387\u8ba1\u7b97\u67b6\u6784\uff0c\u4ee5\u6539\u5584\u8fd9\u4e9b\u4e0d\u8db3\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u57fa\u4e8e\u5168\u6676\u4f53\u7ba1\u7684\u6982\u7387\u8ba1\u7b97\u673a\u67b6\u6784\uff0c\u8fd9\u79cd\u67b6\u6784\u53ef\u4ee5\u5728\u786c\u4ef6\u7ea7\u522b\u5b9e\u73b0\u5148\u8fdb\u7684\u53bb\u566a\u6a21\u578b\uff0c\u5e76\u901a\u8fc7\u7cfb\u7edf\u7ea7\u5206\u6790\u9a8c\u8bc1\u5176\u6027\u80fd\u548c\u80fd\u6548\u3002", "result": "\u6a21\u62df\u7ed3\u679c\u663e\u793a\uff0c\u57fa\u4e8e\u6b64\u67b6\u6784\u7684\u8bbe\u5907\u5728\u7b80\u5355\u7684\u56fe\u50cf\u5904\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53ef\u4ee5\u4e0eGPU\u7ade\u4e89\uff0c\u540c\u65f6\u4f7f\u7528\u7684\u80fd\u91cf\u4ec5\u4e3a\u5176\u4ebf\u5206\u4e4b\u4e00\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u5c55\u793a\u4e86\u4e00\u79cd\u5148\u8fdb\u7684\u6982\u7387\u8ba1\u7b97\u67b6\u6784\uff0c\u5b83\u901a\u8fc7\u5148\u8fdb\u7684\u786c\u4ef6\u6280\u672f\u63d0\u9ad8\u4e86\u6027\u80fd\u548c\u80fd\u6548\u3002"}}
{"id": "2510.23974", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.23974", "abs": "https://arxiv.org/abs/2510.23974", "authors": ["Byeonghu Na", "Minsang Park", "Gyuwon Sim", "Donghyeok Shin", "HeeSun Bae", "Mina Kang", "Se Jung Kwon", "Wanmo Kang", "Il-Chul Moon"], "title": "Diffusion Adaptive Text Embedding for Text-to-Image Diffusion Models", "comment": "Accepted at NeurIPS 2025", "summary": "Text-to-image diffusion models rely on text embeddings from a pre-trained\ntext encoder, but these embeddings remain fixed across all diffusion timesteps,\nlimiting their adaptability to the generative process. We propose Diffusion\nAdaptive Text Embedding (DATE), which dynamically updates text embeddings at\neach diffusion timestep based on intermediate perturbed data. We formulate an\noptimization problem and derive an update rule that refines the text embeddings\nat each sampling step to improve alignment and preference between the mean\npredicted image and the text. This allows DATE to dynamically adapts the text\nconditions to the reverse-diffused images throughout diffusion sampling without\nrequiring additional model training. Through theoretical analysis and empirical\nresults, we show that DATE maintains the generative capability of the model\nwhile providing superior text-image alignment over fixed text embeddings across\nvarious tasks, including multi-concept generation and text-guided image\nediting. Our code is available at https://github.com/aailab-kaist/DATE.", "AI": {"tldr": "DATE\u63d0\u51fa\u4e86\u4e00\u4e2a\u52a8\u6001\u66f4\u65b0\u6587\u672c\u5d4c\u5165\u7684\u65b9\u6cd5\uff0c\u4ee5\u63d0\u9ad8\u6587\u672c\u548c\u751f\u6210\u56fe\u50cf\u4e4b\u95f4\u7684\u5bf9\u9f50\u80fd\u529b\uff0c\u800c\u4e0d\u9700\u8981\u989d\u5916\u7684\u6a21\u578b\u8bad\u7ec3\u3002\u5b9e\u9a8c\u8bc1\u660e\u4e86DATE\u5728\u591a\u6982\u5ff5\u751f\u6210\u548c\u57fa\u4e8e\u6587\u672c\u7684\u56fe\u50cf\u7f16\u8f91\u7b49\u4efb\u52a1\u4e0a\u7684\u4f18\u8d8a\u6027\u3002", "motivation": "\u73b0\u6709\u7684\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u6a21\u578b\u7531\u4e8e\u4f7f\u7528\u56fa\u5b9a\u7684\u6587\u672c\u5d4c\u5165\uff0c\u9650\u5236\u4e86\u751f\u6210\u8fc7\u7a0b\u7684\u9002\u5e94\u6027\uff0c\u4ece\u800c\u5f71\u54cd\u4e86\u6587\u672c\u548c\u751f\u6210\u56fe\u50cf\u7684\u5bf9\u9f50\u3002\u56e0\u6b64\uff0c\u63d0\u51faDATE\u6765\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u901a\u8fc7\u52a8\u6001\u66f4\u65b0\u6587\u672c\u5d4c\u5165\uff0c\u63d0\u9ad8\u751f\u6210\u56fe\u50cf\u7684\u8d28\u91cf\u548c\u6587\u672c\u6307\u5bfc\u80fd\u529b\u3002", "method": "DATE\u901a\u8fc7\u5728\u6bcf\u4e2a\u6269\u6563\u65f6\u95f4\u6b65\u66f4\u65b0\u6587\u672c\u5d4c\u5165\u6765\u6539\u8fdb\u5bf9\u9f50\uff0c\u65b9\u6cd5\u662f\u6839\u636e\u4e2d\u95f4\u5e72\u6270\u7684\u6570\u636e\u4f18\u5316\u6587\u672c\u5d4c\u5165\uff0c\u4ee5\u63d0\u9ad8\u6587\u672c\u548c\u56fe\u50cf\u4e4b\u95f4\u7684\u5bf9\u9f50\u548c\u504f\u597d\u3002\u8fd9\u4e9b\u66f4\u65b0\u57fa\u4e8e\u4e00\u4e2a\u5177\u4f53\u5b9e\u73b0\u7684\u4f18\u5316\u95ee\u9898\u548c\u5bfc\u51fa\u7684\u66f4\u65b0\u89c4\u5219\uff0c\u80fd\u591f\u81ea\u9002\u5e94\u5730\u8c03\u6574\u6587\u672c\u6761\u4ef6\u4ee5\u6700\u4f73\u5730\u9002\u5e94\u9006\u5411\u6269\u6563\u56fe\u50cf\u3002", "result": "DATE\u5728\u591a\u4e2a\u4efb\u52a1\u4e0a\u5c55\u793a\u4e86\u6bd4\u56fa\u5b9a\u6587\u672c\u5d4c\u5165\u66f4\u597d\u7684\u6587\u672c-\u56fe\u50cf\u5bf9\u9f50\u6548\u679c\uff0c\u5305\u62ec\u591a\u6982\u5ff5\u751f\u6210\u548c\u57fa\u4e8e\u6587\u672c\u7684\u56fe\u50cf\u7f16\u8f91\u4efb\u52a1\u3002", "conclusion": "DATE\u901a\u8fc7\u52a8\u6001\u8c03\u6574\u6587\u672c\u5d4c\u5165\uff0c\u63d0\u9ad8\u4e86\u6587\u672c\u6761\u4ef6\u548c\u9006\u5411\u6269\u6563\u56fe\u50cf\u4e4b\u95f4\u7684\u5bf9\u9f50\uff0c\u540c\u65f6\u4fdd\u6301\u751f\u6210\u6a21\u578b\u7684\u80fd\u529b\u3002\u8fd9\u8868\u660e\u52a8\u6001\u8c03\u6574\u6587\u672c\u5d4c\u5165\u53ef\u4ee5\u663e\u8457\u6539\u5584\u751f\u6210\u56fe\u50cf\u7684\u8d28\u91cf\uff0c\u800c\u65e0\u9700\u989d\u5916\u7684\u8bad\u7ec3\u3002"}}
{"id": "2510.24514", "categories": ["cs.CV", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.24514", "abs": "https://arxiv.org/abs/2510.24514", "authors": ["Huanyu Zhang", "Wenshan Wu", "Chengzu Li", "Ning Shang", "Yan Xia", "Yangyu Huang", "Yifan Zhang", "Li Dong", "Zhang Zhang", "Liang Wang", "Tieniu Tan", "Furu Wei"], "title": "Latent Sketchpad: Sketching Visual Thoughts to Elicit Multimodal Reasoning in MLLMs", "comment": null, "summary": "While Multimodal Large Language Models (MLLMs) excel at visual understanding,\nthey often struggle in complex scenarios that require visual planning and\nimagination. Inspired by how humans use sketching as a form of visual thinking\nto develop and communicate ideas, we introduce Latent Sketchpad, a framework\nthat equips MLLMs with an internal visual scratchpad. The internal visual\nrepresentations of MLLMs have traditionally been confined to perceptual\nunderstanding. We repurpose them to support generative visual thought without\ncompromising reasoning ability. Building on frontier MLLMs, our approach\nintegrates visual generation directly into their native autoregressive\nreasoning process. It allows the model to interleave textual reasoning with the\ngeneration of visual latents. These latents guide the internal thought process\nand can be translated into sketch images for interpretability. To realize this,\nwe introduce two components: a Context-Aware Vision Head autoregressively\nproduces visual representations, and a pretrained Sketch Decoder renders these\ninto human-interpretable images. We evaluate the framework on our new dataset\nMazePlanning. Experiments across various MLLMs show that Latent Sketchpad\ndelivers comparable or even superior reasoning performance to their backbone.\nIt further generalizes across distinct frontier MLLMs, including Gemma3 and\nQwen2.5-VL. By extending model's textual reasoning to visual thinking, our\nframework opens new opportunities for richer human-computer interaction and\nbroader applications. More details and resources are available on our project\npage: https://latent-sketchpad.github.io/.", "AI": {"tldr": "Latent Sketchpad \u662f\u4e00\u4e2a\u6846\u67b6\uff0c\u5b83\u4f7f\u591a\u6a21\u6001\u5927\u578b\u8bed\u8a00\u6a21\u578b\u80fd\u591f\u5728\u5185\u90e8\u751f\u6210\u89c6\u89c9\u9690\u542b\u8868\u793a\uff0c\u540c\u65f6\u4fdd\u6301\u6216\u63d0\u9ad8\u5176\u63a8\u7406\u80fd\u529b\uff0c\u4ece\u800c\u589e\u5f3a\u5176\u5728\u590d\u6742\u573a\u666f\u4e2d\u7684\u6027\u80fd\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u6846\u67b6\u5728\u63a8\u7406\u4e0a\u7684\u8868\u73b0\u4e0e\u5e95\u5c42\u6a21\u578b\u76f8\u5f53\u6216\u66f4\u4f18\uff0c\u5e76\u4e14\u5728\u4e0d\u540c\u6a21\u578b\u4e0a\u5177\u6709\u826f\u597d\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u4e3a\u66f4\u4e30\u5bcc\u7684\u4eba\u673a\u4ea4\u4e92\u548c\u66f4\u5e7f\u6cdb\u7684\u5e94\u7528\u6253\u5f00\u4e86\u65b0\u7684\u673a\u4f1a\u3002", "motivation": "\u591a\u6a21\u6001\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u590d\u6742\u7684\u573a\u666f\u4e2d\u9700\u8981\u8fdb\u884c\u89c6\u89c9\u89c4\u5212\u548c\u60f3\u8c61\u65f6\u8868\u73b0\u4e0d\u4f73\uff0c\u8be5\u6846\u67b6\u901a\u8fc7\u5f15\u5165\u5185\u90e8\u89c6\u89c9\u8349\u56fe\u53ca\u89c6\u89c9\u751f\u6210\u80fd\u529b\uff0c\u63d0\u5347\u4e86\u6a21\u578b\u5728\u8fd9\u4e9b\u590d\u6742\u573a\u666f\u4e2d\u7684\u8868\u73b0\u80fd\u529b\u3002\u8fd9\u4e00\u7075\u611f\u6765\u6e90\u4e8e\u4eba\u7c7b\u4f7f\u7528\u8349\u56fe\u4f5c\u4e3a\u4e00\u79cd\u89c6\u89c9\u601d\u8003\u65b9\u5f0f\u6765\u53d1\u5c55\u548c\u6c9f\u901a\u60f3\u6cd5\u3002", "method": "\u5229\u7528\u524d\u6cbf\u7684\u591a\u6a21\u6001\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u8be5\u6846\u67b6\u901a\u8fc7\u89c6\u89c9\u751f\u6210\u6574\u5408\u5230\u6a21\u578b\u7684\u539f\u751f\u81ea\u56de\u5f52\u63a8\u7406\u8fc7\u7a0b\u4e2d\u3002\u5b83\u5141\u8bb8\u6a21\u578b\u5728\u6587\u672c\u63a8\u7406\u548c\u751f\u6210\u89c6\u89c9\u9690\u5f62\u8868\u793a\u4e4b\u95f4\u4ea4\u66ff\uff0c\u8fd9\u4e9b\u8868\u793a\u65e2\u53ef\u4ee5\u6307\u5bfc\u5185\u90e8\u601d\u8003\u8fc7\u7a0b\uff0c\u4e5f\u53ef\u4ee5\u8f6c\u6362\u4e3a\u4eba\u7c7b\u53ef\u7406\u89e3\u7684\u8349\u56fe\u56fe\u50cf\u3002\u6846\u67b6\u5305\u62ec\u4e24\u4e2a\u7ec4\u4ef6\uff1a\u4e00\u4e2a\u4e0a\u4e0b\u6587\u611f\u77e5\u89c6\u89c9\u5934\u90e8\u81ea\u56de\u5f52\u751f\u6210\u89c6\u89c9\u8868\u793a\uff0c\u4ee5\u53ca\u4e00\u4e2a\u9884\u8bad\u7ec3\u7684\u8349\u56fe\u89e3\u7801\u5668\u5c06\u8fd9\u4e9b\u8868\u793a\u8f6c\u5316\u4e3a\u4eba\u7c7b\u53ef\u8bfb\u7684\u56fe\u50cf\u3002\u5b9e\u9a8c\u5728\u65b0\u6570\u636e\u96c6MazePlanning\u4e0a\u5c55\u793a\u4e86\u8be5\u6846\u67b6\u7684\u6548\u679c\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cLatent Sketchpad \u5728\u4e0d\u540c\u591a\u6a21\u6001\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u8868\u73b0\u4e0e\u539f\u59cb\u6a21\u578b\u76f8\u5f53\u6216\u66f4\u52a0\u51fa\u8272\uff0c\u5e76\u4e14\u5728\u4e0d\u540c\u6a21\u578b\u4e0a\u5177\u6709\u826f\u597d\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u901a\u8fc7\u5c06\u6a21\u578b\u7684\u6587\u672c\u63a8\u7406\u6269\u5c55\u5230\u89c6\u89c9\u601d\u8003\uff0c\u8be5\u6846\u67b6\u4e3a\u66f4\u4e30\u5bcc\u7684\u4eba\u673a\u4ea4\u4e92\u548c\u66f4\u5e7f\u6cdb\u7684\u5e94\u7528\u6253\u5f00\u4e86\u65b0\u7684\u673a\u4f1a\u3002"}}
{"id": "2510.23977", "categories": ["cs.LG", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.23977", "abs": "https://arxiv.org/abs/2510.23977", "authors": ["Yohan Abeysinghe", "Muhammad Akhtar Munir", "Sanoojan Baliah", "Ron Sarafian", "Fahad Shahbaz Khan", "Yinon Rudich", "Salman Khan"], "title": "Synergistic Neural Forecasting of Air Pollution with Stochastic Sampling", "comment": null, "summary": "Air pollution remains a leading global health and environmental risk,\nparticularly in regions vulnerable to episodic air pollution spikes due to\nwildfires, urban haze and dust storms. Accurate forecasting of particulate\nmatter (PM) concentrations is essential to enable timely public health warnings\nand interventions, yet existing models often underestimate rare but hazardous\npollution events. Here, we present SynCast, a high-resolution neural\nforecasting model that integrates meteorological and air composition data to\nimprove predictions of both average and extreme pollution levels. Built on a\nregionally adapted transformer backbone and enhanced with a diffusion-based\nstochastic refinement module, SynCast captures the nonlinear dynamics driving\nPM spikes more accurately than existing approaches. Leveraging on harmonized\nERA5 and CAMS datasets, our model shows substantial gains in forecasting\nfidelity across multiple PM variables (PM$_1$, PM$_{2.5}$, PM$_{10}$),\nespecially under extreme conditions. We demonstrate that conventional loss\nfunctions underrepresent distributional tails (rare pollution events) and show\nthat SynCast, guided by domain-aware objectives and extreme value theory,\nsignificantly enhances performance in highly impacted regions without\ncompromising global accuracy. This approach provides a scalable foundation for\nnext-generation air quality early warning systems and supports climate-health\nrisk mitigation in vulnerable regions.", "AI": {"tldr": "SynCast \u662f\u4e00\u79cd\u9ad8\u5206\u8fa8\u7387\u7684\u795e\u7ecf\u9884\u62a5\u6a21\u578b\uff0c\u901a\u8fc7\u96c6\u6210\u6c14\u8c61\u548c\u7a7a\u6c14\u8d28\u91cf\u6570\u636e\u6765\u6539\u5584\u9897\u7c92\u7269\u6d53\u5ea6\u9884\u6d4b\uff0c\u7279\u522b\u662f\u5728\u6781\u7aef\u6c61\u67d3\u4e8b\u4ef6\u4e2d\u8868\u73b0\u4f18\u5f02\u3002\u8be5\u6a21\u578b\u57fa\u4e8e\u533a\u57df\u9002\u5e94\u7684\u53d8\u538b\u5668\u9aa8\u5e72\u548c\u6269\u6563\u589e\u5f3a\u7684\u968f\u673a\u7ec6\u5316\u6a21\u5757\u6784\u5efa\uff0c\u80fd\u591f\u66f4\u51c6\u786e\u5730\u6355\u6349\u9897\u7c92\u7269\u6d53\u5ea6\u7a81\u589e\u7684\u975e\u7ebf\u6027\u52a8\u6001\u3002\u5b83\u5229\u7528\u4e86\u6807\u51c6\u5316\u7684ERA5\u548cCAMS\u6570\u636e\u96c6\uff0c\u5e76\u5bf9\u4f20\u7edf\u635f\u5931\u51fd\u6570\u8fdb\u884c\u4e86\u6539\u8fdb\uff0c\u63d0\u9ad8\u4e86\u5728\u6781\u7aef\u6761\u4ef6\u4e0b\u7684\u9884\u62a5\u7cbe\u5ea6\u3002\u8fd9\u9879\u7814\u7a76\u4e3a\u4e0b\u4e00\u4ee3\u7a7a\u6c14\u8d28\u91cf\u65e9\u671f\u9884\u8b66\u7cfb\u7edf\u63d0\u4f9b\u4e86\u4e00\u4e2a\u53ef\u6269\u5c55\u7684\u57fa\u7840\uff0c\u5e76\u6709\u52a9\u4e8e\u6c14\u5019-\u5065\u5eb7\u98ce\u9669\u7684\u7f13\u89e3\u3002", "motivation": "\u5c3d\u7ba1\u73b0\u6709\u6a21\u578b\u5728\u9884\u6d4b\u7a7a\u6c14\u6c61\u67d3\u65b9\u9762\u5df2\u7ecf\u6709\u6240\u8fdb\u5c55\uff0c\u4f46\u5728\u7f55\u89c1\u4f46\u5371\u9669\u7684\u6c61\u67d3\u4e8b\u4ef6\u9884\u6d4b\u4e0a\u4ecd\u7136\u4e0d\u8db3\u3002\u56e0\u6b64\uff0c\u7814\u7a76\u7684\u52a8\u673a\u5728\u4e8e\u5f00\u53d1\u4e00\u79cd\u80fd\u591f\u66f4\u7cbe\u786e\u9884\u62a5\u5305\u62ec\u6781\u7aef\u6c61\u67d3\u4e8b\u4ef6\u5728\u5185\u7684\u9897\u7c92\u7269\u8d28\u6d53\u5ea6\u7684\u6a21\u578b\u3002", "method": "\u63d0\u51fa\u4e86\u57fa\u4e8e\u5206\u7ea7\u53d8\u538b\u5668\u548c\u6269\u6563\u589e\u5f3a\u968f\u673a\u7ec6\u5316\u6a21\u5757\u7684SynCast\u6a21\u578b\uff0c\u8be5\u6a21\u578b\u5229\u7528\u6807\u51c6\u5316\u7684ERA5\u548cCAMS\u6570\u636e\u96c6\uff0c\u4ee5\u63d0\u9ad8\u9884\u6d4b\u7684\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\u3002\u6b64\u5916\uff0c\u6a21\u578b\u91c7\u7528\u4e86\u6539\u8fdb\u7684\u635f\u5931\u51fd\u6570\u4ee5\u66f4\u597d\u5730\u6355\u6349\u5206\u5e03\u5c3e\u90e8\u7684\u6982\u7387\uff0c\u4ece\u800c\u66f4\u51c6\u786e\u5730\u6a21\u62df\u7f55\u89c1\u6c61\u67d3\u4e8b\u4ef6\u3002", "result": "\u4e0e\u73b0\u6709\u65b9\u6cd5\u76f8\u6bd4\uff0cSynCast\u6a21\u578b\u5728\u9884\u6d4b\u9897\u7c92\u7269\u8d28\u6d53\u5ea6\u65b9\u9762\uff08\u7279\u522b\u662fPM1\u3001PM2.5\u548cPM10\uff09\u5728\u6781\u7aef\u6761\u4ef6\u4e0b\u8868\u73b0\u51fa\u8272\u3002\u5b83\u4e0d\u4ec5\u6ca1\u6709\u727a\u7272\u5168\u7403\u8303\u56f4\u5185\u7684\u9884\u62a5\u51c6\u786e\u6027\uff0c\u8fd8\u663e\u8457\u6539\u5584\u4e86\u9ad8\u66b4\u9732\u5730\u533a\u7684\u9884\u62a5\u6027\u80fd\u3002", "conclusion": "SynCast\u6a21\u578b\u4e3a\u4e0b\u4e00\u4ee3\u7a7a\u6c14\u8d28\u91cf\u65e9\u671f\u9884\u8b66\u7cfb\u7edf\u63d0\u4f9b\u4e86\u575a\u5b9e\u7684\u7406\u8bba\u57fa\u7840\u548c\u6280\u672f\u652f\u6301\u3002\u5b83\u5c06\u5728\u6c14\u5019-\u5065\u5eb7\u98ce\u9669\u7684\u7f13\u89e3\u5de5\u4f5c\u4e2d\u53d1\u6325\u91cd\u8981\u4f5c\u7528\uff0c\u5c24\u5176\u662f\u5728\u6613\u53d7\u7a7a\u6c14\u6c61\u67d3\u5f71\u54cd\u7684\u5730\u533a\u3002"}}
{"id": "2510.24579", "categories": ["cs.CV", "I.4.5; I.5"], "pdf": "https://arxiv.org/pdf/2510.24579", "abs": "https://arxiv.org/abs/2510.24579", "authors": ["Xu Jiang", "Huiying Pan", "Ligen Shi", "Jianing Sun", "Wenfeng Xu", "Xing Zhao"], "title": "Physics-Inspired Gaussian Kolmogorov-Arnold Networks for X-ray Scatter Correction in Cone-Beam CT", "comment": "8 pages, 6 figures", "summary": "Cone-beam CT (CBCT) employs a flat-panel detector to achieve\nthree-dimensional imaging with high spatial resolution. However, CBCT is\nsusceptible to scatter during data acquisition, which introduces CT value bias\nand reduced tissue contrast in the reconstructed images, ultimately degrading\ndiagnostic accuracy. To address this issue, we propose a deep learning-based\nscatter artifact correction method inspired by physical prior knowledge.\nLeveraging the fact that the observed point scatter probability density\ndistribution exhibits rotational symmetry in the projection domain. The method\nuses Gaussian Radial Basis Functions (RBF) to model the point scatter function\nand embeds it into the Kolmogorov-Arnold Networks (KAN) layer, which provides\nefficient nonlinear mapping capabilities for learning high-dimensional scatter\nfeatures. By incorporating the physical characteristics of the scattered photon\ndistribution together with the complex function mapping capacity of KAN, the\nmodel improves its ability to accurately represent scatter. The effectiveness\nof the method is validated through both synthetic and real-scan experiments.\nExperimental results show that the model can effectively correct the scatter\nartifacts in the reconstructed images and is superior to the current methods in\nterms of quantitative metrics.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u6563\u5c04\u4f2a\u5f71\u6821\u6b63\u65b9\u6cd5\uff0c\u901a\u8fc7\u5f15\u5165\u7269\u7406\u5148\u9a8c\u77e5\u8bc6\u548c\u5229\u7528Kolmogorov-Arnold Networks (KAN) \u5c42\u6765\u6709\u6548\u6821\u6b63\u6563\u5c04\u4f2a\u5f71\uff0c\u63d0\u9ad8CT\u56fe\u50cf\u7684\u8d28\u91cf\u548c\u8bca\u65ad\u51c6\u786e\u6027\u3002", "motivation": "Cone-beam CT (CBCT) \u7531\u4e8e\u5728\u6570\u636e\u91c7\u96c6\u8fc7\u7a0b\u4e2d\u5bb9\u6613\u53d7\u5230\u6563\u5c04\u7684\u5f71\u54cd\uff0c\u5bfc\u81f4CT\u503c\u504f\u5dee\u548c\u7ec4\u7ec7\u5bf9\u6bd4\u5ea6\u964d\u4f4e\uff0c\u4ece\u800c\u5f71\u54cd\u8bca\u65ad\u51c6\u786e\u6027\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u65b9\u6cd5\u6765\u6821\u6b63\u6563\u5c04\u4f2a\u5f71\u3002", "method": "\u8be5\u65b9\u6cd5\u5229\u7528\u5e73\u9762\u6563\u5c04\u6982\u7387\u5bc6\u5ea6\u5206\u5e03\u7684\u65cb\u8f6c\u5bf9\u79f0\u6027\uff0c\u91c7\u7528\u9ad8\u65af\u5f84\u5411\u57fa\u51fd\u6570\uff08RBF\uff09\u6765\u5efa\u6a21\u70b9\u6563\u5c04\u51fd\u6570\uff0c\u5e76\u5c06\u5176\u5d4c\u5165\u5230Kolmogorov-Arnold Networks (KAN) \u5c42\u4e2d\uff0c\u901a\u8fc7KAN\u7684\u5f3a\u5927\u6620\u5c04\u80fd\u529b\u6765\u5b66\u4e60\u9ad8\u7ef4\u6563\u5c04\u7279\u5f81\u3002\u7ed3\u5408\u7269\u7406\u4e0a\u6563\u5c04\u5149\u5b50\u5206\u5e03\u7684\u7279\u5f81\uff0c\u8be5\u6a21\u578b\u63d0\u9ad8\u4e86\u5bf9\u6563\u5c04\u4f2a\u5f71\u6821\u6b63\u7684\u51c6\u786e\u6027\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u5408\u6210\u548c\u771f\u5b9e\u6570\u636e\u5b9e\u9a8c\u4e2d\u5747\u80fd\u6709\u6548\u6821\u6b63\u6563\u5c04\u4f2a\u5f71\uff0c\u5e76\u5728\u5b9a\u91cf\u6307\u6807\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684\u65b9\u6cd5\u901a\u8fc7\u7ed3\u5408\u7269\u7406\u5148\u9a8c\u77e5\u8bc6\u548c\u6df1\u5ea6\u5b66\u4e60\u6280\u672f\uff0c\u6709\u6548\u5730\u63d0\u9ad8\u4e86\u6563\u5c04\u4f2a\u5f71\u6821\u6b63\u7684\u51c6\u786e\u6027\u3002"}}
{"id": "2510.24640", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.24640", "abs": "https://arxiv.org/abs/2510.24640", "authors": ["Xin Zhang", "Yuqi Song", "Fei Zuo"], "title": "A Dual-Branch CNN for Robust Detection of AI-Generated Facial Forgeries", "comment": null, "summary": "The rapid advancement of generative AI has enabled the creation of highly\nrealistic forged facial images, posing significant threats to AI security,\ndigital media integrity, and public trust. Face forgery techniques, ranging\nfrom face swapping and attribute editing to powerful diffusion-based image\nsynthesis, are increasingly being used for malicious purposes such as\nmisinformation, identity fraud, and defamation. This growing challenge\nunderscores the urgent need for robust and generalizable face forgery detection\nmethods as a critical component of AI security infrastructure. In this work, we\npropose a novel dual-branch convolutional neural network for face forgery\ndetection that leverages complementary cues from both spatial and frequency\ndomains. The RGB branch captures semantic information, while the frequency\nbranch focuses on high-frequency artifacts that are difficult for generative\nmodels to suppress. A channel attention module is introduced to adaptively fuse\nthese heterogeneous features, highlighting the most informative channels for\nforgery discrimination. To guide the network's learning process, we design a\nunified loss function, FSC Loss, that combines focal loss, supervised\ncontrastive loss, and a frequency center margin loss to enhance class\nseparability and robustness. We evaluate our model on the DiFF benchmark, which\nincludes forged images generated from four representative methods:\ntext-to-image, image-to-image, face swap, and face edit. Our method achieves\nstrong performance across all categories and outperforms average human\naccuracy. These results demonstrate the model's effectiveness and its potential\ncontribution to safeguarding AI ecosystems against visual forgery attacks.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2510.24653", "categories": ["cs.CV", "cs.HC", "J.3"], "pdf": "https://arxiv.org/pdf/2510.24653", "abs": "https://arxiv.org/abs/2510.24653", "authors": ["Veronica Thai", "Rui Li", "Meng Ling", "Shuning Jiang", "Jeremy Wolfe", "Raghu Machiraju", "Yan Hu", "Zaibo Li", "Anil Parwani", "Jian Chen"], "title": "Eye-Tracking, Mouse Tracking, Stimulus Tracking,and Decision-Making Datasets in Digital Pathology", "comment": "16 pages, 9 figures, submitted to Nature Scientific Data", "summary": "Interpretation of giga-pixel whole-slide images (WSIs) is an important but\ndifficult task for pathologists. Their diagnostic accuracy is estimated to\naverage around 70%. Adding a second pathologist does not substantially improve\ndecision consistency. The field lacks adequate behavioral data to explain\ndiagnostic errors and inconsistencies. To fill in this gap, we present\nPathoGaze1.0, a comprehensive behavioral dataset capturing the dynamic visual\nsearch and decision-making processes of the full diagnostic workflow during\ncancer diagnosis. The dataset comprises 18.69 hours of eye-tracking, mouse\ninteraction, stimulus tracking, viewport navigation, and diagnostic decision\ndata (EMSVD) collected from 19 pathologists interpreting 397 WSIs. The data\ncollection process emphasizes ecological validity through an\napplication-grounded testbed, called PTAH. In total, we recorded 171,909\nfixations, 263,320 saccades, and 1,867,362 mouse interaction events. In\naddition, such data could also be used to improve the training of both\npathologists and AI systems that might support human experts. All experiments\nwere preregistered at https://osf.io/hj9a7, and the complete dataset along with\nanalysis code is available at https://go.osu.edu/pathogaze.", "AI": {"tldr": "\u63d0\u51fa\u4e86PathoGaze1.0\uff0c\u8fd9\u662f\u4e00\u4e2a\u5168\u9762\u7684\u884c\u4e3a\u6570\u636e\u96c6\uff0c\u6355\u83b7\u4e86\u5168\u8bca\u65ad\u5de5\u4f5c\u6d41\u7a0b\u4e2d\u764c\u75c7\u8bca\u65ad\u8fc7\u7a0b\u4e2d\u7684\u52a8\u6001\u89c6\u89c9\u641c\u7d22\u548c\u51b3\u7b56\u8fc7\u7a0b\u3002\u6570\u636e\u96c6\u5305\u62ec19\u4f4d\u75c5\u7406\u5b66\u5bb6\u5bf9397\u5f20\u5168\u89c6\u56fe\u56fe\u50cf\u8fdb\u884c\u89e3\u91ca\u5f55\u5236\u768418.69\u5c0f\u65f6\u7684\u884c\u4e3a\u6570\u636e\uff0c\u5176\u4e2d\u5305\u62ec\u773c\u52a8\u8ffd\u8e2a\u3001\u9f20\u6807\u4ea4\u4e92\u8f68\u8ff9\u3001\u89c6\u53e3\u5bfc\u822a\u3001\u523a\u6fc0\u8ddf\u8e2a\u548c\u8bca\u65ad\u51b3\u7b56\u6570\u636e\uff08EMSVD\uff09\u7b49\u3002\u8fd9\u4e9b\u6570\u636e\u8fd8\u80fd\u7528\u4e8e\u63d0\u9ad8\u75c5\u7406\u5b66\u5bb6\u548c\u4eba\u5de5\u667a\u80fd\u7cfb\u7edf\u7684\u57f9\u8bad\u6548\u679c\u3002\u6240\u6709\u5b9e\u9a8c\u5747\u5df2\u5728https://osf.io/hj9a7\u9884\u6ce8\u518c\uff0c\u5e76\u4e14\u5b8c\u6574\u7684\u6570\u636e\u96c6\u548c\u5206\u6790\u4ee3\u7801\u53ef\u5728https://go.osu.edu/pathogaze\u83b7\u53d6\u3002", "motivation": "\u63d0\u51faPathoGaze1.0\u6570\u636e\u96c6\u4e3b\u8981\u662f\u56e0\u4e3a\u76ee\u524d\u75c5\u7406\u5b66\u754c\u7f3a\u4e4f\u8db3\u591f\u7684\u884c\u4e3a\u6570\u636e\u6765\u89e3\u91ca\u8bca\u65ad\u9519\u8bef\u548c\u4e0d\u4e00\u81f4\u6027\u3002\u75c5\u7406\u5b66\u5bb6\u5728\u89e3\u91cagiga-pixel\u5168\u89c6\u56fe\u56fe\u50cf\uff08WSIs\uff09\u65f6\u9762\u4e34\u7740\u91cd\u8981\u7684\u4f46\u53c8\u6781\u5177\u6311\u6218\u6027\u7684\u4efb\u52a1\uff0c\u5176\u8bca\u65ad\u51c6\u786e\u6027\u4f30\u8ba1\u5e73\u5747\u4e3a70%\u3002\u5373\u4f7f\u589e\u52a0\u7b2c\u4e8c\u4f4d\u75c5\u7406\u5b66\u5bb6\u4e5f\u4e0d\u80fd\u663e\u8457\u6539\u5584\u51b3\u7b56\u4e00\u81f4\u6027\u3002\u56e0\u6b64\uff0c\u9700\u8981\u5f00\u53d1\u4e00\u4e2a\u884c\u4e3a\u6570\u636e\u96c6\u6765\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u901a\u8fc7\u6784\u5efa\u4e00\u4e2a\u5e94\u7528\u624e\u6839\u7684\u6d4b\u8bd5\u5e73\u53f0PTAH\uff0c\u4ece19\u4f4d\u75c5\u7406\u5b66\u5bb6\u5bf9397\u5f20\u5168\u89c6\u56fe\u56fe\u50cf\u7684\u89e3\u91ca\u8fc7\u7a0b\u4e2d\u6355\u83b718.69\u5c0f\u65f6\u7684\u773c\u52a8\u8ffd\u8e2a\u3001\u9f20\u6807\u4ea4\u4e92\u8f68\u8ff9\u3001\u89c6\u53e3\u5bfc\u822a\u3001\u523a\u6fc0\u8ddf\u8e2a\u548c\u8bca\u65ad\u51b3\u7b56\u6570\u636e\uff08EMSVD\uff09\uff0c\u5e76\u8bb0\u5f55\u4e86171,909\u6b21\u56fa\u5b9a\u70b9\u3001263,320\u6b21\u8df3\u52a8\u548c1,867,362\u6b21\u9f20\u6807\u4ea4\u4e92\u4e8b\u4ef6\uff0c\u751f\u6210\u4e86PathoGaze1.0\u884c\u4e3a\u6570\u636e\u96c6\u3002\u6240\u6709\u5b9e\u9a8c\u5747\u5df2\u5728https://osf.io/hj9a7\u9884\u6ce8\u518c\uff0c\u5e76\u4e14\u5b8c\u6574\u7684\u6570\u636e\u96c6\u548c\u5206\u6790\u4ee3\u7801\u53ef\u5728https://go.osu.edu/pathogaze\u83b7\u53d6\u3002", "result": "\u6536\u96c6\u4e8619\u4f4d\u75c5\u7406\u5b66\u5bb6\u5bf9397\u5f20\u5168\u89c6\u56fe\u56fe\u50cf\u7684\u89e3\u91ca\u8fc7\u7a0b\u4e2d\u7684\u5927\u91cf\u884c\u4e3a\u6570\u636e\uff0c\u5305\u62ec\u773c\u52a8\u8ffd\u8e2a\u3001\u9f20\u6807\u4ea4\u4e92\u8f68\u8ff9\u3001\u89c6\u53e3\u5bfc\u822a\u3001\u523a\u6fc0\u8ddf\u8e2a\u548c\u8bca\u65ad\u51b3\u7b56\u6570\u636e\uff08EMSVD\uff09\uff0c\u4e3a\u89e3\u91ca\u75c5\u7406\u5b66\u4e2d\u7684\u8bca\u65ad\u9519\u8bef\u548c\u4e0d\u4e00\u81f4\u6027\u63d0\u4f9b\u4e86\u884c\u4e3a\u6570\u636e\uff0c\u540c\u65f6\u4e5f\u53ef\u63d0\u5347\u75c5\u7406\u5b66\u5bb6\u7684\u57f9\u8bad\u6548\u679c\u4ee5\u53ca\u652f\u6301\u75c5\u7406\u5b66\u5bb6\u7684\u4eba\u5de5\u667a\u80fd\u7cfb\u7edf\u7684\u57f9\u8bad\u6548\u679c\u3002", "conclusion": "PathoGaze1.0\u6570\u636e\u96c6\u662f\u7b2c\u4e00\u4e2a\u5168\u9762\u7684\u884c\u4e3a\u6570\u636e\u96c6\uff0c\u6db5\u76d6\u4e86\u764c\u75c7\u8bca\u65ad\u5168\u5de5\u4f5c\u6d41\u7a0b\u7684\u89c6\u89c9\u641c\u7d22\u548c\u51b3\u7b56\u52a8\u6001\u3002\u8be5\u6570\u636e\u96c6\u8bb0\u5f55\u4e8619\u4f4d\u75c5\u7406\u5b66\u5bb6\u89e3\u91ca397\u5f20\u5168\u89c6\u56fe\u56fe\u50cf\u65f6\u7684\u5404\u79cd\u884c\u4e3a\uff0c\u5305\u62ec\u773c\u52a8\u3001\u9f20\u6807\u4ea4\u4e92\u548c\u51b3\u7b56\u8fc7\u7a0b\u3002\u8fd9\u4e9b\u6570\u636e\u4e0d\u4ec5\u6709\u52a9\u4e8e\u63ed\u793a\u8bca\u65ad\u8fc7\u7a0b\u4e2d\u7684\u884c\u4e3a\u7279\u5f81\uff0c\u8fd8\u80fd\u4e3a\u8bad\u7ec3\u6280\u672f\u548c\u652f\u6301\u75c5\u7406\u5b66\u5bb6\u7684\u4eba\u5de5\u667a\u80fd\u7cfb\u7edf\u63d0\u4f9b\u8d44\u6e90\u3002"}}
{"id": "2510.23994", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.23994", "abs": "https://arxiv.org/abs/2510.23994", "authors": ["Geoffery Agorku", "Sarah Hernandez", "Hayley Hames", "Cade Wagner"], "title": "Predicting Barge Tow Size on Inland Waterways Using Vessel Trajectory Derived Features: Proof of Concept", "comment": null, "summary": "Accurate, real-time estimation of barge quantity on inland waterways remains\na critical challenge due to the non-self-propelled nature of barges and the\nlimitations of existing monitoring systems. This study introduces a novel\nmethod to use Automatic Identification System (AIS) vessel tracking data to\npredict the number of barges in tow using Machine Learning (ML). To train and\ntest the model, barge instances were manually annotated from satellite scenes\nacross the Lower Mississippi River. Labeled images were matched to AIS vessel\ntracks using a spatiotemporal matching procedure. A comprehensive set of 30\nAIS-derived features capturing vessel geometry, dynamic movement, and\ntrajectory patterns were created and evaluated using Recursive Feature\nElimination (RFE) to identify the most predictive variables. Six regression\nmodels, including ensemble, kernel-based, and generalized linear approaches,\nwere trained and evaluated. The Poisson Regressor model yielded the best\nperformance, achieving a Mean Absolute Error (MAE) of 1.92 barges using 12 of\nthe 30 features. The feature importance analysis revealed that metrics\ncapturing vessel maneuverability such as course entropy, speed variability and\ntrip length were most predictive of barge count. The proposed approach provides\na scalable, readily implementable method for enhancing Maritime Domain\nAwareness (MDA), with strong potential applications in lock scheduling, port\nmanagement, and freight planning. Future work will expand the proof of concept\npresented here to explore model transferability to other inland rivers with\ndiffering operational and environmental conditions.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u4f7f\u7528AIS\u6570\u636e\u548c\u673a\u5668\u5b66\u4e60\u6765\u9884\u6d4b\u5185\u9646\u6c34\u9053\u4e2d\u62d6\u66f3\u7684\u9a73\u8239\u6570\u91cf\u7684\u65b0\u65b9\u6cd5\u3002\u8be5\u65b9\u6cd5\u5305\u62ec\u4ece\u536b\u661f\u56fe\u50cf\u4e2d\u624b\u52a8\u6ce8\u91ca\u9a73\u8239\u5b9e\u4f8b\uff0c\u4f7f\u7528\u65f6\u7a7a\u5339\u914d\u8fc7\u7a0b\u5c06\u5176\u4e0eAIS\u8239\u8236\u8f68\u8ff9\u5339\u914d\uff0c\u5e76\u751f\u621030\u4e2aAIS\u7279\u5f81\u6765\u8bad\u7ec3\u548c\u6d4b\u8bd5\u6a21\u578b\u3002\u57286\u79cd\u56de\u5f52\u6a21\u578b\u4e2d\uff0c\u6cca\u677e\u56de\u5f52\u6a21\u578b\u8868\u73b0\u51fa\u6700\u4f73\u6027\u80fd\uff0c\u5e73\u5747\u7edd\u5bf9\u8bef\u5dee\u4e3a1.92\u3002\u7ed3\u679c\u8868\u660e\uff0c\u4e0e\u9a73\u8239\u6570\u91cf\u6700\u76f8\u5173\u7684\u7279\u5f81\u5305\u62ec\u8239\u8236\u64cd\u7eb5\u6027\u6307\u6807\uff0c\u5982\u822a\u5411\u71b5\u3001\u901f\u5ea6\u53d8\u5316\u548c\u884c\u7a0b\u957f\u5ea6\u3002", "motivation": "\u7cbe\u786e\u3001\u5b9e\u65f6\u4f30\u8ba1\u5185\u9646\u6c34\u9053\u4e2d\u62d6\u66f3\u7684\u9a73\u8239\u6570\u91cf\u662f\u7531\u4e8e\u9a73\u8239\u7684\u975e\u81ea\u63a8\u6027\u8d28\u4ee5\u53ca\u73b0\u6709\u76d1\u63a7\u7cfb\u7edf\u7684\u5c40\u9650\u6027\u6240\u5e26\u6765\u7684\u5173\u952e\u6311\u6218\u3002\u73b0\u6709\u7684\u7cfb\u7edf\u96be\u4ee5\u63d0\u4f9b\u8db3\u591f\u7684\u51c6\u786e\u6027\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u6765\u6539\u5584\u8fd9\u4e00\u72b6\u51b5\u3002\u672c\u7814\u7a76\u5e94\u8fd0\u800c\u751f\uff0c\u65e8\u5728\u5f00\u53d1\u4e00\u79cd\u57fa\u4e8eAIS\u6570\u636e\u548c\u673a\u5668\u5b66\u4e60\u7684\u65b9\u6cd5\uff0c\u4ee5\u89e3\u51b3\u8fd9\u4e00\u5b9e\u9645\u95ee\u9898\u3002", "method": "\u672c\u7814\u7a76\u901a\u8fc7\u4ece\u536b\u661f\u56fe\u50cf\u4e2d\u624b\u52a8\u6ce8\u91ca\u9a73\u8239\u5b9e\u4f8b\uff0c\u5e76\u5229\u7528\u65f6\u7a7a\u5339\u914d\u7a0b\u5e8f\u5c06\u5176\u4e0eAIS\u8239\u8236\u8f68\u8ff9\u5339\u914d\uff0c\u751f\u6210\u4e8630\u4e2a\u4e30\u5bcc\u7684AIS\u7279\u5f81\u3002\u7136\u540e\uff0c\u901a\u8fc7\u9012\u5f52\u7279\u5f81\u6d88\u9664\u8fc7\u7a0b\u9009\u51fa\u5173\u952e\u7279\u5f81\uff0c\u6700\u540e\u8bad\u7ec3\u5e76\u8bc4\u4f30\u4e866\u79cd\u56de\u5f52\u6a21\u578b\uff0c\u4ee5\u786e\u5b9a\u6700\u4f73\u9884\u6d4b\u65b9\u6cd5\u3002", "result": "\u5728\u6240\u6d4b\u8bd5\u7684\u516d\u79cd\u56de\u5f52\u6a21\u578b\u4e2d\uff0c\u6cca\u677e\u56de\u5f52\u6a21\u578b\u8868\u73b0\u6700\u4f73\uff0c\u5e73\u5747\u7edd\u5bf9\u8bef\u5dee\u4e3a1.92 barges\uff0c\u8bc1\u660e\u4e86AIS\u6570\u636e\u548c\u673a\u5668\u5b66\u4e60\u9884\u6d4b\u9a73\u8239\u6570\u91cf\u7684\u6f5c\u529b\u3002\u91cd\u8981\u7279\u5f81\u5305\u62ec\u8239\u8236\u7684\u64cd\u7eb5\u6027\u6307\u6807\uff08\u5982\u822a\u5411\u71b5\u3001\u901f\u5ea6\u53d8\u5316\u548c\u884c\u7a0b\u957f\u5ea6\uff09\u3002", "conclusion": "\u672c\u7814\u7a76\u8868\u660e\uff0c\u57fa\u4e8eAIS\u6570\u636e\u8bad\u7ec3\u7684\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u5728\u9884\u6d4b\u5185\u9646\u6c34\u9053\u4e2d\u62d6\u66f3\u7684\u9a73\u8239\u6570\u91cf\u65b9\u9762\u5177\u6709\u5f88\u597d\u7684\u7cbe\u786e\u5ea6\u4e0e\u9002\u7528\u6027\u3002\u672a\u6765\u7684\u7814\u7a76\u5c06\u63a2\u7d22\u8be5\u65b9\u6cd5\u5728\u5176\u4ed6\u6cb3\u6d41\u4e0d\u540c\u64cd\u4f5c\u4e0e\u73af\u5883\u6761\u4ef6\u4e0b\u7684\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2510.24657", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.24657", "abs": "https://arxiv.org/abs/2510.24657", "authors": ["Xuanpu Zhang", "Xuesong Niu", "Ruidong Chen", "Dan Song", "Jianhao Zeng", "Penghui Du", "Haoxiang Cao", "Kai Wu", "An-an Liu"], "title": "Group Relative Attention Guidance for Image Editing", "comment": null, "summary": "Recently, image editing based on Diffusion-in-Transformer models has\nundergone rapid development. However, existing editing methods often lack\neffective control over the degree of editing, limiting their ability to achieve\nmore customized results. To address this limitation, we investigate the\nMM-Attention mechanism within the DiT model and observe that the Query and Key\ntokens share a bias vector that is only layer-dependent. We interpret this bias\nas representing the model's inherent editing behavior, while the delta between\neach token and its corresponding bias encodes the content-specific editing\nsignals. Based on this insight, we propose Group Relative Attention Guidance, a\nsimple yet effective method that reweights the delta values of different tokens\nto modulate the focus of the model on the input image relative to the editing\ninstruction, enabling continuous and fine-grained control over editing\nintensity without any tuning. Extensive experiments conducted on existing image\nediting frameworks demonstrate that GRAG can be integrated with as few as four\nlines of code, consistently enhancing editing quality. Moreover, compared to\nthe commonly used Classifier-Free Guidance, GRAG achieves smoother and more\nprecise control over the degree of editing. Our code will be released at\nhttps://github.com/little-misfit/GRAG-Image-Editing.", "AI": {"tldr": "\u63d0\u51fa\u4e86Group Relative Attention Guidance (GRAG)\uff0c\u4e00\u79cd\u7b80\u5355\u800c\u6709\u6548\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u5bf9\u4e0d\u540c\u4ee4\u724c\u7684delta\u503c\u8fdb\u884c\u91cd\u65b0\u52a0\u6743\uff0c\u6765\u8c03\u6574\u6a21\u578b\u5bf9\u8f93\u5165\u56fe\u50cf\u4e0e\u7f16\u8f91\u6307\u4ee4\u7684\u5173\u6ce8\u5ea6\uff0c\u5b9e\u73b0\u8fde\u7eed\u800c\u7cbe\u7ec6\u7684\u7f16\u8f91\u5f3a\u5ea6\u63a7\u5236\u3002\u5b9e\u9a8c\u8868\u660e\uff0cGRAG\u53ef\u4ee5\u8f7b\u677e\u96c6\u6210\u5230\u73b0\u6709\u56fe\u50cf\u7f16\u8f91\u6846\u67b6\u4e2d\uff0c\u5e76\u4e14\u76f8\u6bd4\u73b0\u6709\u65b9\u6cd5\uff0cGRAG\u80fd\u5b9e\u73b0\u66f4\u5e73\u6ed1\u548c\u7cbe\u786e\u7684\u7f16\u8f91\u63a7\u5236\u3002", "motivation": "\u73b0\u6709\u56fe\u50cf\u7f16\u8f91\u65b9\u6cd5\u901a\u5e38\u7f3a\u4e4f\u5bf9\u7f16\u8f91\u5f3a\u5ea6\u7684\u6709\u6548\u63a7\u5236\uff0c\u5f71\u54cd\u4e86\u5b9a\u5236\u5316\u7ed3\u679c\u7684\u5b9e\u73b0\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u6539\u8fdb\u6ce8\u610f\u529b\u673a\u5236\u6765\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u4ece\u800c\u5b9e\u73b0\u66f4\u52a0\u7075\u6d3b\u548c\u7ec6\u81f4\u7684\u56fe\u50cf\u7f16\u8f91\u63a7\u5236\u3002", "method": "\u63d0\u51fa\u4e86\u57fa\u4e8eMM-Attention\u673a\u5236\u7684Group Relative Attention Guidance (GRAG) \u65b9\u6cd5\uff0c\u901a\u8fc7\u5bf9Query\u548cKey\u4ee4\u724c\u4e4b\u95f4\u7684delta\u503c\u91cd\u65b0\u52a0\u6743\uff0c\u6765\u8c03\u6574\u6a21\u578b\u5bf9\u8f93\u5165\u56fe\u50cf\u7684\u5173\u6ce8\u5ea6\uff0c\u5b9e\u73b0\u8fde\u7eed\u4e14\u7cbe\u7ec6\u7684\u7f16\u8f91\u5f3a\u5ea6\u63a7\u5236\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cGRAG\u4e0d\u4ec5\u53ef\u4ee5\u7b80\u5355\u5730\u96c6\u6210\u5230\u73b0\u6709\u56fe\u50cf\u7f16\u8f91\u6846\u67b6\u4e2d\uff0c\u800c\u4e14\u53ef\u4ee5\u5b9e\u73b0\u66f4\u5e73\u6ed1\u548c\u7cbe\u786e\u7684\u7f16\u8f91\u63a7\u5236\uff0c\u76f8\u6bd4\u5e38\u89c1\u7684Classifier-Free Guidance\u65b9\u6cd5\uff0cGRAG\u8868\u73b0\u66f4\u4e3a\u4f18\u8d8a\u3002", "conclusion": "\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u7b80\u5355\u6709\u6548\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u5bf9\u4ee4\u724c\u7684delta\u503c\u91cd\u65b0\u52a0\u6743\uff0c\u4ee5\u5b9e\u73b0\u56fe\u50cf\u7f16\u8f91\u7684\u8fde\u7eed\u4e14\u7cbe\u7ec6\u63a7\u5236\uff0c\u4e14\u4e0e\u73b0\u6709\u65b9\u6cd5\u76f8\u6bd4\uff0cGRAG\u80fd\u591f\u8f7b\u677e\u96c6\u6210\u5230\u73b0\u6709\u56fe\u50cf\u7f16\u8f91\u6846\u67b6\u4e2d\uff0c\u663e\u8457\u63d0\u9ad8\u7f16\u8f91\u8d28\u91cf\u3002"}}
{"id": "2510.24012", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24012", "abs": "https://arxiv.org/abs/2510.24012", "authors": ["Byeonghu Na", "Mina Kang", "Jiseok Kwak", "Minsang Park", "Jiwoo Shin", "SeJoon Jun", "Gayoung Lee", "Jin-Hwa Kim", "Il-Chul Moon"], "title": "Training-Free Safe Text Embedding Guidance for Text-to-Image Diffusion Models", "comment": "Accepted at NeurIPS 2025", "summary": "Text-to-image models have recently made significant advances in generating\nrealistic and semantically coherent images, driven by advanced diffusion models\nand large-scale web-crawled datasets. However, these datasets often contain\ninappropriate or biased content, raising concerns about the generation of\nharmful outputs when provided with malicious text prompts. We propose Safe Text\nembedding Guidance (STG), a training-free approach to improve the safety of\ndiffusion models by guiding the text embeddings during sampling. STG adjusts\nthe text embeddings based on a safety function evaluated on the expected final\ndenoised image, allowing the model to generate safer outputs without additional\ntraining. Theoretically, we show that STG aligns the underlying model\ndistribution with safety constraints, thereby achieving safer outputs while\nminimally affecting generation quality. Experiments on various safety\nscenarios, including nudity, violence, and artist-style removal, show that STG\nconsistently outperforms both training-based and training-free baselines in\nremoving unsafe content while preserving the core semantic intent of input\nprompts. Our code is available at https://github.com/aailab-kaist/STG.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86STG\u65b9\u6cd5\uff0c\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u5373\u53ef\u63d0\u9ad8\u6269\u6563\u6a21\u578b\u7684\u5b89\u5168\u6027\uff0c\u901a\u8fc7\u6307\u5bfc\u6587\u672c\u5d4c\u5165\u6765\u751f\u6210\u66f4\u5b89\u5168\u7684\u8f93\u51fa\u3002\u5b9e\u9a8c\u8868\u660eSTG\u5728\u79fb\u9664\u4e0d\u5b89\u5168\u5185\u5bb9\u7684\u540c\u65f6\u80fd\u591f\u4fdd\u6301\u8f93\u5165\u6307\u4ee4\u7684\u6838\u5fc3\u8bed\u4e49\u610f\u56fe\uff0c\u4f18\u4e8e\u8bad\u7ec3\u57fa\u7ebf\u65b9\u6cd5\u3002", "motivation": "\u5f53\u524d\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u6a21\u578b\u5b58\u5728\u7684\u95ee\u9898\u662f\u751f\u6210\u4e0d\u9002\u5b9c\u6216\u6709\u504f\u89c1\u7684\u5185\u5bb9\uff0c\u672c\u6587\u7684\u52a8\u673a\u662f\u5982\u4f55\u5728\u4e0d\u8fdb\u884c\u989d\u5916\u8bad\u7ec3\u7684\u60c5\u51b5\u4e0b\u63d0\u9ad8\u8fd9\u4e9b\u6a21\u578b\u7684\u5b89\u5168\u6027\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u5b89\u5168\u6587\u672c\u5d4c\u5165\u6307\u5bfc\uff08STG\uff09\u65b9\u6cd5\uff0c\u901a\u8fc7\u57fa\u4e8e\u9884\u671f\u7684\u6700\u7ec8\u53bb\u566a\u56fe\u50cf\u8bc4\u4f30\u7684\u5b89\u5168\u51fd\u6570\u8c03\u6574\u6587\u672c\u5d4c\u5165\uff0c\u4ee5\u751f\u6210\u66f4\u5b89\u5168\u7684\u8f93\u51fa\u3002\u8fd9\u79cd\u65b9\u6cd5\u4e0d\u9700\u8981\u5bf9\u6a21\u578b\u8fdb\u884c\u989d\u5916\u7684\u8bad\u7ec3\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cSTG\u5728\u53bb\u9664\u4e0d\u9002\u5b9c\u5185\u5bb9\uff08\u5982\u88f8\u4f53\u3001\u66b4\u529b\u7b49\uff09\u65b9\u9762\u8868\u73b0\u4f18\u4e8e\u65e2\u6709\u8bad\u7ec3\u548c\u975e\u8bad\u7ec3\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "STG\u901a\u8fc7\u8c03\u6574\u6587\u672c\u5d4c\u5165\u63d0\u9ad8\u4e86\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u6a21\u578b\u7684\u5b89\u5168\u6027\uff0c\u65e2\u4e0d\u9700\u8981\u989d\u5916\u8bad\u7ec3\u53c8\u80fd\u5728\u4fdd\u6301\u6838\u5fc3\u8bed\u4e49\u610f\u56fe\u7684\u540c\u65f6\u79fb\u9664\u4e0d\u5b89\u5168\u5185\u5bb9\u3002"}}
{"id": "2510.24667", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.24667", "abs": "https://arxiv.org/abs/2510.24667", "authors": ["Mia Kan", "Yilin Liu", "Niloy Mitra"], "title": "SAGE: Structure-Aware Generative Video Transitions between Diverse Clips", "comment": "Website: https://kan32501.github.io/sage.github.io/", "summary": "Video transitions aim to synthesize intermediate frames between two clips,\nbut naive approaches such as linear blending introduce artifacts that limit\nprofessional use or break temporal coherence. Traditional techniques\n(cross-fades, morphing, frame interpolation) and recent generative inbetweening\nmethods can produce high-quality plausible intermediates, but they struggle\nwith bridging diverse clips involving large temporal gaps or significant\nsemantic differences, leaving a gap for content-aware and visually coherent\ntransitions. We address this challenge by drawing on artistic workflows,\ndistilling strategies such as aligning silhouettes and interpolating salient\nfeatures to preserve structure and perceptual continuity. Building on this, we\npropose SAGE (Structure-Aware Generative vidEo transitions) as a zeroshot\napproach that combines structural guidance, provided via line maps and motion\nflow, with generative synthesis, enabling smooth, semantically consistent\ntransitions without fine-tuning. Extensive experiments and comparison with\ncurrent alternatives, namely [FILM, TVG, DiffMorpher, VACE, GI], demonstrate\nthat SAGE outperforms both classical and generative baselines on quantitative\nmetrics and user studies for producing transitions between diverse clips. Code\nto be released on acceptance.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86SAGE\uff08\u57fa\u4e8e\u7ed3\u6784\u7684\u751f\u6210\u89c6\u9891\u8fc7\u6e21\uff09\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u901a\u8fc7\u7ed3\u5408\u7ed3\u6784\u6307\u5bfc\u548c\u751f\u6210\u5408\u6210\uff0c\u5b9e\u73b0\u4e86\u4e0d\u540c\u89c6\u9891\u7247\u6bb5\u4e4b\u95f4\u5e73\u6ed1\u3001\u8bed\u4e49\u4e00\u81f4\u7684\u8fc7\u6e21\uff0c\u65e0\u9700\u5fae\u8c03\u3002SAGE\u5728\u5b9a\u91cf\u6307\u6807\u548c\u7528\u6237\u7814\u7a76\u4e2d\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u53ef\u5b9e\u73b0\u5728\u4e0d\u540c\u89c6\u9891\u7247\u6bb5\u95f4\u7684\u5e73\u6ed1\u8fc7\u6e21\u3002", "motivation": "\u73b0\u6709\u7684\u89c6\u9891\u8fc7\u6e21\u6280\u672f\u5728\u5904\u7406\u5305\u542b\u8f83\u5927\u65f6\u95f4\u95f4\u9694\u6216\u663e\u8457\u8bed\u4e49\u5dee\u5f02\u7684\u89c6\u9891\u7247\u6bb5\u65f6\u6548\u679c\u4e0d\u4f73\uff0c\u5bfc\u81f4\u8fc7\u6e21\u6548\u679c\u4e0d\u81ea\u7136\u3002\u4e3a\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u672c\u6587\u63d0\u51fa\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u65e8\u5728\u4ea7\u751f\u89c6\u89c9\u4e0a\u8fde\u8d2f\u7684\u8fc7\u6e21\u6548\u679c\u3002\u901a\u8fc7\u501f\u9274\u827a\u672f\u5de5\u4f5c\u6d41\u7a0b\u4e2d\u7684\u7b56\u7565\uff0c\u5982\u5bf9\u9f50\u8f6e\u5ed3\u548c\u4e92\u8865\u663e\u8457\u7279\u5f81\u6765\u4fdd\u6301\u7ed3\u6784\u548c\u77e5\u89c9\u8fde\u7eed\u6027\uff0c\u521b\u9020\u4e86SAGE\u65b9\u6cd5\u3002", "method": "SAGE\u65b9\u6cd5\u901a\u8fc7\u7ed3\u5408\u7ed3\u6784\u6307\u5bfc\uff08\u63d0\u4f9b\u7ebf\u5730\u56fe\u548c\u8fd0\u52a8\u6d41\uff09\u548c\u751f\u6210\u5408\u6210\uff0c\u5b9e\u73b0\u4e86\u4e0d\u540c\u89c6\u9891\u7247\u6bb5\u4e4b\u95f4\u5e73\u6ed1\u3001\u8bed\u4e49\u4e00\u81f4\u7684\u8fc7\u6e21\u3002\u8be5\u65b9\u6cd5\u65e0\u9700\u8fdb\u884c\u5fae\u8c03\uff0c\u76f4\u63a5\u901a\u8fc7\u96f6\u6837\u672c\u5b66\u4e60\u5b9e\u73b0\u4e86\u76ee\u6807\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cSAGE\u65b9\u6cd5\u5728\u5b9a\u91cf\u6307\u6807\u548c\u7528\u6237\u7814\u7a76\u4e2d\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u80fd\u591f\u66f4\u81ea\u7136\u3001\u66f4\u5e73\u6ed1\u5730\u8fde\u63a5\u4e0d\u540c\u89c6\u9891\u7247\u6bb5\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u89c6\u9891\u8fc7\u6e21\u6280\u672fSAGE\uff0c\u901a\u8fc7\u7ed3\u5408\u7ed3\u6784\u6307\u5bfc\u548c\u751f\u6210\u5408\u6210\uff0c\u4f7f\u5f97\u4e0d\u540c\u89c6\u9891\u7247\u6bb5\u95f4\u7684\u8fc7\u6e21\u66f4\u4e3a\u5e73\u6ed1\u3001\u81ea\u7136\uff0c\u4ece\u800c\u586b\u8865\u4e86\u73b0\u6709\u6280\u672f\u7684\u7a7a\u767d\u3002"}}
{"id": "2510.24688", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.24688", "abs": "https://arxiv.org/abs/2510.24688", "authors": ["Yun Zhang", "Zhaoliang Zheng", "Johnson Liu", "Zhiyu Huang", "Zewei Zhou", "Zonglin Meng", "Tianhui Cai", "Jiaqi Ma"], "title": "MIC-BEV: Multi-Infrastructure Camera Bird's-Eye-View Transformer with Relation-Aware Fusion for 3D Object Detection", "comment": null, "summary": "Infrastructure-based perception plays a crucial role in intelligent\ntransportation systems, offering global situational awareness and enabling\ncooperative autonomy. However, existing camera-based detection models often\nunderperform in such scenarios due to challenges such as multi-view\ninfrastructure setup, diverse camera configurations, degraded visual inputs,\nand various road layouts. We introduce MIC-BEV, a Transformer-based\nbird's-eye-view (BEV) perception framework for infrastructure-based\nmulti-camera 3D object detection. MIC-BEV flexibly supports a variable number\nof cameras with heterogeneous intrinsic and extrinsic parameters and\ndemonstrates strong robustness under sensor degradation. The proposed\ngraph-enhanced fusion module in MIC-BEV integrates multi-view image features\ninto the BEV space by exploiting geometric relationships between cameras and\nBEV cells alongside latent visual cues. To support training and evaluation, we\nintroduce M2I, a synthetic dataset for infrastructure-based object detection,\nfeaturing diverse camera configurations, road layouts, and environmental\nconditions. Extensive experiments on both M2I and the real-world dataset\nRoScenes demonstrate that MIC-BEV achieves state-of-the-art performance in 3D\nobject detection. It also remains robust under challenging conditions,\nincluding extreme weather and sensor degradation. These results highlight the\npotential of MIC-BEV for real-world deployment. The dataset and source code are\navailable at: https://github.com/HandsomeYun/MIC-BEV.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2510.24026", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.24026", "abs": "https://arxiv.org/abs/2510.24026", "authors": ["Jiaqi Luo", "Shixin Xu", "Zhouwang Yang"], "title": "Efficient Global-Local Fusion Sampling for Physics-Informed Neural Networks", "comment": null, "summary": "The accuracy of Physics-Informed Neural Networks (PINNs) critically depends\non the placement of collocation points, as the PDE loss is approximated through\nsampling over the solution domain. Global sampling ensures stability by\ncovering the entire domain but requires many samples and is computationally\nexpensive, whereas local sampling improves efficiency by focusing on\nhigh-residual regions but may neglect well-learned areas, reducing robustness.\nWe propose a Global-Local Fusion (GLF) Sampling Strategy that combines the\nstrengths of both approaches. Specifically, new collocation points are\ngenerated by perturbing training points with Gaussian noise scaled inversely to\nthe residual, thereby concentrating samples in difficult regions while\npreserving exploration. To further reduce computational overhead, a lightweight\nlinear surrogate is introduced to approximate the global residual-based\ndistribution, achieving similar effectiveness at a fraction of the cost.\nTogether, these components, residual-adaptive sampling and residual-based\napproximation, preserve the stability of global methods while retaining the\nefficiency of local refinement. Extensive experiments on benchmark PDEs\ndemonstrate that GLF consistently improves both accuracy and efficiency\ncompared with global and local sampling strategies. This study provides a\npractical and scalable framework for enhancing the reliability and efficiency\nof PINNs in solving complex and high-dimensional PDEs.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2510.24709", "categories": ["cs.CV", "cs.AI", "cs.LG", "q-bio.NC"], "pdf": "https://arxiv.org/pdf/2510.24709", "abs": "https://arxiv.org/abs/2510.24709", "authors": ["Yihao Li", "Saeed Salehi", "Lyle Ungar", "Konrad P. Kording"], "title": "Does Object Binding Naturally Emerge in Large Pretrained Vision Transformers?", "comment": "Accepted as a Spotlight at NeurIPS 2025", "summary": "Object binding, the brain's ability to bind the many features that\ncollectively represent an object into a coherent whole, is central to human\ncognition. It groups low-level perceptual features into high-level object\nrepresentations, stores those objects efficiently and compositionally in\nmemory, and supports human reasoning about individual object instances. While\nprior work often imposes object-centric attention (e.g., Slot Attention)\nexplicitly to probe these benefits, it remains unclear whether this ability\nnaturally emerges in pre-trained Vision Transformers (ViTs). Intuitively, they\ncould: recognizing which patches belong to the same object should be useful for\ndownstream prediction and thus guide attention. Motivated by the quadratic\nnature of self-attention, we hypothesize that ViTs represent whether two\npatches belong to the same object, a property we term IsSameObject. We decode\nIsSameObject from patch embeddings across ViT layers using a similarity probe,\nwhich reaches over 90% accuracy. Crucially, this object-binding capability\nemerges reliably in self-supervised ViTs (DINO, MAE, CLIP), but markedly weaker\nin ImageNet-supervised models, suggesting that binding is not a trivial\narchitectural artifact, but an ability acquired through specific pretraining\nobjectives. We further discover that IsSameObject is encoded in a\nlow-dimensional subspace on top of object features, and that this signal\nactively guides attention. Ablating IsSameObject from model activations\ndegrades downstream performance and works against the learning objective,\nimplying that emergent object binding naturally serves the pretraining\nobjective. Our findings challenge the view that ViTs lack object binding and\nhighlight how symbolic knowledge of \"which parts belong together\" emerges\nnaturally in a connectionist system.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2510.24027", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24027", "abs": "https://arxiv.org/abs/2510.24027", "authors": ["Zibo Liu", "Zhe Jiang", "Zelin Xu", "Tingsong Xiao", "Yupu Zhang", "Zhengkun Xiao", "Haibo Wang", "Shigang Chen"], "title": "Spatio-temporal Multivariate Time Series Forecast with Chosen Variables", "comment": "In submission", "summary": "Spatio-Temporal Multivariate time series Forecast (STMF) uses the time series\nof $n$ spatially distributed variables in a period of recent past to forecast\ntheir values in a period of near future. It has important applications in\nspatio-temporal sensing forecast such as road traffic prediction and air\npollution prediction. Recent papers have addressed a practical problem of\nmissing variables in the model input, which arises in the sensing applications\nwhere the number $m$ of sensors is far less than the number $n$ of locations to\nbe monitored, due to budget constraints. We observe that the state of the art\nassumes that the $m$ variables (i.e., locations with sensors) in the model\ninput are pre-determined and the important problem of how to choose the $m$\nvariables in the input has never been studied. This paper fills the gap by\nstudying a new problem of STMF with chosen variables, which optimally selects\n$m$-out-of-$n$ variables for the model input in order to maximize the forecast\naccuracy. We propose a unified framework that jointly performs variable\nselection and model optimization for both forecast accuracy and model\nefficiency. It consists of three novel technical components: (1) masked\nvariable-parameter pruning, which progressively prunes less informative\nvariables and attention parameters through quantile-based masking; (2)\nprioritized variable-parameter replay, which replays low-loss past samples to\npreserve learned knowledge for model stability; (3) dynamic extrapolation\nmechanism, which propagates information from variables selected for the input\nto all other variables via learnable spatial embeddings and adjacency\ninformation. Experiments on five real-world datasets show that our work\nsignificantly outperforms the state-of-the-art baselines in both accuracy and\nefficiency, demonstrating the effectiveness of joint variable selection and\nmodel optimization.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4f18\u5316\u65f6\u7a7a\u591a\u53d8\u91cf\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\uff08STMF\uff09\u6a21\u578b\u7684\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u80fd\u591f\u9009\u62e9\u8f93\u5165\u4e2d\u7684m\u4e2a\u53d8\u91cf\u4ee5\u6700\u5927\u5316\u9884\u6d4b\u51c6\u786e\u6027\uff0c\u5e76\u901a\u8fc7\u91cf\u5316\u5c4f\u853d\u9010\u6b65\u526a\u679d\u975e\u76f8\u5173\u4fe1\u606f\u3001\u4f18\u5148\u7ea7\u91cd\u64ad\u548c\u52a8\u6001\u5916\u63a8\u673a\u5236\u6765\u5b9e\u73b0\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8fd9\u79cd\u65b9\u6cd5\u5728\u4e94\u4e2a\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u6bd4\u6700\u65b0\u7684\u57fa\u51c6\u65b9\u6cd5\u6709\u66f4\u4f18\u7684\u51c6\u786e\u6027\u548c\u6548\u7387\u3002", "motivation": "\u5f53\u524d\u7684STMF\u6a21\u578b\u9700\u8981\u9884\u5148\u786e\u5b9a\u8f93\u5165\u53d8\u91cf\uff0c\u800c\u5982\u4f55\u9009\u62e9\u8fd9\u4e9b\u53d8\u91cf\u4ee5\u63d0\u9ad8\u9884\u6d4b\u51c6\u786e\u6027\u7684\u95ee\u9898\u5c1a\u672a\u88ab\u7814\u7a76\u3002\u672c\u6587\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u9009\u62e9\u6700\u4f18\u53d8\u91cf\u7684\u65b9\u6cd5\u4ee5\u6700\u5927\u5316\u9884\u6d4b\u51c6\u786e\u6027\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u8499\u9762\u53d8\u91cf\u53c2\u6570\u526a\u679d\u3001\u4f18\u5148\u7ea7\u53d8\u91cf\u53c2\u6570\u91cd\u64ad\u548c\u52a8\u6001\u5916\u63a8\u673a\u5236\u6765\u4f18\u5316\u53d8\u91cf\u9009\u62e9\u548c\u6a21\u578b\u6548\u80fd\uff0c\u5177\u4f53\u5305\u62ec\uff1a1.\u901a\u8fc7\u91cf\u5316\u5c4f\u853d\u9010\u6b65\u526a\u679d\u975e\u76f8\u5173\u4fe1\u606f\uff1b2.\u4f18\u5148\u7ea7\u91cd\u64ad\u4f4e\u635f\u5931\u7684\u8fc7\u53bb\u6837\u672c\u4ee5\u4fdd\u6301\u5b66\u4e60\u5230\u7684\u77e5\u8bc6\uff0c\u63d0\u9ad8\u6a21\u578b\u7a33\u5b9a\u6027\uff1b3.\u52a8\u6001\u5916\u63a8\u673a\u5236\u901a\u8fc7\u53ef\u5b66\u4e60\u7684\u7a7a\u95f4\u5d4c\u5165\u548c\u6280\u672f\u4fe1\u606f\u4f20\u64ad\u9009\u5b9a\u8f93\u5165\u53d8\u91cf\u7684\u4fe1\u606f\u5230\u5176\u4ed6\u53d8\u91cf\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u672c\u6587\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u4e94\u4e2a\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u663e\u8457\u4f18\u4e8e\u6700\u65b0\u7684\u57fa\u51c6\u65b9\u6cd5\uff0c\u663e\u793a\u4e86\u8054\u5408\u53d8\u91cf\u9009\u62e9\u548c\u6a21\u578b\u4f18\u5316\u7684\u6709\u6548\u6027\u3002\u5728\u51c6\u786e\u6027\u548c\u6548\u7387\u65b9\u9762\u90fd\u53d6\u5f97\u4e86\u66f4\u4f18\u7684\u7ed3\u679c\u3002", "conclusion": "\u672c\u6587\u586b\u8865\u4e86\u4e00\u9879\u7a7a\u767d\uff0c\u63d0\u51fa\u4e86\u4e00\u4e2a\u80fd\u591f\u4f18\u5316\u65f6\u7a7a\u591a\u53d8\u91cf\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u6a21\u578b\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u9009\u62e9\u8f93\u5165\u53d8\u91cf\u4ee5\u6700\u5927\u5316\u9884\u6d4b\u51c6\u786e\u6027\uff0c\u5b9e\u9a8c\u7ed3\u679c\u4f50\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2510.24711", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.24711", "abs": "https://arxiv.org/abs/2510.24711", "authors": ["Yujie Wei", "Shiwei Zhang", "Hangjie Yuan", "Yujin Han", "Zhekai Chen", "Jiayu Wang", "Difan Zou", "Xihui Liu", "Yingya Zhang", "Yu Liu", "Hongming Shan"], "title": "Routing Matters in MoE: Scaling Diffusion Transformers with Explicit Routing Guidance", "comment": null, "summary": "Mixture-of-Experts (MoE) has emerged as a powerful paradigm for scaling model\ncapacity while preserving computational efficiency. Despite its notable success\nin large language models (LLMs), existing attempts to apply MoE to Diffusion\nTransformers (DiTs) have yielded limited gains. We attribute this gap to\nfundamental differences between language and visual tokens. Language tokens are\nsemantically dense with pronounced inter-token variation, while visual tokens\nexhibit spatial redundancy and functional heterogeneity, hindering expert\nspecialization in vision MoE. To this end, we present ProMoE, an MoE framework\nfeaturing a two-step router with explicit routing guidance that promotes expert\nspecialization. Specifically, this guidance encourages the router to partition\nimage tokens into conditional and unconditional sets via conditional routing\naccording to their functional roles, and refine the assignments of conditional\nimage tokens through prototypical routing with learnable prototypes based on\nsemantic content. Moreover, the similarity-based expert allocation in latent\nspace enabled by prototypical routing offers a natural mechanism for\nincorporating explicit semantic guidance, and we validate that such guidance is\ncrucial for vision MoE. Building on this, we propose a routing contrastive loss\nthat explicitly enhances the prototypical routing process, promoting\nintra-expert coherence and inter-expert diversity. Extensive experiments on\nImageNet benchmark demonstrate that ProMoE surpasses state-of-the-art methods\nunder both Rectified Flow and DDPM training objectives. Code and models will be\nmade publicly available.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86ProMoE\uff0c\u8fd9\u662f\u4e00\u79cd\u65b0\u7684\u6df7\u5408\u4e13\u5bb6\u6846\u67b6\uff0c\u4e13\u95e8\u9488\u5bf9\u89c6\u89c9Transformer\u4e2d\u7684\u6df7\u5408\u4e13\u5bb6\u95ee\u9898\uff0c\u901a\u8fc7\u663e\u5f0f\u7684\u8def\u7531\u6307\u5bfc\u6765\u4fc3\u8fdb\u4e13\u5bb6\u7684\u4e13\u95e8\u5316\uff0c\u5e76\u4e14\u901a\u8fc7\u5f15\u5165\u8def\u7531\u5bf9\u6bd4\u635f\u5931\u6765\u589e\u5f3a\u8def\u7531\u8fc7\u7a0b\uff0c\u63d0\u9ad8\u4e86\u89c6\u89c9\u4efb\u52a1\u4e2d\u7684\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u5c3d\u7ba1MoE\u5728\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e2d\u53d6\u5f97\u4e86\u663e\u8457\u6210\u529f\uff0c\u4f46\u5728\u5e94\u7528\u4e8e\u89c6\u89c9Transformer\u65f6\u53d6\u5f97\u7684\u6539\u8fdb\u6709\u9650\u3002\u8fd9\u4e3b\u8981\u662f\u7531\u4e8e\u8bed\u8a00\u6807\u8bb0\u548c\u89c6\u89c9\u6807\u8bb0\u4e4b\u95f4\u7684\u56fa\u6709\u5dee\u5f02\u5bfc\u81f4\u7684\uff0c\u89c6\u89c9\u6807\u8bb0\u7684\u7a7a\u95f4\u5197\u4f59\u548c\u529f\u80fd\u5f02\u8d28\u6027\u963b\u788d\u4e86\u89c6\u89c9MoE\u4e2d\u7684\u4e13\u5bb6\u4e13\u95e8\u5316\u3002\u56e0\u800c\u672c\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u4e2a\u5dee\u8ddd\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51fa\u4e86\u5229\u7528\u6761\u4ef6\u8def\u7531\u548c\u539f\u578b\u8def\u7531\u6307\u5bfc\u7684\u4e24\u6b65\u8def\u7531\u5668\uff0c\u5c06\u56fe\u50cf\u6807\u8bb0\u4f9d\u636e\u529f\u80fd\u89d2\u8272\u5206\u4e3a\u6761\u4ef6\u96c6\u548c\u65e0\u6761\u4ef6\u96c6\uff0c\u5e76\u6839\u636e\u8bed\u4e49\u5185\u5bb9\u5b66\u4e60\u53ef\u5b66\u4e60\u7684\u539f\u578b\u6765\u8fdb\u4e00\u6b65\u4f18\u5316\u6761\u4ef6\u6807\u8bb0\u7684\u5206\u914d\uff0c\u540c\u65f6\u5f15\u5165\u4e86\u8def\u7531\u5bf9\u6bd4\u635f\u5931\u6765\u6539\u8fdb\u8fd9\u4e00\u8fc7\u7a0b\u3002\u8fd9\u79cd\u65b9\u6cd5\u5728\u6f5c\u5728\u7a7a\u95f4\u5b9e\u73b0\u4e86\u57fa\u4e8e\u76f8\u4f3c\u6027\u7684\u4e13\u5bb6\u5206\u914d\uff0c\u5e76\u5bf9\u539f\u578b\u8def\u7531\u8fc7\u7a0b\u8fdb\u884c\u663e\u5f0f\u589e\u5f3a\uff0c\u4fc3\u8fdb\u76f8\u4f3c\u6027\u4e13\u5bb6\u4e4b\u95f4\u7684\u5185\u90e8\u4e00\u81f4\u6027\u4ee5\u53ca\u4e0d\u540c\u7c7b\u578b\u4e13\u5bb6\u4e4b\u95f4\u7684\u5dee\u5f02\u6027\u3002", "result": "ProMoE\u5728ImageNet\u6570\u636e\u96c6\u4e0a\u5c55\u73b0\u4e86\u4f18\u8d8a\u7684\u6027\u80fd\uff0c\u8d85\u8d8a\u4e86\u73b0\u6709\u7684\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\uff0c\u7279\u522b\u662f\u5728Rectified Flow\u548cDDPM\u8bad\u7ec3\u76ee\u6807\u4e0b\u3002\u4ee3\u7801\u548c\u6a21\u578b\u5c06\u5728\u53d1\u5e03\u540e\u516c\u5f00\u63d0\u4f9b\u3002", "conclusion": "\u89c6\u89c9\u6807\u8bb0\u4e0e\u8bed\u8a00\u6807\u8bb0\u76f8\u6bd4\u5177\u6709\u72ec\u7279\u7279\u5f81\uff0c\u57fa\u4e8e\u5185\u5bb9\u7684\u7cbe\u7ec6\u5316\u8def\u7531\u7b56\u7565\u5bf9\u4e8e\u6709\u6548\u5730\u5229\u7528\u89c6\u89c9MoE\u7684\u6027\u80fd\u81f3\u5173\u91cd\u8981\u3002\u901a\u8fc7\u663e\u5f0f\u7684\u539f\u578b\u5f62\u8def\u7531\u548c\u8def\u7531\u5bf9\u6bd4\u635f\u5931\uff0cProMoE\u6781\u5927\u5730\u6539\u8fdb\u4e86\u89c6\u89c9\u4efb\u52a1\u4e2d\u7684\u6a21\u578b\u6027\u80fd\uff0c\u5e76\u4e3a\u89c6\u89c9\u4e13\u5bb6\u6df7\u5408\u7684\u7814\u7a76\u5f00\u8f9f\u4e86\u65b0\u7684\u9014\u5f84\u3002"}}
{"id": "2510.24035", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.24035", "abs": "https://arxiv.org/abs/2510.24035", "authors": ["Xinqi Li", "Yiqun Liu", "Shan Jiang", "Enrong Zheng", "Huaijin Zheng", "Wenhao Dai", "Haodong Deng", "Dianhai Yu", "Yanjun Ma"], "title": "GraphNet: A Large-Scale Computational Graph Dataset for Tensor Compiler Research", "comment": null, "summary": "We introduce GraphNet, a dataset of 2.7K real-world deep learning\ncomputational graphs with rich metadata, spanning six major task categories\nacross multiple deep learning frameworks. To evaluate tensor compiler\nperformance on these samples, we propose the benchmark metric Speedup Score\nS(t), which jointly considers runtime speedup and execution correctness under\ntunable tolerance levels, offering a reliable measure of general optimization\ncapability. Furthermore, we extend S(t) to the Error-aware Speedup Score ES(t),\nwhich incorporates error information and helps compiler developers identify key\nperformance bottlenecks. In this report, we benchmark the default tensor\ncompilers, CINN for PaddlePaddle and TorchInductor for PyTorch, on computer\nvision (CV) and natural language processing (NLP) samples to demonstrate the\npracticality of GraphNet. The full construction pipeline with graph extraction\nand compiler evaluation tools is available at\nhttps://github.com/PaddlePaddle/GraphNet .", "AI": {"tldr": "GraphNet \u662f\u4e00\u4e2a\u5305\u542b2.7K\u4e2a\u771f\u5b9e\u4e16\u754c\u6df1\u5ea6\u5b66\u4e60\u8ba1\u7b97\u56fe\u7684\u6570\u636e\u96c6\uff0c\u5305\u62ec\u4e30\u5bcc\u7684\u5143\u6570\u636e\u548c\u516d\u79cd\u4e3b\u8981\u4efb\u52a1\u7c7b\u522b\uff0c\u7528\u4e8e\u8bc4\u4f30\u5f20\u91cf\u7f16\u8bd1\u5668\u7684\u6027\u80fd\u3002\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u4e2a\u65b0\u7684\u57fa\u51c6\u5ea6\u91cf Speedup Score S(t) \u6765\u7efc\u5408\u8003\u8651\u901f\u5ea6\u63d0\u5347\u548c\u6267\u884c\u6b63\u786e\u6027\uff0c\u5e76\u6269\u5c55\u4e3a Error-aware Speedup Score ES(t) \u4ee5\u5e2e\u52a9\u8bc6\u522b\u6027\u80fd\u74f6\u9888\u3002\u6211\u4eec\u5728\u8ba1\u7b97\u673a\u89c6\u89c9\u548c\u81ea\u7136\u8bed\u8a00\u5904\u7406\u6837\u672c\u4e0a\u8fdb\u884c\u4e86\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5c55\u793a\u4e86GraphNet\u7684\u5b9e\u7528\u6027\u3002", "motivation": "\u73b0\u6709\u7684\u4e00\u4e9b\u8ba1\u7b97\u56fe\u6570\u636e\u96c6\u53ef\u80fd\u65e0\u6cd5\u5168\u9762\u8bc4\u4f30\u5f20\u91cf\u7f16\u8bd1\u5668\u7684\u4f18\u5316\u80fd\u529b\uff0c\u800cGraphNet \u80fd\u63d0\u4f9b\u66f4\u4e3a\u5168\u9762\u7684\u8bc4\u4f30\uff0c\u4e0d\u4ec5\u8003\u8651\u901f\u5ea6\u63d0\u5347\uff0c\u8fd8\u8003\u8651\u6267\u884c\u6b63\u786e\u6027\uff0c\u8fdb\u800c\u5e2e\u52a9\u5f00\u53d1\u4eba\u5458\u8bc6\u522b\u6027\u80fd\u74f6\u9888\uff0c\u63d0\u5347\u7f16\u8bd1\u5668\u8d28\u91cf\u3002", "method": "\u901a\u8fc7\u5f15\u5165\u4e00\u4e2a\u65b0\u7684\u57fa\u51c6\u5ea6\u91cf Speedup Score S(t)\uff0c\u5b83\u8003\u8651\u4e86\u6267\u884c\u65f6\u95f4\u548c\u6b63\u786e\u6027\u3002\u5e76\u8fdb\u4e00\u6b65\u63d0\u51fa\u4e86Error-aware Speedup Score ES(t)\uff0c\u8be5\u5ea6\u91cf\u8fdb\u4e00\u6b65\u8003\u8651\u4e86\u9519\u8bef\u4fe1\u606f\u3002\u6211\u4eec\u8fd8\u5c55\u793a\u4e86\u57fa\u4e8eGraphNet\u7684\u6570\u636e\u96c6\u5982\u4f55\u5e94\u7528\u4e8e\u8bc4\u4f30\u4e0d\u540c\u6846\u67b6\u4e0b\u7684\u9ed8\u8ba4\u5f20\u91cf\u7f16\u8bd1\u5668\u7684\u6027\u80fd\u3002", "result": "\u901a\u8fc7\u5728\u8ba1\u7b97\u673a\u89c6\u89c9\u548c\u81ea\u7136\u8bed\u8a00\u5904\u7406\u9886\u57df\u8fdb\u884c\u7684\u6d4b\u8bd5\uff0c\u6211\u4eec\u5c55\u793a\u4e86\u5982\u4f55\u4f7f\u7528GraphNet\u901a\u8fc7Speedup Score S(t)\u548cError-aware Speedup Score ES(t)\u6765\u8bc4\u4f30\u73b0\u6709\u7f16\u8bd1\u5668\u7684\u6027\u80fd\u3002\u6d4b\u8bd5\u7ed3\u679c\u8868\u660eGraphNet\u5bf9\u4e8e\u6539\u8fdb\u5f20\u91cf\u7f16\u8bd1\u5668\u5177\u6709\u6f5c\u529b\u3002", "conclusion": "GraphNet\u63d0\u4f9b\u4e86\u4e00\u4e2a\u771f\u5b9e\u4e16\u754c\u6df1\u5ea6\u5b66\u4e60\u8ba1\u7b97\u56fe\u6570\u636e\u96c6\uff0c\u5e2e\u52a9\u8bc4\u4f30\u5f20\u91cf\u7f16\u8bd1\u5668\u7684\u4f18\u5316\u80fd\u529b\u3002Speedup Score S(t) \u548c Error-aware Speedup Score ES(t) \u63d0\u4f9b\u4e86\u8bc4\u4f30\u6027\u80fd\u65b0\u7b56\u7565\uff0c\u5bf9\u672a\u6765\u53d1\u5c55\u6df1\u5ea6\u5b66\u4e60\u7f16\u8bd1\u5668\u4f18\u5316\u5177\u6709\u91cd\u8981\u4f5c\u7528\u3002"}}
{"id": "2510.24717", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.24717", "abs": "https://arxiv.org/abs/2510.24717", "authors": ["Haoge Deng", "Ting Pan", "Fan Zhang", "Yang Liu", "Zhuoyan Luo", "Yufeng Cui", "Wenxuan Wang", "Chunhua Shen", "Shiguang Shan", "Zhaoxiang Zhang", "Xinlong Wang"], "title": "Uniform Discrete Diffusion with Metric Path for Video Generation", "comment": "19 pages, 10 figures", "summary": "Continuous-space video generation has advanced rapidly, while discrete\napproaches lag behind due to error accumulation and long-context inconsistency.\nIn this work, we revisit discrete generative modeling and present Uniform\ndiscRete diffuSion with metric pAth (URSA), a simple yet powerful framework\nthat bridges the gap with continuous approaches for the scalable video\ngeneration. At its core, URSA formulates the video generation task as an\niterative global refinement of discrete spatiotemporal tokens. It integrates\ntwo key designs: a Linearized Metric Path and a Resolution-dependent Timestep\nShifting mechanism. These designs enable URSA to scale efficiently to\nhigh-resolution image synthesis and long-duration video generation, while\nrequiring significantly fewer inference steps. Additionally, we introduce an\nasynchronous temporal fine-tuning strategy that unifies versatile tasks within\na single model, including interpolation and image-to-video generation.\nExtensive experiments on challenging video and image generation benchmarks\ndemonstrate that URSA consistently outperforms existing discrete methods and\nachieves performance comparable to state-of-the-art continuous diffusion\nmethods. Code and models are available at https://github.com/baaivision/URSA", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2510.24039", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24039", "abs": "https://arxiv.org/abs/2510.24039", "authors": ["Nikolaos Karalias", "Akbar Rafiey", "Yifei Xu", "Zhishang Luo", "Behrooz Tahmasebi", "Connie Jiang", "Stefanie Jegelka"], "title": "Geometric Algorithms for Neural Combinatorial Optimization with Constraints", "comment": null, "summary": "Self-Supervised Learning (SSL) for Combinatorial Optimization (CO) is an\nemerging paradigm for solving combinatorial problems using neural networks. In\nthis paper, we address a central challenge of SSL for CO: solving problems with\ndiscrete constraints. We design an end-to-end differentiable framework that\nenables us to solve discrete constrained optimization problems with neural\nnetworks. Concretely, we leverage algorithmic techniques from the literature on\nconvex geometry and Carath\\'eodory's theorem to decompose neural network\noutputs into convex combinations of polytope corners that correspond to\nfeasible sets. This decomposition-based approach enables self-supervised\ntraining but also ensures efficient quality-preserving rounding of the neural\nnet output into feasible solutions. Extensive experiments in\ncardinality-constrained optimization show that our approach can consistently\noutperform neural baselines. We further provide worked-out examples of how our\nmethod can be applied beyond cardinality-constrained problems to a diverse set\nof combinatorial optimization tasks, including finding independent sets in\ngraphs, and solving matroid-constrained problems.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2510.24718", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.24718", "abs": "https://arxiv.org/abs/2510.24718", "authors": ["Chonghyuk Song", "Michal Stary", "Boyuan Chen", "George Kopanas", "Vincent Sitzmann"], "title": "Generative View Stitching", "comment": "Project website: https://andrewsonga.github.io/gvs", "summary": "Autoregressive video diffusion models are capable of long rollouts that are\nstable and consistent with history, but they are unable to guide the current\ngeneration with conditioning from the future. In camera-guided video generation\nwith a predefined camera trajectory, this limitation leads to collisions with\nthe generated scene, after which autoregression quickly collapses. To address\nthis, we propose Generative View Stitching (GVS), which samples the entire\nsequence in parallel such that the generated scene is faithful to every part of\nthe predefined camera trajectory. Our main contribution is a sampling algorithm\nthat extends prior work on diffusion stitching for robot planning to video\ngeneration. While such stitching methods usually require a specially trained\nmodel, GVS is compatible with any off-the-shelf video model trained with\nDiffusion Forcing, a prevalent sequence diffusion framework that we show\nalready provides the affordances necessary for stitching. We then introduce\nOmni Guidance, a technique that enhances the temporal consistency in stitching\nby conditioning on both the past and future, and that enables our proposed\nloop-closing mechanism for delivering long-range coherence. Overall, GVS\nachieves camera-guided video generation that is stable, collision-free,\nframe-to-frame consistent, and closes loops for a variety of predefined camera\npaths, including Oscar Reutersv\\\"ard's Impossible Staircase. Results are best\nviewed as videos at https://andrewsonga.github.io/gvs.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2510.24043", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.24043", "abs": "https://arxiv.org/abs/2510.24043", "authors": ["Akira Tamamori"], "title": "Localized Kernel Projection Outlyingness: A Two-Stage Approach for Multi-Modal Outlier Detection", "comment": "10 pages, 4 figures; submitted to The IEICE Transactions on\n  Information and Systems", "summary": "This paper presents Two-Stage LKPLO, a novel multi-stage outlier detection\nframework that overcomes the coexisting limitations of conventional\nprojection-based methods: their reliance on a fixed statistical metric and\ntheir assumption of a single data structure. Our framework uniquely synthesizes\nthree key concepts: (1) a generalized loss-based outlyingness measure (PLO)\nthat replaces the fixed metric with flexible, adaptive loss functions like our\nproposed SVM-like loss; (2) a global kernel PCA stage to linearize non-linear\ndata structures; and (3) a subsequent local clustering stage to handle\nmulti-modal distributions. Comprehensive 5-fold cross-validation experiments on\n10 benchmark datasets, with automated hyperparameter optimization, demonstrate\nthat Two-Stage LKPLO achieves state-of-the-art performance. It significantly\noutperforms strong baselines on datasets with challenging structures where\nexisting methods fail, most notably on multi-cluster data (Optdigits) and\ncomplex, high-dimensional data (Arrhythmia). Furthermore, an ablation study\nempirically confirms that the synergistic combination of both the kernelization\nand localization stages is indispensable for its superior performance. This\nwork contributes a powerful new tool for a significant class of outlier\ndetection problems and underscores the importance of hybrid, multi-stage\narchitectures.", "AI": {"tldr": "Two-Stage LKPLO \u662f\u4e00\u79cd\u65b0\u9896\u7684\u591a\u9636\u6bb5\u79bb\u7fa4\u70b9\u68c0\u6d4b\u6846\u67b6\uff0c\u5b83\u7ed3\u5408\u4e86\u4e09\u4e2a\u5173\u952e\u7684\u6982\u5ff5\uff1a\u635f\u5931\u57fa\u7684\u79bb\u7fa4\u70b9\u5ea6\u91cf\u3001\u5168\u5c40\u6838\u4e3b\u6210\u5206\u5206\u6790\u9636\u6bb5\u548c\u540e\u7eed\u7684\u5c40\u90e8\u805a\u7c7b\u9636\u6bb5\u3002\u8be5\u6846\u67b6\u5728\u542b\u6709\u590d\u6742\u7ed3\u6784\u7684\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u7684\u57fa\u4e8e\u6295\u5f71\u7684\u65b9\u6cd5\u4f9d\u8d56\u56fa\u5b9a\u7684\u7edf\u8ba1\u5ea6\u91cf\u5e76\u5047\u8bbe\u5355\u4e00\u6570\u636e\u7ed3\u6784\uff0c\u8fd9\u9650\u5236\u4e86\u5b83\u4eec\u5728\u5904\u7406\u590d\u6742\u6570\u636e\u7ed3\u6784\u4e0a\u7684\u80fd\u529b\u3002\u672c\u7814\u7a76\u7684\u52a8\u673a\u662f\u5f00\u53d1\u4e00\u79cd\u80fd\u591f\u514b\u670d\u8fd9\u4e9b\u5c40\u9650\u6027\u7684\u65b0\u578b\u6846\u67b6\uff0c\u9002\u7528\u4e8e\u66f4\u5e7f\u6cdb\u7684\u6570\u636e\u7c7b\u578b\u3002", "method": "\u4ecb\u7ecd\u4e86 Two-Stage LKPLO \u6846\u67b6\uff0c\u5176\u5728\u5168\u5c40\u6838\u5316PCA\u4e4b\u540e\u8fdb\u884c\u5c40\u90e8\u805a\u7c7b\u5206\u6790\uff0c\u91c7\u7528\u635f\u5931\u57fa\u5ea6\u91cf\u4ee5\u9002\u5e94\u4e0d\u540c\u6570\u636e\u7ed3\u6784\u3002\u5177\u4f53\u5305\u62ec\u63d0\u51faSVM\u4f3c\u635f\u5931\u51fd\u6570\u7b49\u7075\u6d3b\u3001\u81ea\u9002\u5e94\u7684\u635f\u5931\u51fd\u6570\uff0c\u5f62\u6210\u5168\u5c40\u7ebf\u6027\u5316\u4ee5\u53ca\u5904\u7406\u591a\u5cf0\u6570\u636e\u7684\u80fd\u529b\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u6846\u67b6\u5728\u591a\u79cd\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\u3002", "result": "\u572810\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u76845\u6298\u4ea4\u53c9\u9a8c\u8bc1\u5b9e\u9a8c\u8868\u660e\uff0cTwo-Stage LKPLO \u663e\u8457\u4f18\u4e8e\u73b0\u6709\u7684\u65b9\u6cd5\uff0c\u7279\u522b\u662f\u5728\u5904\u7406\u591a\u7c07\u7ed3\u6784\u548c\u9ad8\u590d\u6742\u5ea6\u6570\u636e\u96c6\u65f6\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u63d0\u51fa\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u79bb\u7fa4\u70b9\u68c0\u6d4b\u6846\u67b6\uff0c\u8bc1\u5b9e\u4e86\u6df7\u5408\u591a\u9636\u6bb5\u67b6\u6784\u5728\u79bb\u7fa4\u70b9\u68c0\u6d4b\u4e2d\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2510.24044", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.24044", "abs": "https://arxiv.org/abs/2510.24044", "authors": ["Hui Sun", "Zheng Xie", "Hao-Yuan He", "Ming Li"], "title": "Mitigating Negative Transfer via Reducing Environmental Disagreement", "comment": "13 pages, 5 figures", "summary": "Unsupervised Domain Adaptation~(UDA) focuses on transferring knowledge from a\nlabeled source domain to an unlabeled target domain, addressing the challenge\nof \\emph{domain shift}. Significant domain shifts hinder effective knowledge\ntransfer, leading to \\emph{negative transfer} and deteriorating model\nperformance. Therefore, mitigating negative transfer is essential. This study\nrevisits negative transfer through the lens of causally disentangled learning,\nemphasizing cross-domain discriminative disagreement on non-causal\nenvironmental features as a critical factor. Our theoretical analysis reveals\nthat overreliance on non-causal environmental features as the environment\nevolves can cause discriminative disagreements~(termed \\emph{environmental\ndisagreement}), thereby resulting in negative transfer. To address this, we\npropose Reducing Environmental Disagreement~(RED), which disentangles each\nsample into domain-invariant causal features and domain-specific non-causal\nenvironmental features via adversarially training domain-specific environmental\nfeature extractors in the opposite domains. Subsequently, RED estimates and\nreduces environmental disagreement based on domain-specific non-causal\nenvironmental features. Experimental results confirm that RED effectively\nmitigates negative transfer and achieves state-of-the-art performance.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86RED\u65b9\u6cd5\uff0c\u7528\u4e8e\u51cf\u5c11\u8de8\u57df\u5dee\u5f02\u4e2d\u7684\u73af\u5883\u5206\u6b67\uff0c\u4ece\u800c\u7f13\u89e3\u8d1f\u5411\u8fc1\u79fb\u5e76\u63d0\u9ad8\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u7814\u7a76\u6307\u51fa\uff0c\u8fc7\u4f9d\u8d56\u975e\u56e0\u679c\u73af\u5883\u7279\u5f81\u662f\u5bfc\u81f4\u8d1f\u5411\u8fc1\u79fb\u7684\u5173\u952e\u56e0\u7d20\uff0c\u5e76\u63d0\u51fa\u5c06\u57df\u5212\u5206\u6210\u56e0\u679c\u5171\u56e0\u548c\u7279\u5b9a\u4e8e\u73af\u5883\u7684\u975e\u56e0\u679c\u7279\u5f81\u6765\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u7684\u52a8\u673a\u3002", "method": "RED\u901a\u8fc7\u5bf9\u6297\u6027\u8bad\u7ec3\u7279\u5b9a\u57df\u7684\u73af\u5883\u7279\u5f81\u63d0\u53d6\u5668\u6765\u533a\u5206\u6bcf\u4e2a\u6837\u672c\u7684\u56e0\u679c\u5171\u56e0\u548c\u7279\u5b9a\u4e8e\u73af\u5883\u7684\u975e\u56e0\u679c\u7279\u5f81\uff0c\u7136\u540e\u4f30\u8ba1\u548c\u51cf\u5c11\u57fa\u4e8e\u7279\u5b9a\u57df\u7684\u975e\u56e0\u679c\u73af\u5883\u7279\u5f81\u7684\u73af\u5883\u5206\u6b67\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cRED\u65b9\u6cd5\u80fd\u6709\u6548\u51cf\u8f7b\u8d1f\u5411\u8fc1\u79fb\uff0c\u53d6\u5f97\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "RED\u65b9\u6cd5\u901a\u8fc7\u51cf\u5c11\u73af\u5883\u5206\u6b67\u6210\u529f\u7f13\u89e3\u4e86\u8d1f\u5411\u8fc1\u79fb\u95ee\u9898\uff0c\u8bc1\u660e\u4e86\u5728\u8de8\u9886\u57df\u9002\u5e94\u4e2d\u533a\u5206\u56e0\u679c\u5171\u56e0\u548c\u975e\u56e0\u679c\u73af\u5883\u7279\u5f81\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2510.24046", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24046", "abs": "https://arxiv.org/abs/2510.24046", "authors": ["Tu Anh Hoang Nguyen", "Dang Nguyen", "Tri-Nhan Vo", "Thuc Duy Le", "Sunil Gupta"], "title": "Causal-Aware Generative Adversarial Networks with Reinforcement Learning", "comment": null, "summary": "The utility of tabular data for tasks ranging from model training to\nlarge-scale data analysis is often constrained by privacy concerns or\nregulatory hurdles. While existing data generation methods, particularly those\nbased on Generative Adversarial Networks (GANs), have shown promise, they\nfrequently struggle with capturing complex causal relationship, maintaining\ndata utility, and providing provable privacy guarantees suitable for enterprise\ndeployment. We introduce CA-GAN, a novel generative framework specifically\nengineered to address these challenges for real-world tabular datasets. CA-GAN\nutilizes a two-step approach: causal graph extraction to learn a robust,\ncomprehensive causal relationship in the data's manifold, followed by a custom\nConditional WGAN-GP (Wasserstein GAN with Gradient Penalty) that operates\nexclusively as per the structure of nodes in the causal graph. More\nimportantly, the generator is trained with a new Reinforcement Learning-based\nobjective that aligns the causal graphs constructed from real and fake data,\nensuring the causal awareness in both training and sampling phases. We\ndemonstrate CA-GAN superiority over six SOTA methods across 14 tabular\ndatasets. Our evaluations, focused on core data engineering metrics: causal\npreservation, utility preservation, and privacy preservation. Our method offers\na practical, high-performance solution for data engineers seeking to create\nhigh-quality, privacy-compliant synthetic datasets to benchmark database\nsystems, accelerate software development, and facilitate secure data-driven\nresearch.", "AI": {"tldr": "\u4ecb\u7ecd\u4e86\u4e00\u79cd\u65b0\u7684\u751f\u6210\u6846\u67b6CA-GAN\uff0c\u7528\u4e8e\u751f\u6210\u80fd\u591f\u4fdd\u7559\u56e0\u679c\u5173\u7cfb\u3001\u6570\u636e\u5b9e\u7528\u6027\u548c\u9690\u79c1\u4fdd\u62a4\u7684\u9ad8\u8d28\u91cf\u5408\u6210\u6570\u636e\u96c6\u3002CA-GAN\u7ed3\u5408\u4e86\u56e0\u679c\u56fe\u63d0\u53d6\u548c\u6761\u4ef6WGAN-GP\uff0c\u5e76\u4f7f\u7528\u5f3a\u5316\u5b66\u4e60\u76ee\u6807\u6765\u4fdd\u8bc1\u751f\u6210\u6570\u636e\u7684\u56e0\u679c\u610f\u8bc6\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u572814\u4e2a\u8868\u683c\u6570\u636e\u96c6\u4e0a\u4f18\u4e8e\u516d\u4e2aSOTA\u65b9\u6cd5\u3002", "motivation": "\u5f53\u524d\u7684\u57fa\u4e8eGAN\u7684\u6570\u636e\u751f\u6210\u65b9\u6cd5\u5728\u6355\u83b7\u590d\u6742\u7684\u56e0\u679c\u5173\u7cfb\u3001\u4fdd\u6301\u6570\u636e\u5b9e\u7528\u6027\u548c\u63d0\u4f9b\u4f01\u4e1a\u90e8\u7f72\u6240\u9700\u7684\u9690\u79c1\u4fdd\u8bc1\u65b9\u9762\u5b58\u5728\u6311\u6218\u3002\u56e0\u6b64\uff0c\u5f00\u53d1\u4e00\u79cd\u65b0\u7684\u6846\u67b6\u6765\u751f\u6210\u65e2\u80fd\u4fdd\u62a4\u9690\u79c1\u53c8\u80fd\u4fdd\u6301\u5b9e\u7528\u6027\u7684\u9ad8\u8d28\u91cf\u5408\u6210\u6570\u636e\u5bf9\u4e8e\u6570\u636e\u5de5\u7a0b\u81f3\u5173\u91cd\u8981\u3002", "method": "CA-GAN\u63d0\u51fa\u4e86\u4e00\u79cd\u4e24\u6b65\u65b9\u6cd5\uff1a\u9996\u5148\u63d0\u53d6\u56e0\u679c\u56fe\u6765\u5b66\u4e60\u6570\u636e\u6d41\u5f62\u4e2d\u7684\u56e0\u679c\u5173\u7cfb\uff0c\u7136\u540e\u5229\u7528\u4e00\u4e2a\u7279\u5236\u7684\u6761\u4ef6WGAN-GP\u6839\u636e\u56e0\u679c\u56fe\u7684\u8282\u70b9\u7ed3\u6784\u751f\u6210\u6837\u672c\u3002\u751f\u6210\u5668\u8bad\u7ec3\u4f7f\u7528\u4e00\u4e2a\u65b0\u7684\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u76ee\u6807\uff0c\u4fdd\u8bc1\u751f\u6210\u7684\u548c\u771f\u5b9e\u6570\u636e\u7684\u56e0\u679c\u56fe\u4e00\u81f4\u6027\u3002", "result": "\u5b9e\u9a8c\u572814\u4e2a\u8868\u683c\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\uff0c\u7ed3\u679c\u8868\u660eCA-GAN\u5728\u56e0\u679c\u4fdd\u62a4\u3001\u5b9e\u7528\u6027\u548c\u9690\u79c1\u4fdd\u62a4\u65b9\u9762\u5747\u4f18\u4e8e\u516d\u4e2a\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\uff0c\u8bc1\u660e\u4e86\u5176\u8865\u5145\u73b0\u6709\u6280\u672f\u7684\u80fd\u529b\u3002", "conclusion": "CA-GAN\u4e3a\u6570\u636e\u5de5\u7a0b\u5e08\u63d0\u4f9b\u4e86\u4e00\u79cd\u5b9e\u7528\u7684\u3001\u9ad8\u6027\u80fd\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u53ef\u7528\u4e8e\u521b\u5efa\u9ad8\u8d28\u91cf\u7684\u3001\u7b26\u5408\u9690\u79c1\u8981\u6c42\u7684\u5408\u6210\u6570\u636e\u96c6\uff0c\u4ee5\u652f\u6301\u57fa\u51c6\u6d4b\u8bd5\u6570\u636e\u5e93\u7cfb\u7edf\u3001\u52a0\u901f\u8f6f\u4ef6\u5f00\u53d1\u548c\u4fc3\u8fdb\u5b89\u5168\u7684\u6570\u636e\u9a71\u52a8\u7814\u7a76\u3002"}}
{"id": "2510.24049", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24049", "abs": "https://arxiv.org/abs/2510.24049", "authors": ["Hao Jia", "Penghao Zhao", "Hao Wu", "Yuan Gao", "Yangyu Tao", "Bin Cui"], "title": "Learning from History: A Retrieval-Augmented Framework for Spatiotemporal Prediction", "comment": null, "summary": "Accurate and long-term spatiotemporal prediction for complex physical systems\nremains a fundamental challenge in scientific computing. While deep learning\nmodels, as powerful parametric approximators, have shown remarkable success,\nthey suffer from a critical limitation: the accumulation of errors during\nlong-term autoregressive rollouts often leads to physically implausible\nartifacts. This deficiency arises from their purely parametric nature, which\nstruggles to capture the full constraints of a system's intrinsic dynamics. To\naddress this, we introduce a novel \\textbf{Retrieval-Augmented Prediction\n(RAP)} framework, a hybrid paradigm that synergizes the predictive power of\ndeep networks with the grounded truth of historical data. The core philosophy\nof RAP is to leverage historical evolutionary exemplars as a non-parametric\nestimate of the system's local dynamics. For any given state, RAP efficiently\nretrieves the most similar historical analog from a large-scale database. The\ntrue future evolution of this analog then serves as a \\textbf{reference\ntarget}. Critically, this target is not a hard constraint in the loss function\nbut rather a powerful conditional input to a specialized dual-stream\narchitecture. It provides strong \\textbf{dynamic guidance}, steering the\nmodel's predictions towards physically viable trajectories. In extensive\nbenchmarks across meteorology, turbulence, and fire simulation, RAP not only\nsurpasses state-of-the-art methods but also significantly outperforms a strong\n\\textbf{analog-only forecasting baseline}. More importantly, RAP generates\npredictions that are more physically realistic by effectively suppressing error\ndivergence in long-term rollouts.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684Retrieval-Augmented Prediction (RAP) \u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u548c\u5386\u53f2\u6570\u636e\uff0c\u89e3\u51b3\u4e86\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5728\u957f\u671f\u9884\u6d4b\u4e2d\u7269\u7406\u4e0d\u4e00\u81f4\u7684\u95ee\u9898\uff0c\u63d0\u9ad8\u4e86\u9884\u6d4b\u7684\u51c6\u786e\u6027\u548c\u7269\u7406\u5408\u7406\u6027\u3002", "motivation": "\u89e3\u51b3\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u957f\u671f\u9884\u6d4b\u4e2d\u7269\u7406\u4e0d\u4e00\u81f4\u548c\u8bef\u5dee\u7d2f\u79ef\u7684\u95ee\u9898\uff0c\u63d0\u9ad8\u9884\u6d4b\u7684\u7269\u7406\u771f\u5b9e\u6027\u3002", "method": "\u4ecb\u7ecd\u4e86\u4e00\u79cd\u65b0\u7684Retrieval-Augmented Prediction (RAP) \u6846\u67b6\uff0c\u8be5\u6846\u67b6\u5229\u7528\u5386\u53f2\u6570\u636e\u4e2d\u7c7b\u4f3c\u7684\u6f14\u5316\u4f8b\u5b50\u6765\u4f5c\u4e3a\u7cfb\u7edf\u5c40\u90e8\u52a8\u6001\u7684\u975e\u53c2\u6570\u4f30\u8ba1\uff0c\u5e76\u4f7f\u7528\u8fd9\u79cd\u4f30\u8ba1\u6765\u6307\u5bfc\u6a21\u578b\u7684\u9884\u6d4b\uff0c\u4ee5\u4ea7\u751f\u66f4\u5408\u7406\u7684\u9884\u6d4b\u7ed3\u679c\u3002\u8fd9\u79cd\u65b9\u6cd5\u4f7f\u7528\u4e86\u4e00\u4e2a\u53cc\u6d41\u67b6\u6784\uff0c\u5176\u4e2d\u5386\u53f2\u6570\u636e\u63d0\u4f9b\u7684\u53c2\u8003\u76ee\u6807\u4f5c\u4e3a\u6761\u4ef6\u8f93\u5165\uff0c\u63d0\u4f9b\u52a8\u6001\u6307\u5bfc\u3002", "result": "\u5728\u6c14\u8c61\u5b66\u3001\u6e4d\u6d41\u548c\u706b\u707e\u6a21\u62df\u7684\u5e7f\u6cdb\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cRAP\u4e0d\u4ec5\u8d85\u8d8a\u4e86\u73b0\u6709\u7684\u6700\u4f73\u65b9\u6cd5\uff0c\u800c\u4e14\u663e\u8457\u5730\u8d85\u8fc7\u4e86\u4ec5\u57fa\u4e8e\u6a21\u62df\u7684\u6570\u636e\u9884\u6d4b\u7684\u57fa\u7840\u7ebf\u3002\u66f4\u91cd\u8981\u7684\u662f\uff0cRAP\u6709\u6548\u6291\u5236\u4e86\u957f\u671f\u9884\u6d4b\u4e2d\u7684\u8bef\u5dee\u6269\u6563\uff0c\u751f\u6210\u4e86\u66f4\u52a0\u7269\u7406\u5408\u7406\u7684\u9884\u6d4b\u7ed3\u679c\u3002", "conclusion": "\u63d0\u51fa\u7684RAP\u6846\u67b6\u901a\u8fc7\u7ed3\u5408\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u548c\u5386\u53f2\u6570\u636e\uff0c\u89e3\u51b3\u4e86\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u957f\u671f\u9884\u6d4b\u4e2d\u7684\u7269\u7406\u4e0d\u4e00\u81f4\u548c\u8bef\u5dee\u7d2f\u79ef\u95ee\u9898\uff0c\u4ece\u800c\u63d0\u9ad8\u4e86\u9884\u6d4b\u7684\u51c6\u786e\u6027\u548c\u7269\u7406\u5408\u7406\u6027\u3002"}}
{"id": "2510.24053", "categories": ["cs.LG", "q-bio.QM"], "pdf": "https://arxiv.org/pdf/2510.24053", "abs": "https://arxiv.org/abs/2510.24053", "authors": ["Jacob B. Roberts", "Catherine R. Ji", "Isaac Donnell", "Thomas D. Young", "Allison N. Pearson", "Graham A. Hudson", "Leah S. Keiser", "Mia Wesselkamper", "Peter H. Winegar", "Janik Ludwig", "Sarah H. Klass", "Isha V. Sheth", "Ezechinyere C. Ukabiala", "Maria C. T. Astolfi", "Benjamin Eysenbach", "Jay D. Keasling"], "title": "Low-N Protein Activity Optimization with FolDE", "comment": "18 pages, 4 figures. Preprint. Open-source software available at\n  https://github.com/JBEI/foldy", "summary": "Proteins are traditionally optimized through the costly construction and\nmeasurement of many mutants. Active Learning-assisted Directed Evolution (ALDE)\nalleviates that cost by predicting the best improvements and iteratively\ntesting mutants to inform predictions. However, existing ALDE methods face a\ncritical limitation: selecting the highest-predicted mutants in each round\nyields homogeneous training data insufficient for accurate prediction models in\nsubsequent rounds. Here we present FolDE, an ALDE method designed to maximize\nend-of-campaign success. In simulations across 20 protein targets, FolDE\ndiscovers 23% more top 10% mutants than the best baseline ALDE method (p=0.005)\nand is 55% more likely to find top 1% mutants. FolDE achieves this primarily\nthrough naturalness-based warm-starting, which augments limited activity\nmeasurements with protein language model outputs to improve activity\nprediction. We also introduce a constant-liar batch selector, which improves\nbatch diversity; this is important in multi-mutation campaigns but had limited\neffect in our benchmarks. The complete workflow is freely available as\nopen-source software, making efficient protein optimization accessible to any\nlaboratory.", "AI": {"tldr": "FolDE\u662f\u4e00\u79cd\u6539\u8fdb\u7684ALDE\u65b9\u6cd5\uff0c\u901a\u8fc7\u81ea\u7136\u8d77\u59cb\u548c\u5e38\u91cf\u6492\u8c0e\u6279\u6b21\u9009\u62e9\u5668\u63d0\u9ad8\u86cb\u767d\u8d28\u4f18\u5316\u6548\u7387\uff0c\u53ef\u4ee5\u627e\u5230\u66f4\u591a\u9ad8\u6d3b\u6027\u7a81\u53d8\u4f53\u3002", "motivation": "\u4f20\u7edf\u86cb\u767d\u8d28\u4f18\u5316\u65b9\u6cd5\u6210\u672c\u9ad8\uff0c\u4f9d\u8d56\u4e8e\u5927\u91cf\u7a81\u53d8\u4f53\u7684\u6784\u5efa\u548c\u6d4b\u91cf\u3002\u73b0\u6709\u7684ALDE\u65b9\u6cd5\u5728\u9009\u62e9\u6700\u4f73\u7a81\u53d8\u4f53\u65f6\u9762\u4e34\u5c40\u9650\u6027\uff0c\u5bfc\u81f4\u8bad\u7ec3\u6570\u636e\u540c\u8d28\u5316\uff0c\u9884\u6d4b\u6a21\u578b\u4e0d\u51c6\u786e\u3002\u56e0\u6b64\uff0c\u672c\u7814\u7a76\u63d0\u51fa\u4e86FolDE\u65b9\u6cd5\uff0c\u4ee5\u89e3\u51b3\u4e0a\u8ff0\u95ee\u9898\uff0c\u63d0\u9ad8\u86cb\u767d\u8d28\u4f18\u5316\u6548\u7387\u3002", "method": "FolDE\u901a\u8fc7\u81ea\u7136\u8d77\u59cb\u548c\u5e38\u91cf\u6492\u8c0e\u6279\u6b21\u9009\u62e9\u5668\u6765\u6539\u8fdbALDE\u65b9\u6cd5\u3002\u81ea\u7136\u8d77\u59cb\u901a\u8fc7\u7ed3\u5408\u86cb\u767d\u8d28\u8bed\u8a00\u6a21\u578b\u8f93\u51fa\u548c\u6709\u9650\u7684\u6d3b\u6027\u6d4b\u91cf\u6765\u63d0\u9ad8\u6d3b\u6027\u9884\u6d4b\u3002\u5e38\u91cf\u6492\u8c0e\u6279\u6b21\u9009\u62e9\u5668\u5219\u4f18\u5316\u4e86\u6279\u6b21\u591a\u6837\u6027\uff0c\u4f46\u5728\u7814\u7a76\u4e2d\u7684\u5f71\u54cd\u76f8\u5bf9\u8f83\u5c0f\u3002", "result": "\u5728\u9488\u5bf920\u4e2a\u86cb\u767d\u8d28\u76ee\u6807\u7684\u6a21\u62df\u6d4b\u8bd5\u4e2d\uff0cFolDE\u627e\u5230\u4e86\u6bd4\u6700\u4f73\u57fa\u7ebfALDE\u65b9\u6cd5\u591a23%\u7684\u524d10%\u7a81\u53d8\u4f53\uff08p=0.005\uff09\uff0c\u4e14\u627e\u5230\u524d1%\u7a81\u53d8\u4f53\u7684\u53ef\u80fd\u6027\u9ad855%\u3002", "conclusion": "FolDE\u63d0\u5347\u4e86\u86cb\u767d\u8d28\u4f18\u5316\u7684\u6548\u7387\uff0c\u4e14\u5176\u5b8c\u6574\u5de5\u4f5c\u6d41\u7a0b\u53ef\u4f5c\u4e3a\u5f00\u6e90\u8f6f\u4ef6\u4f7f\u7528\uff0c\u4e3a\u6240\u6709\u5b9e\u9a8c\u5ba4\u63d0\u4f9b\u4e86\u9ad8\u6548\u86cb\u767d\u8d28\u4f18\u5316\u7684\u9014\u5f84\u3002"}}
{"id": "2510.24331", "categories": ["cs.LG", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.24331", "abs": "https://arxiv.org/abs/2510.24331", "authors": ["Gabriel O. dos Santos", "Esther Colombini", "Sandra Avila"], "title": "What do vision-language models see in the context? Investigating multimodal in-context learning", "comment": null, "summary": "In-context learning (ICL) enables Large Language Models (LLMs) to learn tasks\nfrom demonstration examples without parameter updates. Although it has been\nextensively studied in LLMs, its effectiveness in Vision-Language Models (VLMs)\nremains underexplored. In this work, we present a systematic study of ICL in\nVLMs, evaluating seven models spanning four architectures on three image\ncaptioning benchmarks. We analyze how prompt design, architectural choices, and\ntraining strategies influence multimodal ICL. To our knowledge, we are the\nfirst to analyze how attention patterns in VLMs vary with an increasing number\nof in-context demonstrations. Our results reveal that training on imag-text\ninterleaved data enhances ICL performance but does not imply effective\nintegration of visual and textual information from demonstration examples. In\ncontrast, instruction tuning improves instruction-following but can reduce\nreliance on in-context demonstrations, suggesting a trade-off between\ninstruction alignment and in-context adaptation. Attention analyses further\nshow that current VLMs primarily focus on textual cues and fail to leverage\nvisual information, suggesting a limited capacity for multimodal integration.\nThese findings highlight key limitations in the ICL abilities of current VLMs\nand provide insights for enhancing their ability to learn from multimodal\nin-context examples.", "AI": {"tldr": "\u8fd9\u9879\u5de5\u4f5c\u7814\u7a76\u4e86\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u4e2d\u7684in-context learning (ICL)\uff0c\u63ed\u793a\u4e86\u8bad\u7ec3\u7b56\u7565\u3001\u67b6\u6784\u9009\u62e9\u4ee5\u53ca\u63d0\u793a\u8bbe\u8ba1\u5bf9multimodal ICL\u7684\u5f71\u54cd\uff0c\u5e76\u5206\u6790\u4e86\u5728ICL\u8fc7\u7a0b\u4e2d\u6ce8\u610f\u529b\u6a21\u5f0f\u7684\u53d8\u5316\uff0c\u53d1\u73b0\u5f53\u524dVLMs\u5728\u591a\u6a21\u6001\u6574\u5408\u65b9\u9762\u5b58\u5728\u5c40\u9650\u6027\uff0c\u4e3b\u8981\u4f9d\u8d56\u6587\u672c\u7ebf\u7d22\u800c\u975e\u89c6\u89c9\u4fe1\u606f\u3002", "motivation": "\u5c3d\u7ba1in-context learning\uff08ICL\uff09\u5728\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e2d\u88ab\u5e7f\u6cdb\u7814\u7a76\uff0c\u4f46\u5728\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u6548\u679c\u4ecd\u7136\u672a\u88ab\u5145\u5206\u63a2\u7d22\u3002\u5f53\u524d\u5de5\u4f5c\u65e8\u5728\u7cfb\u7edf\u5730\u7814\u7a76VLMs\u4e2d\u7684ICL\u80fd\u529b\uff0c\u7279\u522b\u662f\u5728\u63d0\u793a\u8bbe\u8ba1\u3001\u67b6\u6784\u9009\u62e9\u548c\u8bad\u7ec3\u7b56\u7565\u65b9\u9762\u7684\u6548\u679c\u5982\u4f55\uff0c\u4ee5\u53ca\u5982\u4f55\u901a\u8fc7\u589e\u52a0\u4e0a\u4e0b\u6587\u793a\u8303\u6765\u6539\u5584\u6ce8\u610f\u529b\u6a21\u5f0f\u3002\u6b64\u5916\uff0c\u7814\u7a76\u8fd8\u63a2\u8ba8\u4e86instruction tuning\u5bf9\u4e8e\u7f13\u89e3VLM\u6a21\u578b\u8fc7\u5206\u4f9d\u8d56\u4e0a\u4e0b\u6587\u6837\u672c\u7684\u4f5c\u7528\u3002", "method": "\u6d4b\u8bd5\u4e86\u4e03\u79cd\u6a21\u578b\uff0c\u6db5\u76d6\u56db\u79cd\u67b6\u6784\uff0c\u5e76\u5728\u4e09\u4e2a\u56fe\u50cf\u63cf\u8ff0\u57fa\u51c6\u4e0a\u8fdb\u884c\u8bc4\u4ef7\u3002\u540c\u65f6\uff0c\u5bf9\u6ce8\u610f\u529b\u6a21\u5f0f\u968f\u4e0a\u4e0b\u6587\u6837\u672c\u6570\u91cf\u589e\u52a0\u7684\u53d8\u5316\u8fdb\u884c\u4e86\u5206\u6790\u3002", "result": "\u53d1\u73b0\u8bad\u7ec3\u5728\u56fe\u50cf-\u6587\u672c\u4ea4\u9519\u6570\u636e\u4e0a\u7684ICL\u6027\u80fd\u5f97\u5230\u63d0\u5347\uff0c\u4f46\u5bf9\u89c6\u89c9\u548c\u6587\u672c\u4fe1\u606f\u7684\u6709\u6548\u96c6\u6210\u65e0\u663e\u8457\u8d21\u732e\u3002Instruction tuning\u6709\u52a9\u4e8e\u63d0\u5347\u6307\u4ee4\u9075\u5faa\u80fd\u529b\uff0c\u4f46\u53ef\u80fd\u51cf\u5c11\u5bf9\u4e0a\u4e0b\u6587\u793a\u8303\u7684\u4f9d\u8d56\uff0c\u8868\u660e\u6307\u4ee4\u5bf9\u9f50\u548c\u4e0a\u4e0b\u6587\u81ea\u9002\u5e94\u4e4b\u95f4\u5b58\u5728\u6743\u8861\u3002\u8fdb\u4e00\u6b65\u6ce8\u610f\u5230\uff0c\u5f53\u524dVLMs\u4e3b\u8981\u5173\u6ce8\u6587\u672c\u7ebf\u7d22\uff0c\u65e0\u6cd5\u6709\u6548\u5229\u7528\u89c6\u89c9\u4fe1\u606f\uff0c\u8fd9\u8868\u660e\u591a\u6a21\u6001\u6574\u5408\u80fd\u529b\u6709\u9650\u3002", "conclusion": "\u8fd9\u4e9b\u53d1\u73b0\u63ed\u793a\u4e86\u5f53\u524dVLMs\u5728ICL\u80fd\u529b\u65b9\u9762\u5b58\u5728\u7684\u5173\u952e\u9650\u5236\uff0c\u5e76\u9610\u660e\u4e86\u5982\u4f55\u589e\u5f3a\u5176\u901a\u8fc7\u591a\u6a21\u6001\u4e0a\u4e0b\u6587\u793a\u4f8b\u8fdb\u884c\u5b66\u4e60\u7684\u80fd\u529b\u3002"}}
{"id": "2510.24095", "categories": ["cs.LG", "cs.AI", "cs.RO"], "pdf": "https://arxiv.org/pdf/2510.24095", "abs": "https://arxiv.org/abs/2510.24095", "authors": ["Vedant Gupta", "Haotian Fu", "Calvin Luo", "Yiding Jiang", "George Konidaris"], "title": "Learning Parameterized Skills from Demonstrations", "comment": "Neurips 2025", "summary": "We present DEPS, an end-to-end algorithm for discovering parameterized skills\nfrom expert demonstrations. Our method learns parameterized skill policies\njointly with a meta-policy that selects the appropriate discrete skill and\ncontinuous parameters at each timestep. Using a combination of temporal\nvariational inference and information-theoretic regularization methods, we\naddress the challenge of degeneracy common in latent variable models, ensuring\nthat the learned skills are temporally extended, semantically meaningful, and\nadaptable. We empirically show that learning parameterized skills from\nmultitask expert demonstrations significantly improves generalization to unseen\ntasks. Our method outperforms multitask as well as skill learning baselines on\nboth LIBERO and MetaWorld benchmarks. We also demonstrate that DEPS discovers\ninterpretable parameterized skills, such as an object grasping skill whose\ncontinuous arguments define the grasp location.", "AI": {"tldr": "DEPS \u662f\u4e00\u79cd\u4ece\u4e13\u5bb6\u6f14\u793a\u4e2d\u53d1\u73b0\u53c2\u6570\u5316\u6280\u80fd\u7684\u7aef\u5230\u7aef\u7b97\u6cd5\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u5b66\u4e60\u53c2\u6570\u5316\u6280\u80fd\u4e0e\u9009\u62e9\u9002\u5f53\u79bb\u6563\u6280\u80fd\u548c\u8fde\u7eed\u53c2\u6570\u7684\u5143\u7b56\u7565\uff0c\u89e3\u51b3\u4e86\u6f5c\u5728\u53d8\u91cf\u6a21\u578b\u4e2d\u7684\u9000\u5316\u95ee\u9898\uff0c\u5e76\u8bc1\u660e\u4ece\u591a\u4efb\u52a1\u4e13\u5bb6\u6f14\u793a\u4e2d\u5b66\u4e60\u53c2\u6570\u5316\u6280\u80fd\u53ef\u4ee5\u63d0\u9ad8\u5bf9\u672a\u89c1\u8fc7\u4efb\u52a1\u7684\u6cdb\u5316\u80fd\u529b\u3002DEPS \u5728 LIBERO \u548c MetaWorld \u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u4f18\u4e8e\u591a\u4efb\u52a1\u548c\u6280\u80fd\u5b66\u4e60\u57fa\u7ebf\u7684\u65b9\u6cd5\uff0c\u5e76\u80fd\u591f\u53d1\u73b0\u5982\u7269\u4f53\u6293\u53d6\u6280\u80fd\u7b49\u53ef\u89e3\u91ca\u7684\u53c2\u6570\u5316\u6280\u80fd\u3002", "motivation": "\u4e3a\u4e86\u4ece\u4e13\u5bb6\u6f14\u793a\u4e2d\u53d1\u73b0\u53ef\u6cdb\u5316\u7684\u53c2\u6570\u5316\u6280\u80fd\uff0c\u89e3\u51b3\u6f5c\u5728\u53d8\u91cf\u6a21\u578b\u4e2d\u7684\u9000\u5316\u95ee\u9898\uff0c\u5e76\u63d0\u9ad8\u5bf9\u672a\u89c1\u8fc7\u4efb\u52a1\u7684\u6cdb\u5316\u80fd\u529b\u3002", "method": "DEPS \u7ed3\u5408\u4e86\u65f6\u95f4\u53d8\u5206\u63a8\u65ad\u548c\u4fe1\u606f\u7406\u8bba\u6b63\u5219\u5316\u65b9\u6cd5\uff0c\u5b66\u4e60\u53c2\u6570\u5316\u6280\u80fd\u4ee5\u53ca\u9009\u62e9\u9002\u5f53\u79bb\u6563\u6280\u80fd\u548c\u8fde\u7eed\u53c2\u6570\u7684\u5143\u7b56\u7565\u3002", "result": "DEPS \u65b9\u6cd5\u5728 LIBERO \u548c MetaWorld \u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e86\u6bd4\u591a\u4efb\u52a1\u548c\u6280\u80fd\u5b66\u4e60\u57fa\u7ebf\u66f4\u597d\u7684\u7ed3\u679c\uff0c\u5e76\u4e14\u53ef\u4ee5\u53d1\u73b0\u5982\u7269\u4f53\u6293\u53d6\u6280\u80fd\u7b49\u53ef\u89e3\u91ca\u7684\u53c2\u6570\u5316\u6280\u80fd\u3002", "conclusion": "DEPS \u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u4ece\u4e13\u5bb6\u6f14\u793a\u4e2d\u53d1\u73b0\u53c2\u6570\u5316\u6280\u80fd\u7684\u65b0\u65b9\u6cd5\uff0c\u5177\u6709\u5e7f\u6cdb\u7684\u5e94\u7528\u524d\u666f\u3002"}}
{"id": "2510.24503", "categories": ["cs.LG", "cs.AI", "cs.CV", "cs.DC", "cs.MA"], "pdf": "https://arxiv.org/pdf/2510.24503", "abs": "https://arxiv.org/abs/2510.24503", "authors": ["Mortesa Hussaini", "Jan Thei\u00df", "Anthony Stein"], "title": "Local Performance vs. Out-of-Distribution Generalization: An Empirical Analysis of Personalized Federated Learning in Heterogeneous Data Environments", "comment": null, "summary": "In the context of Federated Learning with heterogeneous data environments,\nlocal models tend to converge to their own local model optima during local\ntraining steps, deviating from the overall data distributions. Aggregation of\nthese local updates, e.g., with FedAvg, often does not align with the global\nmodel optimum (client drift), resulting in an update that is suboptimal for\nmost clients. Personalized Federated Learning approaches address this challenge\nby exclusively focusing on the average local performances of clients' models on\ntheir own data distribution. Generalization to out-of-distribution samples,\nwhich is a substantial benefit of FedAvg and represents a significant component\nof robustness, appears to be inadequately incorporated into the assessment and\nevaluation processes. This study involves a thorough evaluation of Federated\nLearning approaches, encompassing both their local performance and their\ngeneralization capabilities. Therefore, we examine different stages within a\nsingle communication round to enable a more nuanced understanding of the\nconsidered metrics. Furthermore, we propose and incorporate a modified approach\nof FedAvg, designated as Federated Learning with Individualized Updates (FLIU),\nextending the algorithm by a straightforward individualization step with an\nadaptive personalization factor. We evaluate and compare the approaches\nempirically using MNIST and CIFAR-10 under various distributional conditions,\nincluding benchmark IID and pathological non-IID, as well as additional novel\ntest environments with Dirichlet distribution specifically developed to stress\nthe algorithms on complex data heterogeneity.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86FedAvg\u7684\u6539\u826f\u7248Federated Learning with Individualized Updates (FLIU)\uff0c\u901a\u8fc7\u5728\u7b97\u6cd5\u4e2d\u6dfb\u52a0\u4e00\u4e2a\u5177\u6709\u81ea\u9002\u5e94\u4e2a\u4eba\u5316\u56e0\u5b50\u7684\u7b80\u5355\u4e2a\u6027\u5316\u6b65\u9aa4\uff0c\u65e8\u5728\u89e3\u51b3\u8054\u90a6\u5b66\u4e60\u4e2d\u5ba2\u6237\u7aef\u6f02\u79fb\u548c\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\u7684\u95ee\u9898\u3002\u8bc4\u4f30\u662f\u5728MNIST\u548cCIFAR-10\u6570\u636e\u96c6\u7684\u4e0d\u540c\u5206\u5e03\u6761\u4ef6\u4e0b\u8fdb\u884c\u7684\uff0c\u5305\u62ec\u57fa\u51c6\u540c\u5206\u5e03\uff08IID\uff09\u548c\u75c5\u6001\u975e\u540c\u5206\u5e03\uff08non-IID\uff09\uff0c\u4ee5\u53ca\u91c7\u7528\u72c4\u5229\u514b\u96f7\u5206\u5e03\uff08Dirichlet distribution\uff09\u521b\u5efa\u7684\u65b0\u6d4b\u8bd5\u73af\u5883\u6765\u6d4b\u8bd5\u7b97\u6cd5\u5728\u590d\u6742\u6570\u636e\u5f02\u8d28\u6027\u4e0b\u7684\u8868\u73b0\u3002", "motivation": "\u8054\u90a6\u5b66\u4e60\u5728\u5904\u7406\u5f02\u6784\u6570\u636e\u73af\u5883\u4e2d\u7684\u95ee\u9898\uff0c\u4f8b\u5982\u5ba2\u6237\u7aef\u6f02\u79fb\u548c\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\uff0c\u901a\u8fc7\u5f15\u5165FedAvg\u7684\u6539\u826f\u7248FLIU\u6765\u63d0\u5347\u8054\u90a6\u5b66\u4e60\u7684\u6574\u4f53\u6027\u80fd\u548c\u7a33\u5b9a\u6027\u3002", "method": "\u65b9\u6cd5\u5305\u62ec\u4e86\u5bf9\u8054\u90a6\u5b66\u4e60\u5404\u4e2a\u9636\u6bb5\u7684\u8be6\u7ec6\u8bc4\u4f30\uff0c\u4ee5\u53ca\u901a\u8fc7\u6dfb\u52a0\u4e2a\u6027\u5316\u6b65\u9aa4\u6765\u6539\u8fdbFedAvg\u7b97\u6cd5\uff0c\u5f00\u53d1\u51faFLIU\u3002\u8be5\u6b65\u9aa4\u901a\u8fc7\u4e00\u4e2a\u81ea\u9002\u5e94\u7684\u4e2a\u6027\u5316\u56e0\u5b50\u6765\u8c03\u6574\u5c40\u90e8\u6a21\u578b\u7684\u66f4\u65b0\u3002\u8bc4\u4f30\u91c7\u7528MNIST\u548cCIFAR-10\u6570\u636e\u96c6\uff0c\u6db5\u76d6\u4e86\u4e0d\u540c\u5206\u5e03\u6761\u4ef6\u4e0b\u7684\u8868\u73b0\u6d4b\u8bd5\u3002", "result": "FLIU\u5728\u591a\u79cd\u5206\u5e03\u6761\u4ef6\u4e0b\u5c55\u793a\u4e86\u826f\u597d\u7684\u6548\u679c\uff0c\u7279\u522b\u662f\u5728\u75c5\u7406\u6027\u7684\u975e\u540c\u5206\u5e03\u73af\u5883\u4e2d\uff0c\u5176\u6027\u80fd\u4f18\u4e8e\u6807\u51c6FedAvg\uff0c\u8bc1\u660e\u4e86\u5176\u5bf9\u590d\u6742\u6570\u636e\u5f02\u8d28\u6027\u7684\u9002\u5e94\u6027\u3002", "conclusion": "\u672c\u7814\u7a76\u901a\u8fc7\u8be6\u7ec6\u8bc4\u4f30\u4e0d\u540c\u6570\u636e\u5206\u5e03\u73af\u5883\u4e0b\u7684\u8054\u90a6\u5b66\u4e60\u7b97\u6cd5\uff0c\u8bc1\u660e\u4e86FLIU\u5728\u5904\u7406\u5f02\u6784\u6570\u636e\u73af\u5883\u4e2d\u7684\u6709\u6548\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2510.24120", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.24120", "abs": "https://arxiv.org/abs/2510.24120", "authors": ["Ziyu Liu", "Yijing Liu", "Jianfei Yuan", "Minzhi Yan", "Le Yue", "Honghui Xiong", "Yi Yang"], "title": "Graph-Guided Concept Selection for Efficient Retrieval-Augmented Generation", "comment": null, "summary": "Graph-based RAG constructs a knowledge graph (KG) from text chunks to enhance\nretrieval in Large Language Model (LLM)-based question answering. It is\nespecially beneficial in domains such as biomedicine, law, and political\nscience, where effective retrieval often involves multi-hop reasoning over\nproprietary documents. However, these methods demand numerous LLM calls to\nextract entities and relations from text chunks, incurring prohibitive costs at\nscale. Through a carefully designed ablation study, we observe that certain\nwords (termed concepts) and their associated documents are more important.\nBased on this insight, we propose Graph-Guided Concept Selection (G2ConS). Its\ncore comprises a chunk selection method and an LLM-independent concept graph.\nThe former selects salient document chunks to reduce KG construction costs; the\nlatter closes knowledge gaps introduced by chunk selection at zero cost.\nEvaluations on multiple real-world datasets show that G2ConS outperforms all\nbaselines in construction cost, retrieval effectiveness, and answering quality.", "AI": {"tldr": "\u63d0\u51fa\u4e86Graph-Guided Concept Selection (G2ConS) \u65b9\u6cd5\uff0c\u901a\u8fc7\u9009\u62e9\u5173\u952e\u6587\u6863\u7247\u6bb5\u548c\u5efa\u7acb\u65e0\u4f9d\u8d56\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u6982\u5ff5\u56fe\uff0c\u6765\u63d0\u5347\u77e5\u8bc6\u56fe\u6784\u5efa\u7684\u6548\u7387\u5e76\u63d0\u9ad8\u68c0\u7d22\u6548\u679c\u548c\u95ee\u7b54\u8d28\u91cf\u3002\u5b9e\u9a8c\u8868\u660e\u5176\u5728\u6210\u672c\u3001\u68c0\u7d22\u6709\u6548\u6027\u548c\u95ee\u7b54\u8d28\u91cf\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u5728\u7279\u5b9a\u9886\u57df\uff08\u5982\u751f\u7269\u533b\u5b66\u3001\u6cd5\u5f8b\u548c\u653f\u6cbb\u5b66\uff09\uff0c\u57fa\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u95ee\u7b54\u7cfb\u7edf\u901a\u8fc7\u6587\u672c\u7247\u6bb5\u63a8\u7406\u4f1a\u589e\u52a0\u9ad8\u6602\u7684\u8c03\u7528\u6210\u672c\u3002\u56e0\u6b64\uff0c\u9700\u8981\u51cf\u5c11LLM\u8c03\u7528\u7684\u6570\u91cf\u6765\u964d\u4f4e\u6210\u672c\u7684\u540c\u65f6\u4fdd\u8bc1\u68c0\u7d22\u7684\u51c6\u786e\u6027\u548c\u6548\u7387\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u533a\u5757\u9009\u62e9\u7684\u65b9\u6cd5\u548c\u65e0\u4f9d\u8d56\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u6982\u5ff5\u56fe\uff0c\u901a\u8fc7\u9009\u62e9\u5177\u6709\u4ee3\u8868\u6027\u7684\u6587\u672c\u7247\u6bb5\u51cf\u5c11\u77e5\u8bc6\u56fe\u6784\u5efa\u6210\u672c\uff0c\u540c\u65f6\u4f7f\u7528\u8fd9\u4e2a\u6982\u5ff5\u56fe\u586b\u8865\u7531\u533a\u5757\u9009\u62e9\u5e26\u6765\u7684\u77e5\u8bc6\u7a7a\u7f3a\uff0c\u65e0\u9700\u989d\u5916\u7684LLM\u8c03\u7528\u6210\u672c\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684G2ConS\u65b9\u6cd5\u5728\u77e5\u8bc6\u56fe\u6784\u5efa\u6210\u672c\u3001\u68c0\u7d22\u6548\u679c\u548c\u95ee\u7b54\u8d28\u91cf\u4e0a\u90fd\u4f18\u4e8e\u5176\u4ed6\u57fa\u51c6\u65b9\u6cd5\u3002", "conclusion": "G2ConS\u65b9\u6cd5\u4e0d\u4ec5\u80fd\u591f\u663e\u8457\u964d\u4f4e\u77e5\u8bc6\u56fe\u6784\u5efa\u6210\u672c\uff0c\u8fd8\u80fd\u591f\u63d0\u5347\u68c0\u7d22\u548c\u95ee\u7b54\u4efb\u52a1\u7684\u6548\u679c\u3002"}}
{"id": "2510.24125", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.24125", "abs": "https://arxiv.org/abs/2510.24125", "authors": ["Kiran Bacsa", "Wei Liu", "Xudong Jian", "Huangbin Liang", "Eleni Chatzi"], "title": "Causal Convolutional Neural Networks as Finite Impulse Response Filters", "comment": "14 pages, 19 figures, Under review", "summary": "This study investigates the behavior of Causal Convolutional Neural Networks\n(CNNs) with quasi-linear activation functions when applied to time-series data\ncharacterized by multimodal frequency content. We demonstrate that, once\ntrained, such networks exhibit properties analogous to Finite Impulse Response\n(FIR) filters, particularly when the convolutional kernels are of extended\nlength exceeding those typically employed in standard CNN architectures. Causal\nCNNs are shown to capture spectral features both implicitly and explicitly,\noffering enhanced interpretability for tasks involving dynamic systems.\nLeveraging the associative property of convolution, we further show that the\nentire network can be reduced to an equivalent single-layer filter resembling\nan FIR filter optimized via least-squares criteria. This equivalence yields new\ninsights into the spectral learning behavior of CNNs trained on signals with\nsparse frequency content. The approach is validated on both simulated beam\ndynamics and real-world bridge vibration datasets, underlining its relevance\nfor modeling and identifying physical systems governed by dynamic responses.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2510.24135", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.24135", "abs": "https://arxiv.org/abs/2510.24135", "authors": ["Hojin Cheon", "Hyeongseok Seo", "Jihun Jeon", "Wooju Lee", "Dohyun Jeong", "Hongseok Kim"], "title": "Fixed Point Neural Acceleration and Inverse Surrogate Model for Battery Parameter Identification", "comment": "31 pages, 11 figures, submitted to Applied Energy", "summary": "The rapid expansion of electric vehicles has intensified the need for\naccurate and efficient diagnosis of lithium-ion batteries. Parameter\nidentification of electrochemical battery models is widely recognized as a\npowerful method for battery health assessment. However, conventional\nmetaheuristic approaches suffer from high computational cost and slow\nconvergence, and recent machine learning methods are limited by their reliance\non constant current data, which may not be available in practice. To overcome\nthese challenges, we propose deep learning-based framework for parameter\nidentification of electrochemical battery models. The proposed framework\ncombines a neural surrogate model of the single particle model with electrolyte\n(NeuralSPMe) and a deep learning-based fixed-point iteration method. NeuralSPMe\nis trained on realistic EV load profiles to accurately predict lithium\nconcentration dynamics under dynamic operating conditions while a parameter\nupdate network (PUNet) performs fixed-point iterative updates to significantly\nreduce both the evaluation time per sample and the overall number of iterations\nrequired for convergence. Experimental evaluations demonstrate that the\nproposed framework accelerates the parameter identification by more than 2000\ntimes, achieves superior sample efficiency and more than 10 times higher\naccuracy compared to conventional metaheuristic algorithms, particularly under\ndynamic load scenarios encountered in practical applications.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2510.24160", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.24160", "abs": "https://arxiv.org/abs/2510.24160", "authors": ["Aiqing Zhu", "Beatrice W. Soh", "Grigorios A. Pavliotis", "Qianxiao Li"], "title": "Identifiable learning of dissipative dynamics", "comment": null, "summary": "Complex dissipative systems appear across science and engineering, from\npolymers and active matter to learning algorithms. These systems operate far\nfrom equilibrium, where energy dissipation and time irreversibility are key to\ntheir behavior, but are difficult to quantify from data. Learning accurate and\ninterpretable models of such dynamics remains a major challenge: the models\nmust be expressive enough to describe diverse processes, yet constrained enough\nto remain physically meaningful and mathematically identifiable. Here, we\nintroduce I-OnsagerNet, a neural framework that learns dissipative stochastic\ndynamics directly from trajectories while ensuring both interpretability and\nuniqueness. I-OnsagerNet extends the Onsager principle to guarantee that the\nlearned potential is obtained from the stationary density and that the drift\ndecomposes cleanly into time-reversible and time-irreversible components, as\ndictated by the Helmholtz decomposition. Our approach enables us to calculate\nthe entropy production and to quantify irreversibility, offering a principled\nway to detect and quantify deviations from equilibrium. Applications to polymer\nstretching in elongational flow and to stochastic gradient Langevin dynamics\nreveal new insights, including super-linear scaling of barrier heights and\nsub-linear scaling of entropy production rates with the strain rate, and the\nsuppression of irreversibility with increasing batch size. I-OnsagerNet thus\nestablishes a general, data-driven framework for discovering and interpreting\nnon-equilibrium dynamics.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2510.24173", "categories": ["cs.LG", "cs.NA", "math.DS", "math.NA", "physics.flu-dyn"], "pdf": "https://arxiv.org/pdf/2510.24173", "abs": "https://arxiv.org/abs/2510.24173", "authors": ["Yiheng Du", "Aditi S. Krishnapriyan"], "title": "EddyFormer: Accelerated Neural Simulations of Three-Dimensional Turbulence at Scale", "comment": "NeurIPS 2025", "summary": "Computationally resolving turbulence remains a central challenge in fluid\ndynamics due to its multi-scale interactions. Fully resolving large-scale\nturbulence through direct numerical simulation (DNS) is computationally\nprohibitive, motivating data-driven machine learning alternatives. In this\nwork, we propose EddyFormer, a Transformer-based spectral-element (SEM)\narchitecture for large-scale turbulence simulation that combines the accuracy\nof spectral methods with the scalability of the attention mechanism. We\nintroduce an SEM tokenization that decomposes the flow into grid-scale and\nsubgrid-scale components, enabling capture of both local and global features.\nWe create a new three-dimensional isotropic turbulence dataset and train\nEddyFormer to achieves DNS-level accuracy at 256^3 resolution, providing a 30x\nspeedup over DNS. When applied to unseen domains up to 4x larger than in\ntraining, EddyFormer preserves accuracy on physics-invariant metrics-energy\nspectra, correlation functions, and structure functions-showing domain\ngeneralization. On The Well benchmark suite of diverse turbulent flows,\nEddyFormer resolves cases where prior ML models fail to converge, accurately\nreproducing complex dynamics across a wide range of physical conditions.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aEddyFormer\u7684\u57fa\u4e8eTransformer\u7684\u5149\u8c31\u5143\u65b9\u6cd5\u67b6\u6784\uff0c\u7528\u4e8e\u5927\u89c4\u6a21\u6e4d\u6d41\u6a21\u62df\uff0c\u7ed3\u5408\u4e86\u5149\u8c31\u65b9\u6cd5\u7684\u51c6\u786e\u6027\u4e0e\u6ce8\u610f\u529b\u673a\u5236\u7684\u53ef\u6269\u5c55\u6027\u3002EddyFormer\u5728256^3\u5206\u8fa8\u7387\u4e0b\u5b9e\u73b0\u4e86\u76f4\u63a5\u6570\u503c\u6a21\u62df\u7ea7\u522b\u7684\u7cbe\u5ea6\uff0c\u901f\u5ea6\u63d0\u9ad8\u4e8630\u500d\u3002\u5728\u66f4\u5927\u89c4\u6a21\u7684\u65b0\u9886\u57df\u4e2d\u4e5f\u8868\u73b0\u51fa\u5f88\u597d\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u5e76\u5728\u590d\u6742\u7269\u7406\u6761\u4ef6\u4e0b\u51c6\u786e\u518d\u73b0\u4e86\u6e4d\u6d41\u52a8\u529b\u5b66\u3002", "motivation": "\u73b0\u6709\u6280\u672f\u901a\u8fc7\u76f4\u63a5\u6570\u503c\u6a21\u62df\u5b8c\u5168\u89e3\u6790\u5927\u578b\u6e4d\u6d41\u4ece\u8ba1\u7b97\u4e0a\u6765\u8bf4\u662f\u4e0d\u53ef\u884c\u7684\uff0c\u56e0\u6b64\u63d0\u51fa\u4e86\u6570\u636e\u9a71\u52a8\u7684\u673a\u5668\u5b66\u4e60\u66ff\u4ee3\u65b9\u6848\u3002EddyFormer\u65e2\u5177\u6709\u5149\u8c31\u65b9\u6cd5\u7684\u7cbe\u5ea6\uff0c\u53c8\u5177\u6709\u6ce8\u610f\u529b\u673a\u5236\u7684\u53ef\u6269\u5c55\u6027\uff0c\u63d0\u4f9b\u4e86\u66f4\u597d\u7684\u8ba1\u7b97\u6548\u7387\u548c\u51c6\u786e\u6027\u3002", "method": "\u5f15\u5165\u4e86\u5149\u8c31\u5143\u65b9\u6cd5\u6807\u8bb0\uff0c\u5c06\u6d41\u4f53\u5206\u89e3\u4e3a\u7f51\u683c\u5c3a\u5ea6\u548c\u6b21\u7f51\u683c\u5c3a\u5ea6\u7ec4\u4ef6\uff0c\u7ed3\u5408\u4e86\u5c40\u90e8\u548c\u5168\u5c40\u7279\u5f81\u7684\u6355\u6349\u3002\u521b\u5efa\u4e86\u4e00\u4e2a\u65b0\u7684\u4e09\u7ef4\u540c\u6027\u6e4d\u6d41\u6570\u636e\u96c6\uff0c\u8bad\u7ec3EddyFormer\u4ee5\u83b7\u5f97\u76f4\u63a5\u6570\u503c\u6a21\u62df\u7ea7\u522b\u7684\u7cbe\u5ea6\uff0c\u5c55\u73b0\u51fa\u826f\u597d\u7684\u6cdb\u5316\u80fd\u529b\u3002\u540c\u65f6\uff0c\u5728\u591a\u6837\u5316\u7684\u6e4d\u6d41\u6d4b\u8bd5\u5957\u4ef6\uff08The Well\uff09\u4e0a\u8fdb\u884c\u6d4b\u8bd5\uff0c\u4ee5\u786e\u4fdd\u6a21\u578b\u7684\u5168\u9762\u51c6\u786e\u6027\u548c\u7a33\u5b9a\u6027\u3002", "result": "EddyFormer\u5728256^3\u5206\u8fa8\u7387\u4e0b\u5b9e\u73b0\u4e86\u76f4\u63a5\u6570\u503c\u6a21\u62df\u7ea7\u522b\u7684\u7cbe\u5ea6\uff0c\u901f\u5ea6\u63d0\u9ad8\u4e8630\u500d\u3002\u6b64\u5916\uff0c\u5b83\u8fd8\u80fd\u4fdd\u6301\u5728\u6bd4\u8bad\u7ec3\u6570\u636e\u96c6\u59274\u500d\u7684\u672a\u89c1\u9886\u57df\u4e2d\u7684\u51c6\u786e\u5ea6\uff0c\u5e76\u5728\u4e00\u7cfb\u5217\u7269\u7406\u6761\u4ef6\u4e0b\u51c6\u786e\u518d\u73b0\u4e86\u590d\u6742\u7684\u52a8\u529b\u5b66\u3002EddyFormer\u901a\u8fc7\u5728\u5404\u79cd\u6e4d\u6d41\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u6210\u529f\uff0c\u8d85\u8fc7\u4e86\u4ee5\u524d\u7684\u673a\u5668\u5b66\u4e60\u6a21\u578b\u3002", "conclusion": "EddyFormer\u4f5c\u4e3a\u4e00\u79cd\u57fa\u4e8eTransformer\u7684\u65b0\u578b\u67b6\u6784\uff0c\u6210\u529f\u5730\u89e3\u51b3\u4e86\u5927\u89c4\u6a21\u6e4d\u6d41\u6a21\u62df\u4e2d\u7684\u51c6\u786e\u6027\u4e0e\u53ef\u6269\u5c55\u6027\u6311\u6218\uff0c\u63d0\u4f9b\u4e86\u9ad8\u6548\u7684\u8ba1\u7b97\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u5e7f\u6cdb\u7684\u7269\u7406\u6761\u4ef6\u4e0b\u7684\u6d41\u91cf\u9884\u6d4b\u548c\u5206\u6790\u3002"}}
{"id": "2510.24180", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.24180", "abs": "https://arxiv.org/abs/2510.24180", "authors": ["Arpita Kundu", "Joyita Chakraborty", "Anindita Desarkar", "Aritra Sen", "Srushti Anil Patil", "Vishwanathan Raman"], "title": "V-SAT: Video Subtitle Annotation Tool", "comment": null, "summary": "The surge of audiovisual content on streaming platforms and social media has\nheightened the demand for accurate and accessible subtitles. However, existing\nsubtitle generation methods primarily speech-based transcription or OCR-based\nextraction suffer from several shortcomings, including poor synchronization,\nincorrect or harmful text, inconsistent formatting, inappropriate reading\nspeeds, and the inability to adapt to dynamic audio-visual contexts. Current\napproaches often address isolated issues, leaving post-editing as a\nlabor-intensive and time-consuming process. In this paper, we introduce V-SAT\n(Video Subtitle Annotation Tool), a unified framework that automatically\ndetects and corrects a wide range of subtitle quality issues. By combining\nLarge Language Models(LLMs), Vision-Language Models (VLMs), Image Processing,\nand Automatic Speech Recognition (ASR), V-SAT leverages contextual cues from\nboth audio and video. Subtitle quality improved, with the SUBER score reduced\nfrom 9.6 to 3.54 after resolving all language mode issues and F1-scores of\n~0.80 for image mode issues. Human-in-the-loop validation ensures high-quality\nresults, providing the first comprehensive solution for robust subtitle\nannotation.", "AI": {"tldr": "\u63d0\u51fa\u4e86V-SAT\uff08\u89c6\u9891\u5b57\u5e55\u6ce8\u91ca\u5de5\u5177\uff09\uff0c\u8fd9\u662f\u4e00\u4e2a\u7edf\u4e00\u7684\u6846\u67b6\uff0c\u7ed3\u5408\u4e86\u5927\u8bed\u8a00\u6a21\u578b\u3001\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u3001\u56fe\u50cf\u5904\u7406\u548c\u81ea\u52a8\u8bed\u97f3\u8bc6\u522b\uff0c\u81ea\u52a8\u68c0\u6d4b\u548c\u7ea0\u6b63\u653e\u6620\u8d28\u91cf\u95ee\u9898\uff0c\u63d0\u9ad8\u4e86\u5b57\u5e55\u8d28\u91cf\uff0c\u51cf\u5c11\u4e86SUBER\u8bc4\u5206\u4e3a3.54\uff0c\u8bed\u8a00\u6a21\u5f0f\u95ee\u9898\u7684F1\u5206\u6570\u8fbe\u52300.80\u5de6\u53f3\uff0c\u63d0\u4f9b\u4e86\u7b2c\u4e00\u4e2a\u5168\u9762\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u786e\u4fdd\u9ad8\u8d28\u91cf\u7684\u5b57\u5e55\u6ce8\u91ca\u3002", "motivation": "\u9488\u5bf9\u73b0\u6709\u5b57\u5e55\u751f\u6210\u65b9\u6cd5\u5b58\u5728\u7684\u95ee\u9898\uff0c\u4f8b\u5982\u540c\u6b65\u4e0d\u826f\u3001\u6587\u672c\u9519\u8bef\u6216\u6709\u5bb3\u7684\u5185\u5bb9\uff0c\u683c\u5f0f\u4e0d\u4e00\u81f4\uff0c\u8bfb\u53d6\u901f\u5ea6\u4e0d\u5408\u9002\uff0c\u65e0\u6cd5\u9002\u5e94\u8be5\u52a8\u6001\u97f3\u9891-\u89c6\u9891\u4e0a\u4e0b\u6587\uff0c\u5f15\u5165\u4e86V-SAT\u6765\u89e3\u51b3\u8fd9\u4e9b\u5b64\u7acb\u7684\u7279\u5f81\u95ee\u9898\uff0c\u4ee5\u51cf\u5c11\u540e\u671f\u7f16\u8f91\u7684\u8d44\u6e90\u548c\u65f6\u95f4\u6295\u5165\u3002", "method": "\u7ed3\u5408\u4e86\u5927\u8bed\u8a00\u6a21\u578b\u3001\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u3001\u56fe\u50cf\u5904\u7406\u548c\u81ea\u52a8\u8bed\u97f3\u8bc6\u522b\uff0c\u901a\u8fc7\u7ed3\u5408\u97f3\u9891\u548c\u89c6\u9891\u7684\u4e0a\u4e0b\u6587\u63d0\u793a\uff0c\u81ea\u52a8\u68c0\u6d4b\u5e76\u7ea0\u6b63\u524d\u666f\u95ee\u9898\uff0c\u63d0\u9ad8\u5b57\u5e55\u8d28\u91cf\uff0c\u8fbe\u5230\u9ad8\u8d28\u91cf\u7684\u5b57\u5e55\u8f93\u51fa\u3002", "result": "\u5728\u89e3\u51b3\u4e86\u6240\u6709\u8bed\u8a00\u6a21\u5f0f\u95ee\u9898\u540e\u7684SUBER\u5206\u6570\u4ece9.6\u964d\u81f33.54\uff0c\u800c\u56fe\u50cf\u6a21\u5f0f\u95ee\u9898\u7684F1\u5206\u6570\u8fbe\u52300.80\u5de6\u53f3\uff0c\u63d0\u9ad8\u4e86\u5b57\u5e55\u8d28\u91cf\uff0c\u8868\u660e\u4e86V-SAT\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u9996\u6b21\u63d0\u4f9b\u4e86\u5168\u9762\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u786e\u4fdd\u9ad8\u8d28\u91cf\u7684\u5b57\u5e55\u6ce8\u91ca\uff0c\u8bc1\u660e\u4e86V-SAT\u5728\u6539\u5584\u5b57\u5e55\u4e2d\u7684\u7efc\u5408\u8868\u73b0\u3002"}}
{"id": "2510.24200", "categories": ["cs.LG", "cs.CR", "cs.DC", "I.2.11"], "pdf": "https://arxiv.org/pdf/2510.24200", "abs": "https://arxiv.org/abs/2510.24200", "authors": ["Alexander Bakarsky", "Dimitar I. Dimitrov", "Maximilian Baader", "Martin Vechev"], "title": "SPEAR++: Scaling Gradient Inversion via Sparsely-Used Dictionary Learning", "comment": "Published at the Workshop on Regulatable ML at the 39th Conference on\n  Neural Information Processing Systems (NeurIPS 2025)", "summary": "Federated Learning has seen an increased deployment in real-world scenarios\nrecently, as it enables the distributed training of machine learning models\nwithout explicit data sharing between individual clients. Yet, the introduction\nof the so-called gradient inversion attacks has fundamentally challenged its\nprivacy-preserving properties. Unfortunately, as these attacks mostly rely on\ndirect data optimization without any formal guarantees, the vulnerability of\nreal-world systems remains in dispute and requires tedious testing for each new\nfederated deployment. To overcome these issues, recently the SPEAR attack was\nintroduced, which is based on a theoretical analysis of the gradients of linear\nlayers with ReLU activations. While SPEAR is an important theoretical\nbreakthrough, the attack's practicality was severely limited by its exponential\nruntime in the batch size b. In this work, we fill this gap by applying\nState-of-the-Art techniques from Sparsely-Used Dictionary Learning to make the\nproblem of gradient inversion on linear layers with ReLU activations tractable.\nOur experiments demonstrate that our new attack, SPEAR++, retains all desirable\nproperties of SPEAR, such as robustness to DP noise and FedAvg aggregation,\nwhile being applicable to 10x bigger batch sizes.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u7684SPEAR++\u653b\u51fb\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u4f7f\u7528\u7a00\u758f\u5b57\u5178\u5b66\u4e60\u6280\u672f\u4f18\u5316\u4e86SPEAR\u653b\u51fb\uff0c\u4f7f\u5176\u5728\u5904\u7406\u66f4\u5927\u7684\u6279\u91cf\u6570\u636e\u65f6\u53d8\u5f97\u66f4\u4e3a\u5b9e\u9645\u4e14\u6709\u6548\u3002\u540c\u65f6\uff0cSPEAR++\u4fdd\u7559\u4e86SPEAR\u7684\u6240\u6709\u4f18\u70b9\uff0c\u5982\u5bf9\u6297\u5dee\u5206\u9690\u79c1\u566a\u58f0\u548cFedAvg\u805a\u5408\u7684\u9c81\u68d2\u6027\u3002 ", "motivation": "\u4f20\u7edf\u7684\u68af\u5ea6\u53cd\u8f6c\u653b\u51fb\u5728\u5904\u7406\u5b9e\u9645\u6570\u636e\u65f6\u9762\u4e34\u6311\u6218\uff0c\u800cSPEAR\u653b\u51fb\u867d\u5728\u7406\u8bba\u4e0a\u6709\u6240\u7a81\u7834\uff0c\u4f46\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u53d7\u9650\u4e8e\u6307\u6570\u7ea7\u7684\u65f6\u95f4\u590d\u6742\u5ea6\u3002\u4e3a\u6b64\uff0c\u7814\u7a76\u8005\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u653b\u51fb\u65b9\u6cd5SPEAR++\u3002 ", "method": "\u4f7f\u7528\u6765\u81ea\u7a00\u758f\u5b57\u5178\u5b66\u4e60\uff08Sparsely-Used Dictionary Learning\uff09\u7684\u6280\u672f\u6765\u89e3\u51b3\u7ebf\u6027\u5c42\u4e0eReLU\u6fc0\u6d3b\u51fd\u6570\u7684\u68af\u5ea6\u53cd\u8f6c\u95ee\u9898\uff0c\u4f7f\u4e4b\u53ef\u4ee5\u5728\u8f83\u5927\u7684\u6279\u91cf\u6570\u636e\u4e0a\u6709\u6548\u8fd0\u884c\u3002 ", "result": "\u65b0\u7684SPEAR++\u653b\u51fb\u65b9\u6cd5\u5728\u5927\u578b\u6279\u91cf\u6570\u636e\u4e0a\u53ef\u6709\u6548\u8fd0\u884c\uff0c\u5e76\u4e14\u4fdd\u6301\u4e86SPEAR\u7684\u6240\u6709\u4f18\u70b9\uff0c\u4f8b\u5982\u5728\u5dee\u5206\u9690\u79c1\u566a\u58f0\u548cFedAvg\u805a\u5408\u60c5\u666f\u4e0b\u7684\u9c81\u68d2\u6027\u3002 ", "conclusion": "\u901a\u8fc7\u5e94\u7528\u7a00\u758f\u5b57\u5178\u5b66\u4e60\u6280\u672f\uff0cSPEAR++\u6210\u529f\u514b\u670d\u4e86SPEAR\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u9650\u5236\uff0c\u63d0\u4f9b\u4e86\u66f4\u5f3a\u5927\u7684\u653b\u51fb\u624b\u6bb5\u4ee5\u6d4b\u8bd5\u8054\u90a6\u5b66\u4e60\u7cfb\u7edf\u7684\u9690\u79c1\u4fdd\u62a4\u80fd\u529b\u3002 "}}
{"id": "2510.24216", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.24216", "abs": "https://arxiv.org/abs/2510.24216", "authors": ["Fan Xu", "Hao Wu", "Kun Wang", "Nan Wang", "Qingsong Wen", "Xian Wu", "Wei Gong", "Xibin Zhao"], "title": "Unlocking Out-of-Distribution Generalization in Dynamics through Physics-Guided Augmentation", "comment": null, "summary": "In dynamical system modeling, traditional numerical methods are limited by\nhigh computational costs, while modern data-driven approaches struggle with\ndata scarcity and distribution shifts. To address these fundamental\nlimitations, we first propose SPARK, a physics-guided quantitative augmentation\nplugin. Specifically, SPARK utilizes a reconstruction autoencoder to integrate\nphysical parameters into a physics-rich discrete state dictionary. This state\ndictionary then acts as a structured dictionary of physical states, enabling\nthe creation of new, physically-plausible training samples via principled\ninterpolation in the latent space. Further, for downstream prediction, these\naugmented representations are seamlessly integrated with a Fourier-enhanced\nGraph ODE, a combination designed to robustly model the enriched data\ndistribution while capturing long-term temporal dependencies. Extensive\nexperiments on diverse benchmarks demonstrate that SPARK significantly\noutperforms state-of-the-art baselines, particularly in challenging\nout-of-distribution scenarios and data-scarce regimes, proving the efficacy of\nour physics-guided augmentation paradigm.", "AI": {"tldr": "\u6211\u4eec\u63d0\u51fa\u4e86SPARK\uff0c\u4e00\u79cd\u57fa\u4e8e\u7269\u7406\u7684\u589e\u5f3a\u63d2\u4ef6\uff0c\u5b83\u901a\u8fc7\u7269\u7406\u53c2\u6570\u7684\u81ea\u52a8\u7f16\u7801\u5668\u91cd\u5efa\u6765\u751f\u6210\u65b0\u7684\u8bad\u7ec3\u6837\u672c\uff0c\u5e76\u5c06\u5176\u4e0e\u589e\u5f3a\u5085\u91cc\u53f6\u56feODE\u7ed3\u5408\uff0c\u63d0\u9ad8\u4e86\u6a21\u578b\u5728\u5206\u5e03\u53d8\u5316\u548c\u6570\u636e\u7a00\u7f3a\u60c5\u51b5\u4e0b\u7684\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u6570\u503c\u65b9\u6cd5\u7684\u9ad8\u8ba1\u7b97\u6210\u672c\u548c\u73b0\u4ee3\u6570\u636e\u9a71\u52a8\u65b9\u6cd5\u7684\u6570\u636e\u7a00\u7f3a\u548c\u5206\u5e03\u53d8\u5316\u95ee\u9898\u662f\u52a8\u529b\u7cfb\u7edf\u5efa\u6a21\u7684\u9650\u5236\u3002\u4e3a\u4e86\u514b\u670d\u8fd9\u4e9b\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u7269\u7406\u5f15\u5bfc\u7684\u589e\u5f3a\u65b9\u6cd5SPARK\u3002", "method": "SPARK\u5229\u7528\u91cd\u5efa\u81ea\u52a8\u7f16\u7801\u5668\u5c06\u7269\u7406\u53c2\u6570\u96c6\u6210\u5230\u7269\u7406\u4e30\u5bcc\u7684\u79bb\u6563\u72b6\u6001\u5b57\u5178\u4e2d\uff0c\u751f\u6210\u65b0\u7684\u8bad\u7ec3\u6837\u672c\uff0c\u5e76\u4e0e\u589e\u5f3a\u5085\u91cc\u53f6\u56feODE\u7ed3\u5408\uff0c\u7528\u4e8e\u4e0b\u6e38\u9884\u6d4b\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cSPARK\u5728\u5206\u5e03\u53d8\u5316\u548c\u6570\u636e\u7a00\u7f3a\u60c5\u51b5\u4e0b\u663e\u8457\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "\u5b9e\u9a8c\u7ed3\u679c\u8bc1\u660e\u4e86\u57fa\u4e8e\u7269\u7406\u7684\u589e\u5f3a\u65b9\u6cd5SPARK\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2510.24217", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24217", "abs": "https://arxiv.org/abs/2510.24217", "authors": ["Alisher Turubayev", "Anna Shopova", "Fabian Lange", "Mahmut Kamalak", "Paul Mattes", "Victoria Ayvasky", "Bert Arnrich", "Bjarne Pfitzner", "Robin P. van de Water"], "title": "Closing Gaps: An Imputation Analysis of ICU Vital Signs", "comment": "Preprint", "summary": "As more Intensive Care Unit (ICU) data becomes available, the interest in\ndeveloping clinical prediction models to improve healthcare protocols\nincreases. However, the lack of data quality still hinders clinical prediction\nusing Machine Learning (ML). Many vital sign measurements, such as heart rate,\ncontain sizeable missing segments, leaving gaps in the data that could\nnegatively impact prediction performance. Previous works have introduced\nnumerous time-series imputation techniques. Nevertheless, more comprehensive\nwork is needed to compare a representative set of methods for imputing ICU\nvital signs and determine the best practice. In reality, ad-hoc imputation\ntechniques that could decrease prediction accuracy, like zero imputation, are\nstill used. In this work, we compare established imputation techniques to guide\nresearchers in improving the performance of clinical prediction models by\nselecting the most accurate imputation technique. We introduce an extensible\nand reusable benchmark with currently 15 imputation and 4 amputation methods,\ncreated for benchmarking on major ICU datasets. We hope to provide a\ncomparative basis and facilitate further ML development to bring more models\ninto clinical practice.", "AI": {"tldr": "\u672c\u6587\u901a\u8fc7\u6bd4\u8f8315\u79cd\u63d2\u8865\u65b9\u6cd5\u548c4\u79cd\u5220\u5931\u65b9\u6cd5\u5728ICU\u6570\u636e\u96c6\u4e0a\u7684\u8868\u73b0\uff0c\u4e3a\u63d0\u9ad8\u4e34\u5e8a\u9884\u6d4b\u6a21\u578b\u6027\u80fd\u63d0\u4f9b\u4e86\u6307\u5bfc\u548c\u57fa\u51c6\u6bd4\u8f83\u57fa\u7840\uff0c\u4fc3\u8fdb\u4e86\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u5728\u4e34\u5e8a\u5b9e\u8df5\u4e2d\u7684\u5e94\u7528\u3002", "motivation": "\u91cd\u75c7\u76d1\u62a4\u75c5\u623f\uff08ICU\uff09\u6570\u636e\u4e2d\u5b58\u5728\u5927\u91cf\u7684\u7f3a\u5931\u503c\uff0c\u8fd9\u9650\u5236\u4e86\u673a\u5668\u5b66\u4e60\uff08ML\uff09\u5728\u4e34\u5e8a\u9884\u6d4b\u4e2d\u7684\u5e94\u7528\u3002\u4e3a\u4e86\u63d0\u9ad8\u4e34\u5e8a\u9884\u6d4b\u6a21\u578b\u7684\u6027\u80fd\uff0c\u9700\u8981\u9009\u62e9\u6700\u4f73\u7684\u63d2\u8865\u6280\u672f\u6765\u586b\u8865\u7f3a\u5931\u6570\u636e\u3002\u5f53\u524d\u4ecd\u5b58\u5728\u4e00\u4e9b\u964d\u4f4e\u9884\u6d4b\u51c6\u786e\u5ea6\u7684\u624b\u52a8\u63d2\u8865\u65b9\u6cd5\uff0c\u56e0\u6b64\u9700\u8981\u6bd4\u8f83\u73b0\u6709\u7684\u63d2\u8865\u65b9\u6cd5\uff0c\u4ee5\u9274\u522b\u51fa\u6700\u4f73\u5b9e\u8df5\u3002", "method": "\u672c\u6587\u5f15\u5165\u4e86\u4e00\u79cd\u53ef\u6269\u5c55\u4e14\u53ef\u91cd\u590d\u4f7f\u7528\u7684\u57fa\u51c6\uff0c\u7528\u6765\u5728\u4e3b\u8981\u7684ICU\u6570\u636e\u96c6\u4e0a\u8bc4\u4f3015\u79cd\u63d2\u8865\u65b9\u6cd5\u548c4\u79cd\u5220\u5931\u65b9\u6cd5\u7684\u8868\u73b0\uff0c\u5e2e\u52a9\u7814\u7a76\u8005\u9009\u62e9\u6700\u4f18\u7684\u63d2\u8865\u6280\u672f\u3002", "result": "\u7814\u7a76\u8868\u660e\uff0c\u4e0d\u540c\u7684\u63d2\u8865\u65b9\u6cd5\u5728\u4e0d\u540c\u7684\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u51fa\u4e0d\u540c\u6027\u80fd\uff0c\u8fd9\u5c55\u793a\u4e86\u9009\u62e9\u6700\u5408\u9002\u63d2\u8865\u65b9\u6cd5\u7684\u91cd\u8981\u6027\u3002\u8fd9\u4e9b\u7ed3\u679c\u80fd\u591f\u5e2e\u52a9\u7814\u7a76\u8005\u63d0\u9ad8\u4e34\u5e8a\u9884\u6d4b\u6a21\u578b\u7684\u51c6\u786e\u6027\u3002", "conclusion": "\u672c\u7814\u7a76\u63d0\u4f9b\u4e86\u8be6\u7ec6\u7684\u63d2\u8865\u65b9\u6cd5\u6bd4\u8f83\u57fa\u51c6\uff0c\u6709\u52a9\u4e8e\u63d0\u5347\u673a\u5668\u5b66\u4e60\u5728\u4e34\u5e8a\u9884\u6d4b\u4e2d\u7684\u5e94\u7528\uff0c\u4fc3\u8fdb\u4e86\u6a21\u578b\u4ece\u5b9e\u9a8c\u5ba4\u5230\u4e34\u5e8a\u7684\u8f6c\u5316\u3002"}}
{"id": "2510.24233", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.24233", "abs": "https://arxiv.org/abs/2510.24233", "authors": ["Antoine Szatkownik", "Aur\u00e9lien Decelle", "Beatriz Seoane", "Nicolas Bereux", "L\u00e9o Planche", "Guillaume Charpiat", "Burak Yelmen", "Flora Jay", "Cyril Furtlehner"], "title": "PRIVET: Privacy Metric Based on Extreme Value Theory", "comment": null, "summary": "Deep generative models are often trained on sensitive data, such as genetic\nsequences, health data, or more broadly, any copyrighted, licensed or protected\ncontent. This raises critical concerns around privacy-preserving synthetic\ndata, and more specifically around privacy leakage, an issue closely tied to\noverfitting. Existing methods almost exclusively rely on global criteria to\nestimate the risk of privacy failure associated to a model, offering only\nquantitative non interpretable insights. The absence of rigorous evaluation\nmethods for data privacy at the sample-level may hinder the practical\ndeployment of synthetic data in real-world applications. Using extreme value\nstatistics on nearest-neighbor distances, we propose PRIVET, a generic\nsample-based, modality-agnostic algorithm that assigns an individual privacy\nleak score to each synthetic sample. We empirically demonstrate that PRIVET\nreliably detects instances of memorization and privacy leakage across diverse\ndata modalities, including settings with very high dimensionality, limited\nsample sizes such as genetic data and even under underfitting regimes. We\ncompare our method to existing approaches under controlled settings and show\nits advantage in providing both dataset level and sample level assessments\nthrough qualitative and quantitative outputs. Additionally, our analysis\nreveals limitations in existing computer vision embeddings to yield\nperceptually meaningful distances when identifying near-duplicate samples.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2510.24234", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.24234", "abs": "https://arxiv.org/abs/2510.24234", "authors": ["Ludovic Schwartz", "Hamish Flynn", "Gergely Neu"], "title": "Sparse Optimistic Information Directed Sampling", "comment": null, "summary": "Many high-dimensional online decision-making problems can be modeled as\nstochastic sparse linear bandits. Most existing algorithms are designed to\nachieve optimal worst-case regret in either the data-rich regime, where\npolynomial depen- dence on the ambient dimension is unavoidable, or the\ndata-poor regime, where dimension-independence is possible at the cost of worse\ndependence on the num- ber of rounds. In contrast, the sparse Information\nDirected Sampling (IDS) algo- rithm satisfies a Bayesian regret bound that has\nthe optimal rate in both regimes simultaneously. In this work, we explore the\nuse of Sparse Optimistic Informa- tion Directed Sampling (SOIDS) to achieve the\nsame adaptivity in the worst-case setting, without Bayesian assumptions.\nThrough a novel analysis that enables the use of a time-dependent learning\nrate, we show that SOIDS can optimally balance information and regret. Our\nresults extend the theoretical guarantees of IDS, pro- viding the first\nalgorithm that simultaneously achieves optimal worst-case regret in both the\ndata-rich and data-poor regimes. We empirically demonstrate the good\nperformance of SOIDS.", "AI": {"tldr": "\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u7b97\u6cd5Sparse Optimistic Information Directed Sampling (SOIDS)\uff0c\u53ef\u4ee5\u5728\u6570\u636e\u4e30\u5bcc\u548c\u6570\u636e\u8d2b\u4e4f\u7684\u73af\u5883\u4e2d\u540c\u65f6\u5b9e\u73b0\u6700\u4f18\u7684\u6700\u574f\u60c5\u51b5\u9057\u61be\u3002\u901a\u8fc7\u4f7f\u7528\u65f6\u95f4\u4f9d\u8d56\u7684\u5b66\u4e60\u7387\uff0cSOIDS\u80fd\u591f\u4f18\u5316\u4fe1\u606f\u548c\u9057\u61be\u4e4b\u95f4\u7684\u5e73\u8861\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660eSOIDS\u5177\u6709\u826f\u597d\u7684\u6027\u80fd\u3002", "motivation": "\u76ee\u524d\u5927\u591a\u6570\u7b97\u6cd5\u5728\u6570\u636e\u4e30\u5bcc\u7684\u60c5\u51b5\u4e0b\u80fd\u591f\u5b9e\u73b0\u6700\u4f18\u9057\u61be\uff0c\u5728\u6570\u636e\u8d2b\u4e4f\u7684\u60c5\u51b5\u4e0b\u5219\u8868\u73b0\u51fa\u6b21\u4f18\u7684\u9057\u61be\u3002\u672c\u6587\u63d0\u51faSOIDS\uff0c\u610f\u56fe\u5728\u6ca1\u6709\u8d1d\u53f6\u65af\u5047\u8bbe\u7684\u60c5\u51b5\u4e0b\u5b9e\u73b0\u540c\u65f6\u5728\u6570\u636e\u4e30\u5bcc\u548c\u6570\u636e\u8d2b\u4e4f\u4e24\u79cd\u73af\u5883\u4e0b\u5747\u6700\u4f18\u7684\u9057\u61be\u3002", "method": "\u5229\u7528\u65f6\u95f4\u4f9d\u8d56\u7684\u5b66\u4e60\u7387\uff0cSOIDS\u7b97\u6cd5\u53ef\u4ee5\u5b9e\u73b0\u5bf9\u4fe1\u606f\u548c\u9057\u61be\u7684\u4f18\u5316\u5e73\u8861\u3002\u8fd9\u79cd\u65b0\u7684\u5206\u6790\u65b9\u6cd5\u8fdb\u4e00\u6b65\u6269\u5c55\u4e86IDS\u7684\u7406\u8bba\u4fdd\u8bc1\uff0c\u63d0\u4f9b\u4e86\u4e00\u79cd\u65b0\u7684\u4f18\u5316\u7b97\u6cd5\u3002\u7814\u7a76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86SOIDS\u7684\u6709\u6548\u6027\u3002", "result": "SOIDS\u7b97\u6cd5\u6210\u529f\u5b9e\u73b0\u4e86\u5728\u6570\u636e\u4e30\u5bcc\u548c\u6570\u636e\u8d2b\u4e4f\u4e24\u79cd\u73af\u5883\u4e0b\u7684\u7406\u60f3\u9057\u61be\u4f18\u5316\uff0c\u5176\u6548\u679c\u901a\u8fc7\u5b9e\u9a8c\u5f97\u5230\u4e86\u9a8c\u8bc1\uff0c\u5b9e\u73b0\u4e86\u6700\u4f18\u7684\u6700\u574f\u60c5\u51b5\u9057\u61be\u3002\u8fd9\u4e00\u7ed3\u679c\u6269\u5c55\u4e86IDS\u7684\u7406\u8bba\u57fa\u7840\uff0c\u63d0\u4f9b\u4e86\u5728\u8d1d\u53f6\u65af\u5047\u8bbe\u4e4b\u5916\u65b0\u7684\u7406\u8bba\u652f\u6301\u3002", "conclusion": "\u901a\u8fc7\u5efa\u7acbSOIDS\u7b97\u6cd5\uff0c\u7814\u7a76\u5b9e\u73b0\u4e86\u5728\u9ad8\u7ef4\u5ea6\u7a00\u758f\u7ebf\u6027\u8d1d\u53f6\u65af\u95ee\u9898\u4e2d\u540c\u65f6\u5728\u6570\u636e\u4e30\u5bcc\u548c\u6570\u636e\u8d2b\u4e4f\u4e24\u79cd\u73af\u5883\u4e0b\u7684\u6700\u4f18\u9057\u61be\u4f18\u5316\u3002\u8fd9\u4e00\u6210\u679c\u5bf9\u4f18\u5316\u548c\u51b3\u7b56\u5236\u5b9a\u9886\u57df\u662f\u91cd\u8981\u7684\u8d21\u732e\u3002"}}
{"id": "2510.24235", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24235", "abs": "https://arxiv.org/abs/2510.24235", "authors": ["Ai Jian", "Jingqing Ruan", "Xing Ma", "Dailin Li", "QianLin Zhou", "Ke Zeng", "Xunliang Cai"], "title": "PaTaRM: Bridging Pairwise and Pointwise Signals via Preference-Aware Task-Adaptive Reward Modeling", "comment": null, "summary": "Reward models (RMs) are central to reinforcement learning from human feedback\n(RLHF), providing the critical supervision signals that align large language\nmodels (LLMs) with human preferences. While generative reward models (GRMs)\noffer greater interpretability than traditional scalar RMs, current training\nparadigms remain limited. Pair-wise methods rely on binary good-versus-bad\nlabels, which cause mismatches for point-wise inference and necessitate complex\npairing strategies for effective application in RLHF. On the other hand,\npoint-wise methods require more elaborate absolute labeling with rubric-driven\ncriteria, resulting in poor adaptability and high annotation costs. In this\nwork, we propose the Preference-Aware Task-Adaptive Reward Model (PaTaRM), a\nunified framework that integrates a preference-aware reward (PAR) mechanism\nwith dynamic rubric adaptation. PaTaRM leverages relative preference\ninformation from pairwise data to construct robust point-wise training signals,\neliminating the need for explicit point-wise labels. Simultaneously, it employs\na task-adaptive rubric system that flexibly generates evaluation criteria for\nboth global task consistency and instance-specific fine-grained reasoning. This\ndesign enables efficient, generalizable, and interpretable reward modeling for\nRLHF. Extensive experiments show that PaTaRM achieves an average relative\nimprovement of 4.7% on RewardBench and RMBench across Qwen3-8B and Qwen3-14B\nmodels. Furthermore, PaTaRM boosts downstream RLHF performance, with an average\nimprovement of 13.6% across IFEval and InFoBench benchmarks, confirming its\neffectiveness and robustness. Our code is available at\nhttps://github.com/JaneEyre0530/PaTaRM.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5956\u52b1\u6a21\u578bPaTaRM\uff0c\u7ed3\u5408\u4e86\u504f\u597d\u610f\u8bc6\u5956\u52b1\u673a\u5236\u548c\u52a8\u6001\u8bc4\u5206\u6807\u51c6\u9002\u5e94\u7cfb\u7edf\uff0c\u65e8\u5728\u901a\u8fc7\u76f8\u5bf9\u504f\u597d\u4fe1\u606f\u63d0\u9ad8\u5956\u52b1\u6a21\u578b\u7684\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u80fd\u529b\uff0c\u540c\u65f6\u51cf\u5c11\u6807\u7b7e\u9700\u6c42\uff0c\u63d0\u9ad8\u4e86RLHF\u7684\u6548\u679c\u548c\u6548\u7387", "motivation": "\u73b0\u6709\u7684\u5956\u52b1\u6a21\u578b\u5728\u76f4\u63a5\u76d1\u7763\u5b66\u4e60\u65f6\u5b58\u5728\u5c40\u9650\u6027\uff0c\u5305\u62ec\u751f\u6210\u578b\u6a21\u578b\u7f3a\u4e4f\u53ef\u89e3\u91ca\u6027\uff0c\u70b9\u5bf9\u70b9\u65b9\u6cd5\u9700\u8981\u590d\u6742\u7684\u914d\u5bf9\u7b56\u7565\u548c\u6602\u8d35\u7684\u6807\u7b7e\u6210\u672c\u3002\u9700\u8981\u4e00\u79cd\u7ed3\u5408\u4e24\u8005\u7684\u4f18\u70b9\uff0c\u66f4\u9ad8\u6548\u3001\u66f4\u6cdb\u5316\u7684\u6a21\u578b", "method": "PaTaRM\u901a\u8fc7\u76f8\u5bf9\u504f\u597d\u4fe1\u606f\u6784\u5efa\u70b9\u5bf9\u70b9\u8bad\u7ec3\u4fe1\u53f7\uff0c\u540c\u65f6\u91c7\u7528\u4efb\u52a1\u9002\u5e94\u6027\u8bc4\u5206\u7cfb\u7edf\u751f\u6210\u7ec6\u7c92\u5ea6\u8bc4\u4ef7\u6807\u51c6", "result": "\u5b9e\u9a8c\u8bc1\u660e\uff0cPaTaRM\u5728RewardBench\u548cRMBench\u4e0a\u5e73\u5747\u76f8\u5bf9\u63d0\u9ad8\u4e864.7%\uff0c\u5728\u4e0b\u6e38RLHF\u4efb\u52a1\u4e0a\u5e73\u5747\u63d0\u9ad8\u4e8613.6%\u3002\u8fd9\u8868\u660ePaTaRM\u662f\u6709\u6548\u7684\uff0c\u4e14\u5177\u6709\u9c81\u68d2\u6027", "conclusion": "PaTaRM\u63d0\u4f9b\u4e86\u4e00\u79cd\u65b0\u7684\u6846\u67b6\uff0c\u7ed3\u5408\u76f8\u5bf9\u504f\u597d\u4fe1\u606f\u4e0e\u52a8\u6001\u8bc4\u5206\u6807\u51c6\u9002\u5e94\uff0c\u4f7f\u5956\u52b1\u6a21\u578b\u5728RLHF\u4e2d\u66f4\u9ad8\u6548\u3001\u66f4\u6cdb\u5316\u3001\u66f4\u6709\u89e3\u91ca\u6027"}}
{"id": "2510.24240", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.24240", "abs": "https://arxiv.org/abs/2510.24240", "authors": ["Edward Markai", "Sina Molavipour"], "title": "Temporal Knowledge Graph Hyperedge Forecasting: Exploring Entity-to-Category Link Prediction", "comment": null, "summary": "Temporal Knowledge Graphs have emerged as a powerful way of not only modeling\nstatic relationships between entities but also the dynamics of how relations\nevolve over time. As these informational structures can be used to store\ninformation from a real-world setting, such as a news flow, predicting future\ngraph components to a certain extent equates predicting real-world events. Most\nof the research in this field focuses on embedding-based methods, often\nleveraging convolutional neural net architectures. These solutions act as black\nboxes, limiting insight. In this paper, we explore an extension to an\nestablished rule-based framework, TLogic, that yields a high accuracy in\ncombination with explainable predictions. This offers transparency and allows\nthe end-user to critically evaluate the rules applied at the end of the\nprediction stage. The new rule format incorporates entity category as a key\ncomponent with the purpose of limiting rule application only to relevant\nentities. When categories are unknown for building the graph, we propose a\ndata-driven method to generate them with an LLM-based approach. Additionally,\nwe investigate the choice of aggregation method for scores of retrieved\nentities when performing category prediction.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2510.24273", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.24273", "abs": "https://arxiv.org/abs/2510.24273", "authors": ["Junlin Mu", "Hantao Huang", "Jihang Zhang", "Minghui Yu", "Tao Wang", "Yidong Li"], "title": "SALS: Sparse Attention in Latent Space for KV cache Compression", "comment": null, "summary": "Large Language Models capable of handling extended contexts are in high\ndemand, yet their inference remains challenging due to substantial Key-Value\ncache size and high memory bandwidth requirements. Previous research has\ndemonstrated that KV cache exhibits low-rank characteristics within the hidden\ndimension, suggesting the potential for effective compression. However, due to\nthe widely adopted Rotary Position Embedding mechanism in modern LLMs, naive\nlow-rank compression suffers severe accuracy degradation or creates a new speed\nbottleneck, as the low-rank cache must first be reconstructed in order to apply\nRoPE. In this paper, we introduce two key insights: first, the application of\nRoPE to the key vectors increases their variance, which in turn results in a\nhigher rank; second, after the key vectors are transformed into the latent\nspace, they largely maintain their representation across most layers. Based on\nthese insights, we propose the Sparse Attention in Latent Space framework. SALS\nprojects the KV cache into a compact latent space via low-rank projection, and\nperforms sparse token selection using RoPE-free query-key interactions in this\nspace. By reconstructing only a small subset of important tokens, it avoids the\noverhead of full KV cache reconstruction. We comprehensively evaluate SALS on\nvarious tasks using two large-scale models: LLaMA2-7b-chat and Mistral-7b, and\nadditionally verify its scalability on the RULER-128k benchmark with\nLLaMA3.1-8B-Instruct. Experimental results demonstrate that SALS achieves SOTA\nperformance by maintaining competitive accuracy. Under different settings, SALS\nachieves 6.4-fold KV cache compression and 5.7-fold speed-up in the attention\noperator compared to FlashAttention2 on the 4K sequence. For the end-to-end\nthroughput performance, we achieves 1.4-fold and 4.5-fold improvement compared\nto GPT-fast on 4k and 32K sequences, respectively.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aSparse Attention in Latent Space (SALS) \u7684\u6846\u67b6\uff0c\u901a\u8fc7\u5728\u9690\u7a7a\u95f4\u4e2d\u8fdb\u884c\u4f4e\u79e9\u6295\u5f71\u548c\u7a00\u758f\u6807\u8bb0\u9009\u62e9\uff0c\u4ee5\u514b\u670d\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e2d\u7684KV\u7f13\u5b58\u538b\u7f29\u548c\u901f\u5ea6\u74f6\u9888\u95ee\u9898\u3002\u8be5\u65b9\u6cd5\u5728\u591a\u4e2a\u5927\u578b\u6a21\u578b\u548c\u4efb\u52a1\u4e0a\u663e\u793a\u51fa\u4f18\u5f02\u7684\u6027\u80fd\uff0c\u76f8\u6bd4FlashAttention2\u5728\u7f13\u5b58\u538b\u7f29\u548c\u6ce8\u610f\u529b\u64cd\u4f5c\u52a0\u901f\u65b9\u9762\u6709\u663e\u8457\u4f18\u52bf\uff0c\u540c\u65f6\u7ef4\u6301\u4e86\u8f83\u9ad8\u7684\u51c6\u786e\u6027\u3002", "motivation": "\u4e3a\u4e86\u5e94\u5bf9\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e2dKV\u7f13\u5b58\u5927\u5c0f\u548c\u5185\u5b58\u5e26\u5bbd\u9700\u6c42\u9ad8\u7684\u6311\u6218\uff0c\u540c\u65f6\u4fdd\u6301\u51c6\u786e\u6027\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u538b\u7f29\u7b56\u7565\u3002\u8be5\u7b56\u7565\u514b\u670d\u4e86\u4f20\u7edf\u7684\u4f4e\u79e9\u538b\u7f29\u5728\u73b0\u4ee3LLMs\u4e2d\u5e94\u7528\u65f6\u9020\u6210\u7684\u964d\u51c6\u95ee\u9898\uff0c\u5c24\u5176\u662f\u5728\u4f7f\u7528RoPE\u673a\u5236\u65f6\u3002\u901a\u8fc7\u65b0\u7684\u65b9\u6cd5\uff0c\u5728\u4e0d\u5f71\u54cd\u6a21\u578b\u8868\u73b0\u7684\u524d\u63d0\u4e0b\uff0c\u8fdb\u4e00\u6b65\u51cf\u5c0f\u5185\u5b58\u4f7f\u7528\u5e76\u52a0\u901f\u63a8\u7406\u8fc7\u7a0b\u3002", "method": "\u63d0\u51fa\u4e86Sparse Attention in Latent Space\uff08SALS\uff09\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u901a\u8fc7\u4f4e\u79e9\u6295\u5f71\u5c06KV\u7f13\u5b58\u538b\u7f29\u8fdb\u7d27\u51d1\u7684\u9690\u7a7a\u95f4\uff0c\u5e76\u4f7f\u7528RoPE\u514d\u67e5\u8be2-\u952e\u4ea4\u4e92\u8fdb\u884c\u7a00\u758f\u6807\u8bb0\u9009\u62e9\u3002\u8fd9\u79cd\u65b9\u6cd5\u901a\u8fc7\u91cd\u5efa\u91cd\u8981\u6807\u8bb0\u7684\u5173\u952e\u503c\uff0c\u907f\u514d\u4e86\u5b8c\u6574KV\u7f13\u5b58\u91cd\u5efa\u7684\u5f00\u9500\uff0c\u4ece\u800c\u5b9e\u73b0KV\u7f13\u5b58\u538b\u7f29\u548c\u6ce8\u610f\u529b\u64cd\u4f5c\u52a0\u901f\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0cSALS\u5728LLaMA2-7b-chat\u548cMistral-7b\u4e0a\u53d6\u5f97\u4e86\u4f18\u5f02\u7684\u7ed3\u679c\uff0c\u5c24\u5176\u662f\u5728\u5e8f\u5217\u957f\u5ea64K\u7684\u60c5\u51b5\u4e0b\uff0c\u4e0eFlashAttention2\u76f8\u6bd4\uff0c\u7f13\u5b58\u538b\u7f29\u6bd4\u4e3a6.4\uff0c\u6ce8\u610f\u529b\u64cd\u4f5c\u52a0\u901f\u6bd4\u4e3a5.7\u3002\u57284K\u548c32K\u5e8f\u5217\u957f\u5ea6\u60c5\u51b5\u4e0b\uff0c\u4e0eGPT-fast\u76f8\u6bd4\uff0c\u6574\u4f53\u541e\u5410\u91cf\u6027\u80fd\u5206\u522b\u63d0\u9ad8\u4e861.4\u500d\u548c4.5\u500d\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u8f83\u9ad8\u7684\u51c6\u786e\u6027\u3002", "conclusion": "SALS\u901a\u8fc7\u5728\u9690\u7a7a\u95f4\u4e2d\u8fdb\u884c\u4f4e\u79e9\u6295\u5f71\u548cRoPE\u514d\u67e5\u8be2-\u952e\u4ea4\u4e92\uff0c\u6709\u6548\u5b9e\u73b0\u4e86KV\u7f13\u5b58\u7684\u538b\u7f29\u548c\u6ce8\u610f\u529b\u64cd\u4f5c\u7684\u52a0\u901f\uff0c\u63d0\u9ad8\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u5185\u5b58\u6548\u7387\u548c\u63a8\u7406\u901f\u5ea6\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u9ad8\u7684\u51c6\u786e\u7387\u3002"}}
{"id": "2510.24310", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.24310", "abs": "https://arxiv.org/abs/2510.24310", "authors": ["Guus Toussaint", "Arno Knobbe"], "title": "EDC: Equation Discovery for Classification", "comment": "This preprint has not undergone peer review or any post-submission\n  improvements or corrections. The Version of Record of this contribution is\n  published in Lecture Notes in Computer Science, and is available online at\n  https://doi.org/10.1007/978-3-032-05461-6_9", "summary": "Equation Discovery techniques have shown considerable success in regression\ntasks, where they are used to discover concise and interpretable models\n(\\textit{Symbolic Regression}). In this paper, we propose a new ED-based binary\nclassification framework. Our proposed method EDC finds analytical functions of\nmanageable size that specify the location and shape of the decision boundary.\nIn extensive experiments on artificial and real-life data, we demonstrate how\nEDC is able to discover both the structure of the target equation as well as\nthe value of its parameters, outperforming the current state-of-the-art\nED-based classification methods in binary classification and achieving\nperformance comparable to the state of the art in binary classification. We\nsuggest a grammar of modest complexity that appears to work well on the tested\ndatasets but argue that the exact grammar -- and thus the complexity of the\nmodels -- is configurable, and especially domain-specific expressions can be\nincluded in the pattern language, where that is required. The presented grammar\nconsists of a series of summands (additive terms) that include linear,\nquadratic and exponential terms, as well as products of two features (producing\nhyperbolic curves ideal for capturing XOR-like dependencies). The experiments\ndemonstrate that this grammar allows fairly flexible decision boundaries while\nnot so rich to cause overfitting.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2510.24318", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24318", "abs": "https://arxiv.org/abs/2510.24318", "authors": ["Prajit Bhaskaran", "Tom Viering"], "title": "Transformers can do Bayesian Clustering", "comment": null, "summary": "Bayesian clustering accounts for uncertainty but is computationally demanding\nat scale. Furthermore, real-world datasets often contain missing values, and\nsimple imputation ignores the associated uncertainty, resulting in suboptimal\nresults. We present Cluster-PFN, a Transformer-based model that extends\nPrior-Data Fitted Networks (PFNs) to unsupervised Bayesian clustering. Trained\nentirely on synthetic datasets generated from a finite Gaussian Mixture Model\n(GMM) prior, Cluster-PFN learns to estimate the posterior distribution over\nboth the number of clusters and the cluster assignments. Our method estimates\nthe number of clusters more accurately than handcrafted model selection\nprocedures such as AIC, BIC and Variational Inference (VI), and achieves\nclustering quality competitive with VI while being orders of magnitude faster.\nCluster-PFN can be trained on complex priors that include missing data,\noutperforming imputation-based baselines on real-world genomic datasets, at\nhigh missingness. These results show that the Cluster-PFN can provide scalable\nand flexible Bayesian clustering.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eTransformer\u7684\u6a21\u578bCluster-PFN\uff0c\u7528\u4e8e\u65e0\u76d1\u7763\u7684\u8d1d\u53f6\u65af\u805a\u7c7b\uff0c\u8be5\u6a21\u578b\u80fd\u591f\u5904\u7406\u5305\u542b\u7f3a\u5931\u503c\u7684\u6570\u636e\u96c6\uff0c\u5e76\u5728\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002Cluster-PFN\u5728\u4f30\u8ba1\u805a\u7c7b\u6570\u91cf\u7684\u51c6\u786e\u6027\u4e0a\u8d85\u8d8a\u4e86\u624b\u5de5\u6a21\u578b\u9009\u62e9\u65b9\u6cd5\uff0c\u5e76\u4e14\u5728\u901f\u5ea6\u4e0a\u4e5f\u66f4\u5feb\u3002", "motivation": "\u4f20\u7edf\u8d1d\u53f6\u65af\u805a\u7c7b\u65b9\u6cd5\u8ba1\u7b97\u91cf\u5927\uff0c\u96be\u4ee5\u5904\u7406\u5927\u89c4\u6a21\u6570\u636e\u96c6\u3002\u6b64\u5916\uff0c\u5b9e\u9645\u6570\u636e\u96c6\u901a\u5e38\u5305\u542b\u7f3a\u5931\u503c\uff0c\u7b80\u5355\u7684\u6570\u636e\u586b\u8865\u65b9\u6cd5\u5ffd\u7565\u4e86\u8fd9\u79cd\u4e0d\u786e\u5b9a\u6027\uff0c\u53ef\u80fd\u5bfc\u81f4\u6b21\u4f18\u7ed3\u679c\u3002\u56e0\u6b64\uff0c\u8feb\u5207\u9700\u8981\u4e00\u79cd\u65e2\u80fd\u591f\u5904\u7406\u7f3a\u5931\u503c\uff0c\u53c8\u5177\u5907\u9ad8\u6548\u7387\u548c\u51c6\u786e\u6027\u7684\u805a\u7c7b\u65b9\u6cd5\u3002", "method": "\u672c\u6587\u63d0\u51fa\u7684Cluster-PFN\u6a21\u578b\u57fa\u4e8eTransformer\u67b6\u6784\uff0c\u5e76\u901a\u8fc7\u4ece\u6709\u9650\u9ad8\u65af\u6df7\u5408\u6a21\u578b\u751f\u6210\u7684\u5408\u6210\u6570\u636e\u96c6\u8fdb\u884c\u8bad\u7ec3\uff0c\u4ece\u800c\u5b66\u4f1a\u4e86\u4f30\u8ba1\u5173\u4e8e\u805a\u7c7b\u6570\u91cf\u548c\u805a\u7c7b\u5206\u914d\u7684\u540e\u9a8c\u5206\u5e03\u3002\u8be5\u6a21\u578b\u80fd\u5728\u590d\u6742\u5148\u9a8c\u4e2d\u8fdb\u884c\u8bad\u7ec3\uff0c\u5305\u62ec\u5177\u6709\u7f3a\u5931\u6570\u636e\u7684\u60c5\u51b5\u3002", "result": "Cluster-PFN\u5728\u4f30\u8ba1\u805a\u7c7b\u6570\u91cf\u4e0a\u7684\u51c6\u786e\u6027\u4e0a\u8d85\u8d8a\u4e86AIC\u3001BIC\u548c\u53d8\u5206\u63a8\u65ad\u7b49\u4f20\u7edf\u6a21\u578b\u9009\u62e9\u65b9\u6cd5\uff0c\u5e76\u4e14\u5176\u805a\u7c7b\u8d28\u91cf\u4e0e\u53d8\u5206\u63a8\u65ad\u76f8\u5f53\uff0c\u4f46\u901f\u5ea6\u8981\u5feb\u5f97\u591a\u3002\u5728\u771f\u5b9e\u57fa\u56e0\u7ec4\u6570\u636e\u96c6\u4e0a\uff0cCluster-PFN\u5728\u9ad8\u7f3a\u5931\u503c\u7684\u6570\u636e\u4e0a\u8868\u73b0\u4f18\u4e8e\u57fa\u4e8e\u586b\u8865\u7684\u57fa\u672c\u7ebf\u65b9\u6cd5\u3002", "conclusion": "\u8fd9\u4e9b\u7ed3\u679c\u8bf4\u660e\u4e86Cluster-PFN\u80fd\u591f\u63d0\u4f9b\u53ef\u6269\u5c55\u548c\u7075\u6d3b\u7684\u8d1d\u53f6\u65af\u805a\u7c7b\u65b9\u6cd5\u3002"}}
{"id": "2510.24356", "categories": ["cs.LG", "cs.AI", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.24356", "abs": "https://arxiv.org/abs/2510.24356", "authors": ["Suman Sanyal"], "title": "Perception Learning: A Formal Separation of Sensory Representation Learning from Decision Learning", "comment": null, "summary": "We introduce Perception Learning (PeL), a paradigm that optimizes an agent's\nsensory interface $f_\\phi:\\mathcal{X}\\to\\mathcal{Z}$ using task-agnostic\nsignals, decoupled from downstream decision learning\n$g_\\theta:\\mathcal{Z}\\to\\mathcal{Y}$. PeL directly targets label-free\nperceptual properties, such as stability to nuisances, informativeness without\ncollapse, and controlled geometry, assessed via objective\nrepresentation-invariant metrics. We formalize the separation of perception and\ndecision, define perceptual properties independent of objectives or\nreparameterizations, and prove that PeL updates preserving sufficient\ninvariants are orthogonal to Bayes task-risk gradients. Additionally, we\nprovide a suite of task-agnostic evaluation metrics to certify perceptual\nquality.", "AI": {"tldr": "\u6211\u4eec\u5f15\u5165\u4e86\u611f\u77e5\u5b66\u4e60\uff08PeL\uff09\u8303\u5f0f\uff0c\u8be5\u8303\u5f0f\u901a\u8fc7\u4efb\u52a1\u65e0\u5173\u7684\u4fe1\u53f7\u6765\u4f18\u5316\u667a\u80fd\u4f53\u7684\u611f\u5b98\u63a5\u53e3\uff0c\u4f7f\u5176\u7a33\u5b9a\u4e14\u5177\u4fe1\u606f\u91cf\uff0c\u5e76\u4e0d\u53d7\u4e0b\u6e38\u51b3\u7b56\u5b66\u4e60\u7684\u5f71\u54cd\u3002\u6211\u4eec\u8fd8\u63d0\u4f9b\u4e86\u7528\u4e8e\u8bc4\u4f30\u611f\u77e5\u8d28\u91cf\u7684\u4efb\u52a1\u65e0\u5173\u7684\u8bc4\u4ef7\u6307\u6807\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u901a\u5e38\u5c06\u611f\u77e5\u5b66\u4e60\u4e0e\u51b3\u7b56\u5b66\u4e60\u8026\u5408\u5728\u4e00\u8d77\uff0c\u8fd9\u9650\u5236\u4e86\u611f\u77e5\u5b66\u4e60\u7684\u8868\u73b0\u3002\u4e3a\u6b64\uff0c\u6211\u4eec\u5e0c\u671b\u5c06\u611f\u77e5\u4ece\u51b3\u7b56\u4e2d\u89e3\u8026\u51fa\u6765\uff0c\u4f7f\u5176\u80fd\u591f\u72ec\u7acb\u4e8e\u4efb\u52a1\u8fdb\u884c\u4f18\u5316\uff0c\u4ece\u800c\u83b7\u5f97\u66f4\u597d\u7684\u611f\u77e5\u6027\u80fd\u3002", "method": "PeL\u8303\u5f0f\u901a\u8fc7\u4efb\u52a1\u65e0\u5173\u7684\u4fe1\u53f7\u6765\u76f4\u63a5\u4f18\u5316\u611f\u77e5\u63a5\u53e3\uff0c\u800c\u4e0d\u9700\u8981\u4e0b\u6e38\u4efb\u52a1\u7684\u53cd\u9988\u3002\u6211\u4eec\u5b9a\u4e49\u4e86\u4e00\u7cfb\u5217\u611f\u77e5\u5c5e\u6027\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u4f18\u5316\u65b9\u6cd5\uff0c\u8fd9\u79cd\u65b9\u6cd5\u53ef\u4ee5\u4fdd\u6301\u8db3\u591f\u7684\u4e0d\u53d8\u6027\uff0c\u4ece\u800c\u4e0e\u8d1d\u53f6\u65af\u98ce\u9669\u68af\u5ea6\u6b63\u4ea4\u3002\u53e6\u5916\uff0c\u6211\u4eec\u8fd8\u63d0\u4f9b\u4e86\u4e00\u5957\u4efb\u52a1\u65e0\u5173\u7684\u8bc4\u4ef7\u6307\u6807\u6765\u8bc4\u4f30\u611f\u77e5\u8d28\u91cf\u3002 ", "result": "\u6211\u4eec\u7684\u65b9\u6cd5\u53ef\u4ee5\u72ec\u7acb\u5730\u4f18\u5316\u611f\u77e5\u63a5\u53e3\uff0c\u4ece\u800c\u63d0\u9ad8\u611f\u77e5\u8d28\u91cf\u548c\u9c81\u68d2\u6027\u3002\u540c\u65f6\uff0c\u901a\u8fc7\u4efb\u52a1\u65e0\u5173\u7684\u8bc4\u4ef7\u6307\u6807\uff0c\u6211\u4eec\u53ef\u4ee5\u6709\u6548\u5730\u8bc4\u4f30\u611f\u77e5\u63a5\u53e3\u7684\u6027\u80fd\u3002", "conclusion": "PeL\u8303\u5f0f\u4e3a\u4f18\u5316\u611f\u77e5\u63a5\u53e3\u63d0\u4f9b\u4e86\u4e00\u79cd\u65b0\u7684\u9014\u5f84\uff0c\u5b83\u53ef\u4ee5\u72ec\u7acb\u5730\u4f18\u5316\u611f\u77e5\uff0c\u800c\u4e0d\u9700\u8981\u77e5\u9053\u5177\u4f53\u4efb\u52a1\u7684\u7ec6\u8282\u80fd\u3002"}}
{"id": "2510.24368", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.24368", "abs": "https://arxiv.org/abs/2510.24368", "authors": ["Maria Gabriela Valeriano", "David Kohan Marzag\u00e3o", "Alfredo Montelongo", "Carlos Roberto Veiga Kiffer", "Natan Katz", "Ana Carolina Lorena"], "title": "Filtering instances and rejecting predictions to obtain reliable models in healthcare", "comment": "This paper is under review at Machine Learning (Springer)", "summary": "Machine Learning (ML) models are widely used in high-stakes domains such as\nhealthcare, where the reliability of predictions is critical. However, these\nmodels often fail to account for uncertainty, providing predictions even with\nlow confidence. This work proposes a novel two-step data-centric approach to\nenhance the performance of ML models by improving data quality and filtering\nlow-confidence predictions. The first step involves leveraging Instance\nHardness (IH) to filter problematic instances during training, thereby refining\nthe dataset. The second step introduces a confidence-based rejection mechanism\nduring inference, ensuring that only reliable predictions are retained. We\nevaluate our approach using three real-world healthcare datasets, demonstrating\nits effectiveness at improving model reliability while balancing predictive\nperformance and rejection rate. Additionally, we use alternative criteria -\ninfluence values for filtering and uncertainty for rejection - as baselines to\nevaluate the efficiency of the proposed method. The results demonstrate that\nintegrating IH filtering with confidence-based rejection effectively enhances\nmodel performance while preserving a large proportion of instances. This\napproach provides a practical method for deploying ML systems in\nsafety-critical applications.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2510.24375", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.24375", "abs": "https://arxiv.org/abs/2510.24375", "authors": ["Yuanyuan Wu", "Zhenlin Qin", "Zhenliang Ma"], "title": "A Comprehensive Evaluation Framework for Synthetic Trip Data Generation in Public Transport", "comment": null, "summary": "Synthetic data offers a promising solution to the privacy and accessibility\nchallenges of using smart card data in public transport research. Despite rapid\nprogress in generative modeling, there is limited attention to comprehensive\nevaluation, leaving unclear how reliable, safe, and useful synthetic data truly\nare. Existing evaluations remain fragmented, typically limited to\npopulation-level representativeness or record-level privacy, without\nconsidering group-level variations or task-specific utility. To address this\ngap, we propose a Representativeness-Privacy-Utility (RPU) framework that\nsystematically evaluates synthetic trip data across three complementary\ndimensions and three hierarchical levels (record, group, population). The\nframework integrates a consistent set of metrics to quantify similarity,\ndisclosure risk, and practical usefulness, enabling transparent and balanced\nassessment of synthetic data quality. We apply the framework to benchmark\ntwelve representative generation methods, spanning conventional statistical\nmodels, deep generative networks, and privacy-enhanced variants. Results show\nthat synthetic data do not inherently guarantee privacy and there is no\n\"one-size-fits-all\" model, the trade-off between privacy and\nrepresentativeness/utility is obvious. Conditional Tabular generative\nadversarial network (CTGAN) provide the most balanced trade-off and is\nsuggested for practical applications. The RPU framework provides a systematic\nand reproducible basis for researchers and practitioners to compare synthetic\ndata generation techniques and select appropriate methods in public transport\napplications.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2510.24432", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.24432", "abs": "https://arxiv.org/abs/2510.24432", "authors": ["Seyed Mahdi Basiri Azad", "Joschka Boedecker"], "title": "Fill in the Blanks: Accelerating Q-Learning with a Handful of Demonstrations in Sparse Reward Settings", "comment": null, "summary": "Reinforcement learning (RL) in sparse-reward environments remains a\nsignificant challenge due to the lack of informative feedback. We propose a\nsimple yet effective method that uses a small number of successful\ndemonstrations to initialize the value function of an RL agent. By precomputing\nvalue estimates from offline demonstrations and using them as targets for early\nlearning, our approach provides the agent with a useful prior over promising\nactions. The agent then refines these estimates through standard online\ninteraction. This hybrid offline-to-online paradigm significantly reduces the\nexploration burden and improves sample efficiency in sparse-reward settings.\nExperiments on benchmark tasks demonstrate that our method accelerates\nconvergence and outperforms standard baselines, even with minimal or suboptimal\ndemonstration data.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5728\u7a00\u758f\u5956\u52b1\u73af\u5883\u4e2d\uff0c\u901a\u8fc7\u4f7f\u7528\u5c11\u91cf\u6210\u529f\u7684\u6f14\u793a\u6765\u521d\u59cb\u5316\u4ef7\u503c\u51fd\u6570\uff0c\u4ece\u800c\u63d0\u9ad8\u5f3a\u5316\u5b66\u4e60\u6548\u7387\u7684\u65b9\u6cd5\u3002\u8be5\u65b9\u6cd5\u51cf\u8f7b\u4e86\u63a2\u7d22\u8d1f\u62c5\u5e76\u63d0\u9ad8\u4e86\u6837\u672c\u6548\u7387\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\u8be5\u65b9\u6cd5\u52a0\u901f\u4e86\u6536\u655b\u901f\u5ea6\uff0c\u5e76\u4f18\u4e8e\u6807\u51c6\u57fa\u51c6\u7ebf\u65b9\u6cd5\uff0c\u5373\u4fbf\u662f\u4f7f\u7528\u6700\u5c11\u7684\u6216\u6b21\u4f18\u7684\u6f14\u793a\u6570\u636e\u4e5f\u80fd\u53d6\u5f97\u597d\u7ed3\u679c\u3002", "motivation": "\u7a00\u758f\u5956\u52b1\u73af\u5883\u4e0b\u7684\u5f3a\u5316\u5b66\u4e60\u5177\u6709\u6311\u6218\u6027\uff0c\u56e0\u4e3a\u7f3a\u4e4f\u8be6\u7ec6\u7684\u53cd\u9988\u4fe1\u606f\u3002\u4e3a\u4e86\u89e3\u51b3\u8be5\u95ee\u9898\uff0c\u8be5\u6587\u65e8\u5728\u5229\u7528\u5c11\u91cf\u6210\u529f\u7684\u6f14\u793a\u6570\u636e\u6765\u6539\u8fdb\u5f3a\u5316\u5b66\u4e60\u8fc7\u7a0b\uff0c\u4ece\u800c\u63d0\u5347\u5b66\u4e60\u6548\u7387\u548c\u51cf\u5c11\u63a2\u7d22\u671f\u3002", "method": "\u65b9\u6cd5\u4e3b\u8981\u5206\u4e3a\u4e24\u4e2a\u9636\u6bb5\uff1a\u521d\u59cb\u5316\u9636\u6bb5\uff0c\u901a\u8fc7\u79bb\u7ebf\u8ba1\u7b97\u6210\u529f\u7684\u6f14\u793a\u6570\u636e\u7684\u4ef7\u503c\u4f30\u8ba1\u4f5c\u4e3a\u5728\u7ebf\u5b66\u4e60\u7684\u76ee\u6807\uff1b\u4f18\u5316\u9636\u6bb5\uff0c\u901a\u8fc7\u5728\u7ebf\u4e92\u52a8\u8fdb\u4e00\u6b65\u4f18\u5316\u8fd9\u4e9b\u9884\u4f30\u503c\uff0c\u5f62\u6210\u6df7\u5408\u7684\u79bb\u7ebf\u5230\u5728\u7ebf\u7684\u5b66\u4e60\u673a\u5236\u3002", "result": "\u5728\u6b64\u79cd\u7a00\u758f\u5956\u52b1\u7684\u73af\u5883\u4e0b\uff0c\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u80fd\u591f\u663e\u8457\u52a0\u901f\u6536\u655b\u901f\u5ea6\u5e76\u8d85\u8d8a\u6807\u51c6\u57fa\u51c6\u6280\u672f\uff0c\u5373\u4f7f\u662f\u5728\u6f14\u793a\u6570\u636e\u5f88\u5c11\u6216\u6b21\u4f18\u7684\u60c5\u51b5\u4e0b\u4e5f\u8868\u73b0\u51fa\u8272\u3002", "conclusion": "\u8be5\u7814\u7a76\u7684\u7ed3\u679c\u5c55\u793a\u4e86\u901a\u8fc7\u5e26\u6709\u4ef7\u503c\u51fd\u6570\u521d\u59cb\u5316\u7684\u6210\u529f\u6f14\u793a\u6570\u636e\u6765\u63d0\u5347\u7a00\u758f\u5956\u52b1\u73af\u5883\u4e0b\u5f3a\u5316\u5b66\u4e60\u6027\u80fd\u7684\u6f5c\u529b\uff0c\u8fd9\u79cd\u65b9\u6cd5\u53ef\u80fd\u4e3a\u89e3\u51b3\u7c7b\u4f3c\u95ee\u9898\u63d0\u4f9b\u65b0\u7684\u65b9\u6848\u3002"}}
{"id": "2510.24473", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.24473", "abs": "https://arxiv.org/abs/2510.24473", "authors": ["Lucas Buk Cardoso", "Simone Aldrey Angelo", "Yasmin Pacheco Gil Bonilha", "Fernando Maia", "Adeylson Guimar\u00e3es Ribeiro", "Maria Paula Curado", "Gisele Aparecida Fernandes", "Vanderlei Cunha Parro", "Fl\u00e1vio Almeida de Magalh\u00e3es Cipparrone", "Alexandre Dias Porto Chiavegatto Filho", "Tatiana Natasha Toporcov"], "title": "Methodology for Comparing Machine Learning Algorithms for Survival Analysis", "comment": null, "summary": "This study presents a comparative methodological analysis of six machine\nlearning models for survival analysis (MLSA). Using data from nearly 45,000\ncolorectal cancer patients in the Hospital-Based Cancer Registries of S\\~ao\nPaulo, we evaluated Random Survival Forest (RSF), Gradient Boosting for\nSurvival Analysis (GBSA), Survival SVM (SSVM), XGBoost-Cox (XGB-Cox),\nXGBoost-AFT (XGB-AFT), and LightGBM (LGBM), capable of predicting survival\nconsidering censored data. Hyperparameter optimization was performed with\ndifferent samplers, and model performance was assessed using the Concordance\nIndex (C-Index), C-Index IPCW, time-dependent AUC, and Integrated Brier Score\n(IBS). Survival curves produced by the models were compared with predictions\nfrom classification algorithms, and predictor interpretation was conducted\nusing SHAP and permutation importance. XGB-AFT achieved the best performance\n(C-Index = 0.7618; IPCW = 0.7532), followed by GBSA and RSF. The results\nhighlight the potential and applicability of MLSA to improve survival\nprediction and support decision making.", "AI": {"tldr": "\u672c\u7814\u7a76\u5bf9\u6bd4\u5206\u6790\u4e86\u516d\u79cd\u673a\u5668\u5b66\u4e60\u6a21\u578b\u5728\u751f\u5b58\u5206\u6790\u4e2d\u7684\u8868\u73b0\uff0c\u5176\u4e2dXGBoost-AFT\u6a21\u578b\u8868\u73b0\u6700\u4f73\uff0c\u663e\u793a\u51faMLSA\u5728\u63d0\u9ad8\u751f\u5b58\u9884\u6d4b\u548c\u8f85\u52a9\u51b3\u7b56\u65b9\u9762\u7684\u80fd\u529b\u548c\u9002\u7528\u6027\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u5728\u4e8e\u8bc4\u4f30\u4e0d\u540c\u673a\u5668\u5b66\u4e60\u6a21\u578b\u5728\u751f\u5b58\u5206\u6790\u4e2d\u7684\u9884\u6d4b\u6027\u80fd\uff0c\u7279\u522b\u662f\u5728\u5904\u7406\u5220\u5931\u6570\u636e\u65f6\u7684\u8868\u73b0\uff0c\u5e0c\u671b\u627e\u5230\u9002\u7528\u4e8e\u63d0\u9ad8\u751f\u5b58\u9884\u6d4b\u51c6\u786e\u6027\u7684\u6a21\u578b\u6765\u8f85\u52a9\u4e34\u5e8a\u51b3\u7b56\u3002", "method": "\u7814\u7a76\u6bd4\u8f83\u4e86\u516d\u79cd\u673a\u5668\u5b66\u4e60\u6a21\u578b\uff1a\u968f\u673a\u751f\u5b58\u68ee\u6797\u3001\u68af\u5ea6\u63d0\u5347\u751f\u5b58\u5206\u6790\u3001\u751f\u5b58SVM\u3001XGBoost-Cox\u3001XGBoost-AFT\u548cLightGBM\uff0c\u8bc4\u4f30\u4e86\u5b83\u4eec\u7684\u6027\u80fd\u3002\u901a\u8fc7\u4e0d\u540c\u91c7\u6837\u5668\u8fdb\u884c\u8d85\u53c2\u6570\u4f18\u5316\uff0c\u5e76\u4f7f\u7528\u591a\u4e2a\u6307\u6807\u6765\u8bc4\u4f30\u6a21\u578b\u6027\u80fd\uff0c\u8fd8\u5305\u62ec\u4e0e\u5206\u7c7b\u7b97\u6cd5\u9884\u6d4b\u7684\u6bd4\u8f83\uff0c\u4ee5\u53ca\u4f7f\u7528SHAP\u548c\u7f6e\u6362\u91cd\u8981\u6027\u6765\u89e3\u91ca\u9884\u6d4b\u56e0\u5b50\u7684\u4f5c\u7528\u3002", "result": "XGBoost-AFT\u6a21\u578b\u5728\u7ecf\u8fc7\u5bf9\u6bd4\u540e\u663e\u793a\u51fa\u4e86\u6700\u597d\u7684\u6027\u80fd\uff08C-Index = 0.7618; IPCW = 0.7532\uff09\uff0c\u5176\u4ed6\u8868\u73b0\u4e0d\u9519\u7684\u6a21\u578b\u5305\u62ecGBSA\u548cRSF\u3002\u7ed3\u679c\u8868\u660eMLSA\u6709\u6f5c\u5728\u7684\u7528\u4e8e\u6539\u5584\u751f\u5b58\u9884\u6d4b\u548c\u8f85\u52a9\u51b3\u7b56\u7684\u9002\u7528\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u8bc1\u660e\u4e86XGBoost-AFT\u7b49MLSA\u6a21\u578b\u5728\u9884\u6d4b\u5177\u6709\u5220\u5931\u6570\u636e\u96c6\u7684\u751f\u5b58\u65f6\u95f4\u4e0a\u7684\u4f18\u52bf\uff0c\u5e76\u8bc1\u660e\u4e86\u5b83\u4eec\u7684\u5e94\u7528\u6f5c\u529b\uff0c\u7279\u522b\u662f\u5728\u63d0\u9ad8\u764c\u75c7\u60a3\u8005\u751f\u5b58\u9884\u6d4b\u51c6\u786e\u6027\u548c\u63d0\u4f9b\u57fa\u4e8e\u6570\u636e\u9a71\u52a8\u7684\u533b\u7597\u51b3\u7b56\u652f\u6301\u65b9\u9762\u3002"}}
{"id": "2510.24500", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.24500", "abs": "https://arxiv.org/abs/2510.24500", "authors": ["Yong Huang", "Zhongqi Yang", "Amir Rahmani"], "title": "MIMIC-Sepsis: A Curated Benchmark for Modeling and Learning from Sepsis Trajectories in the ICU", "comment": null, "summary": "Sepsis is a leading cause of mortality in intensive care units (ICUs), yet\nexisting research often relies on outdated datasets, non-reproducible\npreprocessing pipelines, and limited coverage of clinical interventions. We\nintroduce MIMIC-Sepsis, a curated cohort and benchmark framework derived from\nthe MIMIC-IV database, designed to support reproducible modeling of sepsis\ntrajectories. Our cohort includes 35,239 ICU patients with time-aligned\nclinical variables and standardized treatment data, including vasopressors,\nfluids, mechanical ventilation and antibiotics. We describe a transparent\npreprocessing pipeline-based on Sepsis-3 criteria, structured imputation\nstrategies, and treatment inclusion-and release it alongside benchmark tasks\nfocused on early mortality prediction, length-of-stay estimation, and shock\nonset classification. Empirical results demonstrate that incorporating\ntreatment variables substantially improves model performance, particularly for\nTransformer-based architectures. MIMIC-Sepsis serves as a robust platform for\nevaluating predictive and sequential models in critical care research.", "AI": {"tldr": "MIMIC-Sepsis \u662f\u4e00\u4e2a\u57fa\u4e8e MIMIC-IV \u6570\u636e\u5e93\u6784\u5efa\u7684\u8113\u6bd2\u75c7\u961f\u5217\u548c\u57fa\u51c6\u6846\u67b6\uff0c\u7528\u4e8e\u652f\u6301\u8113\u6bd2\u75c7\u8f68\u8ff9\u7684\u53ef\u91cd\u590d\u5efa\u6a21\u3002\u7814\u7a76\u8868\u660e\uff0c\u5305\u542b\u6cbb\u7597\u53d8\u91cf\u53ef\u4ee5\u663e\u8457\u63d0\u9ad8\u6a21\u578b\u6027\u80fd\uff0c\u5c24\u5176\u662f\u5728Transformer\u67b6\u6784\u4e2d\u3002", "motivation": "\u5f53\u524d\u5173\u4e8e\u8113\u6bd2\u75c7\u7684\u7814\u7a76\u4f9d\u8d56\u9648\u65e7\u7684\u6570\u636e\u96c6\uff0c\u4e0d\u53ef\u590d\u73b0\u7684\u9884\u5904\u7406\u6d41\u7a0b\u4ee5\u53ca\u4e34\u5e8a\u5e72\u9884\u63aa\u65bd\u6709\u9650\u7684\u8986\u76d6\u9762\u3002\u672c\u6587\u63d0\u51fa\u4e86 MIMIC-Sepsis\uff0c\u4e00\u4e2a\u57fa\u4e8e MIMIC-IV \u6570\u636e\u5e93\u5efa\u7acb\u7684\u8113\u6bd2\u75c7\u961f\u5217\u548c\u57fa\u51c6\u6846\u67b6\uff0c\u4ee5\u652f\u6301\u8113\u6bd2\u75c7\u8f68\u8ff9\u7684\u53ef\u91cd\u590d\u5efa\u6a21\u7684\u7814\u7a76\u9700\u8981\u3002", "method": "\u63cf\u8ff0\u4e86\u4e00\u4e2a\u900f\u660e\u7684\u9884\u5904\u7406\u6d41\u7a0b\uff0c\u57fa\u4e8e Sepsis-3 \u6807\u51c6\uff0c\u5305\u542b\u7ed3\u6784\u5316\u7684\u63d2\u8865\u7b56\u7565\u548c\u6cbb\u7597\u53d8\u91cf\uff0c\u4ee5\u53ca\u53d1\u5e03\u76f8\u5173\u7684\u57fa\u51c6\u4efb\u52a1\uff0c\u5305\u62ec\u65e9\u671f\u6b7b\u4ea1\u7387\u9884\u6d4b\uff0c\u4f4f\u9662\u65f6\u95f4\u4f30\u8ba1\u548c\u4f11\u514b\u53d1\u751f\u5206\u7c7b\u3002\u7814\u7a76\u4e2d\u91c7\u7528\u7684\u6cbb\u7597\u53d8\u91cf\u5305\u62ec\u8840\u7ba1\u52a0\u538b\u836f\uff0c\u6db2\u4f53\uff0c\u673a\u68b0\u901a\u6c14\u548c\u6297\u751f\u7d20\u7b49\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\uff0c\u7eb3\u5165\u6cbb\u7597\u53d8\u91cf\u53ef\u4ee5\u663e\u8457\u63d0\u9ad8\u6a21\u578b\u7684\u6027\u80fd\uff0c\u5c24\u5176\u662f\u5728Transformer\u67b6\u6784\u4e0b\u7684\u6548\u679c\u66f4\u4e3a\u663e\u8457\u3002", "conclusion": "MIMIC-Sepsis \u4f5c\u4e3a\u4e00\u4e2a\u5177\u6709\u7814\u7a76\u548c\u4e34\u754c\u7167\u62a4\u6a21\u578b\u8bc4\u4f30\u5f3a\u5927\u529f\u80fd\u7684\u5e73\u53f0\uff0c\u5728\u91cd\u75c7\u7814\u7a76\u4e2d\u5177\u6709\u6781\u5927\u7684\u4ef7\u503c\u3002"}}
{"id": "2510.24561", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24561", "abs": "https://arxiv.org/abs/2510.24561", "authors": ["Qingyue Zhang", "Chang Chu", "Tianren Peng", "Qi Li", "Xiangyang Luo", "Zhihao Jiang", "Shao-Lun Huang"], "title": "LoRA-DA: Data-Aware Initialization for Low-Rank Adaptation via Asymptotic Analysis", "comment": null, "summary": "With the widespread adoption of LLMs, LoRA has become a dominant method for\nPEFT, and its initialization methods have attracted increasing attention.\nHowever, existing methods have notable limitations: many methods do not\nincorporate target-domain data, while gradient-based methods exploit data only\nat a shallow level by relying on one-step gradient decomposition, which remains\nunsatisfactory due to the weak empirical performance of the one-step\nfine-tuning model that serves as their basis, as well as the fact that these\nmethods either lack a rigorous theoretical foundation or depend heavily on\nrestrictive isotropic assumptions. In this paper, we establish a theoretical\nframework for data-aware LoRA initialization based on asymptotic analysis.\nStarting from a general optimization objective that minimizes the expectation\nof the parameter discrepancy between the fine-tuned and target models, we\nderive an optimization problem with two components: a bias term, which is\nrelated to the parameter distance between the fine-tuned and target models, and\nis approximated using a Fisher-gradient formulation to preserve anisotropy; and\na variance term, which accounts for the uncertainty introduced by sampling\nstochasticity through the Fisher information. By solving this problem, we\nobtain an optimal initialization strategy for LoRA. Building on this\ntheoretical framework, we develop an efficient algorithm, LoRA-DA, which\nestimates the terms in the optimization problem from a small set of target\ndomain samples and obtains the optimal LoRA initialization. Empirical results\nacross multiple benchmarks demonstrate that LoRA-DA consistently improves final\naccuracy over existing initialization methods. Additional studies show faster,\nmore stable convergence, robustness across ranks, and only a small\ninitialization overhead for LoRA-DA. The source code will be released upon\npublication.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6e10\u8fd1\u5206\u6790\u7684LoRA\u521d\u59cb\u5316\u7406\u8bba\u6846\u67b6\uff0c\u79f0\u4e3aLoRA-DA\uff0c\u8be5\u6846\u67b6\u901a\u8fc7\u89e3\u51b3\u4e00\u4e2a\u5305\u542b\u504f\u5dee\u548c\u65b9\u5dee\u9879\u7684\u4f18\u5316\u95ee\u9898\uff0c\u5b9e\u73b0\u6570\u636e\u611f\u77e5\u7684LoRA\u521d\u59cb\u5316\uff0c\u5b9e\u9a8c\u8868\u660eLoRA-DA\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u63d0\u9ad8\u4e86\u6700\u7ec8\u7684\u51c6\u786e\u6027\uff0c\u5177\u6709\u66f4\u5feb\u66f4\u7a33\u5b9a\u7684\u6536\u655b\u6027\u548c\u66f4\u597d\u7684\u79e9\u7a33\u5b9a\u6027\uff0c\u4e14\u521d\u59cb\u5316\u5f00\u9500\u5c0f", "motivation": "\u73b0\u6709LoRA\u521d\u59cb\u5316\u65b9\u6cd5\u7f3a\u4e4f\u4e25\u683c\u7684\u7406\u8bba\u57fa\u7840\u6216\u4f9d\u8d56\u4e8e\u9650\u5236\u6027\u5047\u8bbe\uff0c\u6027\u80fd\u4e0d\u7406\u60f3\u3002\u4e3a\u4e86\u63d0\u4f9b\u4e00\u4e2a\u6570\u636e\u611f\u77e5\u7684LoRA\u521d\u59cb\u5316\u65b9\u6cd5\uff0c\u6784\u5efa\u4e86\u4e00\u4e2a\u7406\u8bba\u6846\u67b6\uff0c\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u7684\u8fd9\u4e9b\u5c40\u9650\u6027", "method": "\u63d0\u51fa\u4e86LoRA\u521d\u59cb\u5316\u7684\u7406\u8bba\u6846\u67b6\uff0c\u6839\u636e\u6e10\u8fd1\u5206\u6790\uff0c\u4ece\u4e00\u4e2a\u4f18\u5316\u76ee\u6807\u51fa\u53d1\uff0c\u8be5\u76ee\u6807\u65e8\u5728\u6700\u5c0f\u5316\u4eceScratch\u6a21\u578b\u4e2d\u8c03\u6574\u540e\u7684\u53c2\u6570\u4e0e\u76ee\u6807\u6a21\u578b\u53c2\u6570\u4e4b\u95f4\u7684\u5dee\u5f02\u7684\u671f\u671b\u503c\u3002\u901a\u8fc7\u5206\u6790\u53c2\u6570\u5dee\u5f02\u7684\u7a7a\u95f4\u6027\u8d28\uff0c\u63d0\u51fa\u4e86\u4e00\u4e2a\u8003\u8651\u4e86\u53c2\u6570\u8ddd\u79bb\u504f\u5dee\u548c\u6837\u54c1\u4e0d\u786e\u5b9a\u6027\u65b9\u5dee\u7684\u4f18\u5316\u95ee\u9898\uff0c\u89e3\u51b3\u8be5\u95ee\u9898\u53ef\u4ee5\u627e\u5230\u6700\u4f18\u7684LoRA\u521d\u59cb\u5316\u7b56\u7565\u3002\u57fa\u4e8e\u8be5\u6846\u67b6\uff0c\u5f00\u53d1\u4e86LoRA-DA\u7b97\u6cd5\uff0c\u8be5\u7b97\u6cd5\u53ef\u4ee5\u4ece\u5c0f\u6837\u672c\u96c6\u4e2d\u4f30\u8ba1\u4f18\u5316\u95ee\u9898\u4e2d\u7684\u9879\uff0c\u5e76\u627e\u5230\u6700\u4f18\u7684LoRA\u521d\u59cb\u5316", "result": "LoRA-DA\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u63d0\u9ad8\u4e86\u6700\u7ec8\u7684\u51c6\u786e\u6027\uff0c\u5c55\u793a\u4e86\u66f4\u5feb\u3001\u66f4\u7a33\u5b9a\u7684\u6536\u655b\u6027\uff0c\u5e76\u4e14\u5177\u6709\u826f\u597d\u7684\u79e9\u7a33\u5b9a\u6027\uff0c\u4e14\u53ea\u6709\u5f88\u5c0f\u7684\u521d\u59cb\u5316\u5f00\u9500", "conclusion": "\u63d0\u51fa\u7684LoRA-DA\u65b9\u6cd5\u53ef\u4ee5\u89e3\u51b3\u73b0\u6709LoRA\u521d\u59cb\u5316\u65b9\u6cd5\u7684\u4e0d\u8db3\uff0c\u63d0\u4f9b\u4e00\u79cd\u66f4\u52a0\u6709\u6548\u7684\u521d\u59cb\u5316\u7b56\u7565\uff0c\u5b9e\u9a8c\u7ed3\u679c\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027"}}
{"id": "2510.24574", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24574", "abs": "https://arxiv.org/abs/2510.24574", "authors": ["Hao Wang", "Licheng Pan", "Yuan Lu", "Zhixuan Chu", "Xiaoxi Li", "Shuting He", "Zhichao Chen", "Haoxuan Li", "Qingsong Wen", "Zhouchen Lin"], "title": "DistDF: Time-Series Forecasting Needs Joint-Distribution Wasserstein Alignment", "comment": null, "summary": "Training time-series forecast models requires aligning the conditional\ndistribution of model forecasts with that of the label sequence. The standard\ndirect forecast (DF) approach resorts to minimize the conditional negative\nlog-likelihood of the label sequence, typically estimated using the mean\nsquared error. However, this estimation proves to be biased in the presence of\nlabel autocorrelation. In this paper, we propose DistDF, which achieves\nalignment by alternatively minimizing a discrepancy between the conditional\nforecast and label distributions. Because conditional discrepancies are\ndifficult to estimate from finite time-series observations, we introduce a\nnewly proposed joint-distribution Wasserstein discrepancy for time-series\nforecasting, which provably upper bounds the conditional discrepancy of\ninterest. This discrepancy admits tractable, differentiable estimation from\nempirical samples and integrates seamlessly with gradient-based training.\nExtensive experiments show that DistDF improves the performance diverse\nforecast models and achieves the state-of-the-art forecasting performance. Code\nis available at https://anonymous.4open.science/r/DistDF-F66B.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86DistDF\u65b9\u6cd5\uff0c\u901a\u8fc7\u6700\u5c0f\u5316\u9884\u6d4b\u5206\u5e03\u548c\u6807\u7b7e\u5206\u5e03\u4e4b\u95f4\u7684\u5dee\u5f02\u6765\u6539\u8fdb\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u6a21\u578b\u7684\u8bad\u7ec3\uff0c\u7279\u522b\u662f\u5f53\u6807\u7b7e\u5e8f\u5217\u5177\u6709\u81ea\u76f8\u5173\u6027\u65f6\u8868\u73b0\u66f4\u4f73\u3002\u5b9e\u9a8c\u7ed3\u679c\u663e\u793aDistDF\u6bd4\u4f20\u7edf\u7684\u76f4\u63a5\u9884\u6d4b\u65b9\u6cd5\u6709\u66f4\u4f18\u7684\u6548\u679c\uff0c\u5e76\u8fbe\u5230\u4e86\u6700\u65b0\u7684\u9884\u6d4b\u6027\u80fd\u6c34\u5e73\u3002", "motivation": "\u4f20\u7edf\u76f4\u63a5\u9884\u6d4b\u65b9\u6cd5\u5728\u5904\u7406\u5177\u6709\u81ea\u76f8\u5173\u6027\u7684\u6807\u7b7e\u5e8f\u5217\u65f6\u4f1a\u51fa\u73b0\u504f\u5dee\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u65b0\u7684\u65b9\u6cd5\u6765\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002\u672c\u6587\u63d0\u51fa\u4e86DistDF\u4ee5\u5e94\u5bf9\u5728\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u4e2d\u51fa\u73b0\u7684\u8fd9\u4e00\u6311\u6218\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u8054\u5408\u5206\u5e03Wasserstein\u5dee\u5f02\u5ea6\u91cf\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u80fd\u591f\u4ece\u6709\u9650\u7684\u65f6\u95f4\u5e8f\u5217\u89c2\u5bdf\u4e2d\u4f30\u8ba1\u6761\u4ef6\u5dee\u5f02\uff0c\u5e76\u4e14\u8be5\u5dee\u5f02\u5ea6\u91cf\u53ef\u4ee5\u88ab\u6709\u6548\u5730\u4ece\u7ecf\u9a8c\u6837\u672c\u4e2d\u4f30\u8ba1\uff0c\u4ece\u800c\u80fd\u591f\u4e0e\u57fa\u4e8e\u68af\u5ea6\u7684\u8bad\u7ec3\u8fc7\u7a0b\u65e0\u7f1d\u7ed3\u5408\u3002", "result": "DistDF\u5728\u5e7f\u6cdb\u7684\u5b9e\u9a8c\u4e2d\u63d0\u9ad8\u4e86\u4e0d\u540c\u7684\u9884\u6d4b\u6a21\u578b\u7684\u6027\u80fd\uff0c\u5e76\u4e14\u5728\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u4e2d\u8fbe\u5230\u4e86\u6700\u65b0\u7684\u6280\u672f\u6c34\u5e73\u3002\u5177\u4f53\u7684\u4ee3\u7801\u5730\u5740\u4e3ahttps://anonymous.4open.science/r/DistDF-F66B\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e86DistDF\uff0c\u4f5c\u4e3a\u4e00\u79cd\u65b0\u7684\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u65b9\u6cd5\uff0c\u5b83\u901a\u8fc7\u6700\u5c0f\u5316\u9884\u6d4b\u5206\u5e03\u548c\u6807\u7b7e\u5206\u5e03\u4e4b\u95f4\u7684\u5dee\u5f02\uff0c\u80fd\u591f\u6709\u6548\u5730\u63d0\u5347\u9884\u6d4b\u6027\u80fd\uff0c\u5c24\u5176\u662f\u5728\u5177\u6709\u81ea\u76f8\u5173\u6027\u7684\u6807\u7b7e\u5e8f\u5217\u4e2d\u8868\u73b0\u66f4\u4f73\u3002"}}
{"id": "2510.24577", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2510.24577", "abs": "https://arxiv.org/abs/2510.24577", "authors": ["He Yang", "Fei Ren", "Hai-Sui Yu", "Xiaohui Chen", "Pei-Zhi Zhuang"], "title": "Physics-Informed Extreme Learning Machine (PIELM): Opportunities and Challenges", "comment": null, "summary": "We are very delighted to see the fast development of physics-informed extreme\nlearning machine (PIELM) in recent years for higher computation efficiency and\naccuracy in physics-informed machine learning. As a summary or review on PIELM\nis currently not available, we would like to take this opportunity to show our\nperspective and experience for this promising research direction. We can see\nmany efforts are made to solve PDEs with sharp gradients, nonlinearities,\nhigh-frequency behavior, hard constraints, uncertainty, multiphysics coupling.\nDespite the success, many urgent challenges remain to be tackled, which also\nprovides us opportunities to develop more robust, interpretable, and\ngeneralizable PIELM frameworks with applications in science and engineering.", "AI": {"tldr": "\u672c\u6587\u603b\u7ed3\u5e76\u56de\u987e\u4e86\u7269\u7406\u4fe1\u606f\u589e\u5f3a\u7684\u6781\u5b66\u4e60\u673a(PIELM)\u7684\u53d1\u5c55\uff0c\u5e76\u8ba8\u8bba\u4e86\u5176\u5728\u89e3\u51b3\u504f\u5fae\u5206\u65b9\u7a0b\u7b49\u95ee\u9898\u4e0a\u7684\u6311\u6218\u4e0e\u673a\u9047\uff0c\u65e8\u5728\u6784\u5efa\u66f4\u4e3a\u7a33\u5065\u3001\u53ef\u89e3\u91ca\u4e14\u901a\u7528\u7684PIELM\u6846\u67b6\uff0c\u5e94\u7528\u4e8e\u79d1\u5b66\u7814\u7a76\u4e0e\u5de5\u7a0b\u9886\u57df\u3002", "motivation": "\u7269\u7406\u4fe1\u606f\u589e\u5f3a\u7684\u6781\u5b66\u4e60\u673a(PIELM)\u8fd1\u5e74\u6765\u53d1\u5c55\u8fc5\u901f\uff0c\u5177\u6709\u9ad8\u6548\u8ba1\u7b97\u53ca\u51c6\u786e\u6027\uff0c\u4f46\u76ee\u524d\u7f3a\u4e4f\u5bf9\u8be5\u9886\u57df\u7684\u603b\u7ed3\u6216\u7efc\u8ff0\uff0c\u56e0\u6b64\u4f5c\u8005\u5e0c\u671b\u901a\u8fc7\u6b64\u8bba\u6587\u5c55\u793a\u4ed6\u4eec\u7684\u89c6\u89d2\u4e0e\u7ecf\u9a8c\uff0c\u5e76\u63a2\u8ba8\u8be5\u7814\u7a76\u65b9\u5411\u6240\u9762\u4e34\u7684\u6311\u6218\u548c\u673a\u9047\uff0c\u63a8\u52a8\u6784\u5efa\u66f4\u4e3a\u7a33\u5065\u3001\u53ef\u89e3\u91ca\u4e14\u901a\u7528\u7684PIELM\u6846\u67b6\uff0c\u5e94\u7528\u4e8e\u79d1\u5b66\u7814\u7a76\u4e0e\u5de5\u7a0b\u9886\u57df\u3002", "method": "\u8be5\u8bba\u6587\u5e76\u4e0d\u662f\u4e00\u4e2a\u5177\u4f53\u7684\u7814\u7a76\u8bba\u6587\uff0c\u800c\u662f\u4f5c\u4e3a\u4e00\u4e2a\u603b\u7ed3\u6216\u7efc\u8ff0\u6765\u5c55\u793a\u73b0\u6709\u7684\u65b9\u6cd5\u548c\u6311\u6218\uff0c\u56e0\u6b64\u6ca1\u6709\u7279\u5b9a\u7684\u65b9\u6cd5\u8bba\u3002\u4e3b\u8981\u6709\u5bf9\u73b0\u6709PIELM\u65b9\u6cd5\u7684\u56de\u987e\u3001\u5206\u6790\u4ee5\u53ca\u63d0\u51fa\u672a\u6765\u7814\u7a76\u65b9\u5411\u7684\u5efa\u8bae\u3002", "result": "\u8be5\u8bba\u6587\u6307\u51fa\u4e86PIELM\u65b9\u6cd5\u5728\u89e3\u51b3\u542b\u6709\u9661\u5ced\u68af\u5ea6\u3001\u975e\u7ebf\u6027\u3001\u9ad8\u9891\u884c\u4e3a\u3001\u786c\u7ea6\u675f\u3001\u4e0d\u786e\u5b9a\u6027\u548c\u591a\u7269\u7406\u8026\u5408\u504f\u5fae\u5206\u65b9\u7a0b\u7b49\u95ee\u9898\u4e0a\u7684\u80fd\u529b\uff0c\u540c\u65f6\u4e5f\u8bc6\u522b\u51fa\u4e86\u4e00\u4e9b\u4e9f\u5f85\u89e3\u51b3\u7684\u95ee\u9898\uff0c\u5982\u63d0\u9ad8\u6a21\u578b\u7684\u7a33\u5065\u6027\u3001\u53ef\u89e3\u91ca\u6027\u53ca\u6cdb\u5316\u80fd\u529b\u7b49\u3002", "conclusion": "\u5c3d\u7ba1PIELM\u5728\u89e3\u51b3\u590d\u6742\u504f\u5fae\u5206\u65b9\u7a0b\u95ee\u9898\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u4ecd\u6709\u8bb8\u591a\u6311\u6218\u9700\u8981\u514b\u670d\uff0c\u5305\u62ec\u63d0\u9ad8\u6a21\u578b\u7684\u7a33\u5065\u6027\u3001\u53ef\u89e3\u91ca\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002\u8fd9\u4e9b\u95ee\u9898\u4e3a\u7814\u7a76\u4eba\u5458\u63d0\u4f9b\u4e86\u5f00\u53d1\u66f4\u52a0\u901a\u7528\u3001\u7a33\u5065\u4e14\u53ef\u89e3\u91ca\u7684PIELM\u6846\u67b6\u7684\u673a\u4f1a\u3002"}}
{"id": "2510.24633", "categories": ["cs.LG", "cs.LO"], "pdf": "https://arxiv.org/pdf/2510.24633", "abs": "https://arxiv.org/abs/2510.24633", "authors": ["Mingyue Liu", "Andrew Cropper"], "title": "Symbolic Snapshot Ensembles", "comment": null, "summary": "Inductive logic programming (ILP) is a form of logical machine learning. Most\nILP algorithms learn a single hypothesis from a single training run. Ensemble\nmethods train an ILP algorithm multiple times to learn multiple hypotheses. In\nthis paper, we train an ILP algorithm only once and save intermediate\nhypotheses. We then combine the hypotheses using a minimum description length\nweighting scheme. Our experiments on multiple benchmarks, including game\nplaying and visual reasoning, show that our approach improves predictive\naccuracy by 4% with less than 1% computational overhead.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2510.24639", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24639", "abs": "https://arxiv.org/abs/2510.24639", "authors": ["Pedro P. Sanchez", "Damian Machlanski", "Steven McDonagh", "Sotirios A. Tsaftaris"], "title": "Causal Ordering for Structure Learning From Time Series", "comment": "32 pages", "summary": "Predicting causal structure from time series data is crucial for\nunderstanding complex phenomena in physiology, brain connectivity, climate\ndynamics, and socio-economic behaviour. Causal discovery in time series is\nhindered by the combinatorial complexity of identifying true causal\nrelationships, especially as the number of variables and time points grow. A\ncommon approach to simplify the task is the so-called ordering-based methods.\nTraditional ordering methods inherently limit the representational capacity of\nthe resulting model. In this work, we fix this issue by leveraging multiple\nvalid causal orderings, instead of a single one as standard practice. We\npropose DOTS (Diffusion Ordered Temporal Structure), using diffusion-based\ncausal discovery for temporal data. By integrating multiple orderings, DOTS\neffectively recovers the transitive closure of the underlying directed acyclic\ngraph, mitigating spurious artifacts inherent in single-ordering approaches. We\nformalise the problem under standard assumptions such as stationarity and the\nadditive noise model, and leverage score matching with diffusion processes to\nenable efficient Hessian estimation. Extensive experiments validate the\napproach. Empirical evaluations on synthetic and real-world datasets\ndemonstrate that DOTS outperforms state-of-the-art baselines, offering a\nscalable and robust approach to temporal causal discovery. On synthetic\nbenchmarks ($d{=}\\!3-\\!6$ variables, $T{=}200\\!-\\!5{,}000$ samples), DOTS\nimproves mean window-graph $F1$ from $0.63$ (best baseline) to $0.81$. On the\nCausalTime real-world benchmark ($d{=}20\\!-\\!36$), while baselines remain the\nbest on individual datasets, DOTS attains the highest average summary-graph\n$F1$ while halving runtime relative to graph-optimisation methods. These\nresults establish DOTS as a scalable and accurate solution for temporal causal\ndiscovery.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aDOTS\u7684\u65b0\u578b\u65f6\u95f4\u56e0\u679c\u5173\u7cfb\u53d1\u73b0\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u901a\u8fc7\u7ed3\u5408\u591a\u4e2a\u6709\u6548\u7684\u56e0\u679c\u987a\u5e8f\uff0c\u6709\u6548\u6062\u590d\u4e86\u6f5c\u5728\u6709\u5411\u65e0\u73af\u56fe\u7684\u4f20\u9012\u95ed\u5305\uff0c\u4ece\u800c\u63d0\u9ad8\u4e86\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u4e2d\u56e0\u679c\u5173\u7cfb\u53d1\u73b0\u7684\u51c6\u786e\u6027\u548c\u6548\u7387\u3002\u5b9e\u9a8c\u8bc1\u660e\u8be5\u65b9\u6cd5\u4f18\u4e8e\u76ee\u524d\u7684\u6700\u4f18\u57fa\u7ebf\u65b9\u6cd5\u3002", "motivation": "\u5f53\u524d\u65f6\u95f4\u5e8f\u5217\u56e0\u679c\u5173\u7cfb\u53d1\u73b0\u65b9\u6cd5\u901a\u5e38\u4f9d\u8d56\u5355\u4e00\u7684\u56e0\u679c\u987a\u5e8f\uff0c\u8fd9\u9650\u5236\u4e86\u6a21\u578b\u7684\u8868\u8fbe\u80fd\u529b\uff0c\u5e76\u4e14\u968f\u7740\u53d8\u91cf\u6570\u548c\u65f6\u95f4\u70b9\u7684\u589e\u957f\uff0c\u8bc6\u522b\u771f\u5b9e\u56e0\u679c\u5173\u7cfb\u53d8\u5f97\u590d\u6742\u3002\u672c\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u901a\u8fc7\u7ed3\u5408\u591a\u4e2a\u56e0\u679c\u987a\u5e8f\uff0c\u63d0\u9ad8\u56e0\u679c\u5173\u7cfb\u53d1\u73b0\u7684\u51c6\u786e\u6027\u548c\u6548\u7387\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u65b9\u6cd5DOTS\uff0c\u5b83\u5229\u7528\u57fa\u4e8e\u6269\u6563\u7684\u56e0\u679c\u53d1\u73b0\u65b9\u6cd5\u5904\u7406\u65f6\u95f4\u5e8f\u5217\u6570\u636e\uff0c\u901a\u8fc7\u6574\u5408\u591a\u4e2a\u6709\u6548\u7684\u56e0\u679c\u987a\u5e8f\uff0c\u6709\u6548\u6062\u590d\u6f5c\u5728\u6709\u5411\u65e0\u73af\u56fe\u7684\u4f20\u9012\u95ed\u5305\uff0c\u4ece\u800c\u89e3\u51b3\u5355\u4e00\u56e0\u679c\u987a\u5e8f\u65b9\u6cd5\u7684\u8bef\u5bfc\u6027\u95ee\u9898\u3002", "result": "DOTS\u65b9\u6cd5\u5728\u5408\u6210\u6570\u636e\u96c6\u548c\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u5e7f\u6cdb\u7684\u5b9e\u9a8c\uff0c\u7ed3\u679c\u8868\u660e\u5176\u5728\u65f6\u95f4\u56e0\u679c\u53d1\u73b0\u4efb\u52a1\u4e0a\u4f18\u4e8e\u76ee\u524d\u7684\u6700\u4f18\u57fa\u7ebf\u65b9\u6cd5\u3002\u5177\u4f53\u800c\u8a00\uff0c\u5728\u5408\u6210\u6570\u636e\u96c6\u4e0a\uff0cDOTS\u65b9\u6cd5\u63d0\u9ad8\u4e86\u7a97\u53e3\u56feF-1\u5f97\u5206\uff0c\u4ece\u57fa\u7ebf\u65b9\u6cd5\u76840.63\u63d0\u9ad8\u5230\u4e860.81\uff1b\u5728CausalTime\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\uff0c\u867d\u7136\u57fa\u7ebf\u65b9\u6cd5\u5728\u5355\u4e2a\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u6700\u597d\uff0c\u4f46DOTS\u5b9e\u73b0\u4e86\u6700\u9ad8\u7684\u5e73\u5747\u603b\u7ed3\u56feF-1\u5f97\u5206\uff0c\u5e76\u4e14\u5c06\u8fd0\u884c\u65f6\u95f4\u76f8\u5bf9\u56fe\u4f18\u5316\u65b9\u6cd5\u51cf\u534a\u3002\u8fd9\u4e9b\u7ed3\u679c\u8868\u660e\uff0cDOTS\u662f\u4e00\u79cd\u53ef\u6269\u5c55\u4e14\u51c6\u786e\u7684\u89e3\u51b3\u65f6\u95f4\u56e0\u679c\u53d1\u73b0\u7684\u65b9\u6cd5\u3002", "conclusion": "DOTS\u65b9\u6cd5\u901a\u8fc7\u7ed3\u5408\u591a\u4e2a\u56e0\u679c\u987a\u5e8f\uff0c\u89e3\u51b3\u4e86\u5355\u4e00\u56e0\u679c\u987a\u5e8f\u65b9\u6cd5\u8868\u8fbe\u80fd\u529b\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u4ece\u800c\u63d0\u9ad8\u4e86\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u4e2d\u56e0\u679c\u5173\u7cfb\u53d1\u73b0\u7684\u51c6\u786e\u6027\u548c\u6548\u7387\u3002\u5b9e\u9a8c\u9a8c\u8bc1\u8bc1\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u51c6\u786e\u6027\u548c\u6548\u7387\u4e0a\u4f18\u4e8e\u5f53\u524d\u6700\u4f18\u65b9\u6cd5\uff0c\u662f\u65f6\u95f4\u56e0\u679c\u53d1\u73b0\u7684\u4e00\u4e2a\u6709\u524d\u9014\u7684\u5de5\u5177\u3002"}}
{"id": "2510.24643", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24643", "abs": "https://arxiv.org/abs/2510.24643", "authors": ["Yujun Kim", "Chaewon Moon", "Chulhee Yun"], "title": "The Cost of Robustness: Tighter Bounds on Parameter Complexity for Robust Memorization in ReLU Nets", "comment": "Accepted to NeurIPS 2025, 72 pages, 8 figures", "summary": "We study the parameter complexity of robust memorization for $\\mathrm{ReLU}$\nnetworks: the number of parameters required to interpolate any given dataset\nwith $\\epsilon$-separation between differently labeled points, while ensuring\npredictions remain consistent within a $\\mu$-ball around each training sample.\nWe establish upper and lower bounds on the parameter count as a function of the\nrobustness ratio $\\rho = \\mu / \\epsilon$. Unlike prior work, we provide a\nfine-grained analysis across the entire range $\\rho \\in (0,1)$ and obtain\ntighter upper and lower bounds that improve upon existing results. Our findings\nreveal that the parameter complexity of robust memorization matches that of\nnon-robust memorization when $\\rho$ is small, but grows with increasing $\\rho$.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2510.24670", "categories": ["cs.LG", "q-bio.QM"], "pdf": "https://arxiv.org/pdf/2510.24670", "abs": "https://arxiv.org/abs/2510.24670", "authors": ["Genesis Research Team", "Alejandro Dobles", "Nina Jovic", "Kenneth Leidal", "Pranav Murugan", "David C. Williams", "Drausin Wulsin", "Nate Gruver", "Christina X. Ji", "Korrawat Pruegsanusak", "Gianluca Scarpellini", "Ansh Sharma", "Wojciech Swiderski", "Andrea Bootsma", "Richard Strong Bowen", "Charlotte Chen", "Jamin Chen", "Marc Andr\u00e9 D\u00e4mgen", "Roy Tal Dew", "Benjamin DiFrancesco", "J. D. Fishman", "Alla Ivanova", "Zach Kagin", "David Li-Bland", "Zuli Liu", "Igor Morozov", "Jeffrey Ouyang-Zhang", "Frank C. Pickard IV", "Kushal S. Shah", "Ben Shor", "Gabriel Monteiro da Silva", "Maxx Tessmer", "Carl Tilbury", "Cyr Vetcher", "Daniel Zeng", "Maruan Al-Shedivat", "Aleksandra Faust", "Evan N. Feinberg", "Michael V. LeVine", "Matteus Pan"], "title": "Pearl: A Foundation Model for Placing Every Atom in the Right Location", "comment": null, "summary": "Accurately predicting the three-dimensional structures of protein-ligand\ncomplexes remains a fundamental challenge in computational drug discovery that\nlimits the pace and success of therapeutic design. Deep learning methods have\nrecently shown strong potential as structural prediction tools, achieving\npromising accuracy across diverse biomolecular systems. However, their\nperformance and utility are constrained by scarce experimental data,\ninefficient architectures, physically invalid poses, and the limited ability to\nexploit auxiliary information available at inference. To address these issues,\nwe introduce Pearl (Placing Every Atom in the Right Location), a foundation\nmodel for protein-ligand cofolding at scale. Pearl addresses these challenges\nwith three key innovations: (1) training recipes that include large-scale\nsynthetic data to overcome data scarcity; (2) architectures that incorporate an\nSO(3)-equivariant diffusion module to inherently respect 3D rotational\nsymmetries, improving generalization and sample efficiency, and (3)\ncontrollable inference, including a generalized multi-chain templating system\nsupporting both protein and non-polymeric components as well as dual\nunconditional/conditional modes. Pearl establishes a new state-of-the-art\nperformance in protein-ligand cofolding. On the key metric of generating\naccurate (RMSD < 2 \\r{A}) and physically valid poses, Pearl surpasses AlphaFold\n3 and other open source baselines on the public Runs N' Poses and PoseBusters\nbenchmarks, delivering 14.5% and 14.2% improvements, respectively, over the\nnext best model. In the pocket-conditional cofolding regime, Pearl delivers\n$3.6\\times$ improvement on a proprietary set of challenging, real-world drug\ntargets at the more rigorous RMSD < 1 \\r{A} threshold. Finally, we demonstrate\nthat model performance correlates directly with synthetic dataset size used in\ntraining.", "AI": {"tldr": "Pearl\u662f\u4e00\u4e2a\u7528\u4e8e\u86cb\u767d\u8d28-\u914d\u4f53\u5171\u6298\u53e0\u7684\u57fa\u7840\u6a21\u578b\uff0c\u76f8\u8f83\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5b83\u5728\u751f\u6210\u51c6\u786e\u7684\u3001\u7269\u7406\u4e0a\u6709\u6548\u7684\u6784\u8c61\u4e0a\u8868\u73b0\u51fa\u663e\u8457\u7684\u6539\u8fdb\u3002\u5728\u5173\u952e\u8bc4\u4f30\u6307\u6807\uff08RMSD < 2 \u00c5\uff09\u4e0a\uff0cPearl\u5728\u516c\u5f00\u7684\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5206\u522b\u8d85\u8d8a\u4e86AlphaFold 3\u548c\u5176\u4ed6\u5f00\u6e90\u57fa\u51c614.5%\u548c14.2%\u3002\u5728\u53e3\u888b\u6761\u4ef6\u5171\u6298\u53e0\u9886\u57df\uff0cPearl\u5728\u4e25\u683c\u7684RMSD < 1 \u00c5\u9608\u503c\u4e0b\uff0c\u5bf9\u73b0\u5b9e\u4e16\u754c\u7684\u836f\u7269\u9776\u6807\u8868\u73b0\u51fa3.6\u500d\u7684\u6539\u8fdb\u3002\u4e0e\u8bad\u7ec3\u65f6\u4f7f\u7528\u7684\u5408\u6210\u6570\u636e\u96c6\u5927\u5c0f\u76f4\u63a5\u76f8\u5173\uff0cPearl\u6a21\u578b\u7684\u6027\u80fd\u6709\u6240\u63d0\u9ad8\u3002", "motivation": "\u8ba1\u7b97\u836f\u7269\u53d1\u73b0\u4e2d\u7cbe\u786e\u9884\u6d4b\u86cb\u767d\u8d28-\u914d\u4f53\u590d\u5408\u7269\u7684\u4e09\u7ef4\u7ed3\u6784\u4f9d\u7136\u662f\u4e00\u9879\u91cd\u5927\u6311\u6218\uff0c\u9650\u5236\u4e86\u6cbb\u7597\u8bbe\u8ba1\u7684\u901f\u5ea6\u548c\u6210\u529f\u7387\u3002\u5c3d\u7ba1\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u5728\u7ed3\u6784\u9884\u6d4b\u5de5\u5177\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u53d7\u9650\u4e8e\u5b9e\u9a8c\u6570\u636e\u7a00\u5c11\u3001\u975e\u9ad8\u6548\u7684\u67b6\u6784\u3001\u7269\u7406\u4e0a\u65e0\u6548\u7684\u6784\u8c61\u4ee5\u53ca\u5728\u63a8\u7406\u65f6\u5229\u7528\u8f85\u52a9\u4fe1\u606f\u7684\u4e0d\u8db3\u3002\u4e3a\u4e86\u514b\u670d\u8fd9\u4e9b\u95ee\u9898\uff0cPearl\u6a21\u578b\u88ab\u5f15\u5165\uff0c\u7528\u4ee5\u6539\u8fdb\u86cb\u767d\u8d28-\u914d\u4f53\u5171\u6298\u53e0\u7684\u80fd\u529b\u3002", "method": "Pearl\u4ee5\u4e09\u79cd\u5173\u952e\u521b\u65b0\u6765\u514b\u670d\u73b0\u5b58\u969c\u788d\uff1a(1) \u5927\u89c4\u6a21\u5408\u6210\u6570\u636e\u7684\u8bad\u7ec3\u914d\u65b9\u53bb\u89e3\u51b3\u6570\u636e\u7a00\u7f3a\u95ee\u9898\uff1b(2) \u5305\u542bSO(3)\u7b49\u53d8\u6269\u6563\u6a21\u5757\u7684\u67b6\u6784\uff0c\u53ef\u4ee5\u5185\u5728\u5730\u5c0a\u91cd\u4e09\u7ef4\u65cb\u8f6c\u5bf9\u79f0\u6027\uff0c\u63d0\u9ad8\u6cdb\u5316\u80fd\u529b\u548c\u6837\u672c\u6548\u7387\uff1b\uff083\uff09\u53ef\u63a7\u7684\u63a8\u7406\uff0c\u5305\u62ec\u652f\u6301\u86cb\u767d\u8d28\u548c\u975e\u805a\u5408\u6210\u5206\u7684\u901a\u7528\u591a\u94fe\u6a21\u677f\u7cfb\u7edf\u4ee5\u53ca\u65e0\u6761\u4ef6/\u6709\u6761\u4ef6\u6a21\u5f0f\u3002", "result": "Pearl\u5728\u86cb\u767d\u8d28-\u914d\u4f53\u5171\u6298\u53e0\u4e0a\u5efa\u7acb\u4e86\u65b0\u7684\u6027\u80fd\u6807\u6746\u3002\u5728\u751f\u6210\u51c6\u786e\u7684\u548c\u7269\u7406\u4e0a\u6709\u6548\u7684\u6784\u8c61\u8fd9\u4e00\u5173\u952e\u6307\u6807\u4e0a\uff0cPearl\u5728\u516c\u5f00\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5206\u522b\u6bd4AlphaFold 3\u53ca\u5176\u4ed6\u5f00\u6e90\u57fa\u51c6\u6a21\u578b\u63d0\u9ad814.5%\u548c14.2%\u3002\u5728\u53e3\u888b\u6761\u4ef6\u5171\u6298\u53e0\u9886\u57df\uff0cPearl\u5728\u66f4\u4e3a\u4e25\u683c\u7684RMSD < 1 \u00c5\u9608\u503c\u4e0b\uff0c\u5bf9\u73b0\u5b9e\u4e16\u754c\u7684\u836f\u7269\u9776\u6807\u8868\u73b0\u51fa3.6\u500d\u7684\u6027\u80fd\u63d0\u5347\u3002\u53e6\u5916\uff0c\u6a21\u578b\u6027\u80fd\u76f4\u63a5\u4e0e\u8bad\u7ec3\u4e2d\u4f7f\u7528\u7684\u5408\u6210\u6570\u636e\u96c6\u5927\u5c0f\u76f8\u5173\u3002", "conclusion": "Pearl\u6a21\u578b\u901a\u8fc7\u5f15\u5165\u5927\u89c4\u6a21\u5408\u6210\u6570\u636e\u3001SO(3)\u7b49\u53d8\u6269\u6563\u6a21\u5757\u53ca\u53ef\u63a7\u63a8\u7406\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u86cb\u767d\u8d28-\u914d\u4f53\u5171\u6298\u53e0\u7684\u51c6\u786e\u6027\u548c\u7269\u7406\u6709\u6548\u6027\uff0c\u5c55\u73b0\u4e86\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u6f5c\u5728\u4ef7\u503c\u3002"}}
{"id": "2510.24672", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2510.24672", "abs": "https://arxiv.org/abs/2510.24672", "authors": ["Burak Var\u0131c\u0131", "Che-Ping Tsai", "Ritabrata Ray", "Nicholas M. Boffi", "Pradeep Ravikumar"], "title": "Eigenfunction Extraction for Ordered Representation Learning", "comment": null, "summary": "Recent advances in representation learning reveal that widely used\nobjectives, such as contrastive and non-contrastive, implicitly perform\nspectral decomposition of a contextual kernel, induced by the relationship\nbetween inputs and their contexts. Yet, these methods recover only the linear\nspan of top eigenfunctions of the kernel, whereas exact spectral decomposition\nis essential for understanding feature ordering and importance. In this work,\nwe propose a general framework to extract ordered and identifiable\neigenfunctions, based on modular building blocks designed to satisfy key\ndesiderata, including compatibility with the contextual kernel and scalability\nto modern settings. We then show how two main methodological paradigms,\nlow-rank approximation and Rayleigh quotient optimization, align with this\nframework for eigenfunction extraction. Finally, we validate our approach on\nsynthetic kernels and demonstrate on real-world image datasets that the\nrecovered eigenvalues act as effective importance scores for feature selection,\nenabling principled efficiency-accuracy tradeoffs via adaptive-dimensional\nrepresentations.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
