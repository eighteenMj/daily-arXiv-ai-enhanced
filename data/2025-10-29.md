<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 56]
- [cs.AI](#cs.AI) [Total: 30]
- [eess.IV](#eess.IV) [Total: 3]
- [cs.NI](#cs.NI) [Total: 3]
- [cs.LG](#cs.LG) [Total: 93]
- [cs.IT](#cs.IT) [Total: 4]
- [eess.SY](#eess.SY) [Total: 10]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Explainable Detection of AI-Generated Images with Artifact Localization Using Faster-Than-Lies and Vision-Language Models for Edge Devices](https://arxiv.org/abs/2510.23775)
*Aryan Mathur,Asaduddin Ahmed,Pushti Amit Vasoya,Simeon Kandan Sonar,Yasir Z,Madesh Kuppusamy*

Main category: cs.CV

TL;DR: 该论文提出了一种可解释的图像真实性检测系统，结合了轻量级卷积分类器和视觉-语言模型，可以分类、定位并解释32x32图像中的伪影，准确率达到96.5%，同时保持了较短的推理时间，提高了解释性和实用性，适用于多个领域如法医、工业检查和社会媒体监控等。


<details>
  <summary>Details</summary>
Motivation: 鉴于AI生成图像的逼真度日益提高，使得验证视觉真实性变得更具挑战性，本论文旨在开发一种能够在低分辨率图像中进行可理解的真实性检测的方法。通过结合视觉和语言领域的模型，提高了检测过程的可解释性，使其不仅有效，而且具备深刻的解释能力。这种方法不仅可以提升检测系统的精度，还可以增强决策者的信任，特别是在部署到法医学、工业检查和社会媒体监控等场景下。

Method: 该论文的方法包括构建一个基于轻量级卷积分类器和视觉-语言模型(Qwen2-VL-7B)的系统，该系统能够对32x32像素大小的图像进行分类并定位其中的伪影。使用自编码器生成重构错误地图，以增强图像中伪影位置的可视化；同时，基于这些定位，生成可解释的文本描述以说明发现的异常情况。该系统不仅能够保证较快的推理速度，以便部署在本地或者边缘设备上，还能提供强烈的解释能力，使系统的结果易于被人类以及后续的决策步骤所理解。

Result: 该系统在加入了对抗性扰动的扩展CiFAKE数据集上实现了96.5%的分类准确率，并且在8核CPU上实现了175毫秒的推理时间，具备了在低功耗环境如边缘设备上的部署能力。系统通过生成自编码器的误差图来增强伪影检测的定位能力，并且能够清晰地说明每个异常被检测的原因，使得系统不仅仅是一个高效的工具，同时也成为一个可解释和富有见解的平台。

Conclusion: 总之，这项工作不仅证明了在低分辨率图像中实现可解释性的真实性检测是可行的，还展示了这种方法如何通过结合视觉和语言信息来增强决策的可信度。这种方法具有广泛的应用潜力，特别是在那些需要快速准确信息的领域，如法医调查、工业质量和安全检查，以及社交媒体内容的监控。未来的研究将进一步探索如何优化模型的性能，以及如何拓展至更高分辨率的图像上的检测任务。

Abstract: The increasing realism of AI-generated imagery poses challenges for verifying
visual authenticity. We present an explainable image authenticity detection
system that combines a lightweight convolutional classifier
("Faster-Than-Lies") with a Vision-Language Model (Qwen2-VL-7B) to classify,
localize, and explain artifacts in 32x32 images. Our model achieves 96.5%
accuracy on the extended CiFAKE dataset augmented with adversarial
perturbations and maintains an inference time of 175ms on 8-core CPUs, enabling
deployment on local or edge devices. Using autoencoder-based reconstruction
error maps, we generate artifact localization heatmaps, which enhance
interpretability for both humans and the VLM. We further categorize 70 visual
artifact types into eight semantic groups and demonstrate explainable text
generation for each detected anomaly. This work highlights the feasibility of
combining visual and linguistic reasoning for interpretable authenticity
detection in low-resolution imagery and outlines potential cross-domain
applications in forensics, industrial inspection, and social media moderation.

</details>


### [2] [CountFormer: A Transformer Framework for Learning Visual Repetition and Structure in Class-Agnostic Object Counting](https://arxiv.org/abs/2510.23785)
*Md Tanvir Hossain,Akif Islam,Mohd Ruhul Ameen*

Main category: cs.CV

TL;DR: 本文介绍了CountFormer，一种基于Transformer的框架，用于无类别身份识别的对象计数，通过自监督基础模型DINOv2和位置嵌入融合提高了复杂场景中的计数准确性，达到了当前最先进的水平，同时推动了向通用和无样例的计数范例的前进。


<details>
  <summary>Details</summary>
Motivation: 现有的计数模型常常在处理具有复杂形状、内部对称性或重叠组件的对象时出错。因此，本文希望通过引入CountFormer来解决这一问题，提升模型在复杂场景中计数的准确性。

Method: CountFormer在CounTR架构的基础上替换视觉编码器为自监督基础模型DINOv2，将位置嵌入融合到模型中保存几何关系，并通过轻量级卷积解码器将这些特征解码为密度图。

Result: 在FSC-147数据集上的评估表明，该模型达到了当前最先进的性能，特别是在结构复杂或密集的场景中表现出更高的准确性。

Conclusion: 研究结果表明，将基础模型如DINOv2集成到计数系统中，可以使系统接近于人类的结构感知能力，推动了向通用和无样例计数范例的前进。

Abstract: Humans can effortlessly count diverse objects by perceiving visual repetition
and structural relationships rather than relying on class identity. However,
most existing counting models fail to replicate this ability; they often
miscount when objects exhibit complex shapes, internal symmetry, or overlapping
components. In this work, we introduce CountFormer, a transformer-based
framework that learns to recognize repetition and structural coherence for
class-agnostic object counting. Built upon the CounTR architecture, our model
replaces its visual encoder with the self-supervised foundation model DINOv2,
which produces richer and spatially consistent feature representations. We
further incorporate positional embedding fusion to preserve geometric
relationships before decoding these features into density maps through a
lightweight convolutional decoder. Evaluated on the FSC-147 dataset, our model
achieves performance comparable to current state-of-the-art methods while
demonstrating superior accuracy on structurally intricate or densely packed
scenes. Our findings indicate that integrating foundation models such as DINOv2
enables counting systems to approach human-like structural perception,
advancing toward a truly general and exemplar-free counting paradigm.

</details>


### [3] [A geometric and deep learning reproducible pipeline for monitoring floating anthropogenic debris in urban rivers using in situ cameras](https://arxiv.org/abs/2510.23798)
*Gauthier Grimmer,Romain Wenger,Clément Flint,Germain Forestier,Gilles Rixhon,Valentin Chardon*

Main category: cs.CV

TL;DR: 该研究提出了一种新方法框架，利用固定摄像机连续量化和监测河流中的漂浮垃圾，并通过深度学习模型识别最佳模型，同时实施几何模型以估计2D图像中检测到对象的实际大小。研究突出了数据集构建协议的重要性，展示了使用投影几何和回归校正进行度量对象估算的可行性。


<details>
  <summary>Details</summary>
Motivation: 该方法旨在解决河流中漂浮的人类废弃物的环境问题，对生物多样性、水质和人类活动造成影响。采用固定摄像技术和深度学习方法来监测和量化这种废弃物。

Method: 研究使用了固定安装的摄像机，通过深度学习模型连续量化和监测漂浮垃圾，并通过实验识别出了最合适的模型。同时，实施了利用摄像机的固有和外在特性估计物体实际大小的几何模型。

Result: 研究表明，利用深度学习模型可以实现漂浮垃圾的有效监测和量化，而几何模型能够准确估计封闭图像中物体的实际大小。同时，强调了数据集构成的重要性，特别是引入负样本和考虑时间泄露的影响。

Conclusion: 该方法证明了利用投影几何和回归修正进行度量物体估算的可行性，为开发城市水环境中低成本、自动化监测系统铺平了道路。

Abstract: The proliferation of floating anthropogenic debris in rivers has emerged as a
pressing environmental concern, exerting a detrimental influence on
biodiversity, water quality, and human activities such as navigation and
recreation. The present study proposes a novel methodological framework for the
monitoring the aforementioned waste, utilising fixed, in-situ cameras. This
study provides two key contributions: (i) the continuous quantification and
monitoring of floating debris using deep learning and (ii) the identification
of the most suitable deep learning model in terms of accuracy and inference
speed under complex environmental conditions. These models are tested in a
range of environmental conditions and learning configurations, including
experiments on biases related to data leakage. Furthermore, a geometric model
is implemented to estimate the actual size of detected objects from a 2D image.
This model takes advantage of both intrinsic and extrinsic characteristics of
the camera. The findings of this study underscore the significance of the
dataset constitution protocol, particularly with respect to the integration of
negative images and the consideration of temporal leakage. In conclusion, the
feasibility of metric object estimation using projective geometry coupled with
regression corrections is demonstrated. This approach paves the way for the
development of robust, low-cost, automated monitoring systems for urban aquatic
environments.

</details>


### [4] [RareFlow: Physics-Aware Flow-Matching for Cross-Sensor Super-Resolution of Rare-Earth Features](https://arxiv.org/abs/2510.23816)
*Forouzan Fallah,Wenwen Li,Chia-Yu Hsu,Hyunho Lee,Yezhou Yang*

Main category: cs.CV

TL;DR: RareFlow 是一种用于遥感图像超分辨率的物理感知框架，旨在提高罕见地形特征下的稳健性。通过双条件架构和多面损失函数，确保生成图像在光谱和辐射度上的物理一致性，同时通过不确定性量化机制减少误造特征，取得显著优于现有方法的性能改善。


<details>
  <summary>Details</summary>
Motivation: 当前超分辨率技术在处理罕见地形特征时效果不佳，视觉上看似合理但物理上并不准确。RareFlow 旨在解决这种由传感器种类多样导致的 out-of-distribution 数据下超分辨率任务的挑战，提高生成结果的质量和准确性。

Method: RareFlow 采用了双条件架构：门控 ControlNet 用于从低分辨率输入中保留细粒度几何保真度，而文本提示用于合成复杂特征；多面损失函数确保输出在光谱和辐射度上的物理一致性；通过随机前向传播量化预测不确定性。

Result: 在新基准测试中，RareFlow 的输出被地理物理专家评为接近真实图像的保真度，显著优于现有最佳方法，体现出优于现有方法的性能，特别是在感知度量方面的 FID 几乎降低了 40%。

Conclusion: RareFlow 为数据稀缺的科学领域提供了高保真合成的稳健框架，为在严苛领域偏移下控制生成提供了一种新范式。

Abstract: Super-resolution (SR) for remote sensing imagery often fails under
out-of-distribution (OOD) conditions, such as rare geomorphic features captured
by diverse sensors, producing visually plausible but physically inaccurate
results. We present RareFlow, a physics-aware SR framework designed for OOD
robustness. RareFlow's core is a dual-conditioning architecture. A Gated
ControlNet preserves fine-grained geometric fidelity from the low-resolution
input, while textual prompts provide semantic guidance for synthesizing complex
features. To ensure physically sound outputs, we introduce a multifaceted loss
function that enforces both spectral and radiometric consistency with sensor
properties. Furthermore, the framework quantifies its own predictive
uncertainty by employing a stochastic forward pass approach; the resulting
output variance directly identifies unfamiliar inputs, mitigating feature
hallucination. We validate RareFlow on a new, curated benchmark of multi-sensor
satellite imagery. In blind evaluations, geophysical experts rated our model's
outputs as approaching the fidelity of ground truth imagery, significantly
outperforming state-of-the-art baselines. This qualitative superiority is
corroborated by quantitative gains in perceptual metrics, including a nearly
40\% reduction in FID. RareFlow provides a robust framework for high-fidelity
synthesis in data-scarce scientific domains and offers a new paradigm for
controlled generation under severe domain shift.

</details>


### [5] [TRELLISWorld: Training-Free World Generation from Object Generators](https://arxiv.org/abs/2510.23880)
*Hanke Chen,Yuan Liu,Minchen Li*

Main category: cs.CV

TL;DR: 提出了一种不依赖于场景级别数据集或重新训练的文本驱动的3D场景生成方法，通过将现有的文本到3D对象扩散模型作为模块化贴图生成器，重新定义场景生成为多贴图去噪问题，实现了大范围、连贯的场景合成，同时保持局部语义控制。该方法支持多样化的场景布局、高效的生成和灵活的编辑。


<details>
  <summary>Details</summary>
Motivation: 现有的3D场景生成方法往往局限于单个对象生成，需要特定领域的训练数据，或者不支持360度的全景视图。这个工作希望通过一种不需要训练的方法来解决这些问题，以合成复杂的3D场景。

Method: 该方法将现有的文本到3D对象扩散模型重新定义为生成模块化贴图的工具，将场景生成问题转化为多贴图去噪问题，通过重叠的3D区域的独立生成和加权平均的无缝融合来生成场景。这使得生成大型连贯的场景变得可能，同时保持局部语义的控制。该方法基于对象级别的先验知识，需要的启发式最少，不需要场景级别数据集或重新训练。

Result: 展示了一种支持多样化场景布局、高效生成和灵活编辑的方法，为通用的、语言驱动的3D场景构建奠定简单而强大的基础。该方法能够合成大型的、连贯的场景，同时保持对场景局部细节的控制能力。

Conclusion: 这项工作提出了一种新颖的、不需要场景级别数据集或重训练的方法，用于文本驱动的3D场景生成。该方法基于对象级别先验，提供了场景布局多样化支持、高效生成及编辑功能，是通用语言驱动3D场景构建的基础。

Abstract: Text-driven 3D scene generation holds promise for a wide range of
applications, from virtual prototyping to AR/VR and simulation. However,
existing methods are often constrained to single-object generation, require
domain-specific training, or lack support for full 360-degree viewability. In
this work, we present a training-free approach to 3D scene synthesis by
repurposing general-purpose text-to-3D object diffusion models as modular tile
generators. We reformulate scene generation as a multi-tile denoising problem,
where overlapping 3D regions are independently generated and seamlessly blended
via weighted averaging. This enables scalable synthesis of large, coherent
scenes while preserving local semantic control. Our method eliminates the need
for scene-level datasets or retraining, relies on minimal heuristics, and
inherits the generalization capabilities of object-level priors. We demonstrate
that our approach supports diverse scene layouts, efficient generation, and
flexible editing, establishing a simple yet powerful foundation for
general-purpose, language-driven 3D scene construction.

</details>


### [6] [DynaStride: Dynamic Stride Windowing with MMCoT for Instructional Multi-Scene Captioning](https://arxiv.org/abs/2510.23907)
*Eddison Pham,Prisha Priyadarshini,Adrian Maliackel,Kanishk Bandi,Cristian Meo,Kevin Zhu*

Main category: cs.CV

TL;DR: DynaStride是一种生成连贯的场景级字幕的流水线，无需手动场景分割，适用于教学视频，通过自适应帧采样、多模态窗口和动态步幅窗口选择算法，更好地捕捉视觉语义和时间推理，生成高质量的字幕。


<details>
  <summary>Details</summary>
Motivation: 当前的教学视频字幕由于缺乏结构而可能缺乏连贯性和质量，造成混淆，破坏视频的教育目的。现有技术需要手动场景分割，这既耗时又不切实际。因此，引入DynaStride来填补这一空白，生成连贯的场景级字幕，以改进AI生成教学内容的能力。

Method: DynaStride采用自适应帧采样和多模态窗口来捕获每个场景的关键过渡，利用多模态链式思维过程生成多个动作-对象对，然后通过动态步幅窗口选择算法进行细化和融合，产生最后的场景级标题。

Result: DynaStride在基于n-gram的指标（如BLEU，METEOR）和语义相似性度量（如BERTScore，CLIPScore）上都优于基线方法（如VLLaMA3和GPT-4o），显示了在提高AI生成的教学内容质量方面的潜力。

Conclusion: DynaStride生成的字幕在时间和信息方面更为连贯，提示了一种改进AI生成教学内容的方法，具有强大的潜力。

Abstract: Scene-level captioning in instructional videos can enhance learning by
requiring an understanding of both visual cues and temporal structure. By
aligning visual cues with textual guidance, this understanding supports
procedural learning and multimodal reasoning, providing a richer context for
skill acquisition. However, captions that fail to capture this structure may
lack coherence and quality, which can create confusion and undermine the
video's educational intent. To address this gap, we introduce DynaStride, a
pipeline to generate coherent, scene-level captions without requiring manual
scene segmentation. Using the YouCookII dataset's scene annotations, DynaStride
performs adaptive frame sampling and multimodal windowing to capture key
transitions within each scene. It then employs a multimodal chain-of-thought
process to produce multiple action-object pairs, which are refined and fused
using a dynamic stride window selection algorithm that adaptively balances
temporal context and redundancy. The final scene-level caption integrates
visual semantics and temporal reasoning in a single instructional caption.
Empirical evaluations against strong baselines, including VLLaMA3 and GPT-4o,
demonstrate consistent gains on both N-gram-based metrics (BLEU, METEOR) and
semantic similarity measures (BERTScore, CLIPScore). Qualitative analyses
further show that DynaStride produces captions that are more temporally
coherent and informative, suggesting a promising direction for improving
AI-powered instructional content generation.

</details>


### [7] [TurboPortrait3D: Single-step diffusion-based fast portrait novel-view synthesis](https://arxiv.org/abs/2510.23929)
*Emily Kim,Julieta Martinez,Timur Bagautdinov,Jessica Hodgins*

Main category: cs.CV

TL;DR: 我们提出了一种方法TurboPortrait3D，用于生成高质量、低延迟的人脸多视角合成图像，该方法结合了图像扩散模型和图像到3D模型的优点，既保持了3D视角的一致性，又提高了生成图像的质量和细节


<details>
  <summary>Details</summary>
Motivation: 现有的图像到3D模型在生成可渲染的3D表示时容易产生视觉伪影，缺乏细节，并且无法完全保留对象的身份。图像扩散模型在生成高质量图像方面表现出色，但计算成本高且不具有3D基础，不能直接生成多视角一致的输出

Method: 该方法首先通过图像到3D模型生成初始3D表示和相应的噪声渲染，然后将这些噪声渲染输入到单步扩散模型进行细化，该模型根据输入图像进行条件训练，以实现多视角一致性。此外，该方法引入了一种新型有效的训练策略，包括在大量合成多视角数据上进行预训练，然后在高质量真实图像上进行微调

Result: 该方法在质量上和数量上都优于当前最先进的人脸多视角合成解决方案，同时在时间效率上保持高效

Conclusion: TurboPortrait3D能够在低延迟下生成高质量的人脸多视角合成图像，并且在质量和效率上都超越了当前的最先进技术

Abstract: We introduce TurboPortrait3D: a method for low-latency novel-view synthesis
of human portraits. Our approach builds on the observation that existing
image-to-3D models for portrait generation, while capable of producing
renderable 3D representations, are prone to visual artifacts, often lack of
detail, and tend to fail at fully preserving the identity of the subject. On
the other hand, image diffusion models excel at generating high-quality images,
but besides being computationally expensive, are not grounded in 3D and thus
are not directly capable of producing multi-view consistent outputs. In this
work, we demonstrate that image-space diffusion models can be used to
significantly enhance the quality of existing image-to-avatar methods, while
maintaining 3D-awareness and running with low-latency. Our method takes a
single frontal image of a subject as input, and applies a feedforward
image-to-avatar generation pipeline to obtain an initial 3D representation and
corresponding noisy renders. These noisy renders are then fed to a single-step
diffusion model which is conditioned on input image(s), and is specifically
trained to refine the renders in a multi-view consistent way. Moreover, we
introduce a novel effective training strategy that includes pre-training on a
large corpus of synthetic multi-view data, followed by fine-tuning on
high-quality real images. We demonstrate that our approach both qualitatively
and quantitatively outperforms current state-of-the-art for portrait novel-view
synthesis, while being efficient in time.

</details>


### [8] [PlanarGS: High-Fidelity Indoor 3D Gaussian Splatting Guided by Vision-Language Planar Priors](https://arxiv.org/abs/2510.23930)
*Xirui Jin,Renbiao Jin,Boying Li,Danping Zou,Wenxian Yu*

Main category: cs.CV

TL;DR: 提出了PlanarGS框架，用于改进室内场景的3D重建，尤其是在大面积低纹理区域中表现更优。该框架结合了语言提示的平面先验和几何先验，优化了3D高斯点云的平面一致性和几何细节。实验结果显示，与现有方法相比，PlanarGS在准确性和细节丰富度方面表现出色。


<details>
  <summary>Details</summary>
Motivation: 光照损失在大量低纹理区域的室内场景中导致3DGS的几何表示出现模棱两可的情况，无法恢复高质量的3D表面，因此需要一种新的方法来解决这一问题。

Method: 开发了一种基于语言提示的平面先验和几何先验方法来优化3D高斯点云，以此改进室内场景的3D重建表现。通过结合预训练的视觉-语言分割模型和跨视图融合技术，生成准确的平面先验并指导3D高斯的优化。此外，还添加了一个几何先验监督项，以引导高斯分布的深度和法线信息。

Result: 实验表明，与当前的先进技术相比，PlanarGS显著提高了重建的准确性和细节丰富度，特别是在包含大面积低纹理区域的室内环境中。

Conclusion: 通过引入语言提示和几何先验的优化方法，PlanarGS框架能够有效地提高3D重建质量，特别是在室内场景中表现更优。

Abstract: Three-dimensional Gaussian Splatting (3DGS) has recently emerged as an
efficient representation for novel-view synthesis, achieving impressive visual
quality. However, in scenes dominated by large and low-texture regions, common
in indoor environments, the photometric loss used to optimize 3DGS yields
ambiguous geometry and fails to recover high-fidelity 3D surfaces. To overcome
this limitation, we introduce PlanarGS, a 3DGS-based framework tailored for
indoor scene reconstruction. Specifically, we design a pipeline for
Language-Prompted Planar Priors (LP3) that employs a pretrained vision-language
segmentation model and refines its region proposals via cross-view fusion and
inspection with geometric priors. 3D Gaussians in our framework are optimized
with two additional terms: a planar prior supervision term that enforces planar
consistency, and a geometric prior supervision term that steers the Gaussians
toward the depth and normal cues. We have conducted extensive experiments on
standard indoor benchmarks. The results show that PlanarGS reconstructs
accurate and detailed 3D surfaces, consistently outperforming state-of-the-art
methods by a large margin. Project page: https://planargs.github.io

</details>


### [9] [Neural USD: An object-centric framework for iterative editing and control](https://arxiv.org/abs/2510.23956)
*Alejandro Escontrela,Shrinu Kushagra,Sjoerd van Steenkiste,Yulia Rubanova,Aleksander Holynski,Kelsey Allen,Kevin Murphy,Thomas Kipf*

Main category: cs.CV

TL;DR: 本文介绍了Neural Universal Scene Descriptor（Neural USD），一种用于精确、迭代地编辑生成图像的技术。Neural USD以结构化的方式表示场景和对象，并通过细调方法确保信号分离，实现了对对象外观、几何形状和姿态的独立控制。


<details>
  <summary>Details</summary>
Motivation: 现有的生成模型在编辑生成的图像时往往会导致全局变化，而非仅在指定区域进行局部修改。本文旨在解决这一问题，尤其是实现精确和迭代的对象编辑能力。

Method: 提出了一种新的框架Neural USD，该框架可以结构化地表示场景和对象，支持多样化的信号输入，最小化特定模型的约束，并实现了对单个对象的外观、几何形状和姿态的分离控制。文章还提出通过细调方法来确保这些控制信号的分离，以实现更精确的编辑。

Result: 实验显示，该框架能够支持迭代和增量的工作流程，从而实现了更精确和控制更具体的迭代对象编辑能力。

Conclusion: 本文推出了Neural USD框架，解决了生成模型在精确和迭代对象编辑方面的挑战，为实现对场景和对象的更细粒度控制提供了一种新的方法。

Abstract: Amazing progress has been made in controllable generative modeling,
especially over the last few years. However, some challenges remain. One of
them is precise and iterative object editing. In many of the current methods,
trying to edit the generated image (for example, changing the color of a
particular object in the scene or changing the background while keeping other
elements unchanged) by changing the conditioning signals often leads to
unintended global changes in the scene. In this work, we take the first steps
to address the above challenges. Taking inspiration from the Universal Scene
Descriptor (USD) standard developed in the computer graphics community, we
introduce the "Neural Universal Scene Descriptor" or Neural USD. In this
framework, we represent scenes and objects in a structured, hierarchical
manner. This accommodates diverse signals, minimizes model-specific
constraints, and enables per-object control over appearance, geometry, and
pose. We further apply a fine-tuning approach which ensures that the above
control signals are disentangled from one another. We evaluate several design
considerations for our framework, demonstrating how Neural USD enables
iterative and incremental workflows. More information at:
https://escontrela.me/neural_usd .

</details>


### [10] [SafeVision: Efficient Image Guardrail with Robust Policy Adherence and Explainability](https://arxiv.org/abs/2510.23960)
*Peiyang Xu,Minzhou Pan,Zhaorun Chen,Shuang Yang,Chaowei Xiao,Bo Li*

Main category: cs.CV

TL;DR: SafeVision 是一种新型的图片守护栏系统，结合了类似人类的推理能力，提高了适应性和透明度。该系统通过有效的数据收集和生成框架、遵循策略的训练流程和自定义损失函数提升了性能。此外，SafeVision通过与VisionHarm（高清数据集）的实验，显示出比GPT-4o等模型更好的表现，并且速度更快，适应新的威胁无需重新训练。


<details>
  <summary>Details</summary>
Motivation: 现有的图片守护栏模型依赖预定义的类别，并且仅仅基于特征进行学习而没有语义推理，导致无法适应新的威胁并且准确率低。SafeVision旨在解决这些问题，通过模仿人类的推理能力提高模型的适应性和透明度。同时，SafeVision提高检测速度和性能，同时动态适应新的威胁。

Method: SafeVision 包括一个有效的数据收集和生成框架，一个遵循策略的训练流水线以及一个定制的损失函数。此外还提出多样化的 QA 生成和训练策略来增强学习效果。 SafeVision 在推理时可以动态地与不断演变的安全策略保持一致，从而消除了对重新训练的需求，同时确保精确的风险评估和解释。

Result: 通过在 VisionHarm 数据集上的广泛实验，SafeVision 展示了其优于 GPT-4o 的性能：在 VisionHarm-T 数据集上优于 GPT-4o 8.6%，在 VisionHarm-C 数据集上优于 GPT-4o 15.5%，同时速度也快了 16 倍以上。 SafeVision 设置了全面、策略遵循和可解释的图像防护栏并能够动态适应新威胁。

Conclusion: SafeVision 是一个全面、遵循策略、可解释的图像保护系统，通过结合人类般的推理使得模型能够适应新的安全威胁。它在处理不断变化的安全威胁时无需重新训练，保持高效和准确的性能。

Abstract: With the rapid proliferation of digital media, the need for efficient and
transparent safeguards against unsafe content is more critical than ever.
Traditional image guardrail models, constrained by predefined categories, often
misclassify content due to their pure feature-based learning without semantic
reasoning. Moreover, these models struggle to adapt to emerging threats,
requiring costly retraining for new threats. To address these limitations, we
introduce SafeVision, a novel image guardrail that integrates human-like
reasoning to enhance adaptability and transparency. Our approach incorporates
an effective data collection and generation framework, a policy-following
training pipeline, and a customized loss function. We also propose a diverse QA
generation and training strategy to enhance learning effectiveness. SafeVision
dynamically aligns with evolving safety policies at inference time, eliminating
the need for retraining while ensuring precise risk assessments and
explanations. Recognizing the limitations of existing unsafe image benchmarks,
which either lack granularity or cover limited risks, we introduce VisionHarm,
a high-quality dataset comprising two subsets: VisionHarm Third-party
(VisionHarm-T) and VisionHarm Comprehensive(VisionHarm-C), spanning diverse
harmful categories. Through extensive experiments, we show that SafeVision
achieves state-of-the-art performance on different benchmarks. SafeVision
outperforms GPT-4o by 8.6% on VisionHarm-T and by 15.5% on VisionHarm-C, while
being over 16x faster. SafeVision sets a comprehensive, policy-following, and
explainable image guardrail with dynamic adaptation to emerging threats.

</details>


### [11] [Reasoning Visual Language Model for Chest X-Ray Analysis](https://arxiv.org/abs/2510.23968)
*Andriy Myronenko,Dong Yang,Baris Turkbey,Mariam Aboian,Sena Azamat,Esra Akcicek,Hongxu Yin,Pavlo Molchanov,Marc Edgar,Yufan He,Pengfei Guo,Yucheng Tang,Daguang Xu*

Main category: cs.CV

TL;DR: 本文提出了一种将链式思维（CoT）推理应用于胸部X光影像解读的框架，该框架通过高保真视觉编码和两阶段训练方法，提高了模型的透明性和可解释性，同时保持了预测准确性。研究结果表明，完整的推理踪迹能增加专家放射科医生的信心，支持错误审计，并减少最终报告的时间，从而提高了人机合作的安全性。


<details>
  <summary>Details</summary>
Motivation: 现有许多视觉语言模型在医学图像分析方面表现出强大的潜力，但多数模型缺乏透明的、逐步推理过程，而这是临床医生所依赖的。因此，研究动机在于开发一种能够提供透明、逐步推理过程的模型，以支持临床可审计性、误差分析及更安全的人机合作。

Method: 该研究引入了一种结合高保真视觉编码和两阶段训练方法的模型，第一阶段为带有推理风格的监督微调（SFT），第二阶段为适用于胸部X光异常现象列表的强化学习（RL）。该模型输出的推理过程反映了放射科医生的系统性思维过程，包括不确定性以及鉴别诊断。

Result: 在分布外评估中，该方法在保持预测准确性的同时，提高了模型的透明性和可解释性。在与专家放射科医生进行的读者研究中，完整的推理踪迹增加了医生的信心，支持了错误审计，减少了最终报告的时间。

Conclusion: 研究结论支持进一步研究和应用在胸部放射成像和其他医疗影像任务中，其中推理质量与预测质量一样重要的可靠、解释性AI的发展。已公开模型NV-Reason-CXR-3B供社区使用，以朝向这一目标迈进。

Abstract: Vision-language models (VLMs) have shown strong promise for medical image
analysis, but most remain opaque, offering predictions without the transparent,
stepwise reasoning clinicians rely on. We present a framework that brings
chain-of-thought (CoT) reasoning to chest X-ray interpretation. Inspired by
reasoning-first training paradigms, our approach is designed to learn how
experts reason, not just what they conclude, by aligning intermediate steps
with observable image evidence and radiology workflow. Beyond accuracy, the
explicit reasoning traces support clinical auditability: they reveal why a
conclusion was reached, which alternatives were considered, and where
uncertainty remains, enabling quality assurance, error analysis, and safer
human-AI collaboration.
  Our model couples high-fidelity visual encoding with a two-stage training
recipe: a reasoning-style supervised fine-tuning (SFT) followed by
reinforcement learning (RL) that uses verifiable rewards over a list of X-ray
abnormalities. The model outputs reasoning that mirrors radiologists systematic
thought process, uncertainty, and differential diagnosis. In
out-of-distribution evaluation, the approach achieves competitive multi-label
classification while improving interpretability. In a reader study with expert
radiologists, full reasoning traces increased confidence, supported error
auditing, and reduced time to finalize reports. We release code and the model
NV-Reason-CXR-3B to support community progress toward trustworthy, explainable
AI in chest radiography and other medical imaging tasks where reasoning quality
is as critical as prediction quality.

</details>


### [12] [TeleEgo: Benchmarking Egocentric AI Assistants in the Wild](https://arxiv.org/abs/2510.23981)
*Jiaqi Yan,Ruilong Ren,Jingren Liu,Shuning Xu,Ling Wang,Yiheng Wang,Yun Wang,Long Zhang,Xiangyu Chen,Changzhi Sun,Jixiang Luo,Dell Zhang,Hao Sun,Chi Zhang,Xuelong Li*

Main category: cs.CV

TL;DR: TeleEgo是一个评价情境智能助手在日常生活中的表现的数据集，包含14小时的视频、音频和文本信息，用于评估背景记忆、理解能力和跨背景推理能力，并设定了真实时间准确性和记忆持久时间这两个关键指标


<details>
  <summary>Details</summary>
Motivation: 现有的基准测试往往在单独评估这些能力、缺乏现实的流计算场景或仅支持短期任务方面存在不足，因此提出TeleEgo来评估情境人工智能助手在真实环境中的表现

Method: 创建一个包含多模态数据集TeleEgo，包括视频、音频和文本等长期实时数据，并定义了12个诊断子任务以及两个评估指标：真实时间准确性及记忆持久时间

Result: 该数据集能够在流计算设置下严格评测，包含3291个人验证的问题涵盖多种形式，具有高度真实性和综合评估

Conclusion: TeleEgo为实际情境智能助手的发展提供了一个现实和全面的评估手段

Abstract: Egocentric AI assistants in real-world settings must process multi-modal
inputs (video, audio, text), respond in real time, and retain evolving
long-term memory. However, existing benchmarks typically evaluate these
abilities in isolation, lack realistic streaming scenarios, or support only
short-term tasks. We introduce \textbf{TeleEgo}, a long-duration, streaming,
omni-modal benchmark for evaluating egocentric AI assistants in realistic daily
contexts. The dataset features over 14 hours per participant of synchronized
egocentric video, audio, and text across four domains: work \& study, lifestyle
\& routines, social activities, and outings \& culture. All data is aligned on
a unified global timeline and includes high-quality visual narrations and
speech transcripts, curated through human refinement.TeleEgo defines 12
diagnostic subtasks across three core capabilities: Memory (recalling past
events), Understanding (interpreting the current moment), and Cross-Memory
Reasoning (linking distant events). It contains 3,291 human-verified QA items
spanning multiple question formats (single-choice, binary, multi-choice, and
open-ended), evaluated strictly in a streaming setting. We propose two key
metrics -- Real-Time Accuracy and Memory Persistence Time -- to jointly assess
correctness, temporal responsiveness, and long-term retention. TeleEgo provides
a realistic and comprehensive evaluation to advance the development of
practical AI assistants.

</details>


### [13] [AdvBlur: Adversarial Blur for Robust Diabetic Retinopathy Classification and Cross-Domain Generalization](https://arxiv.org/abs/2510.24000)
*Heethanjan Kanagalingam,Thenukan Pathmanathan,Mokeeshan Vathanakumar,Tharmakulasingam Mukunthan*

Main category: cs.CV

TL;DR: 这项研究提出了一种新的糖尿病视网膜病变（DR）分类方法，通过集成对抗模糊图像和使用双重损失函数框架来提高对不同分布变化的鲁棒性。实验表明，该方法在未见过的外部数据集上达到了与最先进的领域泛化DR模型相当的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的深度学习模型在预测DR时，由于成像设备、人口统计学差异和成像条件的不同导致的数据分布变化而难以保持鲁棒性。为了解决这一重要限制，提出了AdvBlur方法。

Method: 方法包括将对抗模糊图像集成到数据集中，并应用双重损失函数框架，以解决领域泛化的问题。通过广泛的实验来探索如相机类型、低质量图像和数据集大小等不同因素的影响，并进行消融研究，验证了所选方法的有效性。

Result: 实验结果表明，该方法有效地减轻了未见分布变化的影响，并在未见过的外部数据集上达到了与最先进的领域泛化DR模型相当的性能。

Conclusion: 通过提出的AdvBlur方法，可以更有效地预测DR，特别在解决领域泛化和各种分布变化问题上具有显著的优越性。

Abstract: Diabetic retinopathy (DR) is a leading cause of vision loss worldwide, yet
early and accurate detection can significantly improve treatment outcomes.
While numerous Deep learning (DL) models have been developed to predict DR from
fundus images, many face challenges in maintaining robustness due to
distributional variations caused by differences in acquisition devices,
demographic disparities, and imaging conditions. This paper addresses this
critical limitation by proposing a novel DR classification approach, a method
called AdvBlur. Our method integrates adversarial blurred images into the
dataset and employs a dual-loss function framework to address domain
generalization. This approach effectively mitigates the impact of unseen
distributional variations, as evidenced by comprehensive evaluations across
multiple datasets. Additionally, we conduct extensive experiments to explore
the effects of factors such as camera type, low-quality images, and dataset
size. Furthermore, we perform ablation studies on blurred images and the loss
function to ensure the validity of our choices. The experimental results
demonstrate the effectiveness of our proposed method, achieving competitive
performance compared to state-of-the-art domain generalization DR models on
unseen external datasets.

</details>


### [14] [Towards the Automatic Segmentation, Modeling and Meshing of the Aortic Vessel Tree from Multicenter Acquisitions: An Overview of the SEG.A. 2023 Segmentation of the Aorta Challenge](https://arxiv.org/abs/2510.24009)
*Yuan Jin,Antonio Pepe,Gian Marco Melito,Yuxuan Chen,Yunsu Byeon,Hyeseong Kim,Kyungwon Kim,Doohyun Park,Euijoon Choi,Dosik Hwang,Andriy Myronenko,Dong Yang,Yufan He,Daguang Xu,Ayman El-Ghotni,Mohamed Nabil,Hossam El-Kady,Ahmed Ayyad,Amr Nasr,Marek Wodzinski,Henning Müller,Hyeongyu Kim,Yejee Shin,Abbas Khan,Muhammad Asad,Alexander Zolotarev,Caroline Roney,Anthony Mathur,Martin Benning,Gregory Slabaugh,Theodoros Panagiotis Vagenas,Konstantinos Georgas,George K. Matsopoulos,Jihan Zhang,Zhen Zhang,Liqin Huang,Christian Mayer,Heinrich Mächler,Jan Egger*

Main category: cs.CV

TL;DR: SEGA挑战赛推出了一个大型的公共多机构数据集，用于主动脉血管树的分割，吸引了基于深度学习的方法，特别是3D U-Net，表现出色。模型融合显示出比单一模型更好的性能。这项研究为未来的研究提供了宝贵的资源和新的性能基准，促进了临床可翻译工具的发展。


<details>
  <summary>Details</summary>
Motivation: 自动化分析主动脉血管树(AVT)在临床中具有巨大潜力，但由于缺乏高质量的共享数据，其发展受到阻碍。此研究旨在通过引入新的数据集来推动AVT分割领域的进步。

Method: SEGA挑战赛通过公开大型多机构数据集，以及对隐藏测试集的性能评估，来促进深度学习方法的发展，特别是3D U-Net在AVT分割中的应用，同时也探索了模型融合的方法。

Result: 深度学习方法，特别是3D U-Net架构，表现最优。模型融合相比于单个模型有显著的性能提升。发现算法的设计，特别是定制的后处理步骤，以及训练数据的特性是影响性能的重要因素。

Conclusion: 此次研究为AVT分割提供了新的性能基准，并且为未来的研究提供了一个持久的资源，有助于开发更稳健的临床可翻译工具。

Abstract: The automated analysis of the aortic vessel tree (AVT) from computed
tomography angiography (CTA) holds immense clinical potential, but its
development has been impeded by a lack of shared, high-quality data. We
launched the SEG.A. challenge to catalyze progress in this field by introducing
a large, publicly available, multi-institutional dataset for AVT segmentation.
The challenge benchmarked automated algorithms on a hidden test set, with
subsequent optional tasks in surface meshing for computational simulations. Our
findings reveal a clear convergence on deep learning methodologies, with 3D
U-Net architectures dominating the top submissions. A key result was that an
ensemble of the highest-ranking algorithms significantly outperformed
individual models, highlighting the benefits of model fusion. Performance was
strongly linked to algorithmic design, particularly the use of customized
post-processing steps, and the characteristics of the training data. This
initiative not only establishes a new performance benchmark but also provides a
lasting resource to drive future innovation toward robust, clinically
translatable tools.

</details>


### [15] [Mars-Bench: A Benchmark for Evaluating Foundation Models for Mars Science Tasks](https://arxiv.org/abs/2510.24010)
*Mirali Purohit,Bimal Gajera,Vatsal Malaviya,Irish Mehta,Kunal Kasodekar,Jacob Adler,Steven Lu,Umaa Rebbapragada,Hannah Kerner*

Main category: cs.CV

TL;DR: Mars-Bench 是首个专门为火星相关任务设计的评估基准，涵盖分类、分割和目标检测等任务，首次提供了系统性评估火星任务相关模型的标准框架和数据集


<details>
  <summary>Details</summary>
Motivation: 在地球观测等领域，标准化基准的缺乏限制了火星科学研究的发展。本文旨在通过引入 Mars-Bench 来填补这一空白，推动火星领域基础模型的发展

Method: 提供标准化、即用型的数据集以及基于自然图像、地球卫星数据和视觉-语言模型的预训练模型的基线评估

Result: 所有分析结果表明，特定领域的基础模型对火星任务可能有优势，这鼓励了对领域适应性预训练的进一步探索

Conclusion: Mars-Bench 建立了开发和比较火星科学研究用机器学习模型的标准框架。数据、模型和代码可从项目官网下载

Abstract: Foundation models have enabled rapid progress across many specialized domains
by leveraging large-scale pre-training on unlabeled data, demonstrating strong
generalization to a variety of downstream tasks. While such models have gained
significant attention in fields like Earth Observation, their application to
Mars science remains limited. A key enabler of progress in other domains has
been the availability of standardized benchmarks that support systematic
evaluation. In contrast, Mars science lacks such benchmarks and standardized
evaluation frameworks, which have limited progress toward developing foundation
models for Martian tasks. To address this gap, we introduce Mars-Bench, the
first benchmark designed to systematically evaluate models across a broad range
of Mars-related tasks using both orbital and surface imagery. Mars-Bench
comprises 20 datasets spanning classification, segmentation, and object
detection, focused on key geologic features such as craters, cones, boulders,
and frost. We provide standardized, ready-to-use datasets and baseline
evaluations using models pre-trained on natural images, Earth satellite data,
and state-of-the-art vision-language models. Results from all analyses suggest
that Mars-specific foundation models may offer advantages over general-domain
counterparts, motivating further exploration of domain-adapted pre-training.
Mars-Bench aims to establish a standardized foundation for developing and
comparing machine learning models for Mars science. Our data, models, and code
are available at: https://mars-bench.github.io/.

</details>


### [16] [AutoPrompt: Automated Red-Teaming of Text-to-Image Models via LLM-Driven Adversarial Prompts](https://arxiv.org/abs/2510.24034)
*Yufan Liu,Wanqian Zhang,Huashan Chen,Lin Wang,Xiaojun Jia,Zheng Lin,Weiping Wang*

Main category: cs.CV

TL;DR: 介绍了一种名为APTP的新框架，通过使用大语言模型（LLM）自动生成对人类可读的对抗性后缀，来主动评估文本到图像（T2I）模型的安全漏洞。此方法提高了对抗性生成的安全性与有效性。


<details>
  <summary>Details</summary>
Motivation: 现有多数文本到图像模型的安全机制容易受到对抗性提示的影响，生成不安全的图像。当前的红队测试方法依赖于白盒访问和耗时的逐提示优化，需要改进的生成策略。提出APTP方法，以解决这些问题，通过大语言模型自动生成有效的对抗性提示，提高了红队测试的效率和实用性。

Method: APTP使用大语言模型生成对抗性后缀，通过交替优化和微调模型策略提高生成对抗性提示的有效性。在优化过程中，采用了双规避策略，既保证生成的人类可读性，也绕过了基于困惑度和黑名单词的过滤。具体来说，利用辅助的大语言模型困惑度评分来约束生成的人类可读提示，并引入禁止词惩罚来抑制黑名单词的生成。

Result: 实验结果显示，APTP生成的对抗性提示表现出了优秀的红队测试性能，具有更好的零样本迁移能力，能够对未见提示进行即时适应，并展示了现有商业API（例如Leonardo.Ai）中的关键漏洞。

Conclusion: APTP引入了一种新颖的红队测试方法，通过大语言模型自动生成对抗性提示，验证了其在评估T2I模型安全性能方面的优越性。

Abstract: Despite rapid advancements in text-to-image (T2I) models, their safety
mechanisms are vulnerable to adversarial prompts, which maliciously generate
unsafe images. Current red-teaming methods for proactively assessing such
vulnerabilities usually require white-box access to T2I models, and rely on
inefficient per-prompt optimization, as well as inevitably generate
semantically meaningless prompts easily blocked by filters. In this paper, we
propose APT (AutoPrompT), a black-box framework that leverages large language
models (LLMs) to automatically generate human-readable adversarial suffixes for
benign prompts. We first introduce an alternating optimization-finetuning
pipeline between adversarial suffix optimization and fine-tuning the LLM
utilizing the optimized suffix. Furthermore, we integrates a dual-evasion
strategy in optimization phase, enabling the bypass of both perplexity-based
filter and blacklist word filter: (1) we constrain the LLM generating
human-readable prompts through an auxiliary LLM perplexity scoring, which
starkly contrasts with prior token-level gibberish, and (2) we also introduce
banned-token penalties to suppress the explicit generation of banned-tokens in
blacklist. Extensive experiments demonstrate the excellent red-teaming
performance of our human-readable, filter-resistant adversarial prompts, as
well as superior zero-shot transferability which enables instant adaptation to
unseen prompts and exposes critical vulnerabilities even in commercial APIs
(e.g., Leonardo.Ai.).

</details>


### [17] [Kernelized Sparse Fine-Tuning with Bi-level Parameter Competition for Vision Models](https://arxiv.org/abs/2510.24037)
*Shufan Shen,Junshu Sun,Shuhui Wang,Qingming Huang*

Main category: cs.CV

TL;DR: 本文提出了一种名为SNELLA的一阶段方法，以解决当前稀疏调优方法的限制，通过引入非线性核函数来扩展低秩分解，采用自适应双层稀疏分配机制来寻找与任务相关联的权重，实验表明SNELLA在低内存使用的情况下达到了SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 目前的稀疏调优方法存在两个阶段，即定位任务相关的权重和更新权重，前者的参数调整在微调过程中被忽视，后者的稀疏掩码导致高内存使用。本文提出了一种一阶段方法SNELLA来克服这些限制。

Method: SNELLA选择性更新权重矩阵，通过将其添加到由两个低秩学习矩阵合并而成的稀疏矩阵中，并引入非线性核函数来增加合并矩阵的秩，从而防止权重更新之间的相互依存关系。此外，SNELLA还提出了一种自适应双层稀疏分配机制，以鼓励权重在层间和层内根据他们的重要性分数进行竞争。

Result: 在不同的图像预训练模型上进行分类、分割和生成任务实验后，结果显示，SNELLA在低内存使用的情况下实现了SOTA性能，尤其在FGVC基准测试中比SPT-LoRA获得了更高的Top-1准确性。

Conclusion: SNELLA通过合理的方法克服了现有稀疏调优方法中的问题，并在实验中证明了其在低内存使用情况下达到的SOTA性能。

Abstract: Parameter-efficient fine-tuning (PEFT) aims to adapt pre-trained vision
models to downstream tasks. Among PEFT paradigms, sparse tuning achieves
remarkable performance by adjusting only the weights most relevant to
downstream tasks, rather than densely tuning the entire weight matrix. Current
methods follow a two-stage paradigm. First, it locates task-relevant weights by
gradient information, which overlooks the parameter adjustments during
fine-tuning and limits the performance. Second, it updates only the located
weights by applying a sparse mask to the gradient of the weight matrix, which
results in high memory usage due to the storage of all weight matrices in the
optimizer. In this paper, we propose a one-stage method named SNELLA to
overcome the above limitations. For memory usage, SNELLA selectively updates
the weight matrix by adding it to another sparse matrix that is merged by two
low-rank learnable matrices. We extend the low-rank decomposition by
introducing nonlinear kernel functions, thereby increasing the rank of the
resulting merged matrix to prevent the interdependency among weight updates,
enabling better adaptation to downstream tasks. For locating task-relevant
weights, we propose an adaptive bi-level sparsity allocation mechanism that
encourages weights to compete across and inside layers based on their
importance scores in an end-to-end manner. Extensive experiments are conducted
on classification, segmentation, and generation tasks using different
pre-trained vision models. The results show that SNELLA achieves SOTA
performance with low memory usage. Notably, SNELLA obtains 1.8% (91.9% v.s.
90.1%) higher Top-1 accuracy on the FGVC benchmark compared to SPT-LoRA.
Compared to previous methods, SNELLA achieves a memory reduction of 31.1%-39.9%
across models with parameter scales from 86M to 632M. Our source codes are
available at https://github.com/ssfgunner/SNELL.

</details>


### [18] [Enhancing CLIP Robustness via Cross-Modality Alignment](https://arxiv.org/abs/2510.24038)
*Xingyu Zhu,Beier Zhu,Shuo Wang,Kesen Zhao,Hanwang Zhang*

Main category: cs.CV

TL;DR: 本文提出了COLA框架，通过最优传输解决了CLIP等视觉-语言模型在对抗攻击下性能严重下降的问题。COLA可通过投影对抗图像嵌入和稳定交叉模态对齐来提高其对抗稳健性，实验证明其在多个数据集上都有显著改进。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉-语言模型在对抗攻击下性能显著下降，现有方法主要是微调或提示优化，未能解决编码特征间存在差距的问题，特别是对抗性扰动会加剧文本和图像特征之间的错位。因此，本文提出了COLA框架，以解决对抗性对齐问题。

Method: COLA首先将对抗图像嵌入投影到由类别文本特征张成的子空间中，从而过滤掉非语义扭曲，保存了判别信息。然后，它将图像和文本视为多个增强视图的离散分布，并通过最优传输优化它们的对齐。这种设计确保了即使在对抗条件下，跨模态对齐也是稳定的，并且该技术容许仅使用现成的微调模型，不需要对其进行再训练。

Result: 实验结果表明，COLA在多种零样本分类基准测试中都表现出色，并且在PGD对抗攻击下平均提高了ImageNet和其他变体6.7%的性能。同时，它也具备高精度维护，代表性案例是在干净样本上的高准确性。

Conclusion: 通过使用COLA框架，可以有效地解决视觉-语言模型在对抗性扰动下的性能下降问题，该框架具有训练自由性和与现成模型的兼容性优点，即使没有额外的训练需求也能够显著提高模型的稳健性和性能。

Abstract: Vision-language models (VLMs) such as CLIP demonstrate strong generalization
in zero-shot classification but remain highly vulnerable to adversarial
perturbations. Existing methods primarily focus on adversarial fine-tuning or
prompt optimization; they often overlook the gaps in CLIP's encoded features,
which is shown as the text and image features lie far apart from each other.
This misalignment is significantly amplified under adversarial perturbations,
leading to severe degradation in classification performance. To address this
problem, we propose Cross-modality Alignment, dubbed COLA, an optimal
transport-based framework that explicitly addresses adversarial misalignment by
restoring both global image-text alignment and local structural consistency in
the feature space. (1) COLA first projects adversarial image embeddings onto a
subspace spanned by class text features, effectively filtering out non-semantic
distortions while preserving discriminative information. (2) It then models
images and texts as discrete distributions over multiple augmented views and
refines their alignment via OT, with the subspace projection seamlessly
integrated into the cost computation. This design ensures stable cross-modal
alignment even under adversarial conditions. COLA is training-free and
compatible with existing fine-tuned models. Extensive evaluations across 14
zero-shot classification benchmarks demonstrate the effectiveness of COLA,
especially with an average improvement of 6.7% on ImageNet and its variants
under PGD adversarial attacks, while maintaining high accuracy on clean
samples.

</details>


### [19] [OmniText: A Training-Free Generalist for Controllable Text-Image Manipulation](https://arxiv.org/abs/2510.24093)
*Agus Gunawan,Samuel Teodoro,Yun Chen,Soo Ye Kim,Jihyong Oh,Munchurl Kim*

Main category: cs.CV

TL;DR: 本文提出了一种名为OmniText的方法，用于解决当前基于扩散的文本合成方法在文本去除、文本风格控制以及避免文本幻觉等方面的不足，能够实现多种文本图像操纵任务。


<details>
  <summary>Details</summary>
Motivation: 现有的基于扩散的文本合成方法虽然在插入和编辑图像中的文本时表现良好，但存在无法去除文本、缺乏对渲染文本样式的控制以及生成重复字母这三个主要限制。因此，提出OmniText以解决这些挑战，扩展更多文本图像操纵任务的支持。

Method: 通过研究交叉和自我注意机制的性质，提出OmniText可以去除文本并通过新颖的损失函数控制文本样式和内容。具体包括自我注意反转以去除文本，交叉注意内容损失以提高文本渲染准确性，以及自我注意风格损失以实现样式定制。此外，还创建了一个包含多种TIM任务的OmniText-Bench数据集。

Result: OmniText方法可以解决当前文本图像操纵任务中的三个主要限制，实现了文本去除和文本风格控制，同时降低了文本幻觉的产生。实验表明，该方法在多种任务上表现优越，且随着任务类型和方法的增加，该方法的优越性越为明显。

Conclusion: OmniText是第一个能够处理多种文本图像操纵任务的方法，与专门的方法相比，其性能在多项任务和度量标准上都处于最佳水平。

Abstract: Recent advancements in diffusion-based text synthesis have demonstrated
significant performance in inserting and editing text within images via
inpainting. However, despite the potential of text inpainting methods, three
key limitations hinder their applicability to broader Text Image Manipulation
(TIM) tasks: (i) the inability to remove text, (ii) the lack of control over
the style of rendered text, and (iii) a tendency to generate duplicated
letters. To address these challenges, we propose OmniText, a training-free
generalist capable of performing a wide range of TIM tasks. Specifically, we
investigate two key properties of cross- and self-attention mechanisms to
enable text removal and to provide control over both text styles and content.
Our findings reveal that text removal can be achieved by applying
self-attention inversion, which mitigates the model's tendency to focus on
surrounding text, thus reducing text hallucinations. Additionally, we
redistribute cross-attention, as increasing the probability of certain text
tokens reduces text hallucination. For controllable inpainting, we introduce
novel loss functions in a latent optimization framework: a cross-attention
content loss to improve text rendering accuracy and a self-attention style loss
to facilitate style customization. Furthermore, we present OmniText-Bench, a
benchmark dataset for evaluating diverse TIM tasks. It includes input images,
target text with masks, and style references, covering diverse applications
such as text removal, rescaling, repositioning, and insertion and editing with
various styles. Our OmniText framework is the first generalist method capable
of performing diverse TIM tasks. It achieves state-of-the-art performance
across multiple tasks and metrics compared to other text inpainting methods and
is comparable with specialist methods.

</details>


### [20] [Enhancing Pre-trained Representation Classifiability can Boost its Interpretability](https://arxiv.org/abs/2510.24105)
*Shufan Shen,Zhaobo Qi,Junshu Sun,Qingming Huang,Qi Tian,Shuhui Wang*

Main category: cs.CV

TL;DR: 这篇论文提出了一种新的衡量预训练模型表示可解释性的方法，并发现表示的可解释性和分类性能之间存在正相关性。这表明可以通过增加表示的可解释性来提升其分类性能，同时也减少了准确性的损失。这一发现提高了预训练视觉模型在解释性和分类性上的统一改进的可能性。


<details>
  <summary>Details</summary>
Motivation: 预训练模型的视觉表示倾向于下游任务的分类性能，但为了提高表示的可解释性，提出了新的衡量方法，探讨表示同时提升可解释性和分类性的可能性。

Method: 论文提出了一种衡量预训练视觉表示可解释性的新指标——固有可解释性评分（Inherent Interpretability Score, IIS），该指标通过评估解释中的信息损失来衡量表示中可解释语义的比例。并基于该指标进行表示的可解释性分析。

Result: 研究发现可解释性和分类性之间存在正相关性，表明通过增加解释性可以提升分类性能，并减少准确性损失。

Conclusion: 该研究提出了衡量预训练模型表示可解释性的新方法，并证明了通过优化解释性可以同时提升模型的分类性能。这为进一步改进预训练视觉模型的可解释性和分类性提供了依据。

Abstract: The visual representation of a pre-trained model prioritizes the
classifiability on downstream tasks, while the widespread applications for
pre-trained visual models have posed new requirements for representation
interpretability. However, it remains unclear whether the pre-trained
representations can achieve high interpretability and classifiability
simultaneously. To answer this question, we quantify the representation
interpretability by leveraging its correlation with the ratio of interpretable
semantics within the representations. Given the pre-trained representations,
only the interpretable semantics can be captured by interpretations, whereas
the uninterpretable part leads to information loss. Based on this fact, we
propose the Inherent Interpretability Score (IIS) that evaluates the
information loss, measures the ratio of interpretable semantics, and quantifies
the representation interpretability. In the evaluation of the representation
interpretability with different classifiability, we surprisingly discover that
the interpretability and classifiability are positively correlated, i.e.,
representations with higher classifiability provide more interpretable
semantics that can be captured in the interpretations. This observation further
supports two benefits to the pre-trained representations. First, the
classifiability of representations can be further improved by fine-tuning with
interpretability maximization. Second, with the classifiability improvement for
the representations, we obtain predictions based on their interpretations with
less accuracy degradation. The discovered positive correlation and
corresponding applications show that practitioners can unify the improvements
in interpretability and classifiability for pre-trained vision models. Codes
are available at https://github.com/ssfgunner/IIS.

</details>


### [21] [UHKD: A Unified Framework for Heterogeneous Knowledge Distillation via Frequency-Domain Representations](https://arxiv.org/abs/2510.24116)
*Fengming Yu,Haiwei Pan,Kejia Zhang,Jian Guan,Haiying Jiang*

Main category: cs.CV

TL;DR: Unified Heterogeneous Knowledge Distillation (UHKD) 提出了一种知识蒸馏框架，通过频率域中的中间特征来进行跨架构的知识转移，特别适用于异构模型的场景，实验表明其相较于最新方法在CIFAR-100和ImageNet-1K上有更好的性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有的知识蒸馏方法大多针对同构模型设计，在处理异构模型时效果不佳，尤其是在中间层特征涉及时表现更差。为解决这一问题，提出UHKD方法，旨在利用频率域中的中间特征来减少异构模型中的表示差异，并实现知识的有效传输。

Method: UHKD框架通过傅立叶变换捕捉全局特征信息，使用一个特征转换模块(FTM)产生紧凑的频率域教师特征表征，并使用可学习的特征对齐模块(FAM)将学生特征投影并进行多级匹配对齐。训练过程中，通过中间特征的均方误差和输出上的KL散度的联合目标函数进行。

Result: 实验结果表明，UHKD在CIFAR-100和ImageNet-1K数据集上的性能分别比最新方法高出5.59%和0.83%，证明了其在统一异构表示和高效利用视觉知识方面的有效性。

Conclusion: UHKD是一种有效的知识蒸馏框架，能够通过频率域中的中间特征实现跨架构的知识转移，特别适用于处理异构模型的场景，显著提升了模型的压缩效率和表示能力。

Abstract: Knowledge distillation (KD) is an effective model compression technique that
transfers knowledge from a high-performance teacher to a lightweight student,
reducing cost while maintaining accuracy. In visual applications, where
large-scale image models are widely used, KD enables efficient deployment.
However, architectural diversity introduces semantic discrepancies that hinder
the use of intermediate representations. Most existing KD methods are designed
for homogeneous models and degrade in heterogeneous scenarios, especially when
intermediate features are involved. Prior studies mainly focus on the logits
space, making limited use of the semantic information in intermediate layers.
To address this limitation, Unified Heterogeneous Knowledge Distillation (UHKD)
is proposed as a framework that leverages intermediate features in the
frequency domain for cross-architecture transfer. Fourier transform is applied
to capture global feature information, alleviating representational
discrepancies between heterogeneous teacher-student pairs. A Feature
Transformation Module (FTM) produces compact frequency-domain representations
of teacher features, while a learnable Feature Alignment Module (FAM) projects
student features and aligns them via multi-level matching. Training is guided
by a joint objective combining mean squared error on intermediate features with
Kullback-Leibler divergence on logits. Experiments on CIFAR-100 and ImageNet-1K
demonstrate gains of 5.59% and 0.83% over the latest method, highlighting UHKD
as an effective approach for unifying heterogeneous representations and
enabling efficient utilization of visual knowledge

</details>


### [22] [ETC: training-free diffusion models acceleration with Error-aware Trend Consistency](https://arxiv.org/abs/2510.24129)
*Jiajian Xie,Hubery Yin,Chen Li,Zhou Zhao,Shengyu Zhang*

Main category: cs.CV

TL;DR: 提出了一种名为Error-aware Trend Consistency (ETC)的框架，该框架通过一致的趋势预测器和模型特定的误差容限搜索机制来加速扩散过程，同时保持生成结果的一致性，实验证明ETC可以实现比FLUX快2.65倍的加速，同时保持几乎不变的一致性（-0.074 SSIM得分）


<details>
  <summary>Details</summary>
Motivation: 传统的训练无成本方法虽然加速了扩散过程，但忽视了去噪趋势，缺乏模型特定的误差控制，导致轨迹偏差和结果不一致。ETC框架提出了趋势预测器和误差容限搜索机制来解决这些问题

Method: ETC框架包括一致的趋势预测器和模型特定的误差容限搜索机制，趋势预测器用历史去噪模式预测未来的稳定趋势，误差容限搜索机制识别从语义规划到质量细化的过渡点，确定纠正阈值

Result: 实验显示，ETC可以实现比FLUX快2.65倍的加速，同时保持几乎不变的一致性（-0.074 SSIM得分）

Conclusion: ETC框架通过一致的趋势预测器和模型特定的误差容限搜索机制成功解决了传统训练无成本方法的问题，即在加速扩散过程的同时保持结果的一致性和质量

Abstract: Diffusion models have achieved remarkable generative quality but remain
bottlenecked by costly iterative sampling. Recent training-free methods
accelerate diffusion process by reusing model outputs. However, these methods
ignore denoising trends and lack error control for model-specific tolerance,
leading to trajectory deviations under multi-step reuse and exacerbating
inconsistencies in the generated results. To address these issues, we introduce
Error-aware Trend Consistency (ETC), a framework that (1) introduces a
consistent trend predictor that leverages the smooth continuity of diffusion
trajectories, projecting historical denoising patterns into stable future
directions and progressively distributing them across multiple approximation
steps to achieve acceleration without deviating; (2) proposes a model-specific
error tolerance search mechanism that derives corrective thresholds by
identifying transition points from volatile semantic planning to stable quality
refinement. Experiments show that ETC achieves a 2.65x acceleration over FLUX
with negligible (-0.074 SSIM score) degradation of consistency.

</details>


### [23] [Compositional Image Synthesis with Inference-Time Scaling](https://arxiv.org/abs/2510.24133)
*Minsuk Ji,Sanghyeok Lee,Namhyuk Ahn*

Main category: cs.CV

TL;DR: 我们提出了一种训练无关的框架，结合了对象中心的方法和自我改进，以提高布局忠实性，同时保持美学质量。通过将显式布局与自我扩展的推断时间相结合，我们的框架在与提示的场景对齐方面优于最近的文本到图像模型。代码在https://github.com/gcl-inha/ReFocus上可用。


<details>
  <summary>Details</summary>
Motivation: 尽管现代文本到图像模型具有令人印象深刻的逼真度，但它们仍然在组合性方面遇到困难，经常无法准确渲染对象计数、属性和空间关系。为了解决这一挑战，我们提出了一个结合了对象中心的方法和自我改进的训练无关框架。

Method: 利用大型语言模型(LLMs)从输入提示中合成显式布局，并将这些布局注入图像生成过程，其中对象中心的视觉-语言模型(VLM judge)在迭代过程中重排多个候选者以选择与提示最对齐的结果。

Result: 通过统一显式布局对齐和基于自我扩展的推断时间，我们的框架在与提示的场景对齐方面优于最近的文本到图像模型。

Conclusion: 我们的框架在不牺牲美学质量的情况下提高了布局忠实性，并且优于最近的文本到图像模型。

Abstract: Despite their impressive realism, modern text-to-image models still struggle
with compositionality, often failing to render accurate object counts,
attributes, and spatial relations. To address this challenge, we present a
training-free framework that combines an object-centric approach with
self-refinement to improve layout faithfulness while preserving aesthetic
quality. Specifically, we leverage large language models (LLMs) to synthesize
explicit layouts from input prompts, and we inject these layouts into the image
generation process, where a object-centric vision-language model (VLM) judge
reranks multiple candidates to select the most prompt-aligned outcome
iteratively. By unifying explicit layout-grounding with self-refine-based
inference-time scaling, our framework achieves stronger scene alignment with
prompts compared to recent text-to-image models. The code are available at
https://github.com/gcl-inha/ReFocus.

</details>


### [24] [VC4VG: Optimizing Video Captions for Text-to-Video Generation](https://arxiv.org/abs/2510.24134)
*Yang Du,Zhuoran Lin,Kaiqiang Song,Biao Wang,Zhicheng Zheng,Tiezheng Ge,Bo Zheng,Qin Jin*

Main category: cs.CV

TL;DR: 本文介绍了VC4VG（视频描述优化框架），该框架通过分析视频描述并设计一套方法来优化视频生成模型所需的文字描述，从而提升视频生成质量。为了验证此框架的有效性，作者构建了新的基准VC4VG-Bench，并通过大量实验展示改进的视频描述质量可以提高视频生成的表现。所有基准工具和代码都可在GitHub上获取。


<details>
  <summary>Details</summary>
Motivation: 近年来，文本到视频生成（T2V）领域发展迅速，高质量的视频文本对模型生成一致性和指令符合度高的视频至关重要。然而，专门针对T2V训练优化视频描述的方法尚处于探索阶段。为此，本文提出了VC4VG，旨在通过优化视频描述来提升视频生成质量。

Method: 本文首先从T2V的角度分析了现有视频文本的不足，将用于视频重建所需的基本要素分解为多个维度，并提出了一套系统的描述设计方案。为了支持评估，作者构建了新的基准VC4VG-Bench，该基准包括细粒度的、多维度的、与T2V特定需求相配的度量标准。

Result: 大量的实验表明，改进后的视频描述质量与视频生成表现之间存在显著关联，验证了VC4VG的有效性。

Conclusion: 本文提出的VC4VG框架提供了一种新颖的方法来优化视频描述，以支持更高品质的视频生成。实验结果验证了这种方法的有效性。为进一步的研究，所有基准工具和代码都已公开。

Abstract: Recent advances in text-to-video (T2V) generation highlight the critical role
of high-quality video-text pairs in training models capable of producing
coherent and instruction-aligned videos. However, strategies for optimizing
video captions specifically for T2V training remain underexplored. In this
paper, we introduce VC4VG (Video Captioning for Video Generation), a
comprehensive caption optimization framework tailored to the needs of T2V
models.We begin by analyzing caption content from a T2V perspective,
decomposing the essential elements required for video reconstruction into
multiple dimensions, and proposing a principled caption design methodology. To
support evaluation, we construct VC4VG-Bench, a new benchmark featuring
fine-grained, multi-dimensional, and necessity-graded metrics aligned with
T2V-specific requirements.Extensive T2V fine-tuning experiments demonstrate a
strong correlation between improved caption quality and video generation
performance, validating the effectiveness of our approach. We release all
benchmark tools and code at https://github.com/qyr0403/VC4VG to support further
research.

</details>


### [25] [Enhancing Vision-Language Models for Autonomous Driving through Task-Specific Prompting and Spatial Reasoning](https://arxiv.org/abs/2510.24152)
*Aodi Wu,Xubo Luo*

Main category: cs.CV

TL;DR: 本文提出了一种系统框架，用于评估自主驾驶场景理解中的Vision-Language模型，该框架包含四个核心组件：任务分类与调度、任务特定提示、视觉组装模块和模型推理参数配置，实现了70.87%的平均准确率（干净数据）和72.85%的准确率（损坏数据），证明了结构化提示和空间定位对安全关键任务中VLM表现的显著提升。代码和提示可在GitHub上获取。


<details>
  <summary>Details</summary>
Motivation: 为了评估Vision-Language模型在自主驾驶场景理解中的性能，特别是在感知、预测、规划和干扰检测任务中的表现，提出了一种系统化的解决方案框架。通过采用结构化提示和空间定位方法，旨在提高模型在安全关键任务中的性能。

Method: 提出了一种包含四个核心组件的方法，其中包括：任务分类与调度、任务特定提示、视觉组装模块，以及基于任务的模型推理参数配置。框架实施在Qwen2.5-VL-72B模型上，通过这些组件的设计，实现了对各种场景的理解和处理。

Result: 实施的方法在干净数据集上达到了70.87％的平均准确率，而在损坏数据集上达到了72.85％的准确率，展示了结构化提示和空间定位在安全关键任务中提升VLM性能的重要作用。

Conclusion: 所提的框架及方法证明了结构化提示和空间定位显著提高了VLM在安全关键任务中的性能，特别是在受损数据集上表现突出，表现出较高的鲁棒性和准确性。这种方法为自主驾驶场景理解任务提供了新的研究方向。

Abstract: This technical report presents our solution for the RoboSense Challenge at
IROS 2025, which evaluates Vision-Language Models (VLMs) on autonomous driving
scene understanding across perception, prediction, planning, and corruption
detection tasks. We propose a systematic framework built on four core
components. First, a Mixture-of-Prompts router classifies questions and
dispatches them to task-specific expert prompts, eliminating interference
across diverse question types. Second, task-specific prompts embed explicit
coordinate systems, spatial reasoning rules, role-playing,
Chain-of-Thought/Tree-of-Thought reasoning, and few-shot examples tailored to
each task. Third, a visual assembly module composes multi-view images with
object crops, magenta markers, and adaptive historical frames based on question
requirements. Fourth, we configure model inference parameters (temperature,
top-p, message roles) per task to optimize output quality. Implemented on
Qwen2.5-VL-72B, our approach achieves 70.87% average accuracy on Phase-1 (clean
data) and 72.85% on Phase-2 (corrupted data), demonstrating that structured
prompting and spatial grounding substantially enhance VLM performance on
safety-critical autonomous driving tasks. Code and prompt are available at
https://github.com/wuaodi/UCAS-CSU-phase2.

</details>


### [26] [Vanish into Thin Air: Cross-prompt Universal Adversarial Attacks for SAM2](https://arxiv.org/abs/2510.24195)
*Ziqi Zhou,Yifan Hu,Yufei Song,Zijing Li,Shengshan Hu,Leo Yu Zhang,Dezhong Yao,Long Zheng,Hai Jin*

Main category: cs.CV

TL;DR: 研究首次分析了SAM2在视频分割中的性能，并提出了一种针对SAM2的跨提示通用对抗攻击方法UAP-SAM2，该方法利用双重语义偏差优化通用对抗样本，实验显示其显著优于现有最佳方法。


<details>
  <summary>Details</summary>
Motivation: 现有对SAM的攻击是否可以直接应用于其后继者SAM2尚未明确，本次研究旨在探索SAM2的脆弱性，并提出一种新的对抗攻击方法。

Method: 提出了一种基于双重语义偏差的UAP-SAM2跨提示通用对抗攻击方法，通过设计目标扫描策略减少提示依赖，同时优化通用对抗样本以打破帧间语义一致性。

Result: 实验结果表明，UAP-SAM2在多个数据集上的表现显著优于现有的最佳攻击方法。

Conclusion: 证明了现有针对SAM的攻击无法直接应用于SAM2，通过UAP-SAM2能够有效攻击SAM2，这突显了其脆弱性。

Abstract: Recent studies reveal the vulnerability of the image segmentation foundation
model SAM to adversarial examples. Its successor, SAM2, has attracted
significant attention due to its strong generalization capability in video
segmentation. However, its robustness remains unexplored, and it is unclear
whether existing attacks on SAM can be directly transferred to SAM2. In this
paper, we first analyze the performance gap of existing attacks between SAM and
SAM2 and highlight two key challenges arising from their architectural
differences: directional guidance from the prompt and semantic entanglement
across consecutive frames. To address these issues, we propose UAP-SAM2, the
first cross-prompt universal adversarial attack against SAM2 driven by dual
semantic deviation. For cross-prompt transferability, we begin by designing a
target-scanning strategy that divides each frame into k regions, each randomly
assigned a prompt, to reduce prompt dependency during optimization. For
effectiveness, we design a dual semantic deviation framework that optimizes a
UAP by distorting the semantics within the current frame and disrupting the
semantic consistency across consecutive frames. Extensive experiments on six
datasets across two segmentation tasks demonstrate the effectiveness of the
proposed method for SAM2. The comparative results show that UAP-SAM2
significantly outperforms state-of-the-art (SOTA) attacks by a large margin.

</details>


### [27] [MC-SJD : Maximal Coupling Speculative Jacobi Decoding for Autoregressive Visual Generation Acceleration](https://arxiv.org/abs/2510.24211)
*Junhyuk So,Hyunho Kook,Chaeyeon Jang,Eunhyeok Park*

Main category: cs.CV

TL;DR: 本文提出了一种名为MC-SJD的训练免费且无损的并行解码框架，用于加速AR视觉生成。通过最大化的概率使得连续迭代中的临时标记采样相同，该方法实现了对当前AR生成约4.2倍图像生成加速和13.3倍视频生成加速，且不降低输出质量


<details>
  <summary>Details</summary>
Motivation: 当前AR模型的推理速度慢，每生成一个物品需要数千步，限制了其实际应用。SJD虽有潜力加速AR生成，但因独立采样导致的迭代间临时标记不稳定严重降低了其效率。为解决此问题，我们提出了MC-SJD

Method: MC-SJD是一种基于耦合的信息理论方法，通过最大化连续迭代中临时标记相同采样的概率，以消除迭代间临时标记不稳定的影响，此方法仅需对现有算法进行单一修改就能实现加速

Result: 实验表明，MC-SJD能够实现图像生成约4.2倍和视频生成约13.3倍的加速，且保持输出质量不变

Conclusion: MC-SJD为AR视觉生成提供了一种有效且简单的加速方案，证明了该方法在图像和视频生成中的优越性能

Abstract: While autoregressive (AR) modeling has recently emerged as a new paradigm in
visual generation, its practical adoption is severely constrained by the slow
inference speed of per-token generation, which often requires thousands of
steps to produce a single sample. To address this challenge, we propose MC-SJD,
a training-free, lossless parallel decoding framework designed to accelerate AR
visual generation by extending the recently introduced Speculative Jacobi
Decoding (SJD). Although SJD shows strong potential for accelerating AR
generation, we demonstrate that token instability across iterations
significantly reduces the acceptance rate, a limitation that primarily arises
from the independent sampling process used during draft token generation. To
overcome this, we introduce MC-SJD, an information-theoretic approach based on
coupling, which substantially accelerates standard SJD by maximizing the
probability of sampling identical draft tokens across consecutive iterations,
all while preserving its lossless property. Remarkably, this method requires
only a single-line modification to the existing algorithm, yet achieves
substantial performance gains, delivering up to a ~4.2x acceleration in image
generation and ~13.3x acceleration in video generation compared to standard AR
decoding, without any degradation in output quality.

</details>


### [28] [Beyond Inference Intervention: Identity-Decoupled Diffusion for Face Anonymization](https://arxiv.org/abs/2510.24213)
*Haoxin Yang,Yihong Lin,Jingdan Kang,Xuemiao Xu,Yue Li,Cheng Xu,Shengfeng He*

Main category: cs.CV

TL;DR: 本文提出了一种新的用于人脸匿名化的框架ID	extsuperscript{2}Face，该框架在训练阶段就解耦了身份和非身份属性，不需要推理时的额外优化，显著提高了匿名化后的图像质量和数据利用性。


<details>
  <summary>Details</summary>
Motivation: 主流的扩散模型在推理时需要额外的干预来抑制身份信息，这可能会引发数据分布的变化和身份与非身份信息的纠缠，降低了图像质量和数据的实用性。本文的动机是为了找到一种不需要后处理就能有效抑制身份信息的方法，同时保持图像质量和数据的高利用性。

Method: 提出了一个训练为中心的人脸匿名化框架ID	extsuperscript{2}Face，该方法引入了一个条件扩散模型和身份解耦的潜在空间重组器，通过学习结构化的潜在空间来显式地解耦身份和非身份信息。通过身份变分自编码器来模拟身份特征，同时从具有相同身份的成对数据中提取非身份属性并通过双向潜在对齐进行校准，再通过条件的软门控融合表示以抑制身份泄漏。模型通过一个基于重组的重构损失进行训练以强制执行解耦。推理时，通过从学习到的身份空间中采样随机的身份向量来实现匿名化。为了进一步抑制身份泄露，引入了正交身份映射策略来强制样本身份向量与源身份向量之间的正交性。

Result: 实验表明，ID	extsuperscript{2}Face在视觉质量和身份抑制方面优于现有方法，并且能够更好地保持数据的实用性。

Conclusion: 本文提出了一种新颖且高效的训练中心人脸匿名化框架ID	extsuperscript{2}Face，通过学习结构化的潜在空间以明确地解耦身份和非身份信息，有效提高了匿名后的图像质量和数据的实用性，避免了主流扩散模型中常见的推理时需额外优化的问题。

Abstract: Face anonymization aims to conceal identity information while preserving
non-identity attributes. Mainstream diffusion models rely on inference-time
interventions such as negative guidance or energy-based optimization, which are
applied post-training to suppress identity features. These interventions often
introduce distribution shifts and entangle identity with non-identity
attributes, degrading visual fidelity and data utility. To address this, we
propose \textbf{ID\textsuperscript{2}Face}, a training-centric anonymization
framework that removes the need for inference-time optimization. The rationale
of our method is to learn a structured latent space where identity and
non-identity information are explicitly disentangled, enabling direct and
controllable anonymization at inference. To this end, we design a conditional
diffusion model with an identity-masked learning scheme. An Identity-Decoupled
Latent Recomposer uses an Identity Variational Autoencoder to model identity
features, while non-identity attributes are extracted from same-identity pairs
and aligned through bidirectional latent alignment. An Identity-Guided Latent
Harmonizer then fuses these representations via soft-gating conditioned on
noisy feature prediction. The model is trained with a recomposition-based
reconstruction loss to enforce disentanglement. At inference, anonymization is
achieved by sampling a random identity vector from the learned identity space.
To further suppress identity leakage, we introduce an Orthogonal Identity
Mapping strategy that enforces orthogonality between sampled and source
identity vectors. Experiments demonstrate that ID\textsuperscript{2}Face
outperforms existing methods in visual quality, identity suppression, and
utility preservation.

</details>


### [29] [SCOPE: Saliency-Coverage Oriented Token Pruning for Efficient Multimodel LLMs](https://arxiv.org/abs/2510.24214)
*Jinhong Deng,Wen Li,Joey Tianyi Zhou,Yang He*

Main category: cs.CV

TL;DR: 提出了SCOPE策略，旨在同时考虑视觉令牌的显著性和覆盖率，从而更完整地保留语义。在多个视觉-语言理解基准测试中，该方法表现优于现有方法。代码已开源。


<details>
  <summary>Details</summary>
Motivation: 现有视觉令牌裁剪方法侧重选择最显著的令牌，导致语义不完整。为解决这一问题，提出了SCOPE策略，该策略联合建模所选视觉令牌的显著性和覆盖率以更好地保留语义完整性。

Method: 引入基于令牌关系计算的集合覆盖度，并定义未选令牌的令牌覆盖度增益，量化了包含该令牌所能获得的额外覆盖度。通过将显著度评分整合到令牌覆盖度增益中来提出SCOPE评分，并且迭代选择具有最高SCOPE评分的令牌。实验在LLaVA-1.5和LLaVA-Next模型上的多个视觉-语言理解基准测试中进行。

Result: 实验结果表明，提出的SCOPE策略在多个视觉-语言理解基准测试中一贯地优于现有方法，代码已通过https://github.com/kinredon/SCOPE开源。

Conclusion: 通过联合建模视觉令牌的显著性和覆盖率，优先选择能带来有效信息和覆盖度的令牌，降低冗余度和计算开销，提高了语义完整性和模型效率。

Abstract: Multimodal Large Language Models (MLLMs) typically process a large number of
visual tokens, leading to considerable computational overhead, even though many
of these tokens are redundant. Existing visual token pruning methods primarily
focus on selecting the most salient tokens based on attention scores, resulting
in the semantic incompleteness of the selected tokens. In this paper, we
propose a novel visual token pruning strategy, called
\textbf{S}aliency-\textbf{C}overage \textbf{O}riented token \textbf{P}runing
for \textbf{E}fficient MLLMs (SCOPE), to jointly model both the saliency and
coverage of the selected visual tokens to better preserve semantic
completeness. Specifically, we introduce a set-coverage for a given set of
selected tokens, computed based on the token relationships. We then define a
token-coverage gain for each unselected token, quantifying how much additional
coverage would be obtained by including it. By integrating the saliency score
into the token-coverage gain, we propose our SCOPE score and iteratively select
the token with the highest SCOPE score. We conduct extensive experiments on
multiple vision-language understanding benchmarks using the LLaVA-1.5 and
LLaVA-Next models. Experimental results demonstrate that our method
consistently outperforms prior approaches. Our code is available at
\href{https://github.com/kinredon/SCOPE}{https://github.com/kinredon/SCOPE}.

</details>


### [30] [DeshadowMamba: Deshadowing as 1D Sequential Similarity](https://arxiv.org/abs/2510.24260)
*Zhaotong Yang,Yi Chen,Yanying Li,Shengfeng He,Yangyang Xu,Junyu Dong,Jian Yang,Yong Du*

Main category: cs.CV

TL;DR: 本文提出了一种新的序列建模方法DeshadowMamba，结合Mamba模型和CrossGate机制，以及ColorShift正则化，有效提升了图像去阴影的效果，达到了目前最先进的视觉质量和量化性能。


<details>
  <summary>Details</summary>
Motivation: 现有的基于注意力机制的深度模型在图像去阴影处理中存在固定注意力模式导致的光照线索混杂问题，本文旨在通过序列建模的方式来解决这些问题。

Method: 首先，使用Mamba模型进行全局上下文传播；然后，通过CrossGate机制实现阴影意识相似性的输入门注入，选择性地整合相关上下文；最后，引入ColorShift正则化来确保外观真实。

Result: 在公共基准测试上，DeshadowMamba展示了目前最好的视觉质量和强大的量化性能。

Conclusion: 通过序列建模和创新的方法，有效提高了图像去阴影的准确性和视觉质量，为图像处理领域提供了一个新的解决方案。

Abstract: Recent deep models for image shadow removal often rely on attention-based
architectures to capture long-range dependencies. However, their fixed
attention patterns tend to mix illumination cues from irrelevant regions,
leading to distorted structures and inconsistent colors. In this work, we
revisit shadow removal from a sequence modeling perspective and explore the use
of Mamba, a selective state space model that propagates global context through
directional state transitions. These transitions yield an efficient global
receptive field while preserving positional continuity. Despite its potential,
directly applying Mamba to image data is suboptimal, since it lacks awareness
of shadow-non-shadow semantics and remains susceptible to color interference
from nearby regions. To address these limitations, we propose CrossGate, a
directional modulation mechanism that injects shadow-aware similarity into
Mamba's input gate, allowing selective integration of relevant context along
transition axes. To further ensure appearance fidelity, we introduce ColorShift
regularization, a contrastive learning objective driven by global color
statistics. By synthesizing structured informative negatives, it guides the
model to suppress color contamination and achieve robust color restoration.
Together, these components adapt sequence modeling to the structural integrity
and chromatic consistency required for shadow removal. Extensive experiments on
public benchmarks demonstrate that DeshadowMamba achieves state-of-the-art
visual quality and strong quantitative performance.

</details>


### [31] [Training-free Source Attribution of AI-generated Images via Resynthesis](https://arxiv.org/abs/2510.24278)
*Pietro Bongini,Valentina Molinari,Andrea Costanzo,Benedetta Tondi,Mauro Barni*

Main category: cs.CV

TL;DR: 本文提出了一种无需训练的一次性归因方法，利用图像再合成技术对合成图像来源进行归因。该方法对图像描述生成提示，然后使用该提示重新合成图像，并将图像归因于生成的重构图像与原始图像最接近的模型。


<details>
  <summary>Details</summary>
Motivation: 在数据稀缺条件下，实现合成图像源的一次性归因是一个挑战。目前的方法往往需要大量的训练样本，这在数据稀缺的条件下难以实现。

Method: 提出了一种基于图像再合成的训练自由的一次性归因方法。生成图像描述的提示，然后使用该提示对所有候选源重新合成图像，最后归因于重构结果与原始图像最接近的模型。同时引入了包含商业和开源文本到图像生成器的合成图像数据集。该数据集为开发新的归因模型和测试其在不同生成架构下的能力提供了一个挑战性的框架

Result: 实验结果表明，该方法在只有少量样本用于训练或微调的情况下优于现有的归因技术。此外，新数据集是一个具有挑战性的基准，可用于开发和评价未来的少样本和零样本方法

Conclusion: 该方法提供了一种有效且无需训练的一次性归因手段，针对数据稀缺和模型泛化能力的挑战提供了一个有价值的解决方案。同时新数据集为未来的研究提供了一个有价值的基准。

Abstract: Synthetic image source attribution is a challenging task, especially in data
scarcity conditions requiring few-shot or zero-shot classification
capabilities. We present a new training-free one-shot attribution method based
on image resynthesis. A prompt describing the image under analysis is
generated, then it is used to resynthesize the image with all the candidate
sources. The image is attributed to the model which produced the resynthesis
closest to the original image in a proper feature space. We also introduce a
new dataset for synthetic image attribution consisting of face images from
commercial and open-source text-to-image generators. The dataset provides a
challenging attribution framework, useful for developing new attribution models
and testing their capabilities on different generative architectures. The
dataset structure allows to test approaches based on resynthesis and to compare
them to few-shot methods. Results from state-of-the-art few-shot approaches and
other baselines show that the proposed resynthesis method outperforms existing
techniques when only a few samples are available for training or fine-tuning.
The experiments also demonstrate that the new dataset is a challenging one and
represents a valuable benchmark for developing and evaluating future few-shot
and zero-shot methods.

</details>


### [32] [ViPER: Empowering the Self-Evolution of Visual Perception Abilities in Vision-Language Model](https://arxiv.org/abs/2510.24285)
*Juntian Zhang,Song Jin,Chuanqi Cheng,Yuhan Liu,Yankai Lin,Xun Zhang,Yufei Zhang,Fei Jiang,Guojun Yin,Wei Lin,Rui Yan*

Main category: cs.CV

TL;DR: ViPER提出了一种新的任务结构，用于增强视觉感知学习，并通过自举框架和循环训练策略来提升视觉语言模型的能力，提高了多个任务的性能，尤其是精细视觉感知任务上表现突出。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉语言模型在真实应用场景中面临视觉感知性能不足的问题，主要是因为高质量数据稀缺和现有方法（监督微调和强化学习微调）在视觉感知和文字推理之间难以平衡。为了应对这一挑战，提出了解决方案。

Method: ViPER通过两级强化学习整合图像级别和实例级别的重建，并建立闭环训练模式，利用内部合成数据来提升感知能力。该方法适用于Qwen2.5-VL家族，生成了Qwen-Viper系列。

Result: 通过Qwen-Viper模型在七个综合基准测试上的表现，显示了平均1.7％的性能提升，特别是在精细视觉感知任务上有6.0%的提升，证明了其在视觉语言场景中的优越性。此外，ViPER还展示了生成与理解之间的相互促进关系，这为开发更加自主和有能力的视觉语言模型提供了依据。

Conclusion: ViPER通过自监督和自我预测的闭环训练框架，显著提升了视觉语言模型的感知能力。该方法不仅提升了模型的性能，还加深了对生成与理解之间相互关系的理解，促进了VLM模型的发展。

Abstract: The limited capacity for fine-grained visual perception presents a critical
bottleneck for Vision-Language Models (VLMs) in real-world applications.
Addressing this is challenging due to the scarcity of high-quality data and the
limitations of existing methods: supervised fine-tuning (SFT) often compromises
general capabilities, while reinforcement fine-tuning (RFT) prioritizes textual
reasoning over visual perception. To bridge this gap, we propose a novel
two-stage task that structures visual perception learning as a coarse-to-fine
progressive process. Based on this task formulation, we develop ViPER, a
self-bootstrapping framework specifically designed to enable iterative
evolution through self-critiquing and self-prediction. By synergistically
integrating image-level and instance-level reconstruction with a two-stage
reinforcement learning strategy, ViPER establishes a closed-loop training
paradigm, where internally synthesized data directly fuel the enhancement of
perceptual ability. Applied to the Qwen2.5-VL family, ViPER produces the
Qwen-Viper series. With an average gain of 1.7% on seven comprehensive
benchmarks spanning various tasks and up to 6.0% on fine-grained perception,
Qwen-Viper consistently demonstrates superior performance across different
vision-language scenarios while maintaining generalizability. Beyond enabling
self-improvement in perceptual capabilities, ViPER provides concrete evidence
for the reciprocal relationship between generation and understanding, a
breakthrough to developing more autonomous and capable VLMs.

</details>


### [33] [Few-Shot Remote Sensing Image Scene Classification with CLIP and Prompt Learning](https://arxiv.org/abs/2510.24321)
*Ivica Dimitrovski,Vlatko Spasev,Ivan Kitanovski*

Main category: cs.CV

TL;DR: 本文研究了在远程遥感图像场景分类中使用提示学习作为一种轻量级且高效的适应策略来解决标注数据稀少和成本问题。通过实验，研究展示了提示学习在少样本场景下优于基线方法，尤其在跨域性能方面。这表明提示学习可以作为连接遥感领域的可扩展和高效解决方案。


<details>
  <summary>Details</summary>
Motivation: 目前，远程遥感应用中深度学习方法受限于标记数据稀缺和标注成本高昂的问题。尽管视觉-语言模型如CLIP通过视觉和文本模态的学习提供了有希望的方法，但它们直接应用于远程遥感表现不佳。本文意图通过探索提示学习作为解决这类问题的有效策略。

Method: 本文系统性地探讨了四种基于提示学习的方法：上下文优化、条件上下文优化、多模态提示学习和带有自调节约束的提示。这些方法涵盖了静态上下文优化到条件提示增强泛化、多模态提示和语义正则化的提示。

Result: 通过在多个基准遥感数据集上的广泛实验比较，研究展示了提示学习方法在少样本场景下优于基线。特别地，带有自调节约束的提示方法达到了最稳健的跨域表现。

Conclusion: 本文的研究结果强调了提示学习作为连接遥感领域差距的可扩展和高效解决方案，为未来研究提供了坚实的基础。

Abstract: Remote sensing applications increasingly rely on deep learning for scene
classification. However, their performance is often constrained by the scarcity
of labeled data and the high cost of annotation across diverse geographic and
sensor domains. While recent vision-language models like CLIP have shown
promise by learning transferable representations at scale by aligning visual
and textual modalities, their direct application to remote sensing remains
suboptimal due to significant domain gaps and the need for task-specific
semantic adaptation. To address this critical challenge, we systematically
explore prompt learning as a lightweight and efficient adaptation strategy for
few-shot remote sensing image scene classification. We evaluate several
representative methods, including Context Optimization, Conditional Context
Optimization, Multi-modal Prompt Learning, and Prompting with Self-Regulating
Constraints. These approaches reflect complementary design philosophies: from
static context optimization to conditional prompts for enhanced generalization,
multi-modal prompts for joint vision-language adaptation, and semantically
regularized prompts for stable learning without forgetting. We benchmark these
prompt-learning methods against two standard baselines: zero-shot CLIP with
hand-crafted prompts and a linear probe trained on frozen CLIP features.
Through extensive experiments on multiple benchmark remote sensing datasets,
including cross-dataset generalization tests, we demonstrate that prompt
learning consistently outperforms both baselines in few-shot scenarios.
Notably, Prompting with Self-Regulating Constraints achieves the most robust
cross-domain performance. Our findings underscore prompt learning as a scalable
and efficient solution for bridging the domain gap in satellite and aerial
imagery, providing a strong foundation for future research in this field.

</details>


### [34] [Stroke Lesion Segmentation in Clinical Workflows: A Modular, Lightweight, and Deployment-Ready Tool](https://arxiv.org/abs/2510.24378)
*Yann Kerverdo,Florent Leray,Youwan Mahé,Stéphanie Leplaideur,Francesca Galassi*

Main category: cs.CV

TL;DR: 引入了StrokeSeg框架，用于将研究级别的中风病变分割模型转化为临床可用的应用程序，该框架轻量化且模块化，提高了便携性和临床适用性。与其他框架相比，它在性能上几乎没有损失，而模型大小减小了约50%。


<details>
  <summary>Details</summary>
Motivation: 深度学习框架如nnU-Net虽然在脑病变分割中的表现非常出色，但却因为复杂的依赖关系和单体设计而难以在临床应用。因此，需要一种更加模块化和轻量化的框架，以便于将研究级的模型转化为适合临床使用的工具。

Method: 将预处理，推理和后处理分开，预处理使用Anima工具箱附带的BIDS兼容输出。推理部分使用ONNX Runtime，并采用Float16量化来减小模型大小，将模型大小减少了约50%。开发了图形和命令行界面，并发布了Python脚本和独立的Windows安装程序。

Result: 当使用300名慢性和急性中风患者的抽样数据集进行测试时，与其他原生的PyTorch管道相比，Segmentation性能几乎没有下降，差异小于10^-3，显示了研究级管道可以完全转化为高效、便于携带且适合用于临床的工具。

Conclusion: 通过将深度学习的病变分割模型转化为更便携和有效的工具，StrokeSeg提供了显著的改进，使得过去只能用于研究的高性能工具也能用于临床。

Abstract: Deep learning frameworks such as nnU-Net achieve state-of-the-art performance
in brain lesion segmentation but remain difficult to deploy clinically due to
heavy dependencies and monolithic design. We introduce \textit{StrokeSeg}, a
modular and lightweight framework that translates research-grade stroke lesion
segmentation models into deployable applications. Preprocessing, inference, and
postprocessing are decoupled: preprocessing relies on the Anima toolbox with
BIDS-compliant outputs, and inference uses ONNX Runtime with \texttt{Float16}
quantisation, reducing model size by about 50\%. \textit{StrokeSeg} provides
both graphical and command-line interfaces and is distributed as Python scripts
and as a standalone Windows executable. On a held-out set of 300 sub-acute and
chronic stroke subjects, segmentation performance was equivalent to the
original PyTorch pipeline (Dice difference $<10^{-3}$), demonstrating that
high-performing research pipelines can be transformed into portable, clinically
usable tools.

</details>


### [35] [When are radiology reports useful for training medical image classifiers?](https://arxiv.org/abs/2510.24385)
*Herman Bergström,Zhongqi Yue,Fredrik D. Johansson*

Main category: cs.CV

TL;DR: 研究探讨了在医学图像分类中利用放射科报告的时机和方法，从而在预训练和微调阶段改进分类器性能。发现放射科报告对文本中良好表示的标签有利，但在某些场景下通过显式图像-文本对齐进行预训练可能会有害。同时，在某些情况下，利用报告进行微调甚至能比预训练方法带来更大的改进。


<details>
  <summary>Details</summary>
Motivation: 传统的机器学习模型训练通常依赖放射科报告中的专家注释，但这些报告需要经过训练的放射科医生手动输入，限制了其在临床预测中的即时应用。因此，研究探讨了在何种情况下可以利用放射科报告加强基于图像的分类性能，填补了当前研究的空白。

Method: 研究系统性地考察了放射科报告在预训练和微调阶段的作用，研究了诊断和预后任务中的效果，并测试了不同训练集大小的情况。报告是否与文本文本强相关是研究中考虑的关键因素之一。

Result: 研究发现，在标签在文本中有良好表示的情况下，利用报告进行预训练是有益的，但在与文本弱相关的场景中，显式的图像-文本对齐预训练反而可能效果不佳。此外，利用报告进行微调在某些情况下可以带来显著改进，且改进幅度甚至超过预训练方法。

Conclusion: 该研究提供了有关何时以及如何利用特权文本数据来训练医学图像分类器的实际见解，并指出了当前研究的不足之处。

Abstract: Medical images used to train machine learning models are often accompanied by
radiology reports containing rich expert annotations. However, relying on these
reports as inputs for clinical prediction requires the timely manual work of a
trained radiologist. This raises a natural question: when can radiology reports
be leveraged during training to improve image-only classification? Prior works
are limited to evaluating pre-trained image representations by fine-tuning them
to predict diagnostic labels, often extracted from reports, ignoring tasks with
labels that are weakly associated with the text. To address this gap, we
conduct a systematic study of how radiology reports can be used during both
pre-training and fine-tuning, across diagnostic and prognostic tasks (e.g.,
12-month readmission), and under varying training set sizes. Our findings
reveal that: (1) Leveraging reports during pre-training is beneficial for
downstream classification tasks where the label is well-represented in the
text; however, pre-training through explicit image-text alignment can be
detrimental in settings where it's not; (2) Fine-tuning with reports can lead
to significant improvements and even have a larger impact than the pre-training
method in certain settings. These results provide actionable insights into when
and how to leverage privileged text data to train medical image classifiers
while highlighting gaps in current research.

</details>


### [36] [Unsupervised Detection of Post-Stroke Brain Abnormalities](https://arxiv.org/abs/2510.24398)
*Youwan Mahé,Elise Bannier,Stéphanie Leplaideur,Elisa Fromont,Francesca Galassi*

Main category: cs.CV

TL;DR: REFLECT, a flow-based generative model, is evaluated for unsupervised detection of abnormalities in post-stroke patients, showing improvements in lesion segmentation and sensitivity to non-lesional abnormalities when trained on healthy controls.


<details>
  <summary>Details</summary>
Motivation: Supervised segmentation methods are not good at capturing secondary structural changes after a stroke, such as atrophy and ventricular enlargement. REFLECT aims to address this issue with unsupervised detection of both focal and non-lesional abnormalities.

Method: The study uses REFLECT, a flow-based generative model, to detect abnormalities in post-stroke patients' MRI scans. It compares the model's performance when trained on lesion-free slices from stroke patients versus healthy controls.

Result: The model trained on healthy controls achieved better performance in lesion segmentation and showed higher sensitivity to non-lesional abnormalities compared to the model trained on lesion-free slices from stroke patients.

Conclusion: Training on fully healthy anatomy helps the model better capture normal variability, leading to more reliable detection of structural abnormalities in post-stroke patients.

Abstract: Post-stroke MRI not only delineates focal lesions but also reveals secondary
structural changes, such as atrophy and ventricular enlargement. These
abnormalities, increasingly recognised as imaging biomarkers of recovery and
outcome, remain poorly captured by supervised segmentation methods. We evaluate
REFLECT, a flow-based generative model, for unsupervised detection of both
focal and non-lesional abnormalities in post-stroke patients. Using dual-expert
central-slice annotations on ATLAS data, performance was assessed at the object
level with Free-Response ROC analysis for anomaly maps. Two models were trained
on lesion-free slices from stroke patients (ATLAS) and on healthy controls
(IXI) to test the effect of training data. On ATLAS test subjects, the
IXI-trained model achieved higher lesion segmentation (Dice = 0.37 vs 0.27) and
improved sensitivity to non-lesional abnormalities (FROC = 0.62 vs 0.43).
Training on fully healthy anatomy improves the modelling of normal variability,
enabling broader and more reliable detection of structural abnormalities.

</details>


### [37] [GenTrack: A New Generation of Multi-Object Tracking](https://arxiv.org/abs/2510.24399)
*Toan Van Nguyen,Rasmus G. K. Christiansen,Dirk Kraft,Leon Bodenhagen*

Main category: cs.CV

TL;DR: GenTrack是一种新的多目标跟踪(MOT)方法，通过结合随机和确定性方法来处理未知和随时间变化的目标数量，特别是在维护目标身份一致性方面表现优异。GenTrack利用粒子群优化(PSO)来帮助跟踪，即便面对弱和有噪声的对象检测器，依然能够保持有效的跟踪。同时，该方法还考虑了目标之间的社会交互，提高了跟踪的效率和准确性，甚至在遮挡情况下的表现也十分出色。实验表明，GenTrack在标准基准和真实场景下性能出色，优于现有的跟随者。并提供了一个包含标准比较方法的开源代码实现。


<details>
  <summary>Details</summary>
Motivation: 当处理目标数量未知且随时间变化的情况时，事实证明传统的多目标跟踪（MOT）方法在维护身份一致性以及处理非线性动力学上存在挑战。因此，我们希望提出一种新的MOT方法，可以在这些困难场景下表现出更好的性能和鲁棒性，特别是当目标遮挡时，需要保持正确的身份并减少身份切换和跟踪丢失。同时，该方法还需要能够适应弱和有噪声的对象检测器。

Method: 提出的方法结合了随机和确定性的手段以应对多个目标随时间变化的问题。利用粒子群体优化（PSO）引导粒子找到它们的目标分布模式，这有助于弱目标检测器的有效跟踪。同时考虑了目标之间的社会关系，以提高跟踪精度和更新效率，尤其是当目标遮挡时。提出了一个包括空间一致性，外观检测置信度，跟踪惩罚和社交评分的的全面状态和观测模型，用于系统性和高效的目标更新。提供了一个包含三种变体（基础，PSO和PSO社交）的源代码实现，以便于灵活重新实施。此实施具有最小的依赖性，确保了易于使用性。提供可比较的追踪器的源代码实现，以进行公平比较。除此之外，提供了在GitHub上的开源代码实现：https://github.com/SDU-VelKoTek/GenTrack

Result: 实验结果表明，相比其他最先进的追踪器，GenTrack在标准基准测试和现实生活场景中表现出色，并且在处理各种挑战性场景时表现突出，如未知数量的目标，非线性动力学，弱和有噪声的对象检测器，以及尤其是在处理遮挡情况下的跟踪性能。这些实验结果令人信服地展示了GenTrack在处理多目标跟踪任务方面的优越性能，提供了跟踪任务的优良性能保证。

Conclusion: 通过使用PSO和考虑社会交互的新方法，GenTrack在各种多目标跟踪场景中展现了优秀的性能，特别是在处理复杂的遮挡和环境变化时表现出色。该方法不仅理论上有显著创新，且在实际应用中的表现也令人满意，证明了其在MOT领域的实用性。开源代码的提供进一步强化了其他的科研人员，研究者和开发者对该方法的使用实践，开启了未来多目标跟踪问题的新视角。对于未来的研究，文中也探讨了一些潜在的方向。

Abstract: This paper introduces a novel multi-object tracking (MOT) method, dubbed
GenTrack, whose main contributions include: a hybrid tracking approach
employing both stochastic and deterministic manners to robustly handle unknown
and time-varying numbers of targets, particularly in maintaining target
identity (ID) consistency and managing nonlinear dynamics, leveraging particle
swarm optimization (PSO) with some proposed fitness measures to guide
stochastic particles toward their target distribution modes, enabling effective
tracking even with weak and noisy object detectors, integration of social
interactions among targets to enhance PSO-guided particles as well as improve
continuous updates of both strong (matched) and weak (unmatched) tracks,
thereby reducing ID switches and track loss, especially during occlusions, a
GenTrack-based redefined visual MOT baseline incorporating a comprehensive
state and observation model based on space consistency, appearance, detection
confidence, track penalties, and social scores for systematic and efficient
target updates, and the first-ever publicly available source-code reference
implementation with minimal dependencies, featuring three variants, including
GenTrack Basic, PSO, and PSO-Social, facilitating flexible reimplementation.
Experimental results have shown that GenTrack provides superior performance on
standard benchmarks and real-world scenarios compared to state-of-the-art
trackers, with integrated implementations of baselines for fair comparison.
Potential directions for future work are also discussed. The source-code
reference implementations of both the proposed method and compared-trackers are
provided on GitHub: https://github.com/SDU-VelKoTek/GenTrack

</details>


### [38] [A Hybrid Approach for Visual Multi-Object Tracking](https://arxiv.org/abs/2510.24410)
*Toan Van Nguyen,Rasmus G. K. Christiansen,Dirk Kraft,Leon Bodenhagen*

Main category: cs.CV

TL;DR: 该论文提出了一种结合随机和确定机制的视觉多对象跟踪方法，以确保在非线性动力学下的未知和时变目标数量的标识一致。随机粒子滤波器使用PSO粒子群优化支持，处理非线性动力学和非高斯噪声，通过引入移动一致性、外观相似性和社会互动线索来缓解分歧。确定性关联进一步通过成本矩阵强制标识一致性，该矩阵融入了粒子和当前检测之间的空间一致性、检测置信度和跟踪惩罚。一种新颖的方案被提出，可以平滑地更新目标状态，特别是在与其它目标交互和长时间遮挡时保持其身份。此外，速度回归提高了粒子采样和状态更新的趋势。该跟踪器适用于预录视频和相机实时流，可以提供优于当前最优跟踪器的性能。该研究的代码可通过GitHub获取：https://github.com/SDU-VelKoTek/GenTrack2


<details>
  <summary>Details</summary>
Motivation: 提出了一种结合随机和确定机制的多对象跟踪的方法，目标是确保非线性动力学未知和时变目标数量下的标识一致性，同时优化粒子滤波器在处理非线性动力学和非高斯噪声的能力，通过与确定性关联相结合以保持跟踪一致性和平滑更新目标状态

Method: 方法涉及将随机粒子滤波器与粒子群优化结合，来处理非高斯噪声和非线性动力学，同时还通过一种成本矩阵实现了一种新的确定性关联。一种新颖的方案被提出，目的在于平滑地更新目标状态，特别是在与其它目标交互和长时间遮挡时保持其标识一致性。除此之外，使用速度回归来改善粒子采样和状态更新的趋势

Result: 实验结果表明，所提出的追踪器相较于当前最优方法，具有更加优越的性能

Conclusion: 这种结合随机与确定方法的多目标跟踪手法，在非线性动力学中能够高效地追踪不断变化的动态，特别是在视频或摄像机实时流中处理未知数量的对象时。这种方法不仅处理好了粒子滤波在非高斯噪声和非线性动力学中的应用局限，而且还通过引入确定性关联改善了跟踪一致性和目标状态的更新，展示了优于当前最优方法的实践效果。

Abstract: This paper proposes a visual multi-object tracking method that jointly
employs stochastic and deterministic mechanisms to ensure identifier
consistency for unknown and time-varying target numbers under nonlinear
dynamics. A stochastic particle filter addresses nonlinear dynamics and
non-Gaussian noise, with support from particle swarm optimization (PSO) to
guide particles toward state distribution modes and mitigate divergence through
proposed fitness measures incorporating motion consistency, appearance
similarity, and social-interaction cues with neighboring targets. Deterministic
association further enforces identifier consistency via a proposed cost matrix
incorporating spatial consistency between particles and current detections,
detection confidences, and track penalties. Subsequently, a novel scheme is
proposed for the smooth updating of target states while preserving their
identities, particularly for weak tracks during interactions with other targets
and prolonged occlusions. Moreover, velocity regression over past states
provides trend-seed velocities, enhancing particle sampling and state updates.
The proposed tracker is designed to operate flexibly for both pre-recorded
videos and camera live streams, where future frames are unavailable.
Experimental results confirm superior performance compared to state-of-the-art
trackers. The source-code reference implementations of both the proposed method
and compared-trackers are provided on GitHub:
https://github.com/SDU-VelKoTek/GenTrack2

</details>


### [39] [50 Years of Water Body Monitoring: The Case of Qaraaoun Reservoir, Lebanon](https://arxiv.org/abs/2510.24413)
*Ali Ahmad Faour,Nabil Amacha,Ali J. Ghandour*

Main category: cs.CV

TL;DR: 这项研究提出了一种无需传感器监测的方法，通过对卫星影像进行水体分割和机器学习，实时估计Qaraaoun水库的水面面积和体积。这种方法精度高且成本低，为水库存储监测提供了一种新的解决方案，同时也为气候变化和环境模式的研究提供了多年的数据支持。


<details>
  <summary>Details</summary>
Motivation: 持续性管理黎巴嫩贝卡平原的最大地表水体Qaraaoun水库在传感器频繁故障和维护能力有限的情况下依赖于可靠的监控。然而，传统依赖于传感器的方法难以在资源有限的情况下可靠运行。因此，研究提出了一种传感器独立的监控新方法。

Method: 研究使用Sentinel-2和Landsat卫星提供的影像数据，通过新的水分割指数进行水体边缘确定。再通过支持向量回归模型，使用包含卫星影像提取的水面面积、水位以及通过水库地形测量计算的水体积的训练数据集进行训练，从而在没有地面测量的情况下估计水库体积。

Result: 研究得出的方法在95%以上的水岸线处与实地调查相符，经过Hyperparameter调优后，支持向量回归模型的预测误差低于水库满容量的1.5%，决定系数超过0.98，证明了方法的有效性。

Conclusion: 该新方法有效、经济且无需传感器，为流域管理提供了新的工具，同时对气候变化和环境模式的研究具有长期的数据支持。

Abstract: The sustainable management of the Qaraaoun Reservoir, the largest surface
water body in Lebanon located in the Bekaa Plain, depends on reliable
monitoring of its storage volume despite frequent sensor malfunctions and
limited maintenance capacity. This study introduces a sensor-free approach that
integrates open-source satellite imagery, advanced water-extent segmentation,
and machine learning to estimate the reservoir surface area and volume in near
real time. Sentinel-2 and Landsat images are processed, where surface water is
delineated using a newly proposed water segmentation index. A machine learning
model based on Support Vector Regression (SVR) is trained on a curated dataset
that includes water surface area, water level, and water volume calculations
using a reservoir bathymetry survey. The model is then able to estimate
reservoir volume relying solely on surface area extracted from satellite
imagery, without the need for ground measurements. Water segmentation using the
proposed index aligns with ground truth for more than 95 percent of the
shoreline. Hyperparameter tuning with GridSearchCV yields an optimized SVR
performance with error under 1.5 percent of full reservoir capacity and
coefficients of determination exceeding 0.98. These results demonstrate the
robustness and cost-effectiveness of the method, offering a practical solution
for continuous, sensor-independent monitoring of reservoir storage. The
proposed methodology can be replicated for other water bodies, and the
resulting 50 years of time-series data is valuable for research on climate
change and environmental patterns.

</details>


### [40] [XAI Evaluation Framework for Semantic Segmentation](https://arxiv.org/abs/2510.24414)
*Reem Hammoud,Abdul karim Gizzini,Ali J. Ghandour*

Main category: cs.CV

TL;DR: 论文提出了一种评估可解释性AI在语义分割中应用的框架，该框架通过像素级评估策略和设计指标来评估模型的透明度和可信度，特别是对于空间和上下文任务的复杂性。实验结果显示该方法的有效性、鲁棒性和可靠性，有助于提升语义分割模型的可解释性。


<details>
  <summary>Details</summary>
Motivation: 随着人工智能模型在安全关键和高风险领域中的应用增加，确保这些模型的透明性和信任度至关重要。现有的评估方法普遍存在对于分类任务评估较多，而对于语义分割任务的评估较少。因此，该工作旨在解决这一不足，提出了一种能够全面评估语义分割任务中XAI方法的框架。

Method: 该论文提出了一种新的评估框架，包括像素级别的评估策略和专门设计的评价指标，以适应语义分割任务的特定需求，尤其是对空间和上下文的复杂性的考量。框架中使用了基于类激活映射的XAI方法模拟实验，以验证新提出的方法的有效性。

Result: 研究通过基于类激活映射的XAI方法的模拟实验，展示了新提出的评估框架在效率、鲁棒性和可靠性方面的优势，证明了该框架在语义分割任务中评估XAI方法的适用性。

Conclusion: 该论文提出的框架有助于提高语义分割模型的解释性和透明度，使模型更加可靠和可信。这将对开发更透明、可信赖和负责的语义分割模型产生重要的贡献。

Abstract: Ensuring transparency and trust in artificial intelligence (AI) models is
essential, particularly as they are increasingly applied in safety-critical and
high-stakes domains. Explainable AI (XAI) has emerged as a promising approach
to address this challenge, yet the rigorous evaluation of XAI methods remains
crucial for optimizing the trade-offs between model complexity, predictive
performance, and interpretability. While extensive progress has been achieved
in evaluating XAI techniques for classification tasks, evaluation strategies
tailored to semantic segmentation remain relatively underexplored. This work
introduces a comprehensive and systematic evaluation framework specifically
designed for assessing XAI in semantic segmentation, explicitly accounting for
both spatial and contextual task complexities. The framework employs
pixel-level evaluation strategies and carefully designed metrics to provide
fine-grained interpretability insights. Simulation results using recently
adapted class activation mapping (CAM)-based XAI schemes demonstrate the
efficiency, robustness, and reliability of the proposed methodology. These
findings contribute to advancing transparent, trustworthy, and accountable
semantic segmentation models.

</details>


### [41] [Deeply-Conditioned Image Compression via Self-Generated Priors](https://arxiv.org/abs/2510.24437)
*Zhineng Zhao,Zhihai He,Zikun Zhou,Siwei Ma,Yaowei Wang*

Main category: cs.CV

TL;DR: 通过引入深度条件化图像压缩框架（DCIC-sgp），本文提出了一种新的方法来解决现有学习图像压缩（LIC）在低比特率时出现的严重几何失真问题。该框架利用自生成的先验来整体调制压缩流水线，显著改善了图像的压缩性能。在Kodak、CLIC和Tecnick数据集上，与VVC测试模型VTM-12.1相比，本方法取得了14.4%，15.7%，和15.1%的BD率减少。


<details>
  <summary>Details</summary>
Motivation: 当前的LIC方法难以处理自然图像中的复杂相关结构，特别是在单一整体表示中同时存在不变的全局结构和瞬时的局部纹理的情况下。这种限制导致在低比特率时出现严重的几何失真。因此，需要一种新的方法来解决这个问题。

Method: 提出了一个基于功能分解的框架，称为深度条件化图像压缩（DCIC-sgp）。首先，编码一个强大的自生成先验来捕捉图像的结构基础。然后，利用这个先验来整体调制整个压缩流水线，特别强调深度条件化分析变换，以便它可以专注于高熵残差。这种方法实现了信息流的有效解耦。

Result: 实验表明，与传统的压缩编解码器相比，该方法在低比特率时显著减少了几何失真。在Kodak、CLIC和Tecnick数据集上，该框架相对于VVC测试模型VTM-12.1取得了显著的BD率减少。具体而言，相对于VTM-12.1，该方法分别在三个数据集中取得了14.4%、15.7%和15.1%的BD率减少。

Conclusion: 通过引入深度条件化图像压缩框架（DCIC-sgp），有效地解决了现有LIC方法在低比特率时出现的几何失真问题，并在几种常见数据集上实现了显著的性能提升。

Abstract: Learned image compression (LIC) has shown great promise for achieving high
rate-distortion performance. However, current LIC methods are often limited in
their capability to model the complex correlation structures inherent in
natural images, particularly the entanglement of invariant global structures
with transient local textures within a single monolithic representation. This
limitation precipitates severe geometric deformation at low bitrates. To
address this, we introduce a framework predicated on functional decomposition,
which we term Deeply-Conditioned Image Compression via self-generated priors
(DCIC-sgp). Our central idea is to first encode a potent, self-generated prior
to encapsulate the image's structural backbone. This prior is subsequently
utilized not as mere side-information, but to holistically modulate the entire
compression pipeline. This deep conditioning, most critically of the analysis
transform, liberates it to dedicate its representational capacity to the
residual, high-entropy details. This hierarchical, dependency-driven approach
achieves an effective disentanglement of information streams. Our extensive
experiments validate this assertion; visual analysis demonstrates that our
method substantially mitigates the geometric deformation artifacts that plague
conventional codecs at low bitrates. Quantitatively, our framework establishes
highly competitive performance, achieving significant BD-rate reductions of
14.4%, 15.7%, and 15.1% against the VVC test model VTM-12.1 on the Kodak, CLIC,
and Tecnick datasets.

</details>


### [42] [Rethinking Visual Intelligence: Insights from Video Pretraining](https://arxiv.org/abs/2510.24448)
*Pablo Acuaviva,Aram Davtyan,Mariam Hassan,Sebastian Stapf,Ahmad Rahimi,Alexandre Alahi,Paolo Favaro*

Main category: cs.CV

TL;DR: 研究人员探讨了视频扩散模型（VDM）作为弥合语言模型在视觉理解方面不足的潜在方向。通过在时空数据上进行预训练，VDM被赋予了结构和动态的强大归纳偏差，这在一系列任务中显示出了比语言模型更高的数据效率，说明视频预训练支持视觉基础模型的发展。


<details>
  <summary>Details</summary>
Motivation: 目前大型语言模型在处理语言任务上表现出色，但在视觉任务中却面临挑战，如组成理解、采样效率和通用问题解决等。为了探索视频扩散模型作为桥梁的作用，研究者们进行了这项研究。

Method: 设计了一种控制性评估，其中既包括了经过预训练的语言模型，也包括了经过预训练的视频扩散模型，这两者都配备了轻量级的适配器，并以各自自然模式的任务呈现给模型。

Result: 在ARCGI、ConceptARC、视觉游戏、路线规划和细胞自动机等基准测试下，VDM表现出更高的数据效率，说明视频预训练赋予了模型更强的能力来适应多样化的任务。

Conclusion: 研究结果表明，视频预训练有助于支撑视觉基础模型的发展，提供了重要的归纳偏差，以解决视觉任务中的挑战。

Abstract: Large language models (LLMs) have demonstrated that large-scale pretraining
enables systems to adapt rapidly to new problems with little supervision in the
language domain. This success, however, has not translated as effectively to
the visual domain, where models, including LLMs, continue to struggle with
compositional understanding, sample efficiency, and general-purpose
problem-solving. We investigate Video Diffusion Models (VDMs) as a promising
direction for bridging this gap. Pretraining on spatiotemporal data endows
these models with strong inductive biases for structure and dynamics, which we
hypothesize can support broad task adaptability. To test this, we design a
controlled evaluation in which both a pretrained LLM and a pretrained VDM are
equipped with lightweight adapters and presented with tasks in their natural
modalities. Across benchmarks including ARC-AGI, ConceptARC, visual games,
route planning, and cellular automata, VDMs demonstrate higher data efficiency
than their language counterparts. Taken together, our results indicate that
video pretraining offers inductive biases that support progress toward visual
foundation models.

</details>


### [43] [A Critical Study towards the Detection of Parkinsons Disease using ML Technologies](https://arxiv.org/abs/2510.24456)
*Vivek Chetia,Abdul Taher Khan,Rahish Gogoi,David Kapsian Khual,Purnendu Bikash,Sajal Saha*

Main category: cs.CV

TL;DR: 在本文中，提出了一种基于深度学习的茶树叶病害分类与定位技术，可以识别和定位三种茶树叶病害（即红锈病、Helopeltis 和红蜘蛛害），并计算病害面积。评估了 SSD MobileNet V2 和 Faster R-CNN ResNet50 V1 的性能，Faster R-CNN ResNet50 V1 在 mAP 上略优于 SSD MobileNet V2。还使用了 Mask R-CNN 进行实例分割，以计算病害区域。


<details>
  <summary>Details</summary>
Motivation: 茶叶病害会影响茶叶的品质和产量，因此通过深度学习技术对茶叶病害进行准确、快速的识别与评估，以辅助茶叶生产中的病害管理。

Method: 本研究评估了 SSD MobileNet V2 和 Faster R-CNN ResNet50 V1 模型用于茶树叶病害识别的效果，并使用 Mask R-CNN 进行实例分割，计算病害的面积。为了执行实例分割，实施了一个自定义方法来判定识别的病害区域。

Result: SSD MobileNet V2 在 mAP 上的分数为 20.9%；Faster R-CNN ResNet50 V1 的 mAP 为 25%，提高了识别的精度和召回率，在解决茶叶病害识别的任务上表现稍好。Mask R-CNN 实现了更精确的实例分割，能够计算病害面积。

Conclusion: 利用深度学习方法可以有效地识别和量化茶树叶疾病，从而支持作物健康监测和病害管理。Faster R-CNN ResNet50 V1 和 Mask R-CNN 是较为理想的算法选择。

Abstract: The proposed solution is Deep Learning Technique that will be able classify
three types of tea leaves diseases from which two diseases are caused by the
pests and one due to pathogens (infectious organisms) and environmental
conditions and also show the area damaged by a disease in leaves. Namely Red
Rust, Helopeltis and Red spider mite respectively. In this paper we have
evaluated two models namely SSD MobileNet V2 and Faster R-CNN ResNet50 V1 for
the object detection. The SSD MobileNet V2 gave precision of 0.209 for IOU
range of 0.50:0.95 with recall of 0.02 on IOU 0.50:0.95 and final mAP of 20.9%.
While Faster R-CNN ResNet50 V1 has precision of 0.252 on IOU range of 0.50:0.95
and recall of 0.044 on IOU of 0.50:0.95 with a mAP of 25%, which is better than
SSD. Also used Mask R-CNN for Object Instance Segmentation where we have
implemented our custom method to calculate the damaged diseased portion of
leaves. Keywords: Tea Leaf Disease, Deep Learning, Red Rust, Helopeltis and Red
Spider Mite, SSD MobileNet V2, Faster R-CNN ResNet50 V1 and Mask RCNN.

</details>


### [44] [Kineo: Calibration-Free Metric Motion Capture From Sparse RGB Cameras](https://arxiv.org/abs/2510.24464)
*Charles Javerliat,Pierre Raimbaud,Guillaume Lavoué*

Main category: cs.CV

TL;DR: Kineo是一套针对无标记运动捕捉的全自动、无需校准的流水线，可以在无同步和未经校准的消费级RGB摄像机录制的视频中应用。它可以通过二维关键点同时校准摄像机并重建三维关键点和密集场景点地图，表现出优异的性能，得到比前人校准方法更高的精确度，同时计算成本较低。该流水线已经在EgoHumans和Human3.6M数据集上进行评估，证实其优越性。完整源代码已经公开以助力前沿研究和实际应用。


<details>
  <summary>Details</summary>
Motivation: 现有无需校准的无标记动作捕捉方法计算成本高且重建精度低，限制了其应用。因此，研究开发了一套全自动的无需校准的流水线，可以精确同时校准摄像机和重建三维关键点，适用性广，适应性强。

Method: Kineo采用二维关键点检测来自摄像机的视频帧，利用空间-时间自适应的关键点采样策略和全局三维重建方法，重建出大量的场景点，这些点形成场景的三维结构，同时该方法的计算复杂度与视频序列长度无关。

Result: 与前人方法对比，Kineo显著降低了相机间的错误，以及世界上的平均关节点误差，也证明了在实际场景应用中的高效性。其流水线在两个公开数据集上得到了准确验证。评价代码和源码均已公开，便于在研究和实践中使用。 

Conclusion: Kineo提供了一种全新的无需校准的多视图无标记动作捕捉的方法，能够在低成本高效的环境里取得卓越的重建精度，有助于推动无标记动作捕捉技术的发展。

Abstract: Markerless multiview motion capture is often constrained by the need for
precise camera calibration, limiting accessibility for non-experts and
in-the-wild captures. Existing calibration-free approaches mitigate this
requirement but suffer from high computational cost and reduced reconstruction
accuracy.
  We present Kineo, a fully automatic, calibration-free pipeline for markerless
motion capture from videos captured by unsynchronized, uncalibrated,
consumer-grade RGB cameras. Kineo leverages 2D keypoints from off-the-shelf
detectors to simultaneously calibrate cameras, including Brown-Conrady
distortion coefficients, and reconstruct 3D keypoints and dense scene point
maps at metric scale. A confidence-driven spatio-temporal keypoint sampling
strategy, combined with graph-based global optimization, ensures robust
calibration at a fixed computational cost independent of sequence length. We
further introduce a pairwise reprojection consensus score to quantify 3D
reconstruction reliability for downstream tasks.
  Evaluations on EgoHumans and Human3.6M demonstrate substantial improvements
over prior calibration-free methods. Compared to previous state-of-the-art
approaches, Kineo reduces camera translation error by approximately 83-85%,
camera angular error by 86-92%, and world mean-per-joint error (W-MPJPE) by
83-91%.
  Kineo is also efficient in real-world scenarios, processing multi-view
sequences faster than their duration in specific configuration (e.g., 36min to
process 1h20min of footage). The full pipeline and evaluation code are openly
released to promote reproducibility and practical adoption at
https://liris-xr.github.io/kineo/.

</details>


### [45] [Decoupled MeanFlow: Turning Flow Models into Flow Maps for Accelerated Sampling](https://arxiv.org/abs/2510.24474)
*Kyungmin Lee,Sihyun Yu,Jinwoo Shin*

Main category: cs.CV

TL;DR: 本文引入了一种简单的方法Decoupled MeanFlow，可以将现有的流模型转换为流场模型，而无需任何架构修改。这种方法使得预训练的流模型可以被直接转换为流场模型，从而实现高质量的样本生成，速度也显著提高。实验表明，这种方法在ImageNet数据集上的性能超过了现有技术，同时推理速度提升了约100倍。


<details>
  <summary>Details</summary>
Motivation: 现有的去噪生成模型（如扩散和流模型）虽然能够生成高质量的样本，但因为离散化误差需要多个去噪步骤。本文旨在通过引入Decoupled MeanFlow方法来解决这个问题，同时保持流模型的兼容性。

Method: 本文提出的方法Decoupled MeanFlow，通过在扩散变换器的最后几个块中针对后续时间步长进行条件化，将预训练的流模型转换为流场模型。这种方法无需对架构进行修改即可实现这一转换。结合增强的训练技术，该方法能够在较少的步骤内实现高质量的样本生成。

Result: 实验结果显示，通过本文提出的方法将流模型转换后，能够在ImageNet 256x256和512x512数据集上分别达到1步FID 2.16和2.12，超过了现有技术。当增加到4步时，分别达到FID 1.51和1.68，接近流模型的性能，但推理速度提升了100多倍。

Conclusion: 通过引入Decoupled MeanFlow方法，可以将预训练的流模型直接转换为流场模型，实现高质量生成的同时极大地提升了效率。

Abstract: Denoising generative models, such as diffusion and flow-based models, produce
high-quality samples but require many denoising steps due to discretization
error. Flow maps, which estimate the average velocity between timesteps,
mitigate this error and enable faster sampling. However, their training
typically demands architectural changes that limit compatibility with
pretrained flow models. We introduce Decoupled MeanFlow, a simple decoding
strategy that converts flow models into flow map models without architectural
modifications. Our method conditions the final blocks of diffusion transformers
on the subsequent timestep, allowing pretrained flow models to be directly
repurposed as flow maps. Combined with enhanced training techniques, this
design enables high-quality generation in as few as 1 to 4 steps. Notably, we
find that training flow models and subsequently converting them is more
efficient and effective than training flow maps from scratch. On ImageNet
256x256 and 512x512, our models attain 1-step FID of 2.16 and 2.12,
respectively, surpassing prior art by a large margin. Furthermore, we achieve
FID of 1.51 and 1.68 when increasing the steps to 4, which nearly matches the
performance of flow models while delivering over 100x faster inference.

</details>


### [46] [Latent Sketchpad: Sketching Visual Thoughts to Elicit Multimodal Reasoning in MLLMs](https://arxiv.org/abs/2510.24514)
*Huanyu Zhang,Wenshan Wu,Chengzu Li,Ning Shang,Yan Xia,Yangyu Huang,Yifan Zhang,Li Dong,Zhang Zhang,Liang Wang,Tieniu Tan,Furu Wei*

Main category: cs.CV

TL;DR: Latent Sketchpad 是一个框架，它使多模态大型语言模型能够在内部生成视觉隐含表示，同时保持或提高其推理能力，从而增强其在复杂场景中的性能。实验表明，该框架在推理上的表现与底层模型相当或更优，并且在不同模型上具有良好的泛化能力，为更丰富的人机交互和更广泛的应用打开了新的机会。


<details>
  <summary>Details</summary>
Motivation: 多模态大型语言模型在复杂的场景中需要进行视觉规划和想象时表现不佳，该框架通过引入内部视觉草图及视觉生成能力，提升了模型在这些复杂场景中的表现能力。这一灵感来源于人类使用草图作为一种视觉思考方式来发展和沟通想法。

Method: 利用前沿的多模态大型语言模型，该框架通过视觉生成整合到模型的原生自回归推理过程中。它允许模型在文本推理和生成视觉隐形表示之间交替，这些表示既可以指导内部思考过程，也可以转换为人类可理解的草图图像。框架包括两个组件：一个上下文感知视觉头部自回归生成视觉表示，以及一个预训练的草图解码器将这些表示转化为人类可读的图像。实验在新数据集MazePlanning上展示了该框架的效果。

Result: 实验表明，Latent Sketchpad 在不同多模态大型语言模型中的表现与原始模型相当或更加出色，并且在不同模型上具有良好的泛化能力。

Conclusion: 通过将模型的文本推理扩展到视觉思考，该框架为更丰富的人机交互和更广泛的应用打开了新的机会。

Abstract: While Multimodal Large Language Models (MLLMs) excel at visual understanding,
they often struggle in complex scenarios that require visual planning and
imagination. Inspired by how humans use sketching as a form of visual thinking
to develop and communicate ideas, we introduce Latent Sketchpad, a framework
that equips MLLMs with an internal visual scratchpad. The internal visual
representations of MLLMs have traditionally been confined to perceptual
understanding. We repurpose them to support generative visual thought without
compromising reasoning ability. Building on frontier MLLMs, our approach
integrates visual generation directly into their native autoregressive
reasoning process. It allows the model to interleave textual reasoning with the
generation of visual latents. These latents guide the internal thought process
and can be translated into sketch images for interpretability. To realize this,
we introduce two components: a Context-Aware Vision Head autoregressively
produces visual representations, and a pretrained Sketch Decoder renders these
into human-interpretable images. We evaluate the framework on our new dataset
MazePlanning. Experiments across various MLLMs show that Latent Sketchpad
delivers comparable or even superior reasoning performance to their backbone.
It further generalizes across distinct frontier MLLMs, including Gemma3 and
Qwen2.5-VL. By extending model's textual reasoning to visual thinking, our
framework opens new opportunities for richer human-computer interaction and
broader applications. More details and resources are available on our project
page: https://latent-sketchpad.github.io/.

</details>


### [47] [Physics-Inspired Gaussian Kolmogorov-Arnold Networks for X-ray Scatter Correction in Cone-Beam CT](https://arxiv.org/abs/2510.24579)
*Xu Jiang,Huiying Pan,Ligen Shi,Jianing Sun,Wenfeng Xu,Xing Zhao*

Main category: cs.CV

TL;DR: 本文提出了一种基于深度学习的散射伪影校正方法，通过引入物理先验知识和利用Kolmogorov-Arnold Networks (KAN) 层来有效校正散射伪影，提高CT图像的质量和诊断准确性。


<details>
  <summary>Details</summary>
Motivation: Cone-beam CT (CBCT) 由于在数据采集过程中容易受到散射的影响，导致CT值偏差和组织对比度降低，从而影响诊断准确性。为了解决这一问题，本文提出了一种基于深度学习的方法来校正散射伪影。

Method: 该方法利用平面散射概率密度分布的旋转对称性，采用高斯径向基函数（RBF）来建模点散射函数，并将其嵌入到Kolmogorov-Arnold Networks (KAN) 层中，通过KAN的强大映射能力来学习高维散射特征。结合物理上散射光子分布的特征，该模型提高了对散射伪影校正的准确性。

Result: 实验结果表明，该方法在合成和真实数据实验中均能有效校正散射伪影，并在定量指标上优于现有方法。

Conclusion: 本文提出的方法通过结合物理先验知识和深度学习技术，有效地提高了散射伪影校正的准确性。

Abstract: Cone-beam CT (CBCT) employs a flat-panel detector to achieve
three-dimensional imaging with high spatial resolution. However, CBCT is
susceptible to scatter during data acquisition, which introduces CT value bias
and reduced tissue contrast in the reconstructed images, ultimately degrading
diagnostic accuracy. To address this issue, we propose a deep learning-based
scatter artifact correction method inspired by physical prior knowledge.
Leveraging the fact that the observed point scatter probability density
distribution exhibits rotational symmetry in the projection domain. The method
uses Gaussian Radial Basis Functions (RBF) to model the point scatter function
and embeds it into the Kolmogorov-Arnold Networks (KAN) layer, which provides
efficient nonlinear mapping capabilities for learning high-dimensional scatter
features. By incorporating the physical characteristics of the scattered photon
distribution together with the complex function mapping capacity of KAN, the
model improves its ability to accurately represent scatter. The effectiveness
of the method is validated through both synthetic and real-scan experiments.
Experimental results show that the model can effectively correct the scatter
artifacts in the reconstructed images and is superior to the current methods in
terms of quantitative metrics.

</details>


### [48] [A Dual-Branch CNN for Robust Detection of AI-Generated Facial Forgeries](https://arxiv.org/abs/2510.24640)
*Xin Zhang,Yuqi Song,Fei Zuo*

Main category: cs.CV

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: The rapid advancement of generative AI has enabled the creation of highly
realistic forged facial images, posing significant threats to AI security,
digital media integrity, and public trust. Face forgery techniques, ranging
from face swapping and attribute editing to powerful diffusion-based image
synthesis, are increasingly being used for malicious purposes such as
misinformation, identity fraud, and defamation. This growing challenge
underscores the urgent need for robust and generalizable face forgery detection
methods as a critical component of AI security infrastructure. In this work, we
propose a novel dual-branch convolutional neural network for face forgery
detection that leverages complementary cues from both spatial and frequency
domains. The RGB branch captures semantic information, while the frequency
branch focuses on high-frequency artifacts that are difficult for generative
models to suppress. A channel attention module is introduced to adaptively fuse
these heterogeneous features, highlighting the most informative channels for
forgery discrimination. To guide the network's learning process, we design a
unified loss function, FSC Loss, that combines focal loss, supervised
contrastive loss, and a frequency center margin loss to enhance class
separability and robustness. We evaluate our model on the DiFF benchmark, which
includes forged images generated from four representative methods:
text-to-image, image-to-image, face swap, and face edit. Our method achieves
strong performance across all categories and outperforms average human
accuracy. These results demonstrate the model's effectiveness and its potential
contribution to safeguarding AI ecosystems against visual forgery attacks.

</details>


### [49] [Eye-Tracking, Mouse Tracking, Stimulus Tracking,and Decision-Making Datasets in Digital Pathology](https://arxiv.org/abs/2510.24653)
*Veronica Thai,Rui Li,Meng Ling,Shuning Jiang,Jeremy Wolfe,Raghu Machiraju,Yan Hu,Zaibo Li,Anil Parwani,Jian Chen*

Main category: cs.CV

TL;DR: 提出了PathoGaze1.0，这是一个全面的行为数据集，捕获了全诊断工作流程中癌症诊断过程中的动态视觉搜索和决策过程。数据集包括19位病理学家对397张全视图图像进行解释录制的18.69小时的行为数据，其中包括眼动追踪、鼠标交互轨迹、视口导航、刺激跟踪和诊断决策数据（EMSVD）等。这些数据还能用于提高病理学家和人工智能系统的培训效果。所有实验均已在https://osf.io/hj9a7预注册，并且完整的数据集和分析代码可在https://go.osu.edu/pathogaze获取。


<details>
  <summary>Details</summary>
Motivation: 提出PathoGaze1.0数据集主要是因为目前病理学界缺乏足够的行为数据来解释诊断错误和不一致性。病理学家在解释giga-pixel全视图图像（WSIs）时面临着重要的但又极具挑战性的任务，其诊断准确性估计平均为70%。即使增加第二位病理学家也不能显著改善决策一致性。因此，需要开发一个行为数据集来填补这一空白。

Method: 通过构建一个应用扎根的测试平台PTAH，从19位病理学家对397张全视图图像的解释过程中捕获18.69小时的眼动追踪、鼠标交互轨迹、视口导航、刺激跟踪和诊断决策数据（EMSVD），并记录了171,909次固定点、263,320次跳动和1,867,362次鼠标交互事件，生成了PathoGaze1.0行为数据集。所有实验均已在https://osf.io/hj9a7预注册，并且完整的数据集和分析代码可在https://go.osu.edu/pathogaze获取。

Result: 收集了19位病理学家对397张全视图图像的解释过程中的大量行为数据，包括眼动追踪、鼠标交互轨迹、视口导航、刺激跟踪和诊断决策数据（EMSVD），为解释病理学中的诊断错误和不一致性提供了行为数据，同时也可提升病理学家的培训效果以及支持病理学家的人工智能系统的培训效果。

Conclusion: PathoGaze1.0数据集是第一个全面的行为数据集，涵盖了癌症诊断全工作流程的视觉搜索和决策动态。该数据集记录了19位病理学家解释397张全视图图像时的各种行为，包括眼动、鼠标交互和决策过程。这些数据不仅有助于揭示诊断过程中的行为特征，还能为训练技术和支持病理学家的人工智能系统提供资源。

Abstract: Interpretation of giga-pixel whole-slide images (WSIs) is an important but
difficult task for pathologists. Their diagnostic accuracy is estimated to
average around 70%. Adding a second pathologist does not substantially improve
decision consistency. The field lacks adequate behavioral data to explain
diagnostic errors and inconsistencies. To fill in this gap, we present
PathoGaze1.0, a comprehensive behavioral dataset capturing the dynamic visual
search and decision-making processes of the full diagnostic workflow during
cancer diagnosis. The dataset comprises 18.69 hours of eye-tracking, mouse
interaction, stimulus tracking, viewport navigation, and diagnostic decision
data (EMSVD) collected from 19 pathologists interpreting 397 WSIs. The data
collection process emphasizes ecological validity through an
application-grounded testbed, called PTAH. In total, we recorded 171,909
fixations, 263,320 saccades, and 1,867,362 mouse interaction events. In
addition, such data could also be used to improve the training of both
pathologists and AI systems that might support human experts. All experiments
were preregistered at https://osf.io/hj9a7, and the complete dataset along with
analysis code is available at https://go.osu.edu/pathogaze.

</details>


### [50] [Group Relative Attention Guidance for Image Editing](https://arxiv.org/abs/2510.24657)
*Xuanpu Zhang,Xuesong Niu,Ruidong Chen,Dan Song,Jianhao Zeng,Penghui Du,Haoxiang Cao,Kai Wu,An-an Liu*

Main category: cs.CV

TL;DR: 提出了Group Relative Attention Guidance (GRAG)，一种简单而有效的方法，通过对不同令牌的delta值进行重新加权，来调整模型对输入图像与编辑指令的关注度，实现连续而精细的编辑强度控制。实验表明，GRAG可以轻松集成到现有图像编辑框架中，并且相比现有方法，GRAG能实现更平滑和精确的编辑控制。


<details>
  <summary>Details</summary>
Motivation: 现有图像编辑方法通常缺乏对编辑强度的有效控制，影响了定制化结果的实现。本文旨在通过改进注意力机制来解决这一问题，从而实现更加灵活和细致的图像编辑控制。

Method: 提出了基于MM-Attention机制的Group Relative Attention Guidance (GRAG) 方法，通过对Query和Key令牌之间的delta值重新加权，来调整模型对输入图像的关注度，实现连续且精细的编辑强度控制。

Result: 实验结果表明，GRAG不仅可以简单地集成到现有图像编辑框架中，而且可以实现更平滑和精确的编辑控制，相比常见的Classifier-Free Guidance方法，GRAG表现更为优越。

Conclusion: 我们提出了一种简单有效的方法，通过对令牌的delta值重新加权，以实现图像编辑的连续且精细控制，且与现有方法相比，GRAG能够轻松集成到现有图像编辑框架中，显著提高编辑质量。

Abstract: Recently, image editing based on Diffusion-in-Transformer models has
undergone rapid development. However, existing editing methods often lack
effective control over the degree of editing, limiting their ability to achieve
more customized results. To address this limitation, we investigate the
MM-Attention mechanism within the DiT model and observe that the Query and Key
tokens share a bias vector that is only layer-dependent. We interpret this bias
as representing the model's inherent editing behavior, while the delta between
each token and its corresponding bias encodes the content-specific editing
signals. Based on this insight, we propose Group Relative Attention Guidance, a
simple yet effective method that reweights the delta values of different tokens
to modulate the focus of the model on the input image relative to the editing
instruction, enabling continuous and fine-grained control over editing
intensity without any tuning. Extensive experiments conducted on existing image
editing frameworks demonstrate that GRAG can be integrated with as few as four
lines of code, consistently enhancing editing quality. Moreover, compared to
the commonly used Classifier-Free Guidance, GRAG achieves smoother and more
precise control over the degree of editing. Our code will be released at
https://github.com/little-misfit/GRAG-Image-Editing.

</details>


### [51] [SAGE: Structure-Aware Generative Video Transitions between Diverse Clips](https://arxiv.org/abs/2510.24667)
*Mia Kan,Yilin Liu,Niloy Mitra*

Main category: cs.CV

TL;DR: 本文提出了SAGE（基于结构的生成视频过渡）方法，该方法通过结合结构指导和生成合成，实现了不同视频片段之间平滑、语义一致的过渡，无需微调。SAGE在定量指标和用户研究中均优于现有方法，可实现在不同视频片段间的平滑过渡。


<details>
  <summary>Details</summary>
Motivation: 现有的视频过渡技术在处理包含较大时间间隔或显著语义差异的视频片段时效果不佳，导致过渡效果不自然。为解决这一问题，本文提出了新的解决方案，旨在产生视觉上连贯的过渡效果。通过借鉴艺术工作流程中的策略，如对齐轮廓和互补显著特征来保持结构和知觉连续性，创造了SAGE方法。

Method: SAGE方法通过结合结构指导（提供线地图和运动流）和生成合成，实现了不同视频片段之间平滑、语义一致的过渡。该方法无需进行微调，直接通过零样本学习实现了目标。

Result: 实验结果表明，SAGE方法在定量指标和用户研究中均优于现有方法，能够更自然、更平滑地连接不同视频片段。

Conclusion: 本文提出了一种新的视频过渡技术SAGE，通过结合结构指导和生成合成，使得不同视频片段间的过渡更为平滑、自然，从而填补了现有技术的空白。

Abstract: Video transitions aim to synthesize intermediate frames between two clips,
but naive approaches such as linear blending introduce artifacts that limit
professional use or break temporal coherence. Traditional techniques
(cross-fades, morphing, frame interpolation) and recent generative inbetweening
methods can produce high-quality plausible intermediates, but they struggle
with bridging diverse clips involving large temporal gaps or significant
semantic differences, leaving a gap for content-aware and visually coherent
transitions. We address this challenge by drawing on artistic workflows,
distilling strategies such as aligning silhouettes and interpolating salient
features to preserve structure and perceptual continuity. Building on this, we
propose SAGE (Structure-Aware Generative vidEo transitions) as a zeroshot
approach that combines structural guidance, provided via line maps and motion
flow, with generative synthesis, enabling smooth, semantically consistent
transitions without fine-tuning. Extensive experiments and comparison with
current alternatives, namely [FILM, TVG, DiffMorpher, VACE, GI], demonstrate
that SAGE outperforms both classical and generative baselines on quantitative
metrics and user studies for producing transitions between diverse clips. Code
to be released on acceptance.

</details>


### [52] [MIC-BEV: Multi-Infrastructure Camera Bird's-Eye-View Transformer with Relation-Aware Fusion for 3D Object Detection](https://arxiv.org/abs/2510.24688)
*Yun Zhang,Zhaoliang Zheng,Johnson Liu,Zhiyu Huang,Zewei Zhou,Zonglin Meng,Tianhui Cai,Jiaqi Ma*

Main category: cs.CV

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Infrastructure-based perception plays a crucial role in intelligent
transportation systems, offering global situational awareness and enabling
cooperative autonomy. However, existing camera-based detection models often
underperform in such scenarios due to challenges such as multi-view
infrastructure setup, diverse camera configurations, degraded visual inputs,
and various road layouts. We introduce MIC-BEV, a Transformer-based
bird's-eye-view (BEV) perception framework for infrastructure-based
multi-camera 3D object detection. MIC-BEV flexibly supports a variable number
of cameras with heterogeneous intrinsic and extrinsic parameters and
demonstrates strong robustness under sensor degradation. The proposed
graph-enhanced fusion module in MIC-BEV integrates multi-view image features
into the BEV space by exploiting geometric relationships between cameras and
BEV cells alongside latent visual cues. To support training and evaluation, we
introduce M2I, a synthetic dataset for infrastructure-based object detection,
featuring diverse camera configurations, road layouts, and environmental
conditions. Extensive experiments on both M2I and the real-world dataset
RoScenes demonstrate that MIC-BEV achieves state-of-the-art performance in 3D
object detection. It also remains robust under challenging conditions,
including extreme weather and sensor degradation. These results highlight the
potential of MIC-BEV for real-world deployment. The dataset and source code are
available at: https://github.com/HandsomeYun/MIC-BEV.

</details>


### [53] [Does Object Binding Naturally Emerge in Large Pretrained Vision Transformers?](https://arxiv.org/abs/2510.24709)
*Yihao Li,Saeed Salehi,Lyle Ungar,Konrad P. Kording*

Main category: cs.CV

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Object binding, the brain's ability to bind the many features that
collectively represent an object into a coherent whole, is central to human
cognition. It groups low-level perceptual features into high-level object
representations, stores those objects efficiently and compositionally in
memory, and supports human reasoning about individual object instances. While
prior work often imposes object-centric attention (e.g., Slot Attention)
explicitly to probe these benefits, it remains unclear whether this ability
naturally emerges in pre-trained Vision Transformers (ViTs). Intuitively, they
could: recognizing which patches belong to the same object should be useful for
downstream prediction and thus guide attention. Motivated by the quadratic
nature of self-attention, we hypothesize that ViTs represent whether two
patches belong to the same object, a property we term IsSameObject. We decode
IsSameObject from patch embeddings across ViT layers using a similarity probe,
which reaches over 90% accuracy. Crucially, this object-binding capability
emerges reliably in self-supervised ViTs (DINO, MAE, CLIP), but markedly weaker
in ImageNet-supervised models, suggesting that binding is not a trivial
architectural artifact, but an ability acquired through specific pretraining
objectives. We further discover that IsSameObject is encoded in a
low-dimensional subspace on top of object features, and that this signal
actively guides attention. Ablating IsSameObject from model activations
degrades downstream performance and works against the learning objective,
implying that emergent object binding naturally serves the pretraining
objective. Our findings challenge the view that ViTs lack object binding and
highlight how symbolic knowledge of "which parts belong together" emerges
naturally in a connectionist system.

</details>


### [54] [Routing Matters in MoE: Scaling Diffusion Transformers with Explicit Routing Guidance](https://arxiv.org/abs/2510.24711)
*Yujie Wei,Shiwei Zhang,Hangjie Yuan,Yujin Han,Zhekai Chen,Jiayu Wang,Difan Zou,Xihui Liu,Yingya Zhang,Yu Liu,Hongming Shan*

Main category: cs.CV

TL;DR: 本文提出了ProMoE，这是一种新的混合专家框架，专门针对视觉Transformer中的混合专家问题，通过显式的路由指导来促进专家的专门化，并且通过引入路由对比损失来增强路由过程，提高了视觉任务中的模型性能。


<details>
  <summary>Details</summary>
Motivation: 尽管MoE在大型语言模型中取得了显著成功，但在应用于视觉Transformer时取得的改进有限。这主要是由于语言标记和视觉标记之间的固有差异导致的，视觉标记的空间冗余和功能异质性阻碍了视觉MoE中的专家专门化。因而本文旨在解决这个差距问题，提出了新的解决方案。

Method: 提出了利用条件路由和原型路由指导的两步路由器，将图像标记依据功能角色分为条件集和无条件集，并根据语义内容学习可学习的原型来进一步优化条件标记的分配，同时引入了路由对比损失来改进这一过程。这种方法在潜在空间实现了基于相似性的专家分配，并对原型路由过程进行显式增强，促进相似性专家之间的内部一致性以及不同类型专家之间的差异性。

Result: ProMoE在ImageNet数据集上展现了优越的性能，超越了现有的最先进的方法，特别是在Rectified Flow和DDPM训练目标下。代码和模型将在发布后公开提供。

Conclusion: 视觉标记与语言标记相比具有独特特征，基于内容的精细化路由策略对于有效地利用视觉MoE的性能至关重要。通过显式的原型形路由和路由对比损失，ProMoE极大地改进了视觉任务中的模型性能，并为视觉专家混合的研究开辟了新的途径。

Abstract: Mixture-of-Experts (MoE) has emerged as a powerful paradigm for scaling model
capacity while preserving computational efficiency. Despite its notable success
in large language models (LLMs), existing attempts to apply MoE to Diffusion
Transformers (DiTs) have yielded limited gains. We attribute this gap to
fundamental differences between language and visual tokens. Language tokens are
semantically dense with pronounced inter-token variation, while visual tokens
exhibit spatial redundancy and functional heterogeneity, hindering expert
specialization in vision MoE. To this end, we present ProMoE, an MoE framework
featuring a two-step router with explicit routing guidance that promotes expert
specialization. Specifically, this guidance encourages the router to partition
image tokens into conditional and unconditional sets via conditional routing
according to their functional roles, and refine the assignments of conditional
image tokens through prototypical routing with learnable prototypes based on
semantic content. Moreover, the similarity-based expert allocation in latent
space enabled by prototypical routing offers a natural mechanism for
incorporating explicit semantic guidance, and we validate that such guidance is
crucial for vision MoE. Building on this, we propose a routing contrastive loss
that explicitly enhances the prototypical routing process, promoting
intra-expert coherence and inter-expert diversity. Extensive experiments on
ImageNet benchmark demonstrate that ProMoE surpasses state-of-the-art methods
under both Rectified Flow and DDPM training objectives. Code and models will be
made publicly available.

</details>


### [55] [Uniform Discrete Diffusion with Metric Path for Video Generation](https://arxiv.org/abs/2510.24717)
*Haoge Deng,Ting Pan,Fan Zhang,Yang Liu,Zhuoyan Luo,Yufeng Cui,Wenxuan Wang,Chunhua Shen,Shiguang Shan,Zhaoxiang Zhang,Xinlong Wang*

Main category: cs.CV

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Continuous-space video generation has advanced rapidly, while discrete
approaches lag behind due to error accumulation and long-context inconsistency.
In this work, we revisit discrete generative modeling and present Uniform
discRete diffuSion with metric pAth (URSA), a simple yet powerful framework
that bridges the gap with continuous approaches for the scalable video
generation. At its core, URSA formulates the video generation task as an
iterative global refinement of discrete spatiotemporal tokens. It integrates
two key designs: a Linearized Metric Path and a Resolution-dependent Timestep
Shifting mechanism. These designs enable URSA to scale efficiently to
high-resolution image synthesis and long-duration video generation, while
requiring significantly fewer inference steps. Additionally, we introduce an
asynchronous temporal fine-tuning strategy that unifies versatile tasks within
a single model, including interpolation and image-to-video generation.
Extensive experiments on challenging video and image generation benchmarks
demonstrate that URSA consistently outperforms existing discrete methods and
achieves performance comparable to state-of-the-art continuous diffusion
methods. Code and models are available at https://github.com/baaivision/URSA

</details>


### [56] [Generative View Stitching](https://arxiv.org/abs/2510.24718)
*Chonghyuk Song,Michal Stary,Boyuan Chen,George Kopanas,Vincent Sitzmann*

Main category: cs.CV

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Autoregressive video diffusion models are capable of long rollouts that are
stable and consistent with history, but they are unable to guide the current
generation with conditioning from the future. In camera-guided video generation
with a predefined camera trajectory, this limitation leads to collisions with
the generated scene, after which autoregression quickly collapses. To address
this, we propose Generative View Stitching (GVS), which samples the entire
sequence in parallel such that the generated scene is faithful to every part of
the predefined camera trajectory. Our main contribution is a sampling algorithm
that extends prior work on diffusion stitching for robot planning to video
generation. While such stitching methods usually require a specially trained
model, GVS is compatible with any off-the-shelf video model trained with
Diffusion Forcing, a prevalent sequence diffusion framework that we show
already provides the affordances necessary for stitching. We then introduce
Omni Guidance, a technique that enhances the temporal consistency in stitching
by conditioning on both the past and future, and that enables our proposed
loop-closing mechanism for delivering long-range coherence. Overall, GVS
achieves camera-guided video generation that is stable, collision-free,
frame-to-frame consistent, and closes loops for a variety of predefined camera
paths, including Oscar Reutersv\"ard's Impossible Staircase. Results are best
viewed as videos at https://andrewsonga.github.io/gvs.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [57] [AI and the Decentering of Disciplinary Creativity](https://arxiv.org/abs/2510.23734)
*Eamon Duede*

Main category: cs.AI

TL;DR: 该论文探讨了人工智能在科学问题解决中的作用，特别是它对学科创造力的影响。通过两个数学案例，作者展示了计算可以扩展学科创造力，但某些涉及AI的方法可能会替代人类的创造力，这可能改变（也许减少）科学追求的价值。


<details>
  <summary>Details</summary>
Motivation: 论文旨在探讨人工智能在科学问题解决中的角色及其对学科创造力的影响，从而更好地理解人工智能对科学创新和价值的影响。

Method: 通过两个数学案例，作者分析了计算在扩展学科创造力方面的潜力，同时探讨了某些涉及AI的方法在替代人类创造力方面的可能性。这种方法结合了理论分析和案例研究，旨在揭示AI在科学创新中的作用及其潜在影响。

Result: 研究表明，虽然计算能够扩展学科创造力，但某些涉及AI的方法也可能导致人类创造力的减少，这可能会影响科学追求的价值。

Conclusion: 总之，人工智能可以增强学科创造力，但其替代人类创造力的潜力也提醒我们，需要谨慎评估其对科学创新的影响，以确保其发展能够促进而不是阻碍科学进步。

Abstract: This paper examines the role of artificial intelligence in scientific
problem-solving, with a focus on its implications for disciplinary creativity.
Drawing on recent work in the philosophy of creativity, I distinguish between
creative approaches and creative products, and introduce the concept of
disciplinary creativity -the creative application of discipline-specific
expertise to a valued problem within that field. Through two cases in
mathematics, I show that while computation can extend disciplinary creativity,
certain approaches involving AI can serve to displace it. This displacement has
the potential to alter (and, perhaps, diminish) the value of scientific
pursuit.

</details>


### [58] [Multi-Environment POMDPs: Discrete Model Uncertainty Under Partial Observability](https://arxiv.org/abs/2510.23744)
*Eline M. Bovy,Caleb Probine,Marnix Suilen,Ufuk Topcu,Nils Jansen*

Main category: cs.AI

TL;DR: 研究提出了一种处理多环境部分可观察马尔可夫决策过程(ME-POMDP)的新方法，通过引入对手信念POMDP(AB-POMDP)的概念，扩展了现有的研究，并设计了精确和近似算法来计算对所有POMDP环境都稳健的策略。


<details>
  <summary>Details</summary>
Motivation: 该研究解决的是在存在多个可能环境模型（比如由多个领域专家提出的不同模型）的情况下，如何找到一种在所有可能的POMDP模型下都能表现良好的策略的问题。这种方法使得政策更加稳健，能够处理不确定性和模型差异的问题。

Method: 方法上，研究将ME-POMDP扩展为具有初始信念集合的POMDP，称为对手信念POMDP。进一步展示了任意ME-POMDP可以被简化为仅在过渡函数或观测函数上不同的ME-POMDP，同时保持最优策略。此外，还提出了计算AB-POMDP和ME-POMDP稳健策略的精确和近似（基于点）算法。

Result: 研究开发了计算AB-POMDP和ME-POMDP稳健策略的算法，并展示了这些算法可以在标准POMDP基准测试的多环境设置中计算策略的能力。

Conclusion: 该研究通过引入对手信念POMDP和开发稳健策略计算的算法，为处理不同环境模型中POMDP问题提供了一种新的视角和有效方法。

Abstract: Multi-environment POMDPs (ME-POMDPs) extend standard POMDPs with discrete
model uncertainty. ME-POMDPs represent a finite set of POMDPs that share the
same state, action, and observation spaces, but may arbitrarily vary in their
transition, observation, and reward models. Such models arise, for instance,
when multiple domain experts disagree on how to model a problem. The goal is to
find a single policy that is robust against any choice of POMDP within the set,
i.e., a policy that maximizes the worst-case reward across all POMDPs. We
generalize and expand on existing work in the following way. First, we show
that ME-POMDPs can be generalized to POMDPs with sets of initial beliefs, which
we call adversarial-belief POMDPs (AB-POMDPs). Second, we show that any
arbitrary ME-POMDP can be reduced to a ME-POMDP that only varies in its
transition and reward functions or only in its observation and reward
functions, while preserving (optimal) policies. We then devise exact and
approximate (point-based) algorithms to compute robust policies for AB-POMDPs,
and thus ME-POMDPs. We demonstrate that we can compute policies for standard
POMDP benchmarks extended to the multi-environment setting.

</details>


### [59] [Test-Time Tuned Language Models Enable End-to-end De Novo Molecular Structure Generation from MS/MS Spectra](https://arxiv.org/abs/2510.23746)
*Laura Mismetti,Marvin Alberts,Andreas Krause,Mara Graziani*

Main category: cs.AI

TL;DR: 本文介绍了一种新框架，利用测试时间调优，增强预训练转换器模型，实现从串联质谱和分子式直接生成新分子结构。该框架在两个流行基准上的表现优于现有方法DiffMS，同时提高了模型对未知分子的适应性和准确度。


<details>
  <summary>Details</summary>
Motivation: 现有的分子识别方法依赖于数据库匹配或多步骤流水线，导致对于未知化合物难以找到正确匹配。为了克服这个问题，引入了一种新的框架，旨在直接从串联质谱和分子式中生成新分子结构，不需要手动注释或中间步骤，以提高识别未知化合物的能力。

Method: 通过测试时间调优技术，该框架能够增强预训练的转换器模型，实现端到端的新分子结构生成。这种方法避免了传统的多步骤流程和数据库匹配的方法，使得分子结构识别更加高效和精准。

Result: 该方法在两个基准数据集NPLIB1和MassSpecGym上的表现分别比现有的SOTA方法DiffMS高出100%和20%，并且在实验谱图上的测试时间调优提高了模型的动态适应性，其性能比传统微调高出62%。

Conclusion: 测试时间调优使得模型能够更好地适应未知谱图，并生成与实际结构相近的分子候选结构，为人类理解和统计包括修补误差提供了有价值的指导，大大增强了对未知化合物的识别能力和准确性。

Abstract: Tandem Mass Spectrometry enables the identification of unknown compounds in
crucial fields such as metabolomics, natural product discovery and
environmental analysis. However, current methods rely on database matching from
previously observed molecules, or on multi-step pipelines that require
intermediate fragment or fingerprint prediction. This makes finding the correct
molecule highly challenging, particularly for compounds absent from reference
databases. We introduce a framework that, by leveraging test-time tuning,
enhances the learning of a pre-trained transformer model to address this gap,
enabling end-to-end de novo molecular structure generation directly from the
tandem mass spectra and molecular formulae, bypassing manual annotations and
intermediate steps. We surpass the de-facto state-of-the-art approach DiffMS on
two popular benchmarks NPLIB1 and MassSpecGym by 100% and 20%, respectively.
Test-time tuning on experimental spectra allows the model to dynamically adapt
to novel spectra, and the relative performance gain over conventional
fine-tuning is of 62% on MassSpecGym. When predictions deviate from the ground
truth, the generated molecular candidates remain structurally accurate,
providing valuable guidance for human interpretation and more reliable
identification.

</details>


### [60] [Evaluating In Silico Creativity: An Expert Review of AI Chess Compositions](https://arxiv.org/abs/2510.23772)
*Vivek Veeriah,Federico Barbero,Marcus Chiam,Xidong Feng,Michael Dennis,Ryan Pachauri,Thomas Tumiel,Johan Obando-Ceron,Jiaxin Shi,Shaobo Hou,Satinder Singh,Nenad Tomašev,Tom Zahavy*

Main category: cs.AI

TL;DR: 研究探索了一种AI系统，可以生成具有美学吸引力、新颖性和反直觉解决方案的国际象棋难题，并邀请国际大师对这些难题进行评估。


<details>
  <summary>Details</summary>
Motivation: 研究的动机是探讨生成式AI在创造性和新颖输出方面的能力，特别是在国际象棋难题生成中的应用。

Method: 研究简要提及研究方法，但要求读者参考技术论文以获取更多细节。研究将AI生成的问题集展示给国际象棋权威大师评审，以此方法评估系统的创意能力。

Result: 国际象棋专家们选出了他们最喜欢的问题，并对这些问题进行了评价，考虑到诸如创造性、难度水平或美学设计等方面。结果展示了AI系统在生成具有美学吸引力和新颖性的国际象棋难题方面的潜力。

Conclusion: 研究证明了AI系统在生成具有美学价值和新颖性的问题方面的有效性，这也是未来国际象棋难题生成研究的方向。

Abstract: The rapid advancement of Generative AI has raised significant questions
regarding its ability to produce creative and novel outputs. Our recent work
investigates this question within the domain of chess puzzles and presents an
AI system designed to generate puzzles characterized by aesthetic appeal,
novelty, counter-intuitive and unique solutions. We briefly discuss our method
below and refer the reader to the technical paper for more details. To assess
our system's creativity, we presented a curated booklet of AI-generated puzzles
to three world-renowned experts: International Master for chess compositions
Amatzia Avni, Grandmaster Jonathan Levitt, and Grandmaster Matthew Sadler. All
three are noted authors on chess aesthetics and the evolving role of computers
in the game. They were asked to select their favorites and explain what made
them appealing, considering qualities such as their creativity, level of
challenge, or aesthetic design.

</details>


### [61] [Why Foundation Models in Pathology Are Failing](https://arxiv.org/abs/2510.23807)
*Hamid R. Tizhoosh*

Main category: cs.AI

TL;DR: 本文探讨了在计算病理学中基础模型(FMs)的不足，并指出这些不足源自基础建模与人类组织固有复杂性之间的概念性不匹配。指出了七种与基础模型在计算病理学应用中失效相关的因素，呼吁重新思考基础模型在病理学这个领域的应用思路。


<details>
  <summary>Details</summary>
Motivation: 计算机视觉和语言处理领域中，基础模型通过大规模自我监督和多模态学习革新了非医学领域，人们期望这种模式可以用于癌症诊断、预后和多模态检索中带来相似的突破，但是系统评估显示，基础模型在医学领域存在明显的不足，本文欲探讨其原因。

Method: 通过系统评估识别计算病理学中基础模型的不足，深入探讨这种模式在医学领域的应用挑战，识别基础模型在医学应用中的概念性不匹配。

Result: 本文识别了计算病理学中基础模型的七种失效因素，包括生物复杂性、自我监督无关紧要、过度推广等。主要结论是基础模型与病理学之间的匹配度不足。

Conclusion: 当前病理学基础模型在概念上与组织形态学的本质特性不一致，呼吁对此进行根本性的反思。

Abstract: In non-medical domains, foundation models (FMs) have revolutionized computer
vision and language processing through large-scale self-supervised and
multimodal learning. Consequently, their rapid adoption in computational
pathology was expected to deliver comparable breakthroughs in cancer diagnosis,
prognostication, and multimodal retrieval. However, recent systematic
evaluations reveal fundamental weaknesses: low diagnostic accuracy, poor
robustness, geometric instability, heavy computational demands, and concerning
safety vulnerabilities. This short paper examines these shortcomings and argues
that they stem from deeper conceptual mismatches between the assumptions
underlying generic foundation modeling in mainstream AI and the intrinsic
complexity of human tissue. Seven interrelated causes are identified:
biological complexity, ineffective self-supervision, overgeneralization,
excessive architectural complexity, lack of domain-specific innovation,
insufficient data, and a fundamental design flaw related to tissue patch size.
These findings suggest that current pathology foundation models remain
conceptually misaligned with the nature of tissue morphology and call for a
fundamental rethinking of the paradigm itself.

</details>


### [62] [ReCAP: Recursive Context-Aware Reasoning and Planning for Large Language Model Agents](https://arxiv.org/abs/2510.23822)
*Zhenyu Zhang,Tianyi Chen,Weiran Xu,Alex Pentland,Jiaxin Pei*

Main category: cs.AI

TL;DR: ReCAP是一种新的分层框架，通过结合计划分解、保持上下文一致性以及减少冗余提示的机制，提高了大型语言模型在长时间任务中的表现。实验显示，与旧方法相比，ReCAP在不同基准上实现了显著的改进，特别是在同步Robotouille中实现了32%的改进，在异步Robotouille中实现了29%的改进。


<details>
  <summary>Details</summary>
Motivation: 现有的方法存在上下文漂移、目标信息丢失以及反复失败等挑战，无法有效地实现多步骤推理和动态重规划的任务。作者因此提出了ReCAP框架，旨在提高大语言模型处理此类任务的能力，保持多级上下文的一致性和减少冗余提示。

Method: ReCAP框架利用计划分解、层次上下文重新注入和内存效率执行三种机制，这些机制减少了冗余提示，保持了上下文更新的一致性，并降低了任务深度增加的成本，从而提高了长时间任务中子目标的一致性和成功率。

Result: 实验结果显示，ReCAP相对于以前的方法，显著提高了子目标的一致性和成功率，在同步和异步的Robotouille基准上分别实现了32%和29%的改进。这些结果证明了ReCAP框架在处理多级任务和顺序任务时的有效性。

Conclusion: ReCAP通过有效结合计划分解和重新注入上下文，提供了一种新的方法来处理长时任务，显示出强大的性能提升。这种方法为处理那些需要密集多阶段的推理和计划的任务提供了一种新的有效的处理方式。

Abstract: Long-horizon tasks requiring multi-step reasoning and dynamic re-planning
remain challenging for large language models (LLMs). Sequential prompting
methods are prone to context drift, loss of goal information, and recurrent
failure cycles, while hierarchical prompting methods often weaken cross-level
continuity or incur substantial runtime overhead. We introduce ReCAP (Recursive
Context-Aware Reasoning and Planning), a hierarchical framework with shared
context for reasoning and planning in LLMs. ReCAP combines three key
mechanisms: (i) plan-ahead decomposition, in which the model generates a full
subtask list, executes the first item, and refines the remainder; (ii)
structured re-injection of parent plans, maintaining consistent multi-level
context during recursive return; and (iii) memory-efficient execution, bounding
the active prompt so costs scale linearly with task depth. Together these
mechanisms align high-level goals with low-level actions, reduce redundant
prompting, and preserve coherent context updates across recursion. Experiments
demonstrate that ReCAP substantially improves subgoal alignment and success
rates on various long-horizon reasoning benchmarks, achieving a 32% gain on
synchronous Robotouille and a 29% improvement on asynchronous Robotouille under
the strict pass@1 protocol.

</details>


### [63] [Generating Creative Chess Puzzles](https://arxiv.org/abs/2510.23881)
*Xidong Feng,Vivek Veeriah,Marcus Chiam,Michael Dennis,Ryan Pachauri,Thomas Tumiel,Federico Barbero,Johan Obando-Ceron,Jiaxin Shi,Satinder Singh,Shaobo Hou,Nenad Tomašev,Tom Zahavy*

Main category: cs.AI

TL;DR: 本文提出了一种对抗生成AI在棋类谜题创造性、美学和反常识输出方面挑战的方法，通过引入一个基于国际象棋引擎搜索统计的新型强化学习框架，显著提高反常识谜题生成能力，达到甚至超过人类专家作品的水平。


<details>
  <summary>Details</summary>
Motivation: 生成真正具有创造性、美学价值和反常识的输出是生成式AI的一大挑战，特别是在棋类谜题领域，文中通过引入特定的方法来应对这一问题。 

Method: 该方法首先通过基准评估生成式AI架构，然后引入一个基于强化学习与棋类引擎搜索统计的新型奖励机制，旨在增强谜题的独特性，反常识性，多样性，和真实性。

Result: 该研究成果使反常识谜题的生成增加了10倍，从0.22%（监督）增加到2.5%，超过了现有数据集的率和Lichess训练模型的水平。这些谜题在新颖性，多样性，真实性等方面成果显著，从人类专家角度来看，这些生成的谜题比传统布置的变化更具有创造性，更有趣，并且更具有反常识性。

Conclusion: 生成方法打破了以往对AI所能产生的文本的限制，达到了接近经典作品的效果，并获得了三名世界级专家的认可。

Abstract: While Generative AI rapidly advances in various domains, generating truly
creative, aesthetic, and counter-intuitive outputs remains a challenge. This
paper presents an approach to tackle these difficulties in the domain of chess
puzzles. We start by benchmarking Generative AI architectures, and then
introduce an RL framework with novel rewards based on chess engine search
statistics to overcome some of those shortcomings. The rewards are designed to
enhance a puzzle's uniqueness, counter-intuitiveness, diversity, and realism.
Our RL approach dramatically increases counter-intuitive puzzle generation by
10x, from 0.22\% (supervised) to 2.5\%, surpassing existing dataset rates
(2.1\%) and the best Lichess-trained model (0.4\%). Our puzzles meet novelty
and diversity benchmarks, retain aesthetic themes, and are rated by human
experts as more creative, enjoyable, and counter-intuitive than composed book
puzzles, even approaching classic compositions. Our final outcome is a curated
booklet of these AI-generated puzzles, which is acknowledged for creativity by
three world-renowned experts.

</details>


### [64] [Hybrid Modeling, Sim-to-Real Reinforcement Learning, and Large Language Model Driven Control for Digital Twins](https://arxiv.org/abs/2510.23882)
*Adil Rasheed,Oscar Ravik,Omer San*

Main category: cs.AI

TL;DR: 本文通过一个微型温室测试平台，研究了数字孪生在动态系统建模和控制中的应用，比较了线性模型、基于物理模型、LSTM和混合分析模型（HAM）在插值和外推情况下的表现，并评估了模型预测控制、强化学习和语言模型控制的性能。结果显示，在建模方面，HAM表现最为平衡，而LSTM精度最高但计算资源消耗较大；在控制器方面，模型预测控制稳健且可预测，强化学习具有很强的适应性，语言模型控制提供了灵活的人机交互方式。


<details>
  <summary>Details</summary>
Motivation: 本文的研究动机是通过集成基于物理的、数据驱动的和混合的方法，以及传统和AI驱动的控制器，来研究数字孪生在动态系统建模和控制中的应用效果，从而为相关问题提供解决方案。

Method: 该研究使用微型温室作为测试平台，开发并比较了四种预测模型：线性模型、物理模型、LSTM和混合分析模型（HAM），在插值和外推情况下进行性能评估。同时，实施了三种控制策略：模型预测控制（MPC）、强化学习（RL）和基于语言模型的控制，以评估精度、适应性和实施努力方面的权衡。

Result: 建模方面，HAM模型实现了精度、泛化能力和计算效率之间的最平衡表现，而LSTM模型在较高的资源消耗下获得了更高的精度。控制器方面，MPC提供了稳健且可预测的表现，RL展示了强烈的适应性，基于语言模型的控制器与预测工具结合时提供了灵活的人机交互方式。

Conclusion: 研究表明，数字孪生结合多种方法和控制器可以在动态系统建模和控制中提供有效的解决方案，其中HAM模型提供最为平衡的建模性能，而MPC控制器则提供了稳健和可预测控制结果。这些成果为未来的研究和实际应用提供了有价值的参考。

Abstract: This work investigates the use of digital twins for dynamical system modeling
and control, integrating physics-based, data-driven, and hybrid approaches with
both traditional and AI-driven controllers. Using a miniature greenhouse as a
test platform, four predictive models Linear, Physics-Based Modeling (PBM),
Long Short Term Memory (LSTM), and Hybrid Analysis and Modeling (HAM) are
developed and compared under interpolation and extrapolation scenarios. Three
control strategies Model Predictive Control (MPC), Reinforcement Learning (RL),
and Large Language Model (LLM) based control are also implemented to assess
trade-offs in precision, adaptability, and implementation effort. Results show
that in modeling HAM provides the most balanced performance across accuracy,
generalization, and computational efficiency, while LSTM achieves high
precision at greater resource cost. Among controllers, MPC delivers robust and
predictable performance, RL demonstrates strong adaptability, and LLM-based
controllers offer flexible human-AI interaction when coupled with predictive
tools.

</details>


### [65] [Latent Chain-of-Thought for Visual Reasoning](https://arxiv.org/abs/2510.23925)
*Guohao Sun,Hang Hua,Jian Wang,Jiebo Luo,Sohail Dianat,Majid Rabbani,Raghuveer Rao,Zhiqiang Tao*

Main category: cs.AI

TL;DR: 提出了一种基于近似变分推理的可扩展训练算法，用于改善LVLMs的推理能力，通过引入稀疏奖励函数和贝叶斯推理策略，提高了模型的有效性、泛化性和可解释性


<details>
  <summary>Details</summary>
Motivation: 传统训练算法可能不适用于未见的推理任务，并且依赖于有偏的奖励模型

Method: 将LVLMs中的推理视为后验推理，并提出了一种基于近似变分推理的可扩展训练算法，结合分散寻求型强化学习和贝叶斯缩放策略

Result: 在七个推理基准测试上超过当前最佳LVLMs

Conclusion: 方法提高了LVLMs的有效性、泛化能力和可解释性

Abstract: Chain-of-thought (CoT) reasoning is critical for improving the
interpretability and reliability of Large Vision-Language Models (LVLMs).
However, existing training algorithms such as SFT, PPO, and GRPO may not
generalize well across unseen reasoning tasks and heavily rely on a biased
reward model. To address this challenge, we reformulate reasoning in LVLMs as
posterior inference and propose a scalable training algorithm based on
amortized variational inference. By leveraging diversity-seeking reinforcement
learning algorithms, we introduce a novel sparse reward function for
token-level learning signals that encourage diverse, high-likelihood latent
CoT, overcoming deterministic sampling limitations and avoiding reward hacking.
Additionally, we implement a Bayesian inference-scaling strategy that replaces
costly Best-of-N and Beam Search with a marginal likelihood to efficiently rank
optimal rationales and answers. We empirically demonstrate that the proposed
method enhances the state-of-the-art LVLMs on seven reasoning benchmarks, in
terms of effectiveness, generalization, and interpretability.

</details>


### [66] [Decentralized Causal Discovery using Judo Calculus](https://arxiv.org/abs/2510.23942)
*Sridhar Mahadevan*

Main category: cs.AI

TL;DR: 提出了基于佐武尔夫-泰尔尼(y)模态算子的直观主义分散式因果发现框架，利用柔道(judo)微积分，可以更加精确地处理因果效应的上下文依赖性，且实验显示其在计算效率和性能上优于传统方法


<details>
  <summary>Details</summary>
Motivation: 因果效应在不同的上下文中是不同的，现有的因果发现方法难以精确处理这种依赖性，提出了带有上下文依赖性的柔道微积分以解决这个问题

Method: 提出基于佐武尔夫-泰尔尼(y)模态算子的直观主义分散式因果发现框架，利用柔道微积分，并结合评分、约束和梯度优化的因果发现方法，进行实验验证

Result: 展示了算法在多领域的有效性以及在计算效率和性能上的优势

Conclusion: 通过柔道微积分和分散式的因果发现框架，可以更精确地处理因果效应的上下文依赖性，并且在计算效率和性能上超过了传统方法

Abstract: We describe a theory and implementation of an intuitionistic decentralized
framework for causal discovery using judo calculus, which is formally defined
as j-stable causal inference using j-do-calculus in a topos of sheaves. In
real-world applications -- from biology to medicine and social science --
causal effects depend on regime (age, country, dose, genotype, or lab
protocol). Our proposed judo calculus formalizes this context dependence
formally as local truth: a causal claim is proven true on a cover of regimes,
not everywhere at once. The Lawvere-Tierney modal operator j chooses which
regimes are relevant; j-stability means the claim holds constructively and
consistently across that family. We describe an algorithmic and implementation
framework for judo calculus, combining it with standard score-based,
constraint-based, and gradient-based causal discovery methods. We describe
experimental results on a range of domains, from synthetic to real-world
datasets from biology and economics. Our experimental results show the
computational efficiency gained by the decentralized nature of sheaf-theoretic
causal discovery, as well as improved performance over classical causal
discovery methods.

</details>


### [67] [The Sign Estimator: LLM Alignment in the Face of Choice Heterogeneity](https://arxiv.org/abs/2510.23965)
*Aymane El Gadarri,Ali Aouad,Vivek F. Farias*

Main category: cs.AI

TL;DR: 提出了一种名为符号估计器的新方法，通过在聚合步骤中将交叉熵替换为二元分类损失，提供了一种简单、有保证的一致性和高效的估计方法。这种方法在轻微假设下恢复了一致的序数对齐，并在该环境中首次实现了多项式有限样本误差界限。在使用数字双胞胎模拟的大型语言模型对齐的现实情景中，符号估计器显著减少了模拟人物偏好的扭曲，将（角度）估计误差减少了近35%，在与真实的人口偏好不符的情况下从12%减少到8%。与显式建模用户异质性和需要跟踪个人级偏好数据的陪审团数据启发式方法相比，该方法在保持现有大型语言模型对齐管道的实现简易性的同时表现良好。


<details>
  <summary>Details</summary>
Motivation: 传统LLM对齐方法在面对人类偏好的异质性时存在脆弱性。拟合朴素概率模型到成对比较数据上会获得人口平均效用不一致的估计，这是一个社会福利的标准度量。因此，提出了一种克服这些脆弱性的新方法。

Method: 提出的方法称为符号估计器，通过在聚合步骤中用二元分类损失替换交叉熵来实现人口平均效用的估计。该方法证明了一致性和效率，并且在轻微假设下恢复一致的序数对齐。它适用于真实模拟场景，特别是在使用数字双胞胎模拟的LLM对齐中具有显著的效果。在理论上，这种方法提供了多项式有限样本误差界。

Result: 在一系列模拟场景中，与标准RLHF相比，符号估计器通过减少35%的（角度）估计误差，降低了模拟人物偏好的扭曲。此外，与真实人口偏好不一致的比例从12%减少到了8%。相比于其他需要考虑用户异质性的方法，这种新的符号估计器方法在不增加复杂度的情况下提供了一种更有效的解决方案。

Conclusion: 新的符号估计器方法提供了一种简单、高效且有保证的估计方法，能够在LLM对齐过程中处理人类偏好异质性问题。该方法在模拟实验中成功地减少了估计误差，并且在处理复杂偏好情况时提供了比标准方法更好的效果。

Abstract: Traditional LLM alignment methods are vulnerable to heterogeneity in human
preferences. Fitting a na\"ive probabilistic model to pairwise comparison data
(say over prompt-completion pairs) yields an inconsistent estimate of the
population-average utility -a canonical measure of social welfare. We propose a
new method, dubbed the sign estimator, that provides a simple, provably
consistent, and efficient estimator by replacing cross-entropy with binary
classification loss in the aggregation step. This simple modification recovers
consistent ordinal alignment under mild assumptions and achieves the first
polynomial finite-sample error bounds in this setting. In realistic simulations
of LLM alignment using digital twins, the sign estimator substantially reduces
preference distortion over a panel of simulated personas, cutting (angular)
estimation error by nearly 35% and decreasing disagreement with true population
preferences from 12% to 8% compared to standard RLHF. Our method also compares
favorably to panel data heuristics that explicitly model user heterogeneity and
require tracking individual-level preference data-all while maintaining the
implementation simplicity of existing LLM alignment pipelines.

</details>


### [68] [Learning Individual Movement Shifts After Urban Disruptions with Social Infrastructure Reliance](https://arxiv.org/abs/2510.23989)
*Shangde Gao,Zelin Xu,Zhe Jiang*

Main category: cs.AI

TL;DR: 该研究将个人的社会基础设施韧性(SIR)整合到一个条件深度学习模型中，以捕捉大规模稀疏的个人级数据中个人运动模式与局部空间环境之间的复杂关系。实验表明，该模型能够更准确地预测事件后的个人运动模式，特别是对于在事件前表现出相似运动模式但SIR不同的个体之间的分化运动模式变化的预测具有优势。


<details>
  <summary>Details</summary>
Motivation: 研究旨在预测破坏性事件后个人运动模式的变化，但这一目标的实现面临三大挑战：缺乏衡量个人异质SIR的指标，难以捕捉个人运动模式与空间背景的复杂互动关系，以及个人级空间稀疏运动数据不适用于传统决策算法。基于这些挑战，本文提出了整合个人SIR和空间背景的对策。

Method: 本文开发了一种条件深度学习模型，该模型将个体的SIR和空间背景整合进去，通过大规模、稀疏的个人级数据捕捉个人运动模式与局部空间环境的关系，从而更准确地预测事件后的个人运动模式。

Result: 实验表明，将个体SIR和空间背景元素整合到模型中，可以显著提升模型预测事件后个人运动模式的能力，特别是一些在事件前有过相似运动模式但在SIR上有差异的个体之间的分化运动模式变化。

Conclusion: 该研究通过整合个人SIR和空间背景，证明了条件深度学习模型在预测破裂事件后个人运动模式变化方面的能力，此方法对提高个人运动模式预测的准确性具有重要贡献。

Abstract: Shifts in individual movement patterns following disruptive events can reveal
changing demands for community resources. However, predicting such shifts
before disruptive events remains challenging for several reasons. First,
measures are lacking for individuals' heterogeneous social infrastructure
resilience (SIR), which directly influences their movement patterns, and
commonly used features are often limited or unavailable at scale, e.g.,
sociodemographic characteristics. Second, the complex interactions between
individual movement patterns and spatial contexts have not been sufficiently
captured. Third, individual-level movement may be spatially sparse and not
well-suited to traditional decision-making methods for movement predictions.
This study incorporates individuals' SIR into a conditioned deep learning model
to capture the complex relationships between individual movement patterns and
local spatial context using large-scale, sparse individual-level data. Our
experiments demonstrate that incorporating individuals' SIR and spatial context
can enhance the model's ability to predict post-event individual movement
patterns. The conditioned model can capture the divergent shifts in movement
patterns among individuals who exhibit similar pre-event patterns but differ in
SIR.

</details>


### [69] [OneCast: Structured Decomposition and Modular Generation for Cross-Domain Time Series Forecasting](https://arxiv.org/abs/2510.24028)
*Tingyue Pan,Mingyue Cheng,Shilong Zhang,Zhiding Liu,Xiaoyu Tao,Yucong Luo,Jintao Zhang,Qi Liu*

Main category: cs.AI

TL;DR: 本文提出了一种名为OneCast的跨域时间序列预测框架，该框架通过分解时间序列数据为季节性和趋势性组成部分，并分别采用特定的生成方法来预测。实验显示OneCast在多个数据域上的表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前的跨域时间序列预测方法在面对特定领域的趋势变化和不一致的周期模式时表现不佳，这是因为它们未能明确地区分时间序列的基本结构组件。为了克服这个限制，提出了一个结构化的预测框架来提升预测的准确性和适应性。

Method: OneCast通过一个轻量级的投影模块和一个语义感知的标记器分别提取时间序列中的季节性和趋势性组成部分。季节性部分通过可解释的基函数来重建周期模式，而趋势性部分则被转化为离散标记并通过掩码离散扩散机制来预测。最后将这两部分的输出结合起来生成最终的预测。

Result: 在八个不同领域的广泛实验中，OneCast的表现比当前最先进的基准方法更好。这是首次通过分解时间序列的结构组分来优化跨域时间序列预测的表现。

Conclusion: OneCast框架提供了一种基于结构化的跨域时间序列预测方法，这种方法不仅提高了预测的准确率，还具有更强的跨领域适应性。

Abstract: Cross-domain time series forecasting is a valuable task in various web
applications. Despite its rapid advancement, achieving effective generalization
across heterogeneous time series data remains a significant challenge. Existing
methods have made progress by extending single-domain models, yet often fall
short when facing domain-specific trend shifts and inconsistent periodic
patterns. We argue that a key limitation lies in treating temporal series as
undifferentiated sequence, without explicitly decoupling their inherent
structural components. To address this, we propose OneCast, a structured and
modular forecasting framework that decomposes time series into seasonal and
trend components, each modeled through tailored generative pathways.
Specifically, the seasonal component is captured by a lightweight projection
module that reconstructs periodic patterns via interpretable basis functions.
In parallel, the trend component is encoded into discrete tokens at segment
level via a semantic-aware tokenizer, and subsequently inferred through a
masked discrete diffusion mechanism. The outputs from both branches are
combined to produce a final forecast that captures seasonal patterns while
tracking domain-specific trends. Extensive experiments across eight domains
demonstrate that OneCast mostly outperforms state-of-the-art baselines.

</details>


### [70] [Modeling Electric Vehicle Car-Following Behavior: Classical vs Machine Learning Approach](https://arxiv.org/abs/2510.24085)
*Md. Shihab Uddin,Md Nazmus Shakib,Rahul Bhadani*

Main category: cs.AI

TL;DR: 此研究比较了经典模型和机器学习模型在电动汽车跟随行为预测中的表现，使用随机森林回归器的机器学习模型在预测加速时表现出更高的精度，优于经典物理模型。结果显示，随机森林模型在各种情况下表现最佳。这种模型对于模拟电动汽车行为和分析混合自治交通动态非常重要。


<details>
  <summary>Details</summary>
Motivation: 随着电动汽车的增多，了解其驾驶行为对于提高交通安全及开发智能驾驶系统至关重要。本研究旨在比较经典的物理学模型和基于机器学习的模型在电动汽车跟随行为预测中的表现。

Method: 研究采用了一个包含电动汽车跟随内燃机车辆的实际数据集，通过最小化预测值与实际数据之间的RMSE来校准经典模型的参数。机器学习方法采用随机森林回归器，通过间距、速度和间隙类型作为输入来预测加速。

Result: 随机森林模型在不同类型的间隙下表现优于经典物理模型，如长间隙下RMSE为0.0016，而CACC模型的RMSE为2.67。这些发现表明机器学习模型在所有场景中均表现出色。

Conclusion: 本研究证明了机器学习模型在电动汽车跟随行为预测中的优势，对于模拟电动汽车行为及分析混合自治交通动态具有重要价值。

Abstract: The increasing adoption of electric vehicles (EVs) necessitates an
understanding of their driving behavior to enhance traffic safety and develop
smart driving systems. This study compares classical and machine learning
models for EV car following behavior. Classical models include the Intelligent
Driver Model (IDM), Optimum Velocity Model (OVM), Optimal Velocity Relative
Velocity (OVRV), and a simplified CACC model, while the machine learning
approach employs a Random Forest Regressor. Using a real world dataset of an EV
following an internal combustion engine (ICE) vehicle under varied driving
conditions, we calibrated classical model parameters by minimizing the RMSE
between predictions and real data. The Random Forest model predicts
acceleration using spacing, speed, and gap type as inputs. Results demonstrate
the Random Forest's superior accuracy, achieving RMSEs of 0.0046 (medium gap),
0.0016 (long gap), and 0.0025 (extra long gap). Among physics based models,
CACC performed best, with an RMSE of 2.67 for long gaps. These findings
highlight the machine learning model's performance across all scenarios. Such
models are valuable for simulating EV behavior and analyzing mixed autonomy
traffic dynamics in EV integrated environments.

</details>


### [71] [HistoLens: An Interactive XAI Toolkit for Verifying and Mitigating Flaws in Vision-Language Models for Histopathology](https://arxiv.org/abs/2510.24115)
*Sandeep Vissapragada,Vikrant Sahu,Gagan Raj Gupta,Vandita Singh*

Main category: cs.AI

TL;DR: HistoLens是一个透明协作的AI系统，医生可以通过提问来获取关于组织切片的分析报告，并且能够获得'视觉证据'来理解AI的判断依据。确保AI专注于病人的组织，忽略背景噪声，协助医生更快更自信地做出诊断。



<details>
  <summary>Details</summary>
Motivation: 为了使医生能够信任AI，并理解它的推理过程，就像咨询一位同事一样。


Method: 创建了一个名为HistoLens的系统，支持医生用自然语言提问，系统将其转化为AI引擎的精确查询，AI提供详细的报告；并且可以展示'视觉证据', 即热图，显示出AI在分析时所使用的具体细胞和区域。


Result: 医生可以通过这种方式获得AI辅助的分析，同时得到可视化的解释，这帮助医生更快更自信地做出诊断。


Conclusion: 通过HistoLens，医生可以使用一个值得信赖的AI助手来验证他们的见解，维持专家的角色同时提高效率。


Abstract: For doctors to truly trust artificial intelligence, it can't be a black box.
They need to understand its reasoning, almost as if they were consulting a
colleague. We created HistoLens1 to be that transparent, collaborative partner.
It allows a pathologist to simply ask a question in plain English about a
tissue slide--just as they would ask a trainee. Our system intelligently
translates this question into a precise query for its AI engine, which then
provides a clear, structured report. But it doesn't stop there. If a doctor
ever asks, "Why?", HistoLens can instantly provide a 'visual proof' for any
finding--a heatmap that points to the exact cells and regions the AI used for
its analysis. We've also ensured the AI focuses only on the patient's tissue,
just like a trained pathologist would, by teaching it to ignore distracting
background noise. The result is a workflow where the pathologist remains the
expert in charge, using a trustworthy AI assistant to verify their insights and
make faster, more confident diagnoses.

</details>


### [72] [From Observability Data to Diagnosis: An Evolving Multi-agent System for Incident Management in Cloud Systems](https://arxiv.org/abs/2510.24145)
*Yu Luo,Jiamin Jiang,Jingfei Feng,Lei Tao,Qingliang Zhang,Xidao Wen,Yongqian Sun,Shenglin Zhang,Jielong Huang,Nan Qi,Dan Pei*

Main category: cs.AI

TL;DR: 本文介绍了一种轻量级的自我进化多智能体系统OpsAgent，用于自动处理大规模云系统的事故管理。OpsAgent包括一个训练自由的数据处理器，将异构的可观测性数据转换为结构化文本描述，以及一个多智能体合作框架，使诊断推理变得透明和可审计。实验表明，OpsAgent具有最先进的性能，能够自我进化，具有解释性，成本效益高效，适用于实际部署和长期运营。


<details>
  <summary>Details</summary>
Motivation: 大规模云系统中的事故管理依赖于手动操作，这既耗时又容易出错，因为要处理大量的异构数据。目前的自动化解决方案在泛化性、解释性和部署成本上存在不足。因此，引入OpsAgent来解决这些问题。

Method: OpsAgent利用训练自由的数据处理器将异构观测数据转化为结构化、易于理解的文本描述，并引入多智能体协同框架以便诊断推理更为透明和可审计。同时OpsAgent具备自我升级机制，实现了内部模型更新和外部经验积累的闭合部署循环，从而实现持续的能力提升。

Result: 基于OPENRCA基准实验，OpsAgent展示了最先进的性能，证明了其在自身进化、解释性、成本效益和可部署性方面的优势，特别适用于大规模云系统中的长期可靠运营。

Conclusion: OpsAgent作为一款轻量级且自我进化的多智能体系统，能够有效解决大规模云系统中的事故管理问题，具有广泛的适用性和实践性。

Abstract: Incident management (IM) is central to the reliability of large-scale cloud
systems. Yet manual IM, where on-call engineers examine metrics, logs, and
traces is labor-intensive and error-prone in the face of massive and
heterogeneous observability data. Existing automated IM approaches often
struggle to generalize across systems, provide limited interpretability, and
incur high deployment costs, which hinders adoption in practice. In this paper,
we present OpsAgent, a lightweight, self-evolving multi-agent system for IM
that employs a training-free data processor to convert heterogeneous
observability data into structured textual descriptions, along with a
multi-agent collaboration framework that makes diagnostic inference transparent
and auditable. To support continual capability growth, OpsAgent also introduces
a dual self-evolution mechanism that integrates internal model updates with
external experience accumulation, thereby closing the deployment loop.
Comprehensive experiments on the OPENRCA benchmark demonstrate state-of-the-art
performance and show that OpsAgent is generalizable, interpretable,
cost-efficient, and self-evolving, making it a practically deployable and
sustainable solution for long-term operation in real-world cloud systems.

</details>


### [73] [BLM$_1$: A Boundless Large Model for Cross-Space, Cross-Task, and Cross-Embodiment Learning](https://arxiv.org/abs/2510.24161)
*Wentao Tan,Bowen Wang,Heng Zhi,Chenyu Liu,Zhe Li,Jian Liu,Zengrong Lin,Yukun Dai,Yipeng Chen,Wenjie Yang,Enci Xie,Hao Xue,Baixu Ji,Chen Xu,Zhibin Wang,Tianshi Wang,Lei Zhu,Heng Tao Shen*

Main category: cs.AI

TL;DR: 介绍了一种名为Boundless Large Model (BLM$_1$) 的新型多模态空间基础模型，该模型兼具跨数字-物理空间无缝操作、任务泛化和身体泛化能力。经过两个阶段的训练，BLM$_1$在各种任务中表现优于传统模型家族，特别是在数字和物理任务方面分别提高了6%和3%的性能。


<details>
  <summary>Details</summary>
Motivation: 当前的多模态大型语言模型（MLLM）在数字-物理空间迁移、任务泛化和身体泛化方面存在显著限制，比如在真实世界中表现不佳，而BLM$_1$旨在解决这些问题，通过融合跨空间迁移、跨任务学习和跨身体泛化能力来改善模型性能。

Method: BLM$_1$采用了两阶段训练策略：第一阶段将身体知识注入数字语料库以维持语言能力，第二阶段通过意图桥接接口训练政策模块，从MLLM中提取高层次语义来引导控制，且无需微调MLLM主体部分。

Result: 实验结果表明单个BLM$_1$实例在数字和物理基准测试中均表现优于四种模型家族(MLLMs, ELLMs, VLAs, 和GMLMs)，尤其是在数字和物理任务上分别提高了约6%和3%的性能。

Conclusion: BLM$_1$通过整合跨空间、跨任务和跨身体的能力，克服了现有模型在数字-物理空间迁移和泛化方面的局限性，提升了在真实世界中的应用潜力。

Abstract: Multimodal large language models (MLLMs) have advanced vision-language
reasoning and are increasingly deployed in embodied agents. However,
significant limitations remain: MLLMs generalize poorly across digital-physical
spaces and embodiments; vision-language-action models (VLAs) produce low-level
actions yet lack robust high-level embodied reasoning; and most embodied large
language models (ELLMs) are constrained to digital-space with poor
generalization to the physical world. Thus, unified models that operate
seamlessly across digital and physical spaces while generalizing across
embodiments and tasks remain absent. We introduce the \textbf{Boundless Large
Model (BLM$_1$)}, a multimodal spatial foundation model that preserves
instruction following and reasoning, incorporates embodied knowledge, and
supports robust cross-embodiment control. BLM$_1$ integrates three key
capabilities -- \textit{cross-space transfer, cross-task learning, and
cross-embodiment generalization} -- via a two-stage training paradigm. Stage I
injects embodied knowledge into the MLLM through curated digital corpora while
maintaining language competence. Stage II trains a policy module through an
intent-bridging interface that extracts high-level semantics from the MLLM to
guide control, without fine-tuning the MLLM backbone. This process is supported
by a self-collected cross-embodiment demonstration suite spanning four robot
embodiments and six progressively challenging tasks. Evaluations across digital
and physical benchmarks show that a single BLM$_1$ instance outperforms four
model families -- MLLMs, ELLMs, VLAs, and GMLMs -- achieving
$\sim\!\textbf{6%}$ gains in digital tasks and $\sim\!\textbf{3%}$ in physical
tasks.

</details>


### [74] [Investigating Intra-Abstraction Policies For Non-exact Abstraction Algorithms](https://arxiv.org/abs/2510.24297)
*Robin Schmöcker,Alexander Dockhorn,Bodo Rosenhahn*

Main category: cs.AI

TL;DR: 本文探讨了蒙特卡洛树搜索(MCTS)的一个弱点，即样本效率，并通过使用状态和/或动作抽象来改善上界置信区间(UCB)估值。同时提出并实证评估了几种替代的同抽象策略，其中一些策略在多数环境下和参数设置下优于随机策略。


<details>
  <summary>Details</summary>
Motivation: 本文旨在解决蒙特卡洛树搜索(MCTS)中的样本效率问题，并针对使用相同抽象节点时可能出现的冲突情况给出解决方案。

Method: 本文提出的解决方法是采用几种替代的同抽象策略，这些方法旨在改进上界置信区间(UCB)估值，并避免随机选择导致的冲突问题。

Result: 实验证明，本文提出的一些方法在多数测试环境和参数设置下优于随机策略。

Conclusion: 这项工作澄清了现有抽象算法中的一个未解决问题，并提供了改进蒙特卡洛树搜索样本效率的新途径。

Abstract: One weakness of Monte Carlo Tree Search (MCTS) is its sample efficiency which
can be addressed by building and using state and/or action abstractions in
parallel to the tree search such that information can be shared among nodes of
the same layer. The primary usage of abstractions for MCTS is to enhance the
Upper Confidence Bound (UCB) value during the tree policy by aggregating visits
and returns of an abstract node. However, this direct usage of abstractions
does not take the case into account where multiple actions with the same parent
might be in the same abstract node, as these would then all have the same UCB
value, thus requiring a tiebreak rule. In state-of-the-art abstraction
algorithms such as pruned On the Go Abstractions (pruned OGA), this case has
not been noticed, and a random tiebreak rule was implicitly chosen. In this
paper, we propose and empirically evaluate several alternative
intra-abstraction policies, several of which outperform the random policy
across a majority of environments and parameter settings.

</details>


### [75] [Verifying Large Language Models' Reasoning Paths via Correlation Matrix Rank](https://arxiv.org/abs/2510.24299)
*Jiayu Liu,Wei Dai,Zhenya Huang,Ning Miao,Enhong Chen*

Main category: cs.AI

TL;DR: 本文提出了一个简单有效的自我指示器方法，利用LLM内部行为来评估其推理路径的正确性，这种方法仅依赖于LLM内部数据，避免了额外的训练开销和复杂的提示设计，并且在多个LLM上实验有效，提高了推理准确率。


<details>
  <summary>Details</summary>
Motivation: 现有的检查LLM输出的方法依赖于额外的外部资源，如训练好的验证器或复杂的提示，这导致了高计算开销且仅适用于特定领域。本文旨在探究如何通过LLM内部行为来判断其推理路径的正确性，从而减少额外开销并提高通用性。

Method: 该方法基于输入问题与输出推理路径之间的相关矩阵进行计算，这种计算仅依赖于LLM自身的数据。作者设计了一种简单且易于插件的自我指示器方法，用以重新加权候选的推理路径，从而实现显著的性能提升的同时保持低计算开销。

Result: 实验结果表明，自我指示器方法能将正确推理路径与错误推理路径区分的准确率提高到超过75%，在三个推理基准测试上，其准确度提升超过8%。

Conclusion: 提出了一种轻量级的自我指示器方法，能够在不依赖外部资源的情况下有效评估LLM推理路径的正确性，这种方法不仅适用于各种规模和家族的LLM，而且能够显著提升推理准确度。

Abstract: Despite the strong reasoning ability of large language models~(LLMs), they
are prone to errors and hallucinations. As a result, how to check their outputs
effectively and efficiently has become a critical problem in their
applications. Existing checking methods heavily rely on external resources,
such as trained verifiers (e.g., process/outcome reward models) or elaborate
prompts, which lead to high computational overhead and are only applicable to
specific domains. In this paper, we investigate whether the internal behaviors
of LLMs have already implied the credibility of their reasoning paths.
Specifically, we find that the rank of the correlation matrix between the input
problem and the output reasoning path is a robust indicator of reasoning
correctness. Different from other correctness indicators for LLMs, the
calculation of the correlation matrix only relies on the LLM itself, which
avoids the hassle of training a separate model or designing complicated
prompts. Based on it, we design a simple, plug-and-play Self-Indicator method
to reweight candidate reasoning paths, which achieves significant performance
improvements than other voting and verification methods with very few
computational overhead. Our experiments across multiple LLMs of varying scales
and model families have further shown the effectiveness of Self-Indicator. It
achieves over 75% accuracy in distinguishing correct reasoning paths from
incorrect ones, and, in turn, improves the accuracies on three reasoning
benchmarks by more than 8%.

</details>


### [76] [Generative Large Language Models (gLLMs) in Content Analysis: A Practical Guide for Communication Research](https://arxiv.org/abs/2510.24337)
*Daria Kravets-Meinke,Hannah Schmid-Petri,Sonja Niemann,Ute Schmid*

Main category: cs.AI

TL;DR: gLLMs 如 ChatGPT 在传播研究中的内容分析中表现出色，但对其有效集成仍需解决多个关键挑战。本文总结了 gLLM 辅助的定量内容分析的新研究，并提出了一套全面的最佳实践指南，旨在提升 gLLM 基于内容分析的可达性和保证研究质量标准的遵守。


<details>
  <summary>Details</summary>
Motivation: 研究发现 gLLMs 在多种编码任务中优于众包工人和训练后的编码者。尽管有潜力，但 gLLM 在传播研究中的集成尚未完全发展。本文旨在解决其所面临的关键挑战，促进 gLLM 在定量内容分析中的应用，并确保研究遵守学术标准。

Method: 本文通过总结新兴研究，提出了一个应对七个关键挑战的综合最佳实践指南，这些挑战包括编码本开发、提示工程、模型选择、参数调整、迭代优化、模型可靠性的验证，以及可选的性能增强。

Result: 文章提出了一个详尽的方法来指导传播研究人员在实践中使用 gLLM 进行内容分析，该方法考虑了现有的和未来的挑战，同时也确保了研究的可靠性和可重复性标准的遵守。

Conclusion: 通过实施所提出的最佳实践，研究人员可以更好地利用 gLLM 的优势，同时确保其内容分析符合传播研究的高质量标准要求。

Abstract: Generative Large Language Models (gLLMs), such as ChatGPT, are increasingly
being used in communication research for content analysis. Studies show that
gLLMs can outperform both crowd workers and trained coders, such as research
assistants, on various coding tasks relevant to communication science, often at
a fraction of the time and cost. Additionally, gLLMs can decode implicit
meanings and contextual information, be instructed using natural language,
deployed with only basic programming skills, and require little to no annotated
data beyond a validation dataset - constituting a paradigm shift in automated
content analysis. Despite their potential, the integration of gLLMs into the
methodological toolkit of communication research remains underdeveloped. In
gLLM-assisted quantitative content analysis, researchers must address at least
seven critical challenges that impact result quality: (1) codebook development,
(2) prompt engineering, (3) model selection, (4) parameter tuning, (5)
iterative refinement, (6) validation of the model's reliability, and
optionally, (7) performance enhancement. This paper synthesizes emerging
research on gLLM-assisted quantitative content analysis and proposes a
comprehensive best-practice guide to navigate these challenges. Our goal is to
make gLLM-based content analysis more accessible to a broader range of
communication researchers and ensure adherence to established disciplinary
quality standards of validity, reliability, reproducibility, and research
ethics.

</details>


### [77] [VDSAgents: A PCS-Guided Multi-Agent System for Veridical Data Science Automation](https://arxiv.org/abs/2510.24339)
*Yunxuan Jiang,Silan Hu,Xiaoning Wang,Yuanyuan Zhang,Xiangyu Chang*

Main category: cs.AI

TL;DR: LLMs are widely used in data science workflows but lack scientific guidance. VDSAgents, a multi-agent system based on Predictability-Computability-Stability (PCS) principles, integrates modular workflow for data science tasks with enhanced trustworthiness and robustness, showing superior performance compared to other automated systems.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the limitation of LLM-driven data science systems by grounding them in scientific and theoretical principles, enhancing trustworthiness and robustness.

Method: The method introduces VDSAgents, a multi-agent system that follows PCS principles and implements a modular workflow for data science tasks with built-in validation mechanisms.

Result: VDSAgents outperforms state-of-the-art data science automation tools in various datasets, proving the effectiveness of its methodology.

Conclusion: Embedding PCS principles into LLM-driven data science systems is feasible and improves the trustworthiness and robustness of the systems.

Abstract: Large language models (LLMs) become increasingly integrated into data science
workflows for automated system design. However, these LLM-driven data science
systems rely solely on the internal reasoning of LLMs, lacking guidance from
scientific and theoretical principles. This limits their trustworthiness and
robustness, especially when dealing with noisy and complex real-world datasets.
This paper provides VDSAgents, a multi-agent system grounded in the
Predictability-Computability-Stability (PCS) principles proposed in the
Veridical Data Science (VDS) framework. Guided by PCS principles, the system
implements a modular workflow for data cleaning, feature engineering, modeling,
and evaluation. Each phase is handled by an elegant agent, incorporating
perturbation analysis, unit testing, and model validation to ensure both
functionality and scientific auditability. We evaluate VDSAgents on nine
datasets with diverse characteristics, comparing it with state-of-the-art
end-to-end data science systems, such as AutoKaggle and DataInterpreter, using
DeepSeek-V3 and GPT-4o as backends. VDSAgents consistently outperforms the
results of AutoKaggle and DataInterpreter, which validates the feasibility of
embedding PCS principles into LLM-driven data science automation.

</details>


### [78] [Improving LLM Reasoning via Dependency-Aware Query Decomposition and Logic-Parallel Content Expansion](https://arxiv.org/abs/2510.24390)
*Xianjun Gao,Jianchun Liu,Hongli Xu,Liusheng Huang*

Main category: cs.AI

TL;DR: Orion是一个新的高效推理框架，它通过依赖感知查询分解和逻辑并行内容扩展来解决LLM在Web应用中的瓶颈问题。该框架将查询推理过程分为两个协同阶段：关键点生成和内容并行扩展。实验表明，与基准相比，Orion可以将标记生成速度提高4.33倍，将答案延迟降低3.42倍，推理质量提高18.75%。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在实时Web应用中的集成面临一个基本的Web基础设施挑战，即在满足高质量复杂推理需求的同时，还要适应交互式服务的严格低延迟和高吞吐量要求。现有的LLM推理方法在效率和质量上难以兼得，无法满足现代Web平台的需求。

Method: 提出了一种新的推理框架Orion，它包含依赖感知查询分解和逻辑并行内容扩展两个阶段，同时引入了一种流水线调度机制，利用两个阶段的互补计算特性来跨多个查询实现跨查询并行，从而显著提高推理性能。

Result: 实验结果显示，Orion在多样化的基准测试中表现优于基线，不仅将令牌生成速度提高了4.33倍，还将答案延迟降低了3.42倍，而且通过显式建模点间依赖性，推理质量提高了18.75%。

Conclusion: Orion有效地解决了LLM推理在Web应用中的瓶颈问题，它能够在保证推理质量的同时实现高效的推理性能，对于现代Web平台来说是一个重要的进展。

Abstract: The integration of Large Language Models (LLMs) into real-time Web
applications, such as AI-powered search and conversational agents, presents a
fundamental Web infrastructure challenge: reconciling the demand for
high-quality, complex reasoning with the stringent low-latency and
high-throughput requirements of interactive services. Current LLM reasoning,
hindered by computationally inefficient sequential generation and rigid
reasoning strategies, creates a critical bottleneck for the Web services.
Existing approaches typically optimize the LLM reasoning for either efficiency
or quality but struggle to achieve both, and thus fail to meet the dual
requirements of modern Web platforms. To overcome these limitations, we propose
Orion, a novel and efficient reasoning framework that enables dependency-aware
query decomposition and logic-parallel content expansion. Concretely, Orion
decomposes a single query reasoning process into two synergistic phases: (1)
\textit{key point generation}, which distills logically structured key points
through retrieval-augmented few-shot prompting, and (2) \textit{content
parallel expansion}, which concurrently elaborates on these points based on a
dependency graph to ensure logical consistency. Furthermore, Orion introduces a
pipeline scheduling mechanism that exploits the complementary computational
characteristics of the two phases (generation imposes pressure on GPU computing
and expansion stresses on GPU memory) across multiple queries, enabling
cross-query parallelism and dramatically improving reasoning performance (\ie,
efficiency and quality). Experiments on diverse benchmarks show that Orion not
only delivers up to 4.33x higher token generation speed and 3.42x lower answer
latency over the baselines but also improves reasoning quality by up to 18.75%
through explicitly modeling inter-point dependencies.

</details>


### [79] [Human-Level Reasoning: A Comparative Study of Large Language Models on Logical and Abstract Reasoning](https://arxiv.org/abs/2510.24435)
*Benjamin Grando Moreira*

Main category: cs.AI

TL;DR: 本文评估了多个大型语言模型的推理能力，并将其与人类表现进行比较，结果显示模型在演绎推理上存在不足


<details>
  <summary>Details</summary>
Motivation: 评估大型语言模型（LLMs）的推理能力对于推动人工智能的发展至关重要。这涉及到理解这些模型是否真正理解信息，进行推理并以逻辑和有效的方式得出结论

Method: 通过一组八个自定义设计的推理问题对几种LLM，包括GPT，Claude，DeepSeek，Gemini，Grok，Llama，Mistral，Perplexity和Sabi'a等的逻辑和抽象推理技能进行比较

Result: LLM的表现与人类在相同任务上的表现进行了基准测试，揭示了显著的差异，表明LLM在演绎推理方面存在差距

Conclusion: 研究表明，虽然LLM在多种任务上表现出色，但在推理能力，特别是在演绎推理方面，仍表现出对信息的理解和有效推理的局限性

Abstract: Evaluating reasoning ability in Large Language Models (LLMs) is important for
advancing artificial intelligence, as it transcends mere linguistic task
performance. It involves understanding whether these models truly understand
information, perform inferences, and are able to draw conclusions in a logical
and valid way. This study compare logical and abstract reasoning skills of
several LLMs - including GPT, Claude, DeepSeek, Gemini, Grok, Llama, Mistral,
Perplexity, and Sabi\'a - using a set of eight custom-designed reasoning
questions. The LLM results are benchmarked against human performance on the
same tasks, revealing significant differences and indicating areas where LLMs
struggle with deduction.

</details>


### [80] [Adaptive Surrogate Gradients for Sequential Reinforcement Learning in Spiking Neural Networks](https://arxiv.org/abs/2510.24461)
*Korneel Van den Berghe,Stein Stroobants,Vijay Janapa Reddi,G. C. H. E. de Croon*

Main category: cs.AI

TL;DR: 本文解决了SNN在强化学习应用中的两个关键挑战：优化效果不佳的问题和序列训练早期长度受限的问题。通过分析替代梯度斜率设置，发现较浅的斜率会在强化学习中显著提高训练和最终部署性能。提出了一种新的训练方法，利用特权策略引导学习过程，结合自适应斜率调度，在现实世界中的无人机位置控制任务中达到了400的平均回报，大幅优于前人技术。


<details>
  <summary>Details</summary>
Motivation: 本文的研究动机是解决SNN在执行复杂控制任务时面临的两个主要问题：非可微的特性要求使用替代梯度，但其优化性能尚未清晰；SNN的状态动态需要序列训练，但在强化学习中早期训练时其序列长度受限，这两点阻碍了SNN在实际应用中的性能表现和发展。

Method: 本文的方法包括：1.系统地分析了替代梯度斜率设置对梯度大小和与真实梯度的对齐度的影响。2. 提出了一种新的训练方式，该方式利用一个特权指导政策来启动学习过程，同时利用在线环境交互进行学习。3.采用了自适应斜率调度策略。这些方法被应用于解决无人机位置控制问题，取得了显著的性能提升。

Result: 实验结果显示，在RTS无人机位置控制任务中，本文的方法达到了400的平均回报，这个结果大幅超越了先前的方法，包括行为克隆和TD3BC，这些方法在这种条件下最多只能达到-200的回报。

Conclusion: 本文工作深化了对SNN中替代梯度学习的理论理解，并展示了在实际机器人系统中用于神经形态控制器的实用训练方法。新技术不仅在理论上推进了SNN研究的进步，也在实际应用中证明了其优越的性能。

Abstract: Neuromorphic computing systems are set to revolutionize energy-constrained
robotics by achieving orders-of-magnitude efficiency gains, while enabling
native temporal processing. Spiking Neural Networks (SNNs) represent a
promising algorithmic approach for these systems, yet their application to
complex control tasks faces two critical challenges: (1) the non-differentiable
nature of spiking neurons necessitates surrogate gradients with unclear
optimization properties, and (2) the stateful dynamics of SNNs require training
on sequences, which in reinforcement learning (RL) is hindered by limited
sequence lengths during early training, preventing the network from bridging
its warm-up period.
  We address these challenges by systematically analyzing surrogate gradient
slope settings, showing that shallower slopes increase gradient magnitude in
deeper layers but reduce alignment with true gradients. In supervised learning,
we find no clear preference for fixed or scheduled slopes. The effect is much
more pronounced in RL settings, where shallower slopes or scheduled slopes lead
to a 2.1x improvement in both training and final deployed performance. Next, we
propose a novel training approach that leverages a privileged guiding policy to
bootstrap the learning process, while still exploiting online environment
interactions with the spiking policy. Combining our method with an adaptive
slope schedule for a real-world drone position control task, we achieve an
average return of 400 points, substantially outperforming prior techniques,
including Behavioral Cloning and TD3BC, which achieve at most --200 points
under the same conditions. This work advances both the theoretical
understanding of surrogate gradient learning in SNNs and practical training
methodologies for neuromorphic controllers demonstrated in real-world robotic
systems.

</details>


### [81] [From Cross-Task Examples to In-Task Prompts: A Graph-Based Pseudo-Labeling Framework for In-context Learning](https://arxiv.org/abs/2510.24528)
*Zihan Chen,Song Wang,Xingbo Fu,Chengshuai Shi,Zhenyu Lei,Cong Shen,Jundong Li*

Main category: cs.AI

TL;DR: 提出了一种降低成本的两级管道，用于减少大型语言模型（LLM）的数据标注依赖性。该方法首先利用跨任务示例标注少量目标任务实例，然后引入图标签传播方法，实现无需额外LLM查询的数据标注。实验表明，此方法在降低标注成本的同时，保持了良好的性能。


<details>
  <summary>Details</summary>
Motivation: 收集高质量的新任务或挑战性任务示例既昂贵又耗时，因此提出了减少LLM依赖性的高效数据标注方法，以降低成本。

Method: 本方法采用了两个阶段：第一阶段利用现有跨任务示例标注少量目标任务实例，第二阶段采用图标签传播方法，将标注信息散布给剩余的目标示例，无需额外LLM查询。

Result: 实验结果显示，该方法能够降低数据标注成本，并保持优良的性能。在五个任务上的实验表明，该方法能够在降低标注成本的同时保持很好的表现。

Conclusion: 该管道结合了跨任务监督的灵活性和图标签传播的大规模可扩展性，提出了一种降低成本的标注策略。

Abstract: The capability of in-context learning (ICL) enables large language models
(LLMs) to perform novel tasks without parameter updates by conditioning on a
few input-output examples. However, collecting high-quality examples for new or
challenging tasks can be costly and labor-intensive. In this work, we propose a
cost-efficient two-stage pipeline that reduces reliance on LLMs for data
labeling. Our approach first leverages readily available cross-task examples to
prompt an LLM and pseudo-label a small set of target task instances. We then
introduce a graph-based label propagation method that spreads label information
to the remaining target examples without additional LLM queries. The resulting
fully pseudo-labeled dataset is used to construct in-task demonstrations for
ICL. This pipeline combines the flexibility of cross-task supervision with the
scalability of LLM-free propagation. Experiments across five tasks demonstrate
that our method achieves strong performance while lowering labeling costs.

</details>


### [82] [Generative AI for Healthcare: Fundamentals, Challenges, and Perspectives](https://arxiv.org/abs/2510.24551)
*Gang Chen,Changshuo Liu,Gene Anne Ooi,Marcus Tan,Zhongle Xie,Jianwei Yin,James Wei Luen Yip,Wenqiao Zhang,Jiaqi Zhu,Beng Chin Ooi*

Main category: cs.AI

TL;DR: 提出了一个数据为中心的范式，用于设计和部署面向医疗保健的生成式人工智能（GenAI）系统。通过重新定位医疗数据生命周期，建立一个生态系统以支持各种医疗数据和知识的集成、表示和检索，从而优化上游模型组件和下游临床应用。


<details>
  <summary>Details</summary>
Motivation: 当前生成式人工智能正在革新医疗实践和医疗交付，特别是在临床注释、对话支持以及决策支持等领域。然而，要有效部署GenAI，我们需要深刻理解医疗任务，并明确其应用场景和限制。因此，本文希望通过数据为中心的范式，推动GenAI更加有效地支持医疗保健。

Method: 通过重新定位数据生命周期，使医疗数据生态系统成为生成医疗系统的根基。生态系统能够可持续地支持各式医疗数据和知识的集成、表示和检索，并拥有高效的语义向量搜索和上下文查询等数据处理流程，以支持基于数据的生成式操作。这种生态系统不仅为大规模预训练和特定领域的微调提供高质量的多模态数据，还作为知识检索后端，支持任务特定的推理过程。

Result: 文章提出了一个以数据为中心的架构，旨在改进GenAI在医疗领域中的应用，通过一个强健的数据生态系统来优化GenAI的部署和利用。这种架构提升了医疗交付的高质量和有效性。

Conclusion: 通过倡导数据为中心的创新方法，可以更加有效地利用生成式人工智能来优化医疗保健的交付。未来的研究将更进一步探索其实际应用和改进。

Abstract: Generative Artificial Intelligence (GenAI) is taking the world by storm. It
promises transformative opportunities for advancing and disrupting existing
practices, including healthcare. From large language models (LLMs) for clinical
note synthesis and conversational assistance to multimodal systems that
integrate medical imaging, electronic health records, and genomic data for
decision support, GenAI is transforming the practice of medicine and the
delivery of healthcare, such as diagnosis and personalized treatments, with
great potential in reducing the cognitive burden on clinicians, thereby
improving overall healthcare delivery. However, GenAI deployment in healthcare
requires an in-depth understanding of healthcare tasks and what can and cannot
be achieved. In this paper, we propose a data-centric paradigm in the design
and deployment of GenAI systems for healthcare. Specifically, we reposition the
data life cycle by making the medical data ecosystem as the foundational
substrate for generative healthcare systems. This ecosystem is designed to
sustainably support the integration, representation, and retrieval of diverse
medical data and knowledge. With effective and efficient data processing
pipelines, such as semantic vector search and contextual querying, it enables
GenAI-powered operations for upstream model components and downstream clinical
applications. Ultimately, it not only supplies foundation models with
high-quality, multimodal data for large-scale pretraining and domain-specific
fine-tuning, but also serves as a knowledge retrieval backend to support
task-specific inference via the agentic layer. The ecosystem enables the
deployment of GenAI for high-quality and effective healthcare delivery.

</details>


### [83] [FunReason-MT Technical Report: Overcoming the Complexity Barrier in Multi-Turn Function Calling](https://arxiv.org/abs/2510.24645)
*Zengzhuang Xu,Bingguang Hao,Zechuan Wang,Yuntao Wen,Maolin Wang,Yang Liu,Long Chen,Dong Wang,Yicheng Chen,Cunyin Peng,Chenyi Zhuang,Jinjie Gu,Leilei Gan,Xiangyu Zhao,Shi Gu*

Main category: cs.AI

TL;DR: 本文提出了FunReason-MT框架，用于生成用于现实世界中多轮次工具使用的高质量训练数据，该框架在Berkeley Function-Calling Leaderboard上的评估中表现优异，超过了大多数封闭源模型的性能


<details>
  <summary>Details</summary>
Motivation: 现有的数据合成方法在生成真实世界的高质量训练数据方面存在不足，因此本文提出了FunReason-MT框架，以改进这一问题

Method: 提出了环境API图交互、高级工具查询合成和引导式迭代链这三个组件来生成高质量的多轮次工具使用数据

Result: 基于FunReason-MT生成的数据训练的4B模型在Berkeley Function-Calling Leaderboard上的评估中，相较于同规模模型和其他封闭源模型显示出优越的性能

Conclusion: FunReason-MT框架提供了高质量的数据合成能力，对解决现实世界中的复杂问题具有重要意义

Abstract: Function calling (FC) empowers large language models (LLMs) and autonomous
agents to interface with external tools, a critical capability for solving
complex, real-world problems. As this ability becomes increasingly central to
advanced AI systems, the need for high-quality, multi-turn training data to
develop and refine it cannot be overstated. Existing data synthesis methods,
such as random environment sampling or multi-agent role-playing, are not
powerful enough to generate high-quality data in real-world environments.
Practical challenges come in three folds: targeted model training, isolation of
tool architecture, and multi-turn logical dependency. To address these
structural deficiencies, we present FunReason-MT, a novel data synthesis
framework for real-world multi-turn tool use. FunReason-MT resolves the
complexity barrier in multi-turn FC data by employing 1) Environment-API Graph
Interactions to gather varied high-quality trajectories, 2) Advanced Tool-Query
Synthesis to simplify hard query construction, and 3) Guided Iterative Chain
for sophisticated CoT generation. Evaluations on Berkeley Function-Calling
Leaderboard (BFCLv3) demonstrate the power of our framework: a 4B model built
upon FunReason-MT generated data achieves state-of-the-art performance among
comparable-sized models, outperforming most close-source models. Further
performance improvements on BFCLv4 confirm that FunReason-MT provides a
reliable and robust source for agentic learning.

</details>


### [84] [Advancing site-specific disease and pest management in precision agriculture: From reasoning-driven foundation models to adaptive, feedback-based learning](https://arxiv.org/abs/2510.24650)
*Nitin Rai,Daeun,Choi,Nathan S. Boyd,Arnold W. Schumann*

Main category: cs.AI

TL;DR: 该研究综述了约40篇关于基础模型（FMs）在作物特定疾病管理（SSDM）中的应用文章，重点讨论了大规模语言模型（LLMs）和视觉-语言模型（VLMs）在自适应学习（AL）、强化学习（RL）和数字孪生框架中对精准喷洒的支持，并探讨了机器人在SSDM中的角色以及人机协作的挑战和未来方向。核心发现包括基础模型的文献呈上升趋势、视觉语言模型的增长速度高于大规模语言模型等。


<details>
  <summary>Details</summary>
Motivation: 研究的主要动机是探讨基础模型（FMs）在作物特定疾病管理中的应用和发展，尤其是在自适应学习、强化学习和数字孪生框架中的作用，提高精准喷洒的效率和准确性。

Method: 综述文章筛选了40篇关于基础模型在作物疾病管理中应用的文章，集中在大规模语言模型和视觉语言模型上，并讨论了它们在自适应学习、强化学习和数字孪生框架中的应用。

Result: 发现基础模型的应用文献在2023-2024年呈现上升趋势，视觉语言模型的增长速度快于大规模语言模型。自适应学习和强化学习在智能喷洒上仍处于初级阶段，而数字孪生结合强化学习可以虚拟模拟精准喷洒。解决仿真到现实的转换差距是实际应用中的关键问题。人机协作尤其是在人工在环路方法中仍然具有挑战性，机器人检测早期症状而人类验证不确定情况。多模态基础模型结合实时反馈将驱动下一代特定疾病管理的发展。

Conclusion: 综述强调了基础模型在作物特定疾病管理中的重要应用和未来发展方向，指出多模态基础模型和实时反馈的结合将推动下一代SSDM技术的发展，以实现更高效、更准确的作物疾病管理。

Abstract: Site-specific disease management (SSDM) in crops has advanced rapidly through
machine and deep learning (ML and DL) for real-time computer vision. Research
evolved from handcrafted feature extraction to large-scale automated feature
learning. With foundation models (FMs), crop disease datasets are now processed
in fundamentally new ways. Unlike traditional neural networks, FMs integrate
visual and textual data, interpret symptoms in text, reason about
symptom-management relationships, and support interactive QA for growers and
educators. Adaptive and imitation learning in robotics further enables
field-based disease management. This review screened approx. 40 articles on FM
applications for SSDM, focusing on large-language models (LLMs) and
vision-language models (VLMs), and discussing their role in adaptive learning
(AL), reinforcement learning (RL), and digital twin frameworks for targeted
spraying. Key findings: (a) FMs are gaining traction with surging literature in
2023-24; (b) VLMs outpace LLMs, with a 5-10x increase in publications; (c) RL
and AL are still nascent for smart spraying; (d) digital twins with RL can
simulate targeted spraying virtually; (e) addressing the sim-to-real gap is
critical for real-world deployment; (f) human-robot collaboration remains
limited, especially in human-in-the-loop approaches where robots detect early
symptoms and humans validate uncertain cases; (g) multi-modal FMs with
real-time feedback will drive next-gen SSDM. For updates, resources, and
contributions, visit, https://github.com/nitin-dominic/AgriPathogenDatabase, to
submit papers, code, or datasets.

</details>


### [85] [OrchDAG: Complex Tool Orchestration in Multi-Turn Interactions with Plan DAGs](https://arxiv.org/abs/2510.24663)
*Yifu Lu,Shengjie Liu,Li Dong*

Main category: cs.AI

TL;DR: 介绍了OrchDAG，一种用于生成合成数据的数据生成管线，该管线将工具执行建模为有向无环图（DAG），具有可控的复杂性。使用此数据集，对模型性能进行了基准测试，并提出了基于图的奖励以增强RLVR训练。实验表明，该数据集提供了具有挑战性的基准，并且所提出的奖励在与GRPO样式的算法结合使用时是有效的，强调了在多轮工具使用中利用拓扑结构和数据复杂性的重要性。


<details>
  <summary>Details</summary>
Motivation: 许多现有的工作忽略了多轮工具交互的复杂性，本文旨在通过引入OrchDAG来解决这一问题。OrchDAG生成的合成数据可以模拟具有可控复杂性的工具执行，可以作为研究工具交互复杂性的基准。同时，论文还旨在通过引入基于图的奖励来提高RLVR训练的效果。

Method: 设计了一个合成数据生成管线OrchDAG，将工具执行建模为DAG，并通过实验对该数据集进行了基准测试。引入了一种基于图的奖励，用以增强RLVR训练，并结合GRPO风格的算法进行了实验。

Result: 实验结果展示了该数据集提供了多轮工具使用中的复杂挑战，而提出的基于图的奖励在与GRPO风格算法结合使用时能够显著提升模型性能。

Conclusion: 本研究强调了在多轮工具使用中利用数据生成方法和基于图的奖励机制的重要性，为未来研究提供了一个有效的数据生成和评估工具交互复杂性的框架。

Abstract: Agentic tool use has gained traction with the rise of agentic tool calling,
yet most existing work overlooks the complexity of multi-turn tool
interactions. We introduce OrchDAG, a synthetic data generation pipeline that
models tool execution as directed acyclic graphs (DAGs) with controllable
complexity. Using this dataset, we benchmark model performance and propose a
graph-based reward to enhance RLVR training. Experiments show that the dataset
presents a challenging but solvable benchmark, and the proposed reward is
effective when combined with GRPO-style algorithms, highlighting the importance
of leveraging topological structure and data complexity in multi-turn tool use.

</details>


### [86] [Bridging Tool Dependencies and Domain Knowledge: A Graph-Based Framework for In-Context Planning](https://arxiv.org/abs/2510.24690)
*Shengjie Liu,Li Dong,Zhenyu Zhang*

Main category: cs.AI

TL;DR: 提出了一种框架，通过挖掘和利用工具及文档之间的依赖关系，来增强示例生成。通过DeepResearch启发的分析方法构建工具知识图谱，并结合内部文档知识图谱，采用深稀疏集成策略生成范例计划。实验表明该框架能够有效建模工具交互并提升计划生成质量。


<details>
  <summary>Details</summary>
Motivation: 为了更好地理解和利用工具之间的依赖关系，以增强范例生成的质量和效率。提高工具和文档之间的联系在生成计划中的作用。

Method: 从工具描述、参数和输出负载构建工具知识图谱；从内部文档和操作手册构建知识图谱，并将其与工具图谱融合；采用深稀疏集成策略生成范例计划。

Result: 实验结果表明该框架能有效建模工具交互并提升计划生成质量，特别强调了将工具图谱与领域知识图谱连接的重要性。

Conclusion: 该研究通过框架的有效理论和实验结果验证了将工具图谱与领域知识图谱相结合的合理性，未来的工作可以在此基础上进一步优化和扩展。

Abstract: We present a framework for uncovering and exploiting dependencies among tools
and documents to enhance exemplar artifact generation. Our method begins by
constructing a tool knowledge graph from tool schemas,including descriptions,
arguments, and output payloads, using a DeepResearch-inspired analysis. In
parallel, we derive a complementary knowledge graph from internal documents and
SOPs, which is then fused with the tool graph. To generate exemplar plans, we
adopt a deep-sparse integration strategy that aligns structural tool
dependencies with procedural knowledge. Experiments demonstrate that this
unified framework effectively models tool interactions and improves plan
generation, underscoring the benefits of linking tool graphs with domain
knowledge graphs for tool-augmented reasoning and planning.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [87] [High-Quality and Large-Scale Image Downscaling for Modern Display Devices](https://arxiv.org/abs/2510.24334)
*Suvrojit Mitra,G B Kevin Arjun,Sanjay Ghosh*

Main category: eess.IV

TL;DR: 本文提出了一种新的图像缩小技术，该技术通过共现学习来维持结构和感知信息以减少分辨率。实验结果表明，该方法在视觉质量和性能度量方面优于现代技术。这种方法能够在大规模图像增大后再缩小的过程中保持纹理和边缘清晰，避免传统缩小技术中的锯齿和模糊现象。该技术在多个数据集上得到验证，获得了较高的PSNR和PIQE评分。


<details>
  <summary>Details</summary>
Motivation: 缩小图像时通常会失去结构和视觉真实性，特别是对于大规模图像。这项技术旨在解决在缩小图像时保持图像结构和感知信息的问题，特别是在保留边缘和纹理等方面。本文提出了一种新的图像缩小技术旨在解决传统缩小技术中的问题，同时避免了锯齿和模糊的产生。

Method: 使用输入图像创建一个数据驱动的共现配置文件，该配置文件捕捉了邻域内强度相关性的频率。然后通过一种由该配置文件引导的细化过滤过程，该过程用作内容自适应范围核。输入像素的贡献基于其与邻近像素在强度值上的相似程度。基于此方法，在四个数据集（DIV2K，BSD100，Urban100和RealSR）上进行了验证，并取得了优秀的缩小效果。这种方法称为LSID（大规模图像缩小）。

Result: 方法在Downscaling Capacity of Large-Scale Image方面优于其他现代方法，无论是在视觉质量还是性能度量上。在缩小8x 和16x的情况下，在DIV2K数据集上分别获得了高达39.22 dB的PSNR和高达26.35的PIQE评分。实验结果证明了该方法在视觉质量和性能度量中的卓越表现，特别在保持图像详细结构方面有很大优势。

Conclusion: LSID（大规模图像缩小）方法能够保持大规模图像在缩小后保留高频率结构如边缘、纹理和重复模式，避免了传统缩小技术中的模糊和失真现象。

Abstract: In modern display technology and visualization tools, downscaling images is
one of the most important activities. This procedure aims to maintain both
visual authenticity and structural integrity while reducing the dimensions of
an image at a large scale to fit the dimension of the display devices. In this
study, we proposed a new technique for downscaling images that uses
co-occurrence learning to maintain structural and perceptual information while
reducing resolution. The technique uses the input image to create a data-driven
co-occurrence profile that captures the frequency of intensity correlations in
nearby neighborhoods. A refined filtering process is guided by this profile,
which acts as a content-adaptive range kernel. The contribution of each input
pixel is based on how closely it resembles pair-wise intensity values with it's
neighbors. We validate our proposed technique on four datasets: DIV2K, BSD100,
Urban100, and RealSR to show its effective downscaling capacity. Our technique
could obtain up to 39.22 dB PSNR on the DIV2K dataset and PIQE up to 26.35 on
the same dataset when downscaling by 8x and 16x, respectively. Numerous
experimental findings attest to the ability of the suggested picture
downscaling method to outperform more contemporary approaches in terms of both
visual quality and performance measures. Unlike most existing methods, which
did not focus on the large-scale image resizing scenario, we achieve
high-quality downscaled images without texture loss or edge blurring. Our
method, LSID (large scale image downscaling), successfully preserves
high-frequency structures like edges, textures, and repeating patterns by
focusing on statistically consistent pixels while reducing aliasing and
blurring artifacts that are typical of traditional downscaling techniques.

</details>


### [88] [Fast algorithms enabling optimization and deep learning for photoacoustic tomography in a circular detection geometry](https://arxiv.org/abs/2510.24687)
*Andreas Hauptmann,Leonid Kunyansky,Jenni Poimala*

Main category: eess.IV

TL;DR: 本文开发了应用于圆环采集几何的新算法，能快速计算正向和逆向操作符，复杂度为\(\mathcal{O}(n^2 \log n)\)。这些算法用于迭代图像重建方法，包括经典的变分方法和深度学习方法，并且开源。


<details>
  <summary>Details</summary>
Motivation: 光声层析成像及其他耦合机制模式中的反问题经常通过迭代算法来解决。这些算法基于成本函数最小化。为了进一步改进此类优化方法，正在探索新的深度学习技术。然而，所有这些方法都需要进行多次正向问题操作符及其伴随操作符的计算。因此，论文意图提出快速算法来实现这一操作符的数值评估，提高效率，减少计算量。

Method: 开发两种新的渐进式快速算法，用于数值评估正向和伴随操作符，在圆环几何模式下使用。这些算法可以在\((n \times n)\)图像中计算这些操作符，浮点运算次数为\(\mathcal{O}(n^2 \log n)\)。并使用多种迭代图像重建技术，进行实际演示。包括经典变分方法，如非负最小二乘和全变分正则化最小二乘，以及基于深度学习的方法，如学习的原始双重方法。并将这些算法及其计算示例以Python实现并公开发布。

Result: 在数值仿真实验中，我们的算法以实现几类迭代图像重建方法，表现出了出色的性能。这些算法已实现于Python代码，并提供了给公众使用。

Conclusion: 提出了新算法，提供了快速计算正向和伴随操作符的有效方法，适用于多种图像重建技术，并开源共享。

Abstract: The inverse source problem arising in photoacoustic tomography and in several
other coupled-physics modalities is frequently solved by iterative algorithms.
Such algorithms are based on the minimization of a certain cost functional. In
addition, novel deep learning techniques are currently being investigated to
further improve such optimization approaches. All such methods require multiple
applications of the operator defining the forward problem, and of its adjoint.
In this paper, we present new asymptotically fast algorithms for numerical
evaluation of the forward and adjoint operators, applicable in the circular
acquisition geometry. For an $(n \times n)$ image, our algorithms compute these
operators in $\mathcal{O}(n^2 \log n)$ floating point operations. We
demonstrate the performance of our algorithms in numerical simulations, where
they are used as an integral part of several iterative image reconstruction
techniques: classic variational methods, such as non-negative least squares and
total variation regularized least squares, as well as deep learning methods,
such as learned primal dual. A Python implementation of our algorithms and
computational examples is available to the general public.

</details>


### [89] [Dipole-lets: a new multiscale decomposition for MR phase and quantitative susceptibility mapping](https://arxiv.org/abs/2510.24705)
*Ignacio Contreras-Zúñiga,Mathias Lambert,Benjamín Palacios,Cristian Tejos,Carlos Milovic*

Main category: eess.IV

TL;DR: 该论文提出了一种名为Dipole-lets的多尺度变换方法，用于识别和抑制定量磁敏感成像中的条纹伪影。Dipole-lets通过提取与磁偶极子内核零值双圆锥表面有关的特征尺寸和方向，解决了由于极端噪声和非偶极相位贡献导致的伪影问题。该方法通过Tikhonov和无穷范数实现优化功能正则化。


<details>
  <summary>Details</summary>
Motivation: 定量磁敏感制图中的一个主要挑战是识别和抑制条纹伪影，这是由测量中的极端噪声和非偶极相位贡献造成的。这些问题导致了由偶极子核函数放大后产生的伪影。为了应对这一挑战，该论文引入了Dipole-lets方法。

Method: 通过引入Dipole-lets多尺度变换对测量场数据中的偶极不符合项进行最优分解。Dipole-lets从相位数据中提取特征，这些特征具有不同的尺度和方向，相对于偶极核的双圆锥面。具体地，该方法通过Tikhonov和无穷范数实现优化功能正则化。

Result: 实验表明，Dipole-lets可以从相位数据中通过伪影定位提取非偶极项。使用Dipole-lets作为优化功能正则化，成功地减少了条纹伪影。

Conclusion: 该论文提出了一种有效的方法来解决定量磁敏感制图中的条纹伪影问题。通过使用Dipole-lets，研究者可以更准确地识别和减少由极端噪声和非偶极相位贡献引起的伪影。

Abstract: Identifying and suppressing streaking artifacts is one of the most
challenging problems in quantitative susceptibility mapping. The measured phase
from tissue magnetization is assumed to be the convolution by the magnetic
dipole kernel; direct inversion or standard regularization methods tend to
create streaking artifacts in the estimated susceptibility. This is caused by
extreme noise and by the presence of non-dipolar phase contributions, which are
amplified by the dipole kernel following the streaking pattern. In this work,
we introduce a multiscale transform, called Dipole-lets, as an optimal
decomposition method for identifying dipole incompatibilities in measured field
data by extracting features of different characteristic size and orientation
with respect to the dipole kernel's zero-valued double-cone surface (the magic
cone). We provide experiments that showcase that non-dipolar content can be
extracted by Dipole-lets from phase data through artifact localization. We also
present implementations of Dipole-lets as a optimization functional
regularizator, through simple Tikhonov and infinity norm.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [90] [Enabling Near-realtime Remote Sensing via Satellite-Ground Collaboration of Large Vision-Language Models](https://arxiv.org/abs/2510.24242)
*Zihan Li,Jiahao Yang,Yuxin Zhang,Zhe Chen,Yue Gao*

Main category: cs.NI

TL;DR: Grace 是一个卫星-地面协作系统，用于在遥感任务中进行实时 LVLM 推理。它通过部署紧凑的 LVLM 到卫星上，并在地面站上保留更大的模型，从而保证整体性能。Grace 通过卫星-地面检索增强生成和任务调度算法实现近实时的 LVLM 推理，比最先进的方法减少76-95%的延迟，而不会牺牲推理精度。


<details>
  <summary>Details</summary>
Motivation: 大型视觉-语言模型在低地球轨道卫星的遥感任务中显示出巨大潜力，但受到卫星计算资源有限和卫星与地面间通信短暂的限制，这些模型在实际部署中的应用还处于初步探索阶段。为解决这一问题，提出 Grace 系统。

Method: Grace 系统包含两个主要阶段：卫星-地面检索增强生成（RAG）和任务调度算法。检索增强生成阶段中，使用定制的自适应更新算法将地面站上的知识库传输到卫星上。任务调度算法通过一个基于置信度的测试算法决定任务是在卫星上处理还是下传到地面站处理。

Result: 基于实际卫星轨道数据的实验表明，Grace 系统相较于最先进的方法平均延迟降低了 76-95%，且不会影响推理准确性。

Conclusion: Grace 系统在保持较高推理精度的情况下，能够大幅度降低遥感任务中的延迟，展示了卫星-地面协作系统在近实时大型视觉-语言模型推理上的巨大潜力。

Abstract: Large vision-language models (LVLMs) have recently demonstrated great
potential in remote sensing (RS) tasks (e.g., disaster monitoring) conducted by
low Earth orbit (LEO) satellites. However, their deployment in real-world LEO
satellite systems remains largely unexplored, hindered by limited onboard
computing resources and brief satellite-ground contacts. We propose Grace, a
satellite-ground collaborative system designed for near-realtime LVLM inference
in RS tasks. Accordingly, we deploy compact LVLM on satellites for realtime
inference, but larger ones on ground stations (GSs) to guarantee end-to-end
performance. Grace is comprised of two main phases that are asynchronous
satellite-GS Retrieval-Augmented Generation (RAG), and a task dispatch
algorithm. Firstly, we still the knowledge archive of GS RAG to satellite
archive with tailored adaptive update algorithm during limited satellite-ground
data exchange period. Secondly, propose a confidence-based test algorithm that
either processes the task onboard the satellite or offloads it to the GS.
Extensive experiments based on real-world satellite orbital data show that
Grace reduces the average latency by 76-95% compared to state-of-the-art
methods, without compromising inference accuracy.

</details>


### [91] [A New Hybrid Precoding Approach for Multi-user Massive MIMO over Fading Channels](https://arxiv.org/abs/2510.24595)
*Azadeh Pourkabirian,Kai Li,Photios A. Stavrou,Wei Ni*

Main category: cs.NI

TL;DR: 本文提出了一种结合数字和模拟预编码的新型杂合预编码方法，用于优化多用户大规模多输入多输出系统中的数据传输，实现了最大和率并抑制副瓣干扰。通过将角度和相位视为相关的双变量高斯分布建模，本文首次定义联合角度和相位熵来度量无线信道中的角度和相位变化的不确定性，仿真结果验证了这一方法的准确性和有效性，相较于其他最先进的方法，和率提高了18.31%，鲁棒性提高了11.47%。 


<details>
  <summary>Details</summary>
Motivation: 为了最大化多用户大规模多输入多输出系统的潜力，并提高数据传输的效率，我们提出了一种新方法来优化信号的方向，最终提高传输性能。通过改进预编码方式，能够更好地管理信号干扰，同时保证信号质量。同时，定义一个新的参数来度量无线信道中的不确定性，对于系统动态适应信道变化至关重要。 

Method: 本文结合了数字和模拟预编码技术，通过一种新的方法来优化信号的方向，即最大化和率并减少副瓣干扰。角度和相位的变化是相关联的，我们利用这一特点，在模型中考虑了它们的联系，并首次使用联合角度和相位熵来量化无线信道的不确定性。 

Result: 实验结果表明，对比现有方法，我们提出的方法可以提高18.31%的和率，并且鲁棒性也提高了11.47%，这一结果证明了我们提出的方法的优势及其准确性。 

Conclusion: 新方法通过结合不同类型的预编码技术，不仅提高了系统的性能，还增强了系统的鲁棒性。此外，通过对角度和相位变化的建模和量化，可以更好地理解和适应复杂的无线环境。 

Abstract: Hybrid precoding is an indispensable technique to harness the full potential
of a multi-user massive multiple-input, multiple-output (MU-MMIMO) system. In
this paper, we propose a new hybrid precoding approach that combines digital
and analog precoding to optimize data transmission over multiple antennas. This
approach steers signals in specific directions, leading to maximizing sum-rate
and suppressing side-lobe interference. When dealing with complex signals,
changes in phase are naturally associated with changes in angle, and these
variations are inherently correlated. The correlation between the angle and
phase is essential for accurately determining the channel characteristics. An
important aspect of this approach is that we model the angle and phase as
correlated variables following a bivariate Gaussian distribution, and for the
first time, we define a joint angle and phase entropy to measure the
uncertainty of angle and phase variations in wireless channels. This entropy is
crucial to adapt the proposed precoding method with variations. Simulation
result validate the accuracy of our analytical findings, demonstrating 18.31%
increase in sum-rate and an 11.47% improvement in robustness compared to other
state-of-the-art methods.

</details>


### [92] [Strategic Task Offloading for Delay-Sensitive IoT Applications: A Game-Theory-Based Demand-Supply Mechanism with Participation Incentives](https://arxiv.org/abs/2510.24611)
*Azadeh Pourkabirian,Amir Masoud Rahmani,Kai Li,Wei Ni*

Main category: cs.NI

TL;DR: 本文提出了一种基于经济供需模型的任务卸载方法，用于延迟敏感的物联网应用。这种方法使用VCG拍卖设计了一个博弈论框架，以实现市场平衡并最大化社会福利。实验表明，该方法提高了社会福利、保证了真相性并提供了延迟保障。


<details>
  <summary>Details</summary>
Motivation: 许多延迟敏感的应用程序在运行时遇到了挑战，因为物联网设备的处理资源有限，并且需要实时响应。任务卸载作为一种解决方案，通过将计算密集型任务从物联网设备转移到资源丰富的边缘服务器来减少延迟，确保延迟和性能保证。

Method: 本文建立了一个类似于经济供需模型的任务卸载方法，设计了一个基于VCG拍卖的博弈论框架，鼓励物联网用户和边缘服务器共享资源，从而实现市场平衡和最大化社会福利。

Result: 试验表明，所提出的方法极大提高了社会福利，确保了交易的真相性，并为延迟敏感的物联网应用程序提供了保证的延迟。

Conclusion: 本文提出了一种新型的任务卸载方法，通过基于VCG拍卖的博弈论框架和激励机制，以实现最大的社会福利，保证了市场平衡，为延迟敏感的应用程序提供了延迟保障。

Abstract: Delay-sensitive Internet of Things (IoT) applications have drawn significant
attention. Running many of these applications on IoT devices is challenging due
to the limited processing resources of these devices and the need for real-time
responses. Task offloading can minimize latency by transferring computationally
intensive tasks from IoT devices to resource-rich edge servers, ensuring delay
and performance guarantees. In this paper, we develop a task-offloading
approach for delay-sensitive IoT applications in edge computing environments.
Unlike existing schemes, we model the task offloading problem as an economic
demand and supply model to achieve market balance. The proposed model avoids
under- and over-supply, ensuring the computational resources at edge servers
(supply) are allocated in a manner that best meets the processing and
computational needs of user devices (demand). Given the multi-agent nature of
task offloading involving users and service providers with different
preferences and objectives, we design a game-theoretic framework using a
Vickrey-Clarke-Groves (VCG) auction. This framework analyzes agent interactions
and decision-making processes. Additionally, we develop an incentive mechanism
to encourage both parties to participate in the auction. The mechanism
maximizes user task offloading to edge servers and motivates edge servers to
share their computational resources, achieving profitability for both IoT users
and edge servers. Simulations demonstrate our method maximizes social welfare,
ensures truthfulness, maintains market balance, and provides latency guarantees
for delay-sensitive IoT applications.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [93] [An Enhanced Dual Transformer Contrastive Network for Multimodal Sentiment Analysis](https://arxiv.org/abs/2510.23617)
*Phuong Q. Dao,Mark Roantree,Vuong M. Ngo*

Main category: cs.LG

TL;DR: 本文提出了一种新的模型BERT-ViT-EF，结合BERT和ViT通过早期融合策略共同处理文本和图像，并进一步提出了DTCN，该模型通过对比学习增强跨模态融合效果。在两个公共基准测试中表现优于现有方法，展示了早期融合和深度上下文建模的优势。


<details>
  <summary>Details</summary>
Motivation: 传统的一模态分析方法无法精确捕捉人类情绪，文章提出结合文本和视觉输入的方法来提高情感分析的准确性及有效性。

Method: 提出了BERT-ViT-EF模型，结合BERT和ViT，以及DTCN，通过早期融合和对比学习来加强模态交互效果。

Result: 在MVSA-Single和TumEmo数据集上，DTCN取得了最佳的准确率和F1分值，显示了本文工作的有效性。

Conclusion: 研究证明早期融合和深层次上下文建模对于改善Transformer的多模态情感分析效果具有显著的优势。

Abstract: Multimodal Sentiment Analysis (MSA) seeks to understand human emotions by
jointly analyzing data from multiple modalities typically text and images
offering a richer and more accurate interpretation than unimodal approaches. In
this paper, we first propose BERT-ViT-EF, a novel model that combines powerful
Transformer-based encoders BERT for textual input and ViT for visual input
through an early fusion strategy. This approach facilitates deeper cross-modal
interactions and more effective joint representation learning. To further
enhance the model's capability, we propose an extension called the Dual
Transformer Contrastive Network (DTCN), which builds upon BERT-ViT-EF. DTCN
incorporates an additional Transformer encoder layer after BERT to refine
textual context (before fusion) and employs contrastive learning to align text
and image representations, fostering robust multimodal feature learning.
Empirical results on two widely used MSA benchmarks MVSA-Single and TumEmo
demonstrate the effectiveness of our approach. DTCN achieves best accuracy
(78.4%) and F1-score (78.3%) on TumEmo, and delivers competitive performance on
MVSA-Single, with 76.6% accuracy and 75.9% F1-score. These improvements
highlight the benefits of early fusion and deeper contextual modeling in
Transformer-based multimodal sentiment analysis.

</details>


### [94] [Speeding Up MACE: Low-Precision Tricks for Equivarient Force Fields](https://arxiv.org/abs/2510.23621)
*Alexandre Benoit*

Main category: cs.LG

TL;DR: 本文通过分析MACE模型的计算瓶颈，研究了低精度执行策略在不损害物理准确性的前提下，如何使MACE模型更加经济和快速。研究发现，使用cuEquivariance后端和混合精度推理可以显著加速最先进的力场，对下游的MD影响几乎可以忽略不计。推荐策略是在默认情况下使用cuEquivariance和FP32，并为线性层启用BF16/FP16（保持FP32累加），以达到最大吞吐量，而训练仍保持在FP32精度下进行。在Ampere/Hopper GPU和从内核级FP16/BF16路径和流水线融合中，有望获得进一步的性能提升。


<details>
  <summary>Details</summary>
Motivation: 这篇论文的主要动机是通过寻找和解决计算瓶颈，并评估低精度执行策略，使得SO(3)-等变模型（如MACE）可以在不损害物理准确性的前提下，以更低的成本和更高的速度运行。

Method: 本文采用了性能分析方法，针对MACE模型进行了端到端和按模块的性能分析，比较了e3nn和NVIDIA的cuEquivariance后端，并评估了FP64/FP32/BF16/FP16设置（FP32累加）在推理、短NVT和长NPT水模拟以及玩具训练运行中的效果。

Result: cuEquivariance后端减少了大约3倍的推理延迟。将线性层转换为BF16/FP16的低精度模型在大约4倍的速度上获得额外的加速，同时在NVT/NPT MD中的能量和热力学观测值在运行间的变化范围内。混合精度训练会增加力的均方根误差。混合e3nn和cuEq模块将会导致表示误差。融合等变核函数和混合精度推理可以显著加速最先进的力场，并对下游MD的影响几乎可以忽略不计。

Conclusion: 发现使用cuEquivariance后端和混合精度推理可以显著加快最先进的力场，而对物理准确性的负面影响几乎可以忽略不计。推荐使用cuEquivariance后端，将线性层设置为BF16/FP16，保持FP32累加以实现最大吞吐量。在训练阶段仍建议使用FP32。

以上策略适用于最新版本的GPU（如TF32/BF16）以及从内核级FP16/BF16路径和流水线融合中，有望进一步提高性能。

Abstract: Machine-learning force fields can deliver accurate molecular dynamics (MD) at
high computational cost. For SO(3)-equivariant models such as MACE, there is
little systematic evidence on whether reduced-precision arithmetic and
GPU-optimized kernels can cut this cost without harming physical fidelity. This
thesis aims to make MACE cheaper and faster while preserving accuracy by
identifying computational bottlenecks and evaluating low-precision execution
policies. We profile MACE end-to-end and per block, compare the e3nn and NVIDIA
cuEquivariance backends, and assess FP64/FP32/BF16/FP16 settings (with FP32
accumulation) for inference, short NVT and long NPT water simulations, and toy
training runs under reproducible, steady-state timing. cuEquivariance reduces
inference latency by about $3\times$. Casting only linear layers to BF16/FP16
within an FP32 model yields roughly 4x additional speedups, while energies and
thermodynamic observables in NVT/NPT MD remain within run-to-run variability.
Half-precision weights during training degrade force RMSE. Mixing e3nn and cuEq
modules without explicit adapters causes representation mismatches. Fused
equivariant kernels and mixed-precision inference can substantially accelerate
state-of-the-art force fields with negligible impact on downstream MD. A
practical policy is to use cuEquivariance with FP32 by default and enable
BF16/FP16 for linear layers (keeping FP32 accumulations) for maximum
throughput, while training remains in FP32. Further gains are expected on
Ampere/Hopper GPUs (TF32/BF16) and from kernel-level FP16/BF16 paths and
pipeline fusion.

</details>


### [95] [Optimal Arm Elimination Algorithms for Combinatorial Bandits](https://arxiv.org/abs/2510.23992)
*Yuxiao Wen,Yanjun Han,Zhengyuan Zhou*

Main category: cs.LG

TL;DR: 本文介绍了一种新的消减方案，该方案将臂分为三个类别（肯定，活跃和消除），包含明确的探索以更新这些集合。我们的算法在组合多臂老虎机和组合线性上下文老虎机中表现出色，而UCB方法由于探索不足可能表现不佳。论文还提供了匹配的下限。 


<details>
  <summary>Details</summary>
Motivation: 在像在线推荐和商品选择优化这样的环境中，传统的多臂老虎机框架需要扩展到允许在每一轮选择多个臂的场景中，而本文研究如何将臂消减方法引入该框架中。 

Method: 本论文提出了一种新的消减方案，将臂分为三个类别（肯定，活跃和消除）并进行明确的探索更新这些集合。分别在组合多臂老虎机和组合线性上下文老虎机环境中测试该算法。 

Result: 算法在组合多臂老虎机和组合线性上下文老虎机环境中都能达到近似最优的遗憾值，而在未进行充分探索情况下基于UCB算法的方法表现可能不佳。 

Conclusion: 我们的方法通过集成臂消减方法和明确探索解决了多臂老虎机有关问题，表现上超过了UCB方法并且还提供了相匹配的遗憾下限证明了其有效性。

Abstract: Combinatorial bandits extend the classical bandit framework to settings where
the learner selects multiple arms in each round, motivated by applications such
as online recommendation and assortment optimization. While extensions of upper
confidence bound (UCB) algorithms arise naturally in this context, adapting arm
elimination methods has proved more challenging. We introduce a novel
elimination scheme that partitions arms into three categories (confirmed,
active, and eliminated), and incorporates explicit exploration to update these
sets. We demonstrate the efficacy of our algorithm in two settings: the
combinatorial multi-armed bandit with general graph feedback, and the
combinatorial linear contextual bandit. In both cases, our approach achieves
near-optimal regret, whereas UCB-based methods can provably fail due to
insufficient explicit exploration. Matching lower bounds are also provided.

</details>


### [96] [Adversarially-Aware Architecture Design for Robust Medical AI Systems](https://arxiv.org/abs/2510.23622)
*Alyssa Gerhart,Balaji Iyangar*

Main category: cs.LG

TL;DR: 本文研究了医疗AI系统中对抗攻击的风险，通过在皮肤科数据集上的实验，展示了这些攻击对模型分类准确率的严重影响。尽管对抗训练等防御方法在一定程度上有效，但需要在攻击成功率和模型在干净数据上的性能之间进行权衡。研究强调了技术、伦理和政策相结合的重要性，以构建更安全、更具包容性的医疗AI系统。


<details>
  <summary>Details</summary>
Motivation: 文章旨在评估医疗AI系统在对抗攻击下的脆弱性，并探索有效的防御策略，以提高AI系统的安全性和公平性。通过讨论对抗攻击对患者安全的威胁，特别对于弱势群体的影响，文章强调了这一研究的重要性和紧迫性。

Method: 研究通过对抗性实验和基准测试，对皮肤科数据集上的模型进行了评估，探讨了对抗训练和蒸馏等防御方法的效果。同时进行了详细的安全性建模，以系统地分析威胁的严重性。

Result: 实验结果表明，对抗防御方法可以降低攻击成功率，但可能会影响模型在正常数据上的性能。这表明需要一种综合的方法来解决问题，而不仅仅是依靠单一的防御策略。

Conclusion: 文章总结了研究发现，并强调了在医疗领域建设更安全、更具包容性的AI系统的重要性。相对于单一的技术防御，文章呼吁采用包括伦理和政策在内的综合方法。

Abstract: Adversarial attacks pose a severe risk to AI systems used in healthcare,
capable of misleading models into dangerous misclassifications that can delay
treatments or cause misdiagnoses. These attacks, often imperceptible to human
perception, threaten patient safety, particularly in underserved populations.
Our study explores these vulnerabilities through empirical experimentation on a
dermatological dataset, where adversarial methods significantly reduce
classification accuracy. Through detailed threat modeling, experimental
benchmarking, and model evaluation, we demonstrate both the severity of the
threat and the partial success of defenses like adversarial training and
distillation. Our results show that while defenses reduce attack success rates,
they must be balanced against model performance on clean data. We conclude with
a call for integrated technical, ethical, and policy-based approaches to build
more resilient, equitable AI in healthcare.

</details>


### [97] [Information-Theoretic Discrete Diffusion](https://arxiv.org/abs/2510.24088)
*Moongyu Jeon,Sangwoo Shin,Dongjae Jeon,Albert No*

Main category: cs.LG

TL;DR: 本文提出了一种离散扩散模型的信息理论框架，通过分数匹配损失生成了log似然的原理估测器。提出了离散设置下的I-MDSE关系，将数据和其扩散版本之间的互信息与最小分数熵损失连接起来，并扩展到掩码扩散情况，建立了I-MDCE关系。实验结果确定了这些估测器的准确性、方差稳定性和实用性，改善了常见的分数熵和交叉熵损失在模型估测中的局限性。


<details>
  <summary>Details</summary>
Motivation: 本文动机在于提供一个基于信息理论的离散扩散模型框架，通过分数匹配损失方法来估计模型的log似然。为了挑战传统的仅作为变分界的分数熵和交叉熵损失，本文的目标是通过新的理论关系提供更实用的log似然估测器。

Method: 研究提出的方法主要包括了通过新的信息-最小分数熵损失理论关系和信息-最小交叉熵损失理论关系来估计log似然。在离散数据的掩码扩散过程中，进一步推导出这些关系，并构建了一个时间自由公式和条件似然的估计方法。此外，还引入了蒙地卡罗方法来估计似然比。

Result: 通过在合成数据和真实世界数据上的实验，实验结果证明了新提出的估测器的准确性和方差稳定性，并且也强调了其在实际应用中的实用性。比如，通过I-MDCE分解，我们可以得到时间自由的似然估计，这在实际任务中很有用处。

Conclusion: 本文通过提出一种新的信息理论框架，超越了传统的分数熵和交叉熵损失作为变分界的定位，展示了它们作为log似然的准确估测器的实用性。将来可以在更多应用中探索该框架的价值和潜力，提高学界对信息理论在离散模型和扩散模型应用中的理解。

Abstract: We present an information-theoretic framework for discrete diffusion models
that yields principled estimators of log-likelihood using score-matching
losses. Inspired by the I-MMSE identity for the Gaussian setup, we derive
analogous results for the discrete setting. Specifically, we introduce the
Information-Minimum Denoising Score Entropy (I-MDSE) relation, which links
mutual information between data and its diffused version to the minimum
denoising score entropy (DSE) loss. We extend this theory to masked diffusion
and establish the Information-Minimum Denoising Cross-Entropy (I-MDCE)
relation, connecting cross-entropy losses to mutual information in discrete
masked processes. These results provide a time-integral decomposition of the
log-likelihood of the data in terms of optimal score-based losses, showing that
commonly used losses such as DSE and DCE are not merely variational bounds but
tight and principled estimators of log-likelihood. The I-MDCE decomposition
further enables practical extensions, including time-free formula, conditional
likelihood estimation in prompt-response tasks, and coupled Monte Carlo
estimation of likelihood ratios. Experiments on synthetic and real-world data
confirm the accuracy, variance stability, and utility of our estimators. The
code is publicly available at https://github.com/Dongjae0324/infodis.

</details>


### [98] [DiNo and RanBu: Lightweight Predictions from Shallow Random Forests](https://arxiv.org/abs/2510.23624)
*Tiago Mendonça dos Santos,Rafael Izbicki,Luís Gustavo Esteves*

Main category: cs.LG

TL;DR: 介绍了一种新的浅层森林方法DiNo和RanBu，这些方法提供了与深森林相似的预测能力，但具有更快的推断速度和更低的内存需求。在多个数据集上，RanBu在准确性上匹配或超过完整深度的随机森林，并减少训练和推断时间最多达95％。DiNo在低噪声环境中提供最佳偏差-方差权衡。两个方法均适用于分位数回归，并保持准确性的同时带来巨大的速度提升。相关的开源实现可在GitHub上找到。


<details>
  <summary>Details</summary>
Motivation: 原始的随机森林虽然在标注预测任务中表现良好，但是由于依赖于大量深度树，会导致推理延时过高和内存需求激增，不适合在延迟敏感或资源受限的环境中部署。在此背景下，研究者介绍了DiNo（基于节点的距离）和RanBu（随机灌木丛），旨在解决这些挑战。

Method: 新的方法将一组深度受限的树转换为高效的、基于距离加权的预测器。DiNo通过计算观测对的最近公共祖先来测量科芬尼克距离，而RanBu则通过应用核平滑到Breiman的经典邻近度测量来实现。这些方法完全在森林训练后操作，无需生长额外的树，且带宽参数$h$的调整仅需要轻量级的矩阵-向量运算。此外，这两个方法均可以应用于分位数回归。

Result: 在三个合成基准和25个公共数据集上的实验结果表明，RanBu不仅达到了甚至超过了完整深度的随机森林的准确性，尤其是在高噪声环境下，同时还实现了最多95%的训练加推断时间的减少。DiNo在低噪声场景中达到了最佳的偏差-方差权衡，且只需很小的计算成本。两种方法均适用于分位数回归，并可以在保持准确性的同时带来巨大的速度提升。

Conclusion: 新的浅层森林方法不仅在准确性上可以匹配或超过原始的随机森林，同时在延迟和内存要求上要低得多。这使其可以更有效地部署在延迟敏感或资源受限的环境中。此外，实验结果表明，在保持准确性的同时，两种方法在推断速度上有显著提升。

Abstract: Random Forest ensembles are a strong baseline for tabular prediction tasks,
but their reliance on hundreds of deep trees often results in high inference
latency and memory demands, limiting deployment in latency-sensitive or
resource-constrained environments. We introduce DiNo (Distance with Nodes) and
RanBu (Random Bushes), two shallow-forest methods that convert a small set of
depth-limited trees into efficient, distance-weighted predictors. DiNo measures
cophenetic distances via the most recent common ancestor of observation pairs,
while RanBu applies kernel smoothing to Breiman's classical proximity measure.
Both approaches operate entirely after forest training: no additional trees are
grown, and tuning of the single bandwidth parameter $h$ requires only
lightweight matrix-vector operations. Across three synthetic benchmarks and 25
public datasets, RanBu matches or exceeds the accuracy of full-depth random
forests-particularly in high-noise settings-while reducing training plus
inference time by up to 95\%. DiNo achieves the best bias-variance trade-off in
low-noise regimes at a modest computational cost. Both methods extend directly
to quantile regression, maintaining accuracy with substantial speed gains. The
implementation is available as an open-source R/C++ package at
https://github.com/tiagomendonca/dirf. We focus on structured tabular random
samples (i.i.d.), leaving extensions to other modalities for future work.

</details>


### [99] [Noise is All You Need: Solving Linear Inverse Problems by Noise Combination Sampling with Diffusion Models](https://arxiv.org/abs/2510.23633)
*Xun Su,Hiroyuki Kasai*

Main category: cs.LG

TL;DR: 提出了一种新的方法，名为Noise Combination Sampling，该方法可以将条件信息自然嵌入扩散模型的生成过程中，解决了过度和不足融合观测信息的难题，适用于多种逆问题求解任务，特别是在步骤数较少时性能更优且计算开销几乎可以忽略不计，提高了模型的鲁棒性和稳定性。


<details>
  <summary>Details</summary>
Motivation: 预训练扩散模型通过将观测信息融入生成过程来解决逆问题，但过度或不足的融合导致模型性能不佳。为了平衡这两者，提出了一个新的方法。

Method: 提出的方法是Noise Combination Sampling，该方法从一个噪声子空间合成一个最佳的噪声向量，来近似测量得分。替代标准的去噪扩散概率模型中的噪声项，从而使条件信息自然纳入生成过程中。此方法适用于各种逆问题求解器。

Result: 该方法在逆问题求解中实现了优异的性能，尤其是在生成步骤较少时，且计算开销几乎可以忽略不计，同时提高了模型的鲁棒性和稳定性。

Conclusion: Noise Combination Sampling 提供了一种有效的方法来克服预训练扩散模型中的难题，适用于广泛的逆问题求解任务，如图像压缩等。

Abstract: Pretrained diffusion models have demonstrated strong capabilities in
zero-shot inverse problem solving by incorporating observation information into
the generation process of the diffusion models. However, this presents an
inherent dilemma: excessive integration can disrupt the generative process,
while insufficient integration fails to emphasize the constraints imposed by
the inverse problem. To address this, we propose \emph{Noise Combination
Sampling}, a novel method that synthesizes an optimal noise vector from a noise
subspace to approximate the measurement score, replacing the noise term in the
standard Denoising Diffusion Probabilistic Models process. This enables
conditional information to be naturally embedded into the generation process
without reliance on step-wise hyperparameter tuning. Our method can be applied
to a wide range of inverse problem solvers, including image compression, and,
particularly when the number of generation steps $T$ is small, achieves
superior performance with negligible computational overhead, significantly
improving robustness and stability.

</details>


### [100] [From Detection to Discovery: A Closed-Loop Approach for Simultaneous and Continuous Medical Knowledge Expansion and Depression Detection on Social Media](https://arxiv.org/abs/2510.23626)
*Shuang Geng,Wenli Zhang,Jiaheng Xie,Rui Wang,Sudha Ram*

Main category: cs.LG

TL;DR: 本文开发了一种闭环的大规模语言模型（LLM）-知识图谱框架，通过预测和知识扩展的迭代学习循环来改善抑郁症检测和医学知识。



<details>
  <summary>Details</summary>
Motivation: 现有的研究忽略了通过预测过程同时扩展医学知识的机会。


Method: 该框架包含知识驱动的抑郁检测和知识提炼与扩展两个阶段。在第一个阶段，LLM进行抑郁检测并提取实体，知识图谱被用来代表和加权这些实体，提高预测性能。在第二个阶段，新的实体、关系和实体类型在专家监督下并入知识图谱，使知识能够不断进化。


Result: 利用大规模的UGC，该框架提高了预测的准确性并深化了医学理解。


Conclusion: 专家评估确认了该框架发现新的临床有意义的症状、共病和社交触发因素。这种预测和学习的相互强化过程既推进了方法论理解也推进了理论理解。该框架展示了计算模型与领域知识的共同进化，可以应用于其他动态风险管理场合。


Abstract: Social media user-generated content (UGC) provides real-time, self-reported
indicators of mental health conditions such as depression, offering a valuable
source for predictive analytics. While prior studies integrate medical
knowledge to improve prediction accuracy, they overlook the opportunity to
simultaneously expand such knowledge through predictive processes. We develop a
Closed-Loop Large Language Model (LLM)-Knowledge Graph framework that
integrates prediction and knowledge expansion in an iterative learning cycle.
In the knowledge-aware depression detection phase, the LLM jointly performs
depression detection and entity extraction, while the knowledge graph
represents and weights these entities to refine prediction performance. In the
knowledge refinement and expansion phase, new entities, relationships, and
entity types extracted by the LLM are incorporated into the knowledge graph
under expert supervision, enabling continual knowledge evolution. Using
large-scale UGC, the framework enhances both predictive accuracy and medical
understanding. Expert evaluations confirmed the discovery of clinically
meaningful symptoms, comorbidities, and social triggers complementary to
existing literature. We conceptualize and operationalize
prediction-through-learning and learning-through-prediction as mutually
reinforcing processes, advancing both methodological and theoretical
understanding in predictive analytics. The framework demonstrates the
co-evolution of computational models and domain knowledge, offering a
foundation for adaptive, data-driven knowledge systems applicable to other
dynamic risk monitoring contexts.

</details>


### [101] [Chain of Execution Supervision Promotes General Reasoning in Large Language Models](https://arxiv.org/abs/2510.23629)
*Nuo Chen,Zehua Li,Keqin Bao,Junyang Lin,Dayiheng Liu*

Main category: cs.LG

TL;DR: 本文提出了TracePile，一个将代码执行转化为明确的、逐步的推理链的大型数据集，以解决代码推理中隐式表达和实施噪音带来的挑战。实验表明，TracePile能显著提升模型在数学、代码、逻辑和算法等方面的性能。


<details>
  <summary>Details</summary>
Motivation: 现有大型语言模型通过代码训练逻辑推理能力时，遇到代码推理表达隐式及与语法或实现噪音混杂的问题，直接在原始代码上训练效果不佳，因此提出了TracePile来改善这一状况。

Method: 构建了一个命名为TracePile的大规模数据集，该数据集将代码执行转化为显式的、逐步的推理链，称为执行链（CoE），并使用从继续预训练到指令微调的不同训练设置评估其效果。

Result: 通过实验验证，TracePile能显著提升模型在数学、代码、逻辑和算法等领域的表现，特别是在数学领域实现了7.1%的平均性能提升。

Conclusion: TracePile通过将代码执行转化为推理链，解决了代码推理的隐式表达和实施噪音问题，显著增强了模型的逻辑推理能力。

Abstract: Building robust and general reasoning ability is a central goal in the
development of large language models (LLMs). Recent efforts increasingly turn
to code as a rich training source, given its inherent logical structure and
diverse reasoning paradigms such as divide-and-conquer, topological ordering,
and enumeration. However, reasoning in code is often expressed implicitly and
entangled with syntactic or implementation noise, making direct training on raw
code suboptimal.To address this, we introduce TracePile, a large-scale corpus
of 2.6 million samples that transforms code execution into explicit,
step-by-step chain-of-thought-style rationales, which we call Chain of
Execution (CoE). The corpus spans domains including mathematics, classical
algorithms and algorithmic competition, and is enriched with variable-tracing
questions and code rewritings to enhance logical granularity and code
diversity. We evaluate TracePile using three training setups:
continue-pretraining, instruction tuning after pretraining, and two-stage
finetuning. Experiments across four base models (LLaMA 3, LLaMA 3.1, Qwen-2.5,
and Qwen-2.5 Coder) and 20 benchmarks covering math, code, logic, and
algorithms demonstrate consistent improvements. Notably, TracePile boosts
LLaMA3.1-8B by 7.1\% on average across nine math datasets and delivers clear
gains on LiveCodeBench, CRUX, and MMLU under two-stage fine-tuning.

</details>


### [102] [Beyond Pairwise: Empowering LLM Alignment With Ranked Choice Modeling](https://arxiv.org/abs/2510.23631)
*Yuxuan Tang,Yifan Feng*

Main category: cs.LG

TL;DR: 提出了一种新的优化框架 Ranked Choice Preference Optimization (RCPO)，它能够利用排名偏好数据进行语言模型的训练，实现了更好的模型对齐效果。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要采用成对偏好优化，忽视了从更丰富的用户反馈（如多选比较和前 k 排名）中学习的机会。因此，需要一种更全面的框架来支持这类反馈格式的学习。

Method: RCPO 通过最大似然估计将偏好优化与（排名）选择模型统一起来，支持实用性和等级性选择模型，包括多元对数几率模型和 Mallows-RMJ 模型。

Result: 在多个基准数据集上，RCPO 的性能均优于现有的竞争基线模型。

Conclusion: RCPO 能够直接利用排名偏好的数据，并结合适当的选择模型，从而提升语言模型的对齐效果，为将选择模型纳入 LLM 训练提供了一个灵活且可扩展的基础。

Abstract: Alignment of large language models (LLMs) has predominantly relied on
pairwise preference optimization, where annotators select the better of two
responses to a prompt. While simple, this approach overlooks the opportunity to
learn from richer forms of human feedback, such as multiwise comparisons and
top-$k$ rankings. We propose Ranked Choice Preference Optimization (RCPO), a
unified framework that bridges preference optimization with (ranked) choice
modeling via maximum likelihood estimation. The framework is flexible,
supporting both utility-based and rank-based choice models. It subsumes several
existing pairwise methods (e.g., DPO, SimPO), while providing principled
training objectives for richer feedback formats. We instantiate this framework
with two representative ranked choice models (Multinomial Logit and
Mallows-RMJ). Empirical studies on Llama-3-8B-Instruct and Gemma-2-9B-it across
AlpacaEval 2 and Arena-Hard benchmarks show that RCPO consistently outperforms
competitive baselines. RCPO shows how directly leveraging ranked preference
data, combined with the right choice models, yields more effective alignment.
It offers a versatile and extensible foundation for incorporating (ranked)
choice modeling into LLM training.

</details>


### [103] [LLMComp: A Language Modeling Paradigm for Error-Bounded Scientific Data Compression](https://arxiv.org/abs/2510.23632)
*Guozhong Li,Muhannad Alhumaidi,Spiros Skiadopoulos,Panos Kalnis*

Main category: cs.LG

TL;DR: LLMCOMP 是一种利用解码器模型对科学数据进行压缩的新方法，通过令牌量化、Z排序曲线和覆盖引导采样进行训练，实现高效且误差允许的压缩。实验表明，LLMCOMP 在保持严格误差限制的情况下，比现有最佳压缩算法高出最多 30% 的压缩比。


<details>
  <summary>Details</summary>
Motivation: 基于高分辨率科学模拟和观测系统产生的大量时空数据，开发一种效率高、误差可控的压缩方法。解码器大型语言模型 (LLM) 在处理复杂序列数据方面表现出色，适合用于科学数据压缩。

Method: LLMCOMP 先将3D场量化为离散标记，使用Z顺序曲线保留局部相关性，并通过覆盖引导采样提高训练效率。随后使用具有时空嵌入的自回归 Transformer 模型对令牌转换进行建模。在压缩过程中，模型通过Top-k预测保存等级索引并添加恢复更正，以确保严格的错误界限。

Result: 实验结果表明，LLMCOMP 在多个再分析数据集上均能超越现有最佳压缩器，实现了最高达 30% 的压缩比，且所有结果都在严格的误差限制内。这表明将LLM作为用于高保真科学数据的一般压缩器具有潜在价值。

Conclusion: LLMCOMP 证明了基于解码器的大型语言模型可有效用于科学数据压缩，同时实现了高度的压缩效率与误差控制。

Abstract: The rapid growth of high-resolution scientific simulations and observation
systems is generating massive spatiotemporal datasets, making efficient,
error-bounded compression increasingly important. Meanwhile, decoder-only large
language models (LLMs) have demonstrated remarkable capabilities in modeling
complex sequential data. In this paper, we propose LLMCOMP, a novel lossy
compression paradigm that leverages decoder-only large LLMs to model scientific
data. LLMCOMP first quantizes 3D fields into discrete tokens, arranges them via
Z-order curves to preserve locality, and applies coverage-guided sampling to
enhance training efficiency. An autoregressive transformer is then trained with
spatial-temporal embeddings to model token transitions. During compression, the
model performs top-k prediction, storing only rank indices and fallback
corrections to ensure strict error bounds. Experiments on multiple reanalysis
datasets show that LLMCOMP consistently outperforms state-of-the-art
compressors, achieving up to 30% higher compression ratios under strict error
bounds. These results highlight the potential of LLMs as general-purpose
compressors for high-fidelity scientific data.

</details>


### [104] [Monotone and Separable Set Functions: Characterizations and Neural Models](https://arxiv.org/abs/2510.23634)
*Soutrik Sarangi,Yonatan Sverdlov,Nadav Dym,Abir De*

Main category: cs.LG

TL;DR: 研究提出了一种称为Monotone and Separating (MAS)的集合到向量函数的概念，并探讨了其在集合包含问题中的应用，同时提供了一个名为'弱MAS'的替代模型。我们的实验结果表明，使用这种方法可以更好地处理集合包含任务，对比传统的不包含集合包含作为归纳偏置的模型有了明显改进。我们的代码可在https://github.com/yonatansverdlov/Monotone-Embedding获取


<details>
  <summary>Details</summary>
Motivation: 为了应用到集合包含问题，研究希望找到一种保持自然偏序关系的集合到向量函数

Method: 设计和分析MAS集合函数的概念，评估其在有限和无限基数下的可行性，并提出一种'弱MAS'替代模型，衡量其与原始MAS函数的差异

Result: 证明了在无限基数中MAS函数不存在，但是'弱MAS'函数经证具有稳定性和Holder连续性。此外，实验数据表明使用MAS函数构建的模型在处理集合包含任务上具有明显的优势

Conclusion: 研究揭示了集合-向量函数在保持集合包含关系上具有重要的应用价值，特别是'弱MAS'模型展现了很好的实用性和理论基础

Abstract: Motivated by applications for set containment problems, we consider the
following fundamental problem: can we design set-to-vector functions so that
the natural partial order on sets is preserved, namely $S\subseteq T \text{ if
and only if } F(S)\leq F(T) $. We call functions satisfying this property
Monotone and Separating (MAS) set functions. % We establish lower and upper
bounds for the vector dimension necessary to obtain MAS functions, as a
function of the cardinality of the multisets and the underlying ground set. In
the important case of an infinite ground set, we show that MAS functions do not
exist, but provide a model called our which provably enjoys a relaxed MAS
property we name "weakly MAS" and is stable in the sense of Holder continuity.
We also show that MAS functions can be used to construct universal models that
are monotone by construction and can approximate all monotone set functions.
Experimentally, we consider a variety of set containment tasks. The experiments
show the benefit of using our our model, in comparison with standard set models
which do not incorporate set containment as an inductive bias. Our code is
available in https://github.com/yonatansverdlov/Monotone-Embedding.

</details>


### [105] [Help the machine to help you: an evaluation in the wild of egocentric data cleaning via skeptical learning](https://arxiv.org/abs/2510.23635)
*Andrea Bontempelli,Matteo Busso,Leonardo Javier Malcotti,Fausto Giunchiglia*

Main category: cs.LG

TL;DR: 本研究评估了Skeptical Learning（SKEL）在现实条件下的性能，特别是结合实际用户的视角和需求来调整输入标签。研究涉及的参与者是使用iLog移动应用程序的大学生。结果强调了在用户努力和数据质量之间找到平衡的挑战，也提出了SKEL在减少标注工作量和提升数据质量方面的潜在优势。


<details>
  <summary>Details</summary>
Motivation: 现有研究中的Skeptical Learning（SKEL）通过对比离线积极标注与被动数据来解决噪声标签问题，但是并未引入最终用户的确认，而后者才是判断自己所在的环境的最佳评判者。本研究旨在引入最终用户的反馈，以评估SKEL在真实条件下的性能。

Method: 本研究通过让使用iLog移动应用程序的大学生提供数据，观察并验证SKEL在实际中的应用情况，为期四周。参与者会根据自己的处境和需求调整输入标签，以便评估数据的质量和准确性。

Result: 研究结果表明，寻找用户努力与数据质量之间的平衡具有挑战性，并且SKEL有可能降低标注工作量以及提升数据质量。

Conclusion: 研究强调了使用SKEL技术以精简用户参与的数据标注过程和提高数据质量的重要性，同时也指出了过程中面临的挑战。

Abstract: Any digital personal assistant, whether used to support task performance,
answer questions, or manage work and daily life, including fitness schedules,
requires high-quality annotations to function properly. However, user
annotations, whether actively produced or inferred from context (e.g., data
from smartphone sensors), are often subject to errors and noise. Previous
research on Skeptical Learning (SKEL) addressed the issue of noisy labels by
comparing offline active annotations with passive data, allowing for an
evaluation of annotation accuracy. However, this evaluation did not include
confirmation from end-users, the best judges of their own context. In this
study, we evaluate SKEL's performance in real-world conditions with actual
users who can refine the input labels based on their current perspectives and
needs. The study involves university students using the iLog mobile application
on their devices over a period of four weeks. The results highlight the
challenges of finding the right balance between user effort and data quality,
as well as the potential benefits of using SKEL, which include reduced
annotation effort and improved quality of collected data.

</details>


### [106] [Integrating Genomics into Multimodal EHR Foundation Models](https://arxiv.org/abs/2510.23639)
*Jonathan Amar,Edward Liu,Alessandra Breschi,Liangliang Zhang,Pouya Kheradpour,Sylvia Li,Lisa Soleymani Lehmann,Alessandro Giulianelli,Matt Edwards,Yugang Jia,David Nola,Raghav Mani,Pankaj Vats,Jesse Tetreault,T. J. Chen,Cory Y. McLean*

Main category: cs.LG

TL;DR: 该论文介绍了一种新的医疗电子记录（EHR）基础模型，整合了多基因风险评分（PRS），超越了传统的仅依赖EHR的方法，构建了更全面的健康档案。通过从“所有人的研究计划”（AoU）中获得的数据，该多模态框架旨在学习临床数据和遗传易感性之间的复杂关系，并在EHR基础模型领域推进了生成式AI的应用，提升了预测能力和可解释性。在AoU数据上的评估表明，该模型对于各种疾病的预测具有价值，特别是2型糖尿病（T2D），同时展示了PRS和EHR数据之间的相互作用。这项工作还探索了为定制分类任务进行迁移学习，展示了该架构的多功能性和效率。该方法为疾病预测、主动健康管理和个性化治疗策略奠定了基础，开启了更个性化、更公平和更具操作性的现实世界证据生成的前景。


<details>
  <summary>Details</summary>
Motivation: 传统基于EHR的数据分析方法具有局限性，而多基因风险评分（PRS）的引入可以填补遗传信息方面的空白。论文动机在于创新性地将PRS整合到EHR中，构建更全面的健康档案，并通过提升预测能力和可解释性，实现更好的疾病管理和个性化医疗。为了实现这一目标，论文作者采用了多模态框架和先进的电子健康记录（EHR）基础模型，利用生成式AI技术，并进行了广泛的实验验证以展示其优势。这项工作的最终目标是为疾病的预测、管理及治疗提供更强有力的工具，推动医学研究的进步。

Method: 利用All of Us (AoU) 研究计划中丰富的多模态数据，研究团队开发了一种将多基因风险评分 (PRS) 整合为电子健康记录（EHR）基础模型一部分的技术。该方法不仅利用EHR里的临床数据，还将其与遗传数据相结合，通过运用先进的人工智能技术进行数据整合与模式识别。此外，还在EHR基础模型中激活了生成式AI技术，以提升整体分析的预测性和解释性。最后，对AoU数据进行了实验评估，呈现了利用新模式进行健康状况预测的实际效果，并探讨了其在不同应用场景中应用的潜力。

Result: 实验表明，将PRS整合到EHR中的方法能够提升对诸如T2D这样的疾病发生率的预测准确性，有效的结合了临床和遗传的数据，明白展现两者之间复杂的关系。模型不仅能够良好地预测疾病的发生，还展示了它在定制流程中的适用性和多功能性，包括潜在的转移学习应用以支持更为专业化的特定任务分类。此外，初步研究结果表明该模型可能会在预测其他类型的疾病上也有较为广泛的应用场景。

Conclusion: 将PRS数据整合进EHR基础模型中，为疾病预测提供了新的方向，有望进一步推动主动健康管理和个性化治疗策略的发展。也预示着更加个性化、精准和可行的医疗保健数据生成。同时，这项研究也为未来研究提供了方向，即通过更多的数据和更多的算法迭代来提高模型的准确性、稳健性和解释性。

Abstract: This paper introduces an innovative Electronic Health Record (EHR) foundation
model that integrates Polygenic Risk Scores (PRS) as a foundational data
modality, moving beyond traditional EHR-only approaches to build more holistic
health profiles. Leveraging the extensive and diverse data from the All of Us
(AoU) Research Program, this multimodal framework aims to learn complex
relationships between clinical data and genetic predispositions. The
methodology extends advancements in generative AI to the EHR foundation model
space, enhancing predictive capabilities and interpretability. Evaluation on
AoU data demonstrates the model's predictive value for the onset of various
conditions, particularly Type 2 Diabetes (T2D), and illustrates the interplay
between PRS and EHR data. The work also explores transfer learning for custom
classification tasks, showcasing the architecture's versatility and efficiency.
This approach is pivotal for unlocking new insights into disease prediction,
proactive health management, risk stratification, and personalized treatment
strategies, laying the groundwork for more personalized, equitable, and
actionable real-world evidence generation in healthcare.

</details>


### [107] [Structure-Aware Fusion with Progressive Injection for Multimodal Molecular Representation Learning](https://arxiv.org/abs/2510.23640)
*Zihao Jing,Yan Sun,Yan Yi Li,Sugitha Janarthanan,Alana Deng,Pingzhao Hu*

Main category: cs.LG

TL;DR: MuMo 提出了一种结构化多模态融合框架，通过减少依赖于构象的融合不稳定性和避免模态塌陷，改进了分子表示。该框架在多基准测试任务中表现出色，验证了其在分子表示中的多模态融合的有效性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 多模态分子模型经常受到3D构象不稳定性和模态塌陷的限制，影响其鲁棒性和泛化能力。因此提出了MuMo框架来解决这些问题。

Method: MuMo框架设计了结构化融合管道(SFP)，将2D拓扑和3D几何结合为统一且稳定的结构性先验；还引入了渐进注入（PI）机制，不对称地将这种先验合并到序列流中，保持模态特定建模同时促进跨模式丰富性。框架基于状态空间骨干，支持长程依赖建模和鲁棒信息传播。

Result: 在29个基准任务中，MuMo相较最佳基线模型平均改进了2.7%的性能，其中在LD50任务中提升了27%，在22个任务中排名首位。结果验证了框架对3D构象噪声的鲁棒性和多模态融合的有效性。

Conclusion: MuMo通过提供一种稳定且有效的多模态融合方法，显著提高了分子表示的质量，展示了其在药物研发等领域的应用潜力。

Abstract: Multimodal molecular models often suffer from 3D conformer unreliability and
modality collapse, limiting their robustness and generalization. We propose
MuMo, a structured multimodal fusion framework that addresses these challenges
in molecular representation through two key strategies. To reduce the
instability of conformer-dependent fusion, we design a Structured Fusion
Pipeline (SFP) that combines 2D topology and 3D geometry into a unified and
stable structural prior. To mitigate modality collapse caused by naive fusion,
we introduce a Progressive Injection (PI) mechanism that asymmetrically
integrates this prior into the sequence stream, preserving modality-specific
modeling while enabling cross-modal enrichment. Built on a state space
backbone, MuMo supports long-range dependency modeling and robust information
propagation. Across 29 benchmark tasks from Therapeutics Data Commons (TDC) and
MoleculeNet, MuMo achieves an average improvement of 2.7% over the
best-performing baseline on each task, ranking first on 22 of them, including a
27% improvement on the LD50 task. These results validate its robustness to 3D
conformer noise and the effectiveness of multimodal fusion in molecular
representation. The code is available at: github.com/selmiss/MuMo.

</details>


### [108] [Spatially Aware Linear Transformer (SAL-T) for Particle Jet Tagging](https://arxiv.org/abs/2510.23641)
*Aaron Wang,Zihan Zhao,Subash Katel,Vivekanand Gyanchand Sahu,Elham E Khoda,Abhijith Gandrakota,Jennifer Ngadiuba,Richard Cavanaugh,Javier Duarte*

Main category: cs.LG

TL;DR: 本文提出了一种改进的变压器架构——空间感知线性变压器(SAL-T)，该架构旨在解决高能粒子碰撞分析中在高数据吞吐量环境下的应用难题。SAL-T利用了物理学知识，并通过空间感知分区和卷积层来降低计算复杂度，同时保持了高准确度。实验表明，SAL-T在资源消耗和推断延迟上低于标准的线性变压器，而在准确度上则与全注意力变压器相当，并在一定程度上超越了它们。


<details>
  <summary>Details</summary>
Motivation: 传统的变压器模型在处理高数据吞吐量（如CERN LHC实验）时存在计算复杂度过高、资源需求大以及推断延迟等问题，从而阻碍了它们在高能物理分析中的应用。因此，作者提出了SAL-T，旨在降低计算复杂度同时保持或提高模型的预测性能，以此解决这些挑战。此外，作者希望通过将物理知识融入模型设计中，可以提升模型在高能粒子碰撞数据中的应用效果。

Method: SAL-T结合物理学原理，通过空间感知分区，基于颗粒物的运动学特性，划分物理显著区域，并使用卷积层捕捉局部相关性。该方法巧妙融合了物理知识，降低了计算复杂度，并保留了线性注意机制。在具体的任务中，SAL-T展示了较好的性能，尤其是在资源分配和推断延迟上相较于标准的线性变压器模型有显著提升。此外，实验结果也验证了这种新的架构在非专门化的点云分类任务上的潜力。

Result: SAL-T在资源消耗、推断过程的延迟以及性能上相较于传统的线性变压器模型有显著优势，同时在一些特定任务上的性能甚至能达到全注意力变压器的效果。实验表明，在高能粒子碰撞的数据分析上，以及在通用的点云分类数据集上（比如ModelNet10），SAL-T都显示出优越的表现。

Conclusion: SAL-T作为一种改进的物理感知变压器架构，展示了在高能粒子碰撞分析和通用数据集上的应用潜力，它的创新设计有望解决大规模数据集分析中的复杂性和效率问题，为高能物理学以及其他需要处理大规模数据的应用领域提供了新的解决方案。

Abstract: Transformers are very effective in capturing both global and local
correlations within high-energy particle collisions, but they present
deployment challenges in high-data-throughput environments, such as the CERN
LHC. The quadratic complexity of transformer models demands substantial
resources and increases latency during inference. In order to address these
issues, we introduce the Spatially Aware Linear Transformer (SAL-T), a
physics-inspired enhancement of the linformer architecture that maintains
linear attention. Our method incorporates spatially aware partitioning of
particles based on kinematic features, thereby computing attention between
regions of physical significance. Additionally, we employ convolutional layers
to capture local correlations, informed by insights from jet physics. In
addition to outperforming the standard linformer in jet classification tasks,
SAL-T also achieves classification results comparable to full-attention
transformers, while using considerably fewer resources with lower latency
during inference. Experiments on a generic point cloud classification dataset
(ModelNet10) further confirm this trend. Our code is available at
https://github.com/aaronw5/SAL-T4HEP.

</details>


### [109] [Learning to Drive Safely with Hybrid Options](https://arxiv.org/abs/2510.24674)
*Bram De Cooman,Johan Suykens*

Main category: cs.LG

TL;DR: 本文提出了在高速公路自动驾驶任务中使用选项框架的方法，并通过结合纵向和横向控制的动作选择，提高了灵活性和可解释性，且在不同的交通条件下优于基础动作策略。


<details>
  <summary>Details</summary>
Motivation: 现有的深度强化学习方法很少使用选项框架，虽后者非常适合分层控制应用且易于纳入先验知识。因此，本研究试图将选项框架应用到高速公路自动驾驶任务中，并通过引入嵌入安全性和舒适性约束的专门选项来优化学习过程和控制行为。

Method: 定义了纵向和横向运动控制的动作，并用这些动作设置了分层控制的几种体系，执行基于深度强化学习技术的实用算法，以分别选择纵向和横向控制的动作。这些方案具有与人类司机相同的表达能力和灵活性。

Result: 在各种交通条件下，灵活的混合选项策略优于基础动作策略。

Conclusion: 通过引入嵌入先验知识的专门选项，可以增强自动驾驶决策系统的表达能力和灵活性，提升其性能，同时维护易解释性，这为自动驾驶任务提供了一种新颖且高效的解决方案。

Abstract: Out of the many deep reinforcement learning approaches for autonomous
driving, only few make use of the options (or skills) framework. That is
surprising, as this framework is naturally suited for hierarchical control
applications in general, and autonomous driving tasks in specific. Therefore,
in this work the options framework is applied and tailored to autonomous
driving tasks on highways. More specifically, we define dedicated options for
longitudinal and lateral manoeuvres with embedded safety and comfort
constraints. This way, prior domain knowledge can be incorporated into the
learning process and the learned driving behaviour can be constrained more
easily. We propose several setups for hierarchical control with options and
derive practical algorithms following state-of-the-art reinforcement learning
techniques. By separately selecting actions for longitudinal and lateral
control, the introduced policies over combined and hybrid options obtain the
same expressiveness and flexibility that human drivers have, while being easier
to interpret than classical policies over continuous actions. Of all the
investigated approaches, these flexible policies over hybrid options perform
the best under varying traffic conditions, outperforming the baseline policies
over actions.

</details>


### [110] [Beyond Hidden-Layer Manipulation: Semantically-Aware Logit Interventions for Debiasing LLMs](https://arxiv.org/abs/2510.23650)
*Wei Xia*

Main category: cs.LG

TL;DR: 本文提出了两种零样本 logits 层去偏方法：静态和动态。动态方法可以在几乎不损失流畅度的情况下减少高达 70% 的偏差。本文证明了基于语义的 logits 干预方法对于去偏对齐的大语言模型是稳定且有效的；


<details>
  <summary>Details</summary>
Motivation: 动机在于开发一种可以有效减少大语言模型预测偏见的方法，同时保持其生成文本的流畅性；

Method: 提出了静态和动态两种零样本 logits 层去偏方法，其中动态方法通过减少偏差同时保持较高流畅度显示出优越性；

Result: 动态方法显著减少了大语言模型中的预测偏见，减少高达 70%，同时也展示了基于语义的 logits 干预的有效性和稳定性；

Conclusion: 零样本 logits 层去偏方法是一种稳定且有效的方式，能够实现大语言模型的去偏且不显著影响其生成文本的质量。

Abstract: We proposed Static and Dynamic -- two zero-shot logits-layer debiasing
methods. Dynamic reduces bias by up to 70% with minimal fluency loss. Logits
intervention outperforms hidden-layer approaches. We show semantic-aware logits
intervention is stable and effective for debiasing aligned LLMs.

</details>


### [111] [The Structural Scalpel: Automated Contiguous Layer Pruning for Large Language Models](https://arxiv.org/abs/2510.23652)
*Yao Lu,Yuqi Li,Wenbin Xie,Shanqing Yu,Qi Xuan,Zhaowei Zhu,Shiping Wen*

Main category: cs.LG

TL;DR: 本文提出了一种称为CLP的连续层剪枝框架，以解决大型语言模型在边缘设备部署时的计算成本高的问题。CLP使用可微分的凹门算法自动识别最佳的连续层段进行剪枝，并通过调整剪枝段附近的层来恢复模型性能。实验表明，CLP显著优于现有方法，且在与量化结合时能够进一步压缩模型，仅造成微小的性能下降。


<details>
  <summary>Details</summary>
Motivation: 现有的层剪枝方法依赖于手工制定的度量来评估和去除单个层，这会导致模型信息流的破坏并严重降低性能。CLP旨在解决这一问题，通过引入连续层剪枝框架，改进层剪枝技术，以更好地适应资源受限的边缘设备上部署的大模型。

Method: CLP框架有两个关键创新点：一种可微分的凹门算法用于自动识别最佳连续层段进行剪枝；以及一种剪枝末点调整策略，通过微调剪枝段附近的层来恢复模型性能。

Result: 实验显示，CLP在多个模型架构和大小上均显著优于现有基线方法。如在20%的剪枝率下，CLP在LLaMA3-70B模型上实现了95.34%的性能保留率，超过基线方法4.29%-30.52%。此外，CLP还可以与量化技术结合，进一步压缩模型，同时保持接近原始模型的性能。

Conclusion: CLP是一种有效且性能优越的连续层剪枝框架，适用于大型语言模型在资源受限边缘设备上的部署。

Abstract: Although large language models (LLMs) have achieved revolutionary
breakthroughs in many fields, their large model size and high computational
cost pose significant challenges for practical deployment on
resource-constrained edge devices. To this end, layer pruning has been proposed
to reduce the computational overhead by directly removing redundant layers.
However, existing layer pruning methods typically rely on hand-crafted metrics
to evaluate and remove individual layers, while ignoring the dependencies
between layers. This can disrupt the model's information flow and severely
degrade performance. To address these issues, we propose CLP, a novel
continuous layer pruning framework that introduces two key innovations: a
differentiable concave gate algorithm that automatically identifies the best
continuous layer segments for pruning via gradient-based optimization; and a
cutoff endpoint tuning strategy that effectively restores model performance by
fine-tuning only the layers adjacent to the pruned segments. Extensive
experiments across multiple model architectures (including LLaMA2, LLaMA3 and
Qwen) and sizes (from $7$B to $70$B parameters) show that CLP significantly
outperforms existing state-of-the-art baselines. For example, at a pruning rate
of $20\%$, CLP achieves an average performance retention of $95.34\%$ on
LLaMA3-70B, outperforming baselines by $4.29\%$-$30.52\%$. Furthermore, CLP can
be seamlessly combined with quantization to further compress the model with
only a slight performance loss.

</details>


### [112] [A machine learning framework integrating seed traits and plasma parameters for predicting germination uplift in crops](https://arxiv.org/abs/2510.23657)
*Saklain Niam,Tashfiqur Rahman,Md. Amjad Patwary,Mukarram Hossain*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Cold plasma (CP) is an eco-friendly method to enhance seed germination, yet
outcomes remain difficult to predict due to complex seed--plasma--environment
interactions. This study introduces the first machine learning framework to
forecast germination uplift in soybean, barley, sunflower, radish, and tomato
under dielectric barrier discharge (DBD) plasma. Among the models tested (GB,
XGB, ET, and hybrids), Extra Trees (ET) performed best (R\textsuperscript{2} =
0.919; RMSE = 3.21; MAE = 2.62), improving to R\textsuperscript{2} = 0.925
after feature reduction. Engineering analysis revealed a hormetic response:
negligible effects at $<$7 kV or $<$200 s, maximum germination at 7--15 kV for
200--500 s, and reduced germination beyond 20 kV or prolonged exposures.
Discharge power was also a dominant factor, with germination rate maximizing at
$\geq$100 W with low exposure time. Species and cultivar-level predictions
showed radish (MAE = 1.46) and soybean (MAE = 2.05) were modeled with high
consistency, while sunflower remained slightly higher variable (MAE = 3.80).
Among cultivars, Williams (MAE = 1.23) and Sari (1.33) were well predicted,
while Arian (2.86) and Ny\'{\i}rs\'{e}gi fekete (3.74) were comparatively
poorly captured. This framework was also embedded into MLflow, providing a
decision-support tool for optimizing CP seed germination in precision
agriculture.

</details>


### [113] [Quantum Machine Learning for Image Classification: A Hybrid Model of Residual Network with Quantum Support Vector Machine](https://arxiv.org/abs/2510.23659)
*Md. Farhan Shahriyar,Gazi Tanbhir,Abdullah Md Raihan Chy*

Main category: cs.LG

TL;DR: 研究结合了经典深度学习和量子机器学习的方法，使用ResNet-50提取特征，然后通过量子支持向量机进行分类，特别是在马铃薯疾病检测中取得了很好的效果。实验结果表明，基于Z特征映射的量子支持向量机超越了传统模型，准确率达到99.23%。


<details>
  <summary>Details</summary>
Motivation: 在此研究中，为了提高复杂高维数据集下的分类效率，结合了量子计算和传统机器学习方法，特别是在图像分类任务中。提出了在马铃薯疾病检测中的具体应用，并通过实验证实了其优越性。

Method: 研究中采用ResNet-50用于特征提取，之后用PCA降维，再经由量子支持向量机进行分类。量子支持向量机使用了ZZ、Z和Pauli-X等特征映射，将经典数据转换为量子状态。实验中还对比了传统支持向量机和随机森林模型，评价方法为五折交叉验证。

Result: 经过五折交叉验证，基于Z特征映射的量子支持向量机模型取得了最佳效果，准确率达99.23%，优于支持向量机和随机森林模型。

Conclusion: 研究表明，将量子计算融入图像分类任务中可以提高分类效率，并为疾病的检测提供了潜在的解决方案。

Abstract: Recently, there has been growing attention on combining quantum machine
learning (QML) with classical deep learning approaches, as computational
techniques are key to improving the performance of image classification tasks.
This study presents a hybrid approach that uses ResNet-50 (Residual Network)
for feature extraction and Quantum Support Vector Machines (QSVM) for
classification in the context of potato disease detection. Classical machine
learning as well as deep learning models often struggle with high-dimensional
and complex datasets, necessitating advanced techniques like quantum computing
to improve classification efficiency. In our research, we use ResNet-50 to
extract deep feature representations from RGB images of potato diseases. These
features are then subjected to dimensionality reduction using Principal
Component Analysis (PCA). The resulting features are processed through QSVM
models which apply various quantum feature maps such as ZZ, Z, and Pauli-X to
transform classical data into quantum states. To assess the model performance,
we compared it with classical machine learning algorithms such as Support
Vector Machine (SVM) and Random Forest (RF) using five-fold stratified
cross-validation for comprehensive evaluation. The experimental results
demonstrate that the Z-feature map-based QSVM outperforms classical models,
achieving an accuracy of 99.23 percent, surpassing both SVM and RF models. This
research highlights the advantages of integrating quantum computing into image
classification and provides a potential disease detection solution through
hybrid quantum-classical modeling.

</details>


### [114] [AI-Driven Carbon Monitoring: Transformer-Based Reconstruction of Atmospheric CO2 in Canadian Poultry Regions](https://arxiv.org/abs/2510.23663)
*Padmanabhan Jagannathan Prajesh,Kaliaperumal Ragunath,Miriam Gordon,Bruce Rathgeber,Suresh Neethirajan*

Main category: cs.LG

TL;DR: 本文提出了一种名为Spatiotemporal Vision Transformer with Wavelets (ST-ViWT)的框架，该框架能够通过融合小波时间频率表示和变压器注意力，使用较少的观测数据来重建连续且可量化的XCO2场。该模型在2024 OCO-2数据集上表现优异，具备良好的推广能力。结果显示，在高密度家禽养殖区，XCO2排放更高，季节变化更大，夏季变化更显著。ST-ViWT为卫星数据与国家排放清单和精准畜牧业平台的结合提供了可能，有助于精准排放定位和政策制定。


<details>
  <summary>Details</summary>
Motivation: 准确绘制农业景观中的CO2平均柱浓度（XCO2）对于制定减排策略至关重要。本文旨在开发一种基于小波时频表示与变压器注意力的模型，以利用稀疏的卫星观测数据来重建连续、量化不确定性XCO2场，尤其是在家禽密集养殖地区

Method: 本文提出了一种名为Spatiotemporal Vision Transformer with Wavelets (ST-ViWT)的框架。该框架结合了小波时间频谱表示和Transformer注意机制，使用气象学、植被指数、地形和土地覆盖等多源信息来重建XCO2场。该模型能在较少的观测数据下，生成连续且可量化的XCO2场，具备良好的推广能力和不确定性量化能力

Result: 实验结果表明，在2024 OCO-2数据集上，ST-ViWT框架表现出了显著的优势（R² = 0.984, RMSE = 0.468 ppm；92.3%的预测落在±1 ppm范围内）。独立验证表明该模型具备良好的推广能力（偏置=-0.14 ppm，r=0.928）。并且在高密度家禽养殖区，ST-ViWT能够更准确地捕捉到XCO2的季节变化和夏季变化

Conclusion: 通过ST-ViWT模型，可以依据稀疏的观测数据，生成无缝隙的0.25度CO2表面，并具备明确的不确定性量化。该方法支持将卫星约束条件融入国家排放清单中，对特定区域的排放进行精确量化，并验证减排措施。此外，该方法有助于透明、主体明确的碳核算，热点地区优先识别，以及制定具有政策意义的减排评估

Abstract: Accurate mapping of column-averaged CO2 (XCO2) over agricultural landscapes
is essential for guiding emission mitigation strategies. We present a
Spatiotemporal Vision Transformer with Wavelets (ST-ViWT) framework that
reconstructs continuous, uncertainty-quantified XCO2 fields from OCO-2 across
southern Canada, emphasizing poultry-intensive regions. The model fuses wavelet
time-frequency representations with transformer attention over meteorology,
vegetation indices, topography, and land cover. On 2024 OCO-2 data, ST-ViWT
attains R2 = 0.984 and RMSE = 0.468 ppm; 92.3 percent of gap-filled predictions
lie within +/-1 ppm. Independent validation with TCCON shows robust
generalization (bias = -0.14 ppm; r = 0.928), including faithful reproduction
of the late-summer drawdown. Spatial analysis across 14 poultry regions reveals
a moderate positive association between facility density and XCO2 (r = 0.43);
high-density areas exhibit larger seasonal amplitudes (9.57 ppm) and enhanced
summer variability. Compared with conventional interpolation and standard
machine-learning baselines, ST-ViWT yields seamless 0.25 degree CO2 surfaces
with explicit uncertainties, enabling year-round coverage despite sparse
observations. The approach supports integration of satellite constraints with
national inventories and precision livestock platforms to benchmark emissions,
refine region-specific factors, and verify interventions. Importantly,
transformer-based Earth observation enables scalable, transparent, spatially
explicit carbon accounting, hotspot prioritization, and policy-relevant
mitigation assessment.

</details>


### [115] [Transformers from Compressed Representations](https://arxiv.org/abs/2510.23665)
*Juan C. Leon Alcazar,Mattia Soldan,Mohammad Saatialsoruji,Alejandro Pardo,Hani Itani,Juan Camilo Perez,Bernard Ghanem*

Main category: cs.LG

TL;DR: 我们提出了TEMPEST方法，利用压缩文件的字节流结构设计有效的tokenization和编码策略，使得Transformer可以直接从压缩数据流中学习语义表示，从而减少所需token数量，降低计算复杂度和内存使用。实验结果显示，TEMPEST在准确率上与当前最优方法相当，同时在内存和计算资源上更具高效性。


<details>
  <summary>Details</summary>
Motivation: 目前压缩文件格式在数据存储和传输中的高效性已为人所知，然而其在表示学习中的潜力尚未被充分探索。我们希望利用压缩文件的字节流结构来设计出一种新的tokenization和编码策略，以提高数据表示的学习效率。

Method: 提出了TEMPEST方法，该方法通过利用压缩文件的字节流结构，设计了一种有效的tokenization和编码策略，使得标准的Transformer可以直接从压缩数据流中学习语义表示，而不需要原始字节级处理或完整的媒体解码过程。这减少了语义分类所需的token数量，从而降低了计算复杂度和内存使用。

Result: 实验结果表明，TEMPEST方法在多个不同数据集、编码方案和模态上取得了与现有最优方法相当的准确率，同时在内存和计算资源效率方面具有显著优势。

Conclusion: 我们的方法通过利用压缩文件的字节流结构，可以直接从压缩数据流中进行语义表示学习，这降低了计算复杂度和内存使用，同时保持了与现有最优方法相当的准确率。

Abstract: Compressed file formats are the corner stone of efficient data storage and
transmission, yet their potential for representation learning remains largely
underexplored. We introduce TEMPEST (TransformErs froM comPressed
rEpreSenTations), a method that exploits the inherent byte-stream structure of
compressed files to design an effective tokenization and encoding strategy. By
leveraging this compact encoding, a standard transformer can directly learn
semantic representations from compressed data streams, bypassing the need for
raw byte-level processing or full media decoding. Our proposal substantially
reduces the number of tokens required for semantic classification, thereby
lowering both computational complexity and memory usage. Through extensive
experiments across diverse datasets, coding schemes, and modalities, we show
that TEMPEST achieves accuracy competitive wit the state-of-the-art while
delivering efficiency gains in memory and compute.

</details>


### [116] [Optimize Any Topology: A Foundation Model for Shape- and Resolution-Free Structural Topology Optimization](https://arxiv.org/abs/2510.23667)
*Amin Heyrani Nobari,Lyle Regenwetter,Cyril Picard,Ligong Han,Faez Ahmed*

Main category: cs.LG

TL;DR: OAT是一种基础模型框架，用于预测不同条件下的最小合规布局，提高结构拓扑优化的效率和广度，与现有最佳模型相比，可将平均合规性降低多达90%。OAT应用范围广，速度快，不分分辨率，适用于物理感知的拓扑优化。代码和数据可在https://github.com/ahnobari/OptimizeAnyTopology中找到。


<details>
  <summary>Details</summary>
Motivation: 现有的深度学习方法在固定的方格网格、有限的手工编码边界条件以及后期优化的限制下，难以实现普遍适用。因此，研究团队提出了OAT，以解决这些限制并实现对任意结构的高效优化。

Method: OAT将分辨率和形状无关的自动编码器与隐式神经场解码器和条件潜在扩散模型相结合，训练采用了一个新的OpenTO数据集，该数据集包含220万个优化结构，覆盖了200万个独特的边界条件配置。

Result: 在四个公共基准测试和两个具有挑战性的未见过的测试中，OAT相比于最佳的先前方法，平均合规性降低了多达90%。OAT在单个GPU上实现了小于1秒的推理时间，适用于从64x64到256x256的分辨率以及高达10:1的宽高比。

Conclusion: OAT被确立为一种通用、快速且不分分辨率的物理感知拓扑优化框架，并为逆向设计中的生成模型研究提供了一个大规模的数据集。

Abstract: Structural topology optimization (TO) is central to engineering design but
remains computationally intensive due to complex physics and hard constraints.
Existing deep-learning methods are limited to fixed square grids, a few
hand-coded boundary conditions, and post-hoc optimization, preventing general
deployment. We introduce Optimize Any Topology (OAT), a foundation-model
framework that directly predicts minimum-compliance layouts for arbitrary
aspect ratios, resolutions, volume fractions, loads, and fixtures. OAT combines
a resolution- and shape-agnostic autoencoder with an implicit neural-field
decoder and a conditional latent-diffusion model trained on OpenTO, a new
corpus of 2.2 million optimized structures covering 2 million unique
boundary-condition configurations. On four public benchmarks and two
challenging unseen tests, OAT lowers mean compliance up to 90% relative to the
best prior models and delivers sub-1 second inference on a single GPU across
resolutions from 64 x 64 to 256 x 256 and aspect ratios as high as 10:1. These
results establish OAT as a general, fast, and resolution-free framework for
physics-aware topology optimization and provide a large-scale dataset to spur
further research in generative modeling for inverse design. Code & data can be
found at https://github.com/ahnobari/OptimizeAnyTopology.

</details>


### [117] [Traffic flow forecasting, STL decomposition, Hybrid model, LSTM, ARIMA, XGBoost, Intelligent transportation systems](https://arxiv.org/abs/2510.23668)
*Fujiang Yuan,Yangrui Fan,Xiaohuan Bing,Zhen Tian,Chunhong Yuan,Yankang Li*

Main category: cs.LG

TL;DR: 本研究提出了一种基于分解的混合框架，用于交通流量预测，该框架结合了STL分解和LSTM、ARIMA、XGBoost三种模型，分别用于长时趋势、季节周期性和非线性残差波动的预测，提高了预测精度和鲁棒性


<details>
  <summary>Details</summary>
Motivation: 单一模型方法难以捕捉交通流量数据的复杂非线性和跨尺度时间模式，因此需要一种新的方法来提高交通流量预测的准确性

Method: 采用STL分解将原始时间序列分解为趋势、季节性和残差成分，然后分别使用LSTM、ARIMA和XGBoost模型进行预测，并将各子模型预测结果通过乘法集成得到最终预测结果

Result: 基于纽约市一个交叉口2015年11月至12月的998条交通流量记录，结果显示LSTM-ARIMA-XGBoost混合模型在MAE、RMSE和R²等指标上明显优于单独的LSTM、ARIMA和XGBoost模型

Conclusion: 分解策略有效隔离了时间特征，使得每种模型可以专业化，从而提高了预测准确性、可解释性和鲁棒性

Abstract: Accurate traffic flow forecasting is essential for intelligent transportation
systems and urban traffic management. However, single model approaches often
fail to capture the complex, nonlinear, and multi scale temporal patterns in
traffic flow data. This study proposes a decomposition driven hybrid framework
that integrates Seasonal Trend decomposition using Loess (STL) with three
complementary predictive models. STL first decomposes the original time series
into trend, seasonal, and residual components. Then, a Long Short Term Memory
(LSTM) network models long term trends, an Autoregressive Integrated Moving
Average (ARIMA) model captures seasonal periodicity, and an Extreme Gradient
Boosting (XGBoost) algorithm predicts nonlinear residual fluctuations. The
final forecast is obtained through multiplicative integration of the sub model
predictions. Using 998 traffic flow records from a New York City intersection
between November and December 2015, results show that the LSTM ARIMA XGBoost
hybrid model significantly outperforms standalone models including LSTM, ARIMA,
and XGBoost across MAE, RMSE, and R squared metrics. The decomposition strategy
effectively isolates temporal characteristics, allowing each model to
specialize, thereby improving prediction accuracy, interpretability, and
robustness.

</details>


### [118] [DBLoss: Decomposition-based Loss Function for Time Series Forecasting](https://arxiv.org/abs/2510.23672)
*Xiangfei Qiu,Xingjian Wu,Hanyin Cheng,Xvyuan Liu,Chenjuan Guo,Jilin Hu,Bin Yang*

Main category: cs.LG

TL;DR: 提出了一种新的损失函数DBLoss，通过分解时间序列中的季节性和趋势来改进时间序列预测准确性，并在多种真实数据集上验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有基于均方误差（MSE）的损失函数在预测季节性和趋势时存在不足，即使通过正向传播中的分解模块单独建模这些因素也无法完全解决这些问题。因此，提出一种能够更准确地捕捉时间序列中季节性和趋势的损失函数是必要的。

Method: DBLoss使用指数移动平均值将时间序列分解为季节性和趋势分量，在预测范围内分别计算每个分量的损失，并将这些损失加权。作为一种通用的损失函数，DBLoss可以与任何深度学习预测模型结合使用。

Result: 实验结果表明，DBLoss显著改善了最先进的模型在整个真实数据集上的性能，展示了在时间序列损失函数设计上的新视角。

Conclusion: 通过使用DBLoss损失函数，可以有效地提高时间序列预测模型的性能，特别是对于捕捉时间序列中的季节性和趋势具有显著优势。

Abstract: Time series forecasting holds significant value in various domains such as
economics, traffic, energy, and AIOps, as accurate predictions facilitate
informed decision-making. However, the existing Mean Squared Error (MSE) loss
function sometimes fails to accurately capture the seasonality or trend within
the forecasting horizon, even when decomposition modules are used in the
forward propagation to model the trend and seasonality separately. To address
these challenges, we propose a simple yet effective Decomposition-Based Loss
function called DBLoss. This method uses exponential moving averages to
decompose the time series into seasonal and trend components within the
forecasting horizon, and then calculates the loss for each of these components
separately, followed by weighting them. As a general loss function, DBLoss can
be combined with any deep learning forecasting model. Extensive experiments
demonstrate that DBLoss significantly improves the performance of
state-of-the-art models across diverse real-world datasets and provides a new
perspective on the design of time series loss functions.

</details>


### [119] [Informed Initialization for Bayesian Optimization and Active Learning](https://arxiv.org/abs/2510.23681)
*Carl Hvarfner,David Eriksson,Eytan Bakshy,Max Balandat*

Main category: cs.LG

TL;DR: 提出了一种新的先验知识引导的预测探索(HIPE)策略，改进了贝叶斯优化中初始化阶段的性能，特别是在大批次、少量样本的情况下，能够提供更高的预测准确性、更好的超参数识别，进而提升后续优化的表现。


<details>
  <summary>Details</summary>
Motivation: 当前的贝叶斯优化方法多采用空间填充设计初始化，但这种方法忽视了两方面：减少预测不确定性与高效学习超参数之间的冲突；以及空间填充设计可能不适用于所有场景。因此，本文旨在解决这些限制，提出一种新的初始化策略。

Method: 本文提出了一种新的获取策略，称为Hyperparameter-Informed Predictive Exploration (HIPE)，该策略利用信息论原理来平衡减少预测不确定性与超参数学习，从而优化高成本黑箱函数。此外，还提供了Gaussian Process设置下的HIPE的闭式表达式。

Result: 通过广泛的实验，证明了HIPE在主动学习和少量样本贝叶斯优化中的有效性，特别是在大批次、少量样本的场景下，该方法表现明显优于传统的初始化策略。

Conclusion: 该研究提出的新策略HIPE，通过平衡减少预测不确定性与学习超参数之间的关系，在广泛的实验验证中显示出了卓越的性能。

Abstract: Bayesian Optimization is a widely used method for optimizing expensive
black-box functions, relying on probabilistic surrogate models such as Gaussian
Processes. The quality of the surrogate model is crucial for good optimization
performance, especially in the few-shot setting where only a small number of
batches of points can be evaluated. In this setting, the initialization plays a
critical role in shaping the surrogate's predictive quality and guiding
subsequent optimization. Despite this, practitioners typically rely on
(quasi-)random designs to cover the input space. However, such approaches
neglect two key factors: (a) space-filling designs may not be desirable to
reduce predictive uncertainty, and (b) efficient hyperparameter learning during
initialization is essential for high-quality prediction, which may conflict
with space-filling designs. To address these limitations, we propose
Hyperparameter-Informed Predictive Exploration (HIPE), a novel acquisition
strategy that balances predictive uncertainty reduction with hyperparameter
learning using information-theoretic principles. We derive a closed-form
expression for HIPE in the Gaussian Process setting and demonstrate its
effectiveness through extensive experiments in active learning and few-shot BO.
Our results show that HIPE outperforms standard initialization strategies in
terms of predictive accuracy, hyperparameter identification, and subsequent
optimization performance, particularly in large-batch, few-shot settings
relevant to many real-world Bayesian Optimization applications.

</details>


### [120] [On the Societal Impact of Machine Learning](https://arxiv.org/abs/2510.23693)
*Joachim Baumann*

Main category: cs.LG

TL;DR: 该博士论文探讨了机器学习（ML）的社会影响，指出这些数据驱动系统在没有明确公平性考量的情况下开发，可能带来歧视性后果。论文提出了更合适的测量方法，系统分解方法以预防偏见动态，并提出了减少算法歧视的有效干预措施，同时保持系统效用。论文还讨论了ML系统（包括生成性人工智能）在未来社会集成过程中面临的挑战和研究方向，强调了确保ML社会影响与广泛社会价值一致的重要性。


<details>
  <summary>Details</summary>
Motivation: 论文指出随着机器学习系统在决策中的应用越来越广泛，其缺乏公平考虑可能导致歧视性后果，因此论文旨在提出更合适的公平性测量方法和有效的减少歧视的干预措施，以确保机器学习的社会影响与社会价值一致。

Method: 该论文提出了更为合适测量ML系统公平性的方法，进行了系统的ML系统分解来预测偏见动态，提出了有效的减少算法歧视的干预措施，同时保持系统效用。

Result: 论文的贡献在于提出了更合适的测量ML公平性的方法，系统性地分解了ML系统的偏见动态，并提出了减少算法歧视的有效措施，同时保持系统的效用。

Conclusion: 本文强调了在ML系统（包括生成性AI）愈发融入社会的同时，面临的挑战和未来研究方向。也为确保机器学习的社会影响与社会价值一致提供了基础。

Abstract: This PhD thesis investigates the societal impact of machine learning (ML). ML
increasingly informs consequential decisions and recommendations, significantly
affecting many aspects of our lives. As these data-driven systems are often
developed without explicit fairness considerations, they carry the risk of
discriminatory effects. The contributions in this thesis enable more
appropriate measurement of fairness in ML systems, systematic decomposition of
ML systems to anticipate bias dynamics, and effective interventions that reduce
algorithmic discrimination while maintaining system utility. I conclude by
discussing ongoing challenges and future research directions as ML systems,
including generative artificial intelligence, become increasingly integrated
into society. This work offers a foundation for ensuring that ML's societal
impact aligns with broader social values.

</details>


### [121] [MUStReason: A Benchmark for Diagnosing Pragmatic Reasoning in Video-LMs for Multimodal Sarcasm Detection](https://arxiv.org/abs/2510.23727)
*Anisha Saha,Varsha Suresh,Timothy Hospedales,Vera Demberg*

Main category: cs.LG

TL;DR: 介绍了一种新的多模态模型 Benchmark MUStReason 和框架 PragCoT，用于分析检测视频中的讽刺内容，解决了现有模型在多模态依赖和意图推理中的不足


<details>
  <summary>Details</summary>
Motivation: 当前的多模态模型在处理需要跨模态识别相关线索并进行意图推理的复杂任务时存在困难，因此引入了新的 Benchmark 和框架来改善这一问题

Method: 提出了 MUStReason Benchmark，其包含针对不同模态的相关线索注释和推理步骤，以及 PragCoT 框架，该框架使得 VideoLMs 更关注隐含意图而不是字面意思

Result: 新的 Benchmark 和框架能够改善视频中讽刺内容的检测，改进了现有模型在多模态依赖和意图推理上的不足

Conclusion: 通过引入新的 Benchmark 和框架，研究者可以更好地诊断和提升现有模型在处理讽刺语言任务的能力

Abstract: Sarcasm is a specific type of irony which involves discerning what is said
from what is meant. Detecting sarcasm depends not only on the literal content
of an utterance but also on non-verbal cues such as speaker's tonality, facial
expressions and conversational context. However, current multimodal models
struggle with complex tasks like sarcasm detection, which require identifying
relevant cues across modalities and pragmatically reasoning over them to infer
the speaker's intention. To explore these limitations in VideoLMs, we introduce
MUStReason, a diagnostic benchmark enriched with annotations of
modality-specific relevant cues and underlying reasoning steps to identify
sarcastic intent. In addition to benchmarking sarcasm classification
performance in VideoLMs, using MUStReason we quantitatively and qualitatively
evaluate the generated reasoning by disentangling the problem into perception
and reasoning, we propose PragCoT, a framework that steers VideoLMs to focus on
implied intentions over literal meaning, a property core to detecting sarcasm.

</details>


### [122] [Debiasing Reward Models by Representation Learning with Guarantees](https://arxiv.org/abs/2510.23751)
*Ignavier Ng,Patrick Blöbaum,Siddharth Bhandari,Kun Zhang,Shiva Kasiviswanathan*

Main category: cs.LG

TL;DR: 本文提出了一种框架，旨在减少奖励模型中的偏差，同时保持反映预期偏好的潜在因素。通过变分推理来恢复这些潜在变量，并利用它们来训练奖励模型，实验表明，该方法有效地缓解了虚假相关性问题，并产生了更稳健的奖励模型。


<details>
  <summary>Details</summary>
Motivation: 现有的对齐技术，如从人类反馈中学习的强化学习，存在利用虚假相关性的问题，例如响应长度、歧视、奉承和概念偏差。本文旨在解决这些偏见问题。

Method: 首先，给出了一个数据生成过程的公式化，假设观察到的数据是由虚假和非虚假潜在变量生成的。设计了使用变分推理来恢复这些变量，从而训练奖励模型的方法。

Result: 实验结果表明，该方法能够有效缓解虚假相关性问题，并提高奖励模型的鲁棒性。

Conclusion: 提出的方法能够通过理论手段识别非虚假潜在变量，并通过变分推理恢复这些变量来训练奖励模型，最终提高了奖励模型的鲁棒性和准确性。

Abstract: Recent alignment techniques, such as reinforcement learning from human
feedback, have been widely adopted to align large language models with human
preferences by learning and leveraging reward models. In practice, these models
often exploit spurious correlations, involving, e.g., response length,
discrimination, sycophancy, and conceptual bias, which is a problem that has
received increasing attention. In this work, we propose a principled framework
that mitigates these biases in reward models while preserving the underlying
factors that reflect intended preferences. We first provide a formulation of
the data-generating process, assuming that the observed data (e.g., text) is
generated from both spurious and non-spurious latent variables. We show that,
interestingly, these non-spurious latent variables can be theoretically
identified from data, regardless of whether a surrogate for the spurious latent
variables is available. This further inspires a practical method that uses
variational inference to recover these variables and leverages them to train
reward models. Experiments on synthetic and real-world datasets demonstrate
that our method effectively mitigates spurious correlation issues and yields
more robust reward models.

</details>


### [123] [Revealing the Potential of Learnable Perturbation Ensemble Forecast Model for Tropical Cyclone Prediction](https://arxiv.org/abs/2510.23794)
*Jun Liu,Tao Zhou,Jiarui Li,Xiaohui Zhong,Peng Zhang,Jie Feng,Lei Chen,Hao Li*

Main category: cs.LG

TL;DR: FuXi-ENS, a learnable perturbation scheme for ensemble generation, shows clear advantages in predicting tropical cyclone-related physical variables and track forecasts compared to ECMWF-ENS, while also better capturing large-scale circulation and moisture turbulent energy distribution.


<details>
  <summary>Details</summary>
Motivation: 当前的集合预报系统受制于高计算成本和有限的能力来充分表示大气非线性。FuXi-ENS作为一种新型的AI基于预报范式，旨在通过可学习的扰动方案来改进热带气旋的预报性能, 它的应用有助于提高极端天气事件的社会影响预报技巧.

Method: 该研究系统地比较了FuXi-ENS和ECMWF-ENS在2018年全球90个热带气旋中的表现，特别是在热带气旋相关的物理变量、路径和强度预测，以及相关的动力和热力场.

Result: FuXi-ENS在预测热带气旋相关物理变量和路径预测方面有明显的优势，尽管在强度预测方面仍低于观测值。动力和热力分析进一步揭示，FuXi-ENS更好地捕捉了大尺度环流，并且将与热带气旋暖核心相关的湿润湍流能量更集中地聚集在一起，而ECMWF-ENS则显示出更分散的分布.

Conclusion: 这项研究强调了可学习扰动方案在提高热带气旋预报技巧方面的潜在作用，并为通过AI方法提高极端天气事件预测的社会影响提供了有价值的见解.

Abstract: Tropical cyclones (TCs) are highly destructive and inherently uncertain
weather systems. Ensemble forecasting helps quantify these uncertainties, yet
traditional systems are constrained by high computational costs and limited
capability to fully represent atmospheric nonlinearity. FuXi-ENS introduces a
learnable perturbation scheme for ensemble generation, representing a novel
AI-based forecasting paradigm. Here, we systematically compare FuXi-ENS with
ECMWF-ENS using all 90 global TCs in 2018, examining their performance in
TC-related physical variables, track and intensity forecasts, and the
associated dynamical and thermodynamical fields. FuXi-ENS demonstrates clear
advantages in predicting TC-related physical variables, and achieves more
accurate track forecasts with reduced ensemble spread, though it still
underestimates intensity relative to observations. Further dynamical and
thermodynamical analyses reveal that FuXi-ENS better captures large-scale
circulation, with moisture turbulent energy more tightly concentrated around
the TC warm core, whereas ECMWF-ENS exhibits a more dispersed distribution.
These findings highlight the potential of learnable perturbations to improve TC
forecasting skill and provide valuable insights for advancing AI-based ensemble
prediction of extreme weather events that have significant societal impacts.

</details>


### [124] [Learning Interpretable Features in Audio Latent Spaces via Sparse Autoencoders](https://arxiv.org/abs/2510.23802)
*Nathan Paek,Yongyi Zang,Qihui Yang,Randal Leistikow*

Main category: cs.LG

TL;DR: 本文提出了一种框架，通过将音频生成模型的潜在表示映射到人类可理解的声学概念来解释这些模型。该方法依赖于稀疏自编码器，旨在使音频生成过程中的声学属性（如音高、响度和音色）更容易被控制和分析。该框架被验证在连续和离散的音频潜在空间上都有效果，并且可以扩展到可视化的潜在空间生成模型的解释性分析中。


<details>
  <summary>Details</summary>
Motivation: 音频的密集性质使得自动特征刻画变得困难，并且应用稀疏自编码器到音频生成时会面临挑战。该研究目的是将音频生成模型的潜在表示映射到人类可理解的声学概念上，从而使得音频生成这一过程中的声学属性更容易被理解和控制。

Method: 训练稀疏自编码器来处理音频自动编码器的潜在表示，然后学习从稀疏自编码器特征到离散化声学属性（音高、振幅、音色）的线性映射，从而实现可控的操控及分析过程。这个方法验证了在连续和离散音频潜在空间中的有效性，并且展示了如何分析其中声学属性的生成过程。此外，这个框架还可以被扩展到可视化潜在空间生成模型的解释性分析当中。

Result: 该研究成功实现了将音频自编码器潜在表示映射到人类可理解的声学概念上的目标，揭示了音高等属性在音生成过程中的演化，从而提高了音频生成模型的可解释性和可控性。

Conclusion: 这项研究提出的一个框架，能够使音频生成模型中的声学属性变得易于人理解。这一方法验证了在连续和离散音频潜在空间上的有效性，并且显示出可以在可视化的潜在空间生成模型的解释性分析中应用。

Abstract: While sparse autoencoders (SAEs) successfully extract interpretable features
from language models, applying them to audio generation faces unique
challenges: audio's dense nature requires compression that obscures semantic
meaning, and automatic feature characterization remains limited. We propose a
framework for interpreting audio generative models by mapping their latent
representations to human-interpretable acoustic concepts. We train SAEs on
audio autoencoder latents, then learn linear mappings from SAE features to
discretized acoustic properties (pitch, amplitude, and timbre). This enables
both controllable manipulation and analysis of the AI music generation process,
revealing how acoustic properties emerge during synthesis. We validate our
approach on continuous (DiffRhythm-VAE) and discrete (EnCodec, WavTokenizer)
audio latent spaces, and analyze DiffRhythm, a state-of-the-art text-to-music
model, to demonstrate how pitch, timbre, and loudness evolve throughout
generation. While our work is only done on audio modality, our framework can be
extended to interpretable analysis of visual latent space generation models.

</details>


### [125] [How do simple rotations affect the implicit bias of Adam?](https://arxiv.org/abs/2510.23804)
*Adela DePavia,Vasileios Charisopoulos,Rebecca Willett*

Main category: cs.LG

TL;DR: 自适应梯度法在机器学习中广泛应用，但其对所学习模型泛化性能的影响仍不清楚。与梯度下降相比，自适应梯度法存在丰富的偏差，可以帮助模型学习更接近贝叶斯最优决策边界的非线性决策边界。但这种方法对特征空间的正交变换敏感，可以通过正交变换来恢复该方法对丰富决策边界偏向。


<details>
  <summary>Details</summary>
Motivation: 研究自适应梯度法（如Adam和Adagrad）对学习到的模型泛化性能的影响，对比其与常规梯度下降方法的表现差异，探索如何缓解Adam方法对特征空间正交变换敏感的问题及给出解决方案以保留其优点。

Method: 针对自适应梯度法存在的问题，提出了一种新的参数化方法，对优化目标应用正交变换，使其对数据旋转具有等变性。

Result: 实验结果显示，提出的参数化方法能够恢复Adam方法对丰富决策边界的偏向，改善其学习曲线及最终模型的决策边界。

Conclusion: 自适应梯度方法在学习过程中展现出倾向于复杂度较高的决策边界的特点，但其对特征空间的旋转是脆弱的。通过引入正交变换可以增强其鲁棒性同时保留其原本的优点。

Abstract: Adaptive gradient methods such as Adam and Adagrad are widely used in machine
learning, yet their effect on the generalization of learned models -- relative
to methods like gradient descent -- remains poorly understood. Prior work on
binary classification suggests that Adam exhibits a ``richness bias,'' which
can help it learn nonlinear decision boundaries closer to the Bayes-optimal
decision boundary relative to gradient descent. However, the coordinate-wise
preconditioning scheme employed by Adam renders the overall method sensitive to
orthogonal transformations of feature space. We show that this sensitivity can
manifest as a reversal of Adam's competitive advantage: even small rotations of
the underlying data distribution can make Adam forfeit its richness bias and
converge to a linear decision boundary that is farther from the Bayes-optimal
decision boundary than the one learned by gradient descent. To alleviate this
issue, we show that a recently proposed reparameterization method -- which
applies an orthogonal transformation to the optimization objective -- endows
any first-order method with equivariance to data rotations, and we empirically
demonstrate its ability to restore Adam's bias towards rich decision
boundaries.

</details>


### [126] [A Physics-informed Multi-resolution Neural Operator](https://arxiv.org/abs/2510.23810)
*Sumanta Roy,Bahador Bahmani,Ioannis G. Kevrekidis,Michael D. Shields*

Main category: cs.LG

TL;DR: 介绍了一种基于物理信息的算子学习方法，通过扩展分辨率无关神经算子框架，使其在完全不需要数据的情况下工作，该方法能够处理不同分辨率的数据，并使用多层感知器进行算子逼近。


<details>
  <summary>Details</summary>
Motivation: 在实际工程应用中，高精度训练数据难以获取，且训练数据的网格分辨率可能在不同样本之间存在差异。因此，开发无需训练数据并且能够处理不同分辨率数据的算子学习方法是必要的。

Method: 利用预训练的基函数将输入函数投影到一个隐含嵌入空间中，然后使用多层感知器基于隐含码和时空坐标生成物理空间中的解。部分微分方程在物理空间中通过有限差分求解器强制执行。此外，这种方法不需要任何训练数据，仅依赖于物理知识。

Result: 在多分辨率数据的数值示例中验证了该方法的性能，输入函数在不同分辨率上采样，包括粗略和精细离散化。实验结果显示该方法有效且具有潜力。

Conclusion: 提出了一种新的算子学习方法，即扩展的分辨率无关神经算子框架，能够在完全数据自由的情况下工作，有效解决了不同分辨率数据的挑战。

Abstract: The predictive accuracy of operator learning frameworks depends on the
quality and quantity of available training data (input-output function pairs),
often requiring substantial amounts of high-fidelity data, which can be
challenging to obtain in some real-world engineering applications. These
datasets may be unevenly discretized from one realization to another, with the
grid resolution varying across samples. In this study, we introduce a
physics-informed operator learning approach by extending the Resolution
Independent Neural Operator (RINO) framework to a fully data-free setup,
addressing both challenges simultaneously. Here, the arbitrarily (but
sufficiently finely) discretized input functions are projected onto a latent
embedding space (i.e., a vector space of finite dimensions), using pre-trained
basis functions. The operator associated with the underlying partial
differential equations (PDEs) is then approximated by a simple multi-layer
perceptron (MLP), which takes as input a latent code along with spatiotemporal
coordinates to produce the solution in the physical space. The PDEs are
enforced via a finite difference solver in the physical space. The validation
and performance of the proposed method are benchmarked on several numerical
examples with multi-resolution data, where input functions are sampled at
varying resolutions, including both coarse and fine discretizations.

</details>


### [127] [Combining SHAP and Causal Analysis for Interpretable Fault Detection in Industrial Processes](https://arxiv.org/abs/2510.23817)
*Pedro Cortes dos Santos,Matheus Becali Rocha,Renato A Krohling*

Main category: cs.LG

TL;DR: 研究开发了一个新的故障检测框架，结合SHAP和因果分析，来改善工业过程中的故障检测性能和可解释性，特别针对复杂的数据环境和TE过程的挑战。生成的因果结构与SHAP的发现相吻合，提高了故障检测的准确性，并为操作人员提供了清晰的见解。


<details>
  <summary>Details</summary>
Motivation: 工业过程产生了复杂的数据，传统的故障检测系统面临性能和解释性的挑战。为了更好地解决这些问题，研究人员试图通过引入SHAP和因果图的方法来提高精度和可解释性，特别是在TE过程这种具有复杂动态特性的典型场景中。

Method: 该研究首先使用标准模型尝试故障检测，但发现性能和解释性有限。然后使用SHAP将问题转化为更易管理和透明的形式，并利用多种算法生成的有向无环图进行因果分析，揭示故障传播的机制。

Result: 通过这种方法，研究成功地提高了故障检测的准确性和解释性。生成的因果图与SHAP的结果一致，都强调了冷却系统和分离系统等关键过程因素对故障发展的重要性。这些方法为操作人员提供了更清晰、可操作的关于故障来源的洞察。

Conclusion: 该研究提出了一种结合SHAP和因果图的方法框架，以提高故障检测的准确性和可解释性。这种方法不仅增强了预测能力，还提供了对潜在故障机制的理解，有助于改进复杂制造环境下的监测和故障预防。

Abstract: Industrial processes generate complex data that challenge fault detection
systems, often yielding opaque or underwhelming results despite advanced
machine learning techniques. This study tackles such difficulties using the
Tennessee Eastman Process, a well-established benchmark known for its intricate
dynamics, to develop an innovative fault detection framework. Initial attempts
with standard models revealed limitations in both performance and
interpretability, prompting a shift toward a more tractable approach. By
employing SHAP (SHapley Additive exPlanations), we transform the problem into a
more manageable and transparent form, pinpointing the most critical process
features driving fault predictions. This reduction in complexity unlocks the
ability to apply causal analysis through Directed Acyclic Graphs, generated by
multiple algorithms, to uncover the underlying mechanisms of fault propagation.
The resulting causal structures align strikingly with SHAP findings,
consistently highlighting key process elements-like cooling and separation
systems-as pivotal to fault development. Together, these methods not only
enhance detection accuracy but also provide operators with clear, actionable
insights into fault origins, a synergy that, to our knowledge, has not been
previously explored in this context. This dual approach bridges predictive
power with causal understanding, offering a robust tool for monitoring complex
manufacturing environments and paving the way for smarter, more interpretable
fault detection in industrial systems.

</details>


### [128] [ScaLoRA: Optimally Scaled Low-Rank Adaptation for Efficient High-Rank Fine-Tuning](https://arxiv.org/abs/2510.23818)
*Yilang Zhang,Xiaodong Yang,Yiwei Cai,Georgios B. Giannakis*

Main category: cs.LG

TL;DR: 提出了一种新的方法，通过累积连续的低秩增量来构建高秩权重更新，从而在不过度增加计算成本的前提下改进LLM的微调过程，实验证明这种方法在多种任务上都取得了更快的收敛速度和更好的性能。


<details>
  <summary>Details</summary>
Motivation: 提出这种方法是为了减少大规模语言模型在特定任务上进行微调时的计算成本，同时避免传统低秩适配方法在有效性上可能存在的问题。

Method: 通过累积连续的低秩增量来构建高秩权重更新，每个更新迭代中寻找最优的低秩矩阵来最小化损失函数并接近全微调效果。同时，通过适当地调整低秩矩阵的列来实现高效的优化并避免重新开始过程。

Result: 实验结果表明该方法在多个不同的任务上的性能表现优于现有的低秩适配方法，同时具有更快的收敛速度。

Conclusion: 该提出的方法提供了一种有效的方法，在降低大规模语言模型计算成本的同时，保持了较好的微调效果和快速的收敛速度。

Abstract: As large language models (LLMs) continue to scale in size, the computational
overhead has become a major bottleneck for task-specific fine-tuning. While
low-rank adaptation (LoRA) effectively curtails this cost by confining the
weight updates to a low-dimensional subspace, such a restriction can hinder
effectiveness and slow convergence. This contribution deals with these
limitations by accumulating progressively a high-rank weight update from
consecutive low-rank increments. Specifically, the per update optimal low-rank
matrix is identified to minimize the loss function and closely approximate full
fine-tuning. To endow efficient and seamless optimization without restarting,
this optimal choice is formed by appropriately scaling the columns of the
original low-rank matrix. Rigorous performance guarantees reveal that the
optimal scaling can be found analytically. Extensive numerical tests with
popular LLMs scaling up to 12 billion parameters demonstrate a consistent
performance gain and fast convergence relative to state-of-the-art LoRA
variants on diverse tasks including natural language understanding, commonsense
reasoning, and mathematical problem solving.

</details>


### [129] [A PDE-Informed Latent Diffusion Model for 2-m Temperature Downscaling](https://arxiv.org/abs/2510.23866)
*Paul Rosu,Muchang Bahng,Erick Jiang,Rico Zhu,Vahid Tarokh*

Main category: cs.LG

TL;DR: 本文提出了一种基于物理条件的潜在扩散模型，用于大气数据的动力下采样，重点重构2米温度场。通过在模型中加入PDE损失项来增强物理一致性。实验表明，这种额外的损失项可以进一步正则化模型，提高生成场的物理合理性。代码开源在Github上。


<details>
  <summary>Details</summary>
Motivation: 为了提高大气数据动力下采样的物理一致性，特别是在2米温度场的高分辨率重建中，本文尝试在已有的扩散模型基础上，引入物理条件的约束。物理过程的引入有助于提升模型生成数据的物理合理性。

Method: 基于现有的扩散架构，并结合参考UNet的残差形式，将PDE损失项引入到模型训练目标中，PDE损失通过有限差分法计算全分辨率（像素级别）的空间残差，进而用来强制模型输出数据的物理一致性。

Result: 实验发现，即使没有引入PDE损失项，常规的扩散训练也能达到很低的PDE残差。进一步的微调实验表明，引入PDE损失项能进一步正则化模型，并提高生成场的物理可信度。

Conclusion: 本文提出的方法在物理条件下，利用扩散模型和PDE损失项来改进大气数据的动力下采样，这种方法不仅保留了原有的数据生成能力，还提高了数据的物理一致性。

Abstract: This work presents a physics-conditioned latent diffusion model tailored for
dynamical downscaling of atmospheric data, with a focus on reconstructing
high-resolution 2-m temperature fields. Building upon a pre-existing diffusion
architecture and employing a residual formulation against a reference UNet, we
integrate a partial differential equation (PDE) loss term into the model's
training objective. The PDE loss is computed in the full resolution (pixel)
space by decoding the latent representation and is designed to enforce physical
consistency through a finite-difference approximation of an effective
advection-diffusion balance. Empirical observations indicate that conventional
diffusion training already yields low PDE residuals, and we investigate how
fine-tuning with this additional loss further regularizes the model and
enhances the physical plausibility of the generated fields. The entirety of our
codebase is available on Github, for future reference and development.

</details>


### [130] [GIFT: Group-relative Implicit Fine Tuning Integrates GRPO with DPO and UNA](https://arxiv.org/abs/2510.23868)
*Zhichao Wang*

Main category: cs.LG

TL;DR: 提出了GIFT框架，一种新的对齐LLM的强化学习框架，通过最小化隐式奖励模型和显式奖励模型之间的差异来对齐大型语言模型，而不是直接最大化累积奖励。GIFT将三种关键思想相结合，解决了以往方法中的某些问题，并且在数学基准测试中表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有方法如PPO、GRPO等直接最大化累积奖励或存在过度拟合、非凸优化问题，需要更多超参数。为解决这些问题并提高收敛速度和泛化能力，提出了基于最小化奖励模型差异的新型对齐框架GIFT，同时保持效率和稳定性。

Method: GIFT结合了GRPO的在线多响应生成和归一化、DPO的隐式奖励公式化以及UNA的隐式-显式奖励校准原则。通过同时归一化隐式和显式奖励，将复杂的奖励最大化目标转换为简化的均方误差损失函数，从而将非凸优化问题转化为凸、稳定、可分析的微分形式。该方法是非离线方法，因此保留了探索能力。

Result: 实验表明，GIFT在数学基准测试中的推理和对齐性能更优，且计算效率更高。与GRPO相比，它需要更少的超参数，收敛速度更快，且泛化能力更强，显著减少训练过拟合。

Conclusion: 提出了一种新的对齐大型语言模型（LLM）的优化框架GIFT，通过结合三种关键思想，解决了现有方法存在的问题，达到了更好的性能。

Abstract: I propose \textbf{G}roup-relative \textbf{I}mplicit \textbf{F}ine
\textbf{T}uning (GIFT), a novel reinforcement learning framework for aligning
LLMs. Instead of directly maximizing cumulative rewards like PPO or GRPO, GIFT
minimizes the discrepancy between implicit and explicit reward models. It
combines three key ideas: (1) the online multi-response generation and
normalization of GRPO, (2) the implicit reward formulation of DPO, and (3) the
implicit-explicit reward alignment principle of UNA. By jointly normalizing the
implicit and explicit rewards, GIFT eliminates an otherwise intractable term
that prevents effective use of implicit rewards. This normalization transforms
the complex reward maximization objective into a simple mean squared error
(MSE) loss between the normalized reward functions, converting a non-convex
optimization problem into a convex, stable, and analytically differentiable
formulation. Unlike offline methods such as DPO and UNA, GIFT remains on-policy
and thus retains exploration capability. Compared to GRPO, it requires fewer
hyperparameters, converges faster, and generalizes better with significantly
reduced training overfitting. Empirically, GIFT achieves superior reasoning and
alignment performance on mathematical benchmarks while remaining
computationally efficient.

</details>


### [131] [Preference Learning with Response Time: Robust Losses and Guarantees](https://arxiv.org/abs/2505.22820)
*Ayush Sawarni,Sahasrajit Sarmasarkar,Vasilis Syrgkanis*

Main category: cs.LG

TL;DR: 本文研究了将响应时间数据整合到人类偏好学习框架中的方法，以更有效地提取奖励模型。提出了一种新的方法，通过结合二元选择数据和响应时间信息来改善奖励模型学习，这种新方法提高了样本效率，尤其在处理线性奖励函数时，其误差率仅为指数级增长的一小部分，在非参数奖励函数空间中也得到了收敛性保证。实验表明该方法在处理图像偏好学习时的有效性得到了验证。


<details>
  <summary>Details</summary>
Motivation: 传统的人类偏好学习框架主要依赖二元偏好数据，但忽略了用户决策过程中有价值的时间信息。本文希望通过引入响应时间数据，以期提高奖励模型的提取效率和准确性，尤其是在处理线性奖励函数时克服指数级误差率的短板。

Method: 该研究基于证据积累漂移扩散(EZ)模型的原理，提出了利用响应时间数据的新方法，通过构建Neyman正交损失函数实现了奖励模型学习的最优理论收敛率，同时延伸到了非参数奖励函数空间，保证了在更多现实奖励模型中的收敛性。实验在图像偏好学习中进行了验证。

Result: 通过将响应时间信息融入偏好学习，显著提高了线性奖励函数的学习效率，从指数级的误差率下降到了多阶多项式级，其在非参数奖励函数空间中的收敛性也得到了保证，实验结果证明了所提方法的有效性。

Conclusion: 本文提出的新方法设法通过引入响应时间数据提高偏好学习的效率，有效降低了误差率，并在多种奖励函数中都表现出较好的收敛性，为更复杂、现实的实际应用提供了理论基础和实验验证。

Abstract: This paper investigates the integration of response time data into human
preference learning frameworks for more effective reward model elicitation.
While binary preference data has become fundamental in fine-tuning foundation
models, generative AI systems, and other large-scale models, the valuable
temporal information inherent in user decision-making remains largely
unexploited. We propose novel methodologies to incorporate response time
information alongside binary choice data, leveraging the Evidence Accumulation
Drift Diffusion (EZ) model, under which response time is informative of the
preference strength. We develop Neyman-orthogonal loss functions that achieve
oracle convergence rates for reward model learning, matching the theoretical
optimal rates that would be attained if the expected response times for each
query were known a priori. Our theoretical analysis demonstrates that for
linear reward functions, conventional preference learning suffers from error
rates that scale exponentially with reward magnitude. In contrast, our response
time-augmented approach reduces this to polynomial scaling, representing a
significant improvement in sample efficiency. We extend these guarantees to
non-parametric reward function spaces, establishing convergence properties for
more complex, realistic reward models. Our extensive experiments validate our
theoretical findings in the context of preference learning over images.

</details>


### [132] [RS-ORT: A Reduced-Space Branch-and-Bound Algorithm for Optimal Regression Trees](https://arxiv.org/abs/2510.23901)
*Cristobal Heredia,Pedro Chumpitaz-Flores,Kaixun Hua*

Main category: cs.LG

TL;DR: 文章提出了Reduced-Space Optimal Regression Trees (RS-ORT)，这是一种专门针对回归任务设计的分支定界算法，能够在处理大规模连续数据时保证全局最优性，同时保持较低的树深度和良好的泛化能力。此算法通过节点级别的分解和多种边界收紧技术加速训练过程，并可以实现并行执行，从而提升计算效率，适用于包含百万样本的数据集。


<details>
  <summary>Details</summary>
Motivation: 现有的MIP回归任务方法要么仅限于二进制特征，要么在处理连续大规模数据时变得计算上无法解决。单纯将连续特征二值化会损失全局最优性。文章意在解决这些问题，提出一种新的方法来学习最优决策树。

Method: 文章将最优回归树训练重新定义为一个两阶段优化问题，并提出了一种专门的分支定界算法RS-ORT，该算法只在树结构变量上分支。通过利用模型结构，引入了多种边界收紧技术来加速训练过程，同时该方法还能实现并行执行，提高计算效率。

Result: 在多个包含二进制和连续特征的回归基准数据集上的实验证明，RS-ORT在训练性能和测试性能方面均超过了现有方法，并且在具有连续特征的数据集上，能够在4小时内，获得保证的训练性能，具有更简单的树结构和更强的泛化能力。

Conclusion: RS-ORT通过专门的分支定界算法设计和一系列优化技术，保证了算法在处理大规模连续数据集时的全局最优性、计算效率和泛化能力。

Abstract: Mixed-integer programming (MIP) has emerged as a powerful framework for
learning optimal decision trees. Yet, existing MIP approaches for regression
tasks are either limited to purely binary features or become computationally
intractable when continuous, large-scale data are involved. Naively binarizing
continuous features sacrifices global optimality and often yields needlessly
deep trees. We recast the optimal regression-tree training as a two-stage
optimization problem and propose Reduced-Space Optimal Regression Trees
(RS-ORT) - a specialized branch-and-bound (BB) algorithm that branches
exclusively on tree-structural variables. This design guarantees the
algorithm's convergence and its independence from the number of training
samples. Leveraging the model's structure, we introduce several bound
tightening techniques - closed-form leaf prediction, empirical threshold
discretization, and exact depth-1 subtree parsing - that combine with
decomposable upper and lower bounding strategies to accelerate the training.
The BB node-wise decomposition enables trivial parallel execution, further
alleviating the computational intractability even for million-size datasets.
Based on the empirical studies on several regression benchmarks containing both
binary and continuous features, RS-ORT also delivers superior training and
testing performance than state-of-the-art methods. Notably, on datasets with up
to 2,000,000 samples with continuous features, RS-ORT can obtain guaranteed
training performance with a simpler tree structure and a better generalization
ability in four hours.

</details>


### [133] [Group Interventions on Deep Networks for Causal Discovery in Subsystems](https://arxiv.org/abs/2510.23906)
*Wasim Ahmad,Maha Shadaydeh,Joachim Denzler*

Main category: cs.LG

TL;DR: 提出了一种新型的多组因果发现方法gCDMI，利用深度学习对时间序列组的结构关系进行建模，并通过群组干预和模型不变性测试来推断因果关系，该方法在识别群组因果关系方面优于现有方法，并在实际数据集上验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 大多数现有的因果发现方法主要关注变量对之间的因果关系，忽略了变量组之间的集体因果影响。本文提出了一种新的方法来解决这个问题，从而更好地理解复杂系统的因果结构。

Method: 该方法包括三个步骤：第一，使用深度学习技术对所有时间序列组的结构关系进行联合建模。第二，对训练后的模型进行群组干预。第三，通过模型不变性测试来确定变量组之间的因果关系的存在性。

Result: 我们在模拟和实际数据集上对方法进行了评估，证明了本方法在识别群组因果关系方面的优越性，特别是在神经科学和气候科学等领域的复杂因果结构方面。

Conclusion: 本研究展示了gCDMI在处理多变量时间序列数据中的复杂因果结构方面的有效性。该方法通过深度学习模型中的群组干预和模型不变性测试，提供了一种新颖而强大的方法来识别因果关系。

Abstract: Causal discovery uncovers complex relationships between variables, enhancing
predictions, decision-making, and insights into real-world systems, especially
in nonlinear multivariate time series. However, most existing methods primarily
focus on pairwise cause-effect relationships, overlooking interactions among
groups of variables, i.e., subsystems and their collective causal influence. In
this study, we introduce gCDMI, a novel multi-group causal discovery method
that leverages group-level interventions on trained deep neural networks and
employs model invariance testing to infer causal relationships. Our approach
involves three key steps. First, we use deep learning to jointly model the
structural relationships among groups of all time series. Second, we apply
group-wise interventions to the trained model. Finally, we conduct model
invariance testing to determine the presence of causal links among variable
groups. We evaluate our method on simulated datasets, demonstrating its
superior performance in identifying group-level causal relationships compared
to existing methods. Additionally, we validate our approach on real-world
datasets, including brain networks and climate ecosystems. Our results
highlight that applying group-level interventions to deep learning models,
combined with invariance testing, can effectively reveal complex causal
structures, offering valuable insights for domains such as neuroscience and
climate science.

</details>


### [134] [Key and Value Weights Are Probably All You Need: On the Necessity of the Query, Key, Value weight Triplet in Decoder-Only Transformers](https://arxiv.org/abs/2510.23912)
*Marko Karbevski,Antonij Mijoski*

Main category: cs.LG

TL;DR: 理论上证明了查询权重在注意力机制中的冗余性，并在简化假设下验证了减少查询权重可以降低模型参数数量而不会影响性能，为大规模模型中的查询权重冗余性研究提供了理论依据和实证支持。


<details>
  <summary>Details</summary>
Motivation: 探讨查询、键值三元组在注意力机制中的必要性，特别是查询权重是否是必需的，这一理论研究为大模型参数优化提供了可能的方向。

Method: 在简化假设下理论证明查询权重的冗余性，并通过实际训练验证了减少查询权重后的模型在GPT-3小模型架构中的性能。

Result: 减少了超过8%的非嵌入层/lm头参数，且验证损失与标准基准相似，表明减少查询权重不会显著影响模型性能。

Conclusion: 减少了模型中的参数数量，同时没有牺牲模型的性能，为大规模语言模型的参数优化提供了新的思路。

Abstract: The Query, Key, Value weight triplet is a building block of current attention
mechanisms in state-of-the-art LLMs. We theoretically investigate whether this
triplet can be reduced, proving under simplifying assumptions that the Query
weights are redundant, thereby reducing the number of non-embedding/lm-head
parameters by over 8%. We validate the theory on full-complexity GPT-3 small
architectures (with layer normalization, skip connections, and weight decay)
trained from scratch, demonstrating that the reduced model achieves comparable
validation loss to standard baselines. These findings motivate the
investigation of the Query weight redundancy at scale.

</details>


### [135] [Geometry-Inspired Unified Framework for Discounted and Average Reward MDPs](https://arxiv.org/abs/2510.23914)
*Arsenii Mustafin,Xinyi Sheng,Dominik Baumann*

Main category: cs.LG

TL;DR: 本文将马尔可夫决策过程（MDP）的几何解释从折扣奖励情况扩展到平均奖励情况，并证明了在唯一的、遍历的最优策略下，价值迭代算法达到了几何收敛速率.


<details>
  <summary>Details</summary>
Motivation: 传统上，马尔可夫决策过程（MDP）的理论分析分为折扣奖励情况和平均奖励情况，尽管这两者有相似之处，但通常分别进行分析。通过将几何解释从折扣奖励情况扩展到平均奖励情况，统一了这两者的分析方法，从而可以将折扣奖励情况下的一个重要结果延伸到平均奖励情况.

Method: 通过引入几何方法解释平均奖励情况下的马尔可夫决策过程问题，进而分析价值迭代算法在唯一且遍历的最优策略下的收敛性质.

Result: 在唯一的、遍历的最优策略下，价值迭代算法在平均奖励情况下的收敛速率是几何的，这与折扣奖励情况下的结果一致.

Conclusion: 通过为平均奖励情况下的马尔可夫决策过程引入几何解释，得出了一个与折扣奖励情况相应的结果，即在唯一且遍历的最优策略下，价值迭代算法达到几何收敛速率.

Abstract: The theoretical analysis of Markov Decision Processes (MDPs) is commonly
split into two cases - the average-reward case and the discounted-reward case -
which, while sharing similarities, are typically analyzed separately. In this
work, we extend a recently introduced geometric interpretation of MDPs for the
discounted-reward case to the average-reward case, thereby unifying both. This
allows us to extend a major result known for the discounted-reward case to the
average-reward case: under a unique and ergodic optimal policy, the Value
Iteration algorithm achieves a geometric convergence rate.

</details>


### [136] [Differential Privacy: Gradient Leakage Attacks in Federated Learning Environments](https://arxiv.org/abs/2510.23931)
*Miguel Fernandez-de-Retana,Unai Zulaika,Rubén Sánchez-Corcuera,Aitor Almeida*

Main category: cs.LG

TL;DR: 研究了差分隐私机制（DP-SGD和PDP-SGD）在联邦学习中的有效性，以防御梯度泄露攻击。结果表明，DP-SGD显著减少攻击风险，但有轻微的模型性能损失。而PDP-SGD虽然保持了分类性能，但对还原攻击无效。这强调了在分布式学习中实证评估隐私机制的重要性，而不仅仅是理论保证。


<details>
  <summary>Details</summary>
Motivation: 研究差分隐私机制作为防御联邦学习中的梯度泄露攻击的方法的有效性。

Method: 评估了几个计算机视觉模型在不同隐私条件下使用DP-SGD和PDP-SGD训练的结果，并分析了在模拟的联邦学习环境中通过截获的梯度恢复的私人数据质量。

Result: DP-SGD显著减轻了梯度泄露攻击的风险，尽管会导致一定的模型性能损失。PDP-SGD在分类性能上表现良好，但对还原攻击无效。

Conclusion: 实证评估隐私机制在防御梯度泄露攻击中的有效性至关重要，特别是在联邦学习等分布式学习场景中。

Abstract: Federated Learning (FL) allows for the training of Machine Learning models in
a collaborative manner without the need to share sensitive data. However, it
remains vulnerable to Gradient Leakage Attacks (GLAs), which can reveal private
information from the shared model updates. In this work, we investigate the
effectiveness of Differential Privacy (DP) mechanisms - specifically, DP-SGD
and a variant based on explicit regularization (PDP-SGD) - as defenses against
GLAs. To this end, we evaluate the performance of several computer vision
models trained under varying privacy levels on a simple classification task,
and then analyze the quality of private data reconstructions obtained from the
intercepted gradients in a simulated FL environment. Our results demonstrate
that DP-SGD significantly mitigates the risk of gradient leakage attacks,
albeit with a moderate trade-off in model utility. In contrast, PDP-SGD
maintains strong classification performance but proves ineffective as a
practical defense against reconstruction attacks. These findings highlight the
importance of empirically evaluating privacy mechanisms beyond their
theoretical guarantees, particularly in distributed learning scenarios where
information leakage may represent an unassumable critical threat to data
security and privacy.

</details>


### [137] [ChessQA: Evaluating Large Language Models for Chess Understanding](https://arxiv.org/abs/2510.23948)
*Qianfeng Wen,Zhenwei Tang,Ashton Anderson*

Main category: cs.LG

TL;DR: ChessQA 是一个用于评估 LLM 在国际象棋理解方面的基准，涵盖了从基本规则到高阶概念等多个抽象级别的任务。这项研究还发现现代 LLM 在这五个类别中存在持续存在的弱点，并将在未来发布代码、数据集和排行榜以支持进一步的研究。


<details>
  <summary>Details</summary>
Motivation: 当前对LLM在国际象棋中理解能力的评估是临时的，范围狭窄。有必要通过一个全面的基准来进行更准确的评估，以便更准确地测量LLM对国际象棋的理解以及它们随规模、训练后方法或架构选择的变化情况。

Method: 提出了ChessQA，这是一个在五个任务类别（结构化、模式、短战术、位置判断和语义）下衡量LLM国际象棋理解能力的综合性基准。这些任务类别按照玩家随着棋艺知识积累而掌握的逐步上升的抽象层次来大致对应。此外，ChessQA具有动态性质，其提示、答案键和构建脚本可根据模型改进而不断发展。本研究评估了一系列现成的LLM，提供了每个类别的结果和错误分析。设立了一个公共排行榜来支持未来的研究。

Result: 本研究发现，尽管各LLM在某些特定任务上表现出色，但在五个类别中均存在持续存在的弱点。每个类别的评估结果和错误分析都说明了这些模型的局限性。

Conclusion: 本研究引入了ChessQA作为国际象棋理解和能力更全面评估的基准，提供了一个在LLM国际象棋理解能力诊断和比较中的控制和一致环境。LLM在其技能灵活性和广泛性上表现出一定的局限性，这为未来的模型改进和研究提供了有价值的见解。

Abstract: Chess provides an ideal testbed for evaluating the reasoning, modeling, and
abstraction capabilities of large language models (LLMs), as it has
well-defined structure and objective ground truth while admitting a wide
spectrum of skill levels. However, existing evaluations of LLM ability in chess
are ad hoc and narrow in scope, making it difficult to accurately measure LLM
chess understanding and how it varies with scale, post-training methodologies,
or architecture choices. We present ChessQA, a comprehensive benchmark that
assesses LLM chess understanding across five task categories (Structural,
Motifs, Short Tactics, Position Judgment, and Semantic), which approximately
correspond to the ascending abstractions that players master as they accumulate
chess knowledge, from understanding basic rules and learning tactical motifs to
correctly calculating tactics, evaluating positions, and semantically
describing high-level concepts. In this way, ChessQA captures a more
comprehensive picture of chess ability and understanding, going significantly
beyond the simple move quality evaluations done previously, and offers a
controlled, consistent setting for diagnosis and comparison. Furthermore,
ChessQA is inherently dynamic, with prompts, answer keys, and construction
scripts that can evolve as models improve. Evaluating a range of contemporary
LLMs, we find persistent weaknesses across all five categories and provide
results and error analyses by category. We will release the code, periodically
refreshed datasets, and a public leaderboard to support further research.

</details>


### [138] [A Pragmatic Way to Measure Chain-of-Thought Monitorability](https://arxiv.org/abs/2510.23966)
*Scott Emmons,Roland S. Zimmermann,David K. Elson,Rohin Shah*

Main category: cs.LG

TL;DR: 为了保持AI安全中的Chain-of-Thought (CoT)的可监控性，研究提出了一种实用的方法来衡量其可读性和覆盖率，以此作为工具来监控设计决策的影响。这种方法可以通过对前沿模型的应用，发现这些模型在具有挑战性的基准测试中具有高可监控性，但需要与对抗压力测试结合以全面评估模型的稳健性。


<details>
  <summary>Details</summary>
Motivation: 由于训练实践或模型架构的变化可能会导致Chain-of-Thought (CoT)监控的机会丧失，因此研究旨在提出一种实用的方法，来衡量和保持CoT的可监控性，以保障AI的安全性。

Method: 研究提出了一种测量CoT可读性和覆盖率的方法：通过一个自动评分提示，使得任何有能力的LLM可以计算CoT的可读性和覆盖率。然后，研究通过使用合成的CoT退化对自动评分器进行了准确性检查，并将其应用于几款前沿模型，以验证其在具有挑战性的基准测试中的性能。

Result: 研究发现，前沿模型在具有挑战性的基准测试中具有高可监控性。同时，研究也提出，这些自动评分的提示技术，应作为对抗压力测试的补充，而不是替代品，以全面评估模型的稳健性。

Conclusion: 研究提供了一套用于测量CoT可监控性的工具，这是对AI安全维护的一种重要贡献，但同时提出，该方法需要与对抗压力测试相结合，才能确保模型的安全性和稳健性。

Abstract: While Chain-of-Thought (CoT) monitoring offers a unique opportunity for AI
safety, this opportunity could be lost through shifts in training practices or
model architecture. To help preserve monitorability, we propose a pragmatic way
to measure two components of it: legibility (whether the reasoning can be
followed by a human) and coverage (whether the CoT contains all the reasoning
needed for a human to also produce the final output). We implement these
metrics with an autorater prompt that enables any capable LLM to compute the
legibility and coverage of existing CoTs. After sanity-checking our prompted
autorater with synthetic CoT degradations, we apply it to several frontier
models on challenging benchmarks, finding that they exhibit high
monitorability. We present these metrics, including our complete autorater
prompt, as a tool for developers to track how design decisions impact
monitorability. While the exact prompt we share is still a preliminary version
under ongoing development, we are sharing it now in the hopes that others in
the community will find it useful. Our method helps measure the default
monitorability of CoT - it should be seen as a complement, not a replacement,
for the adversarial stress-testing needed to test robustness against
deliberately evasive models.

</details>


### [139] [An efficient probabilistic hardware architecture for diffusion-like models](https://arxiv.org/abs/2510.23972)
*Andraž Jelinčič,Owen Lockwood,Akhil Garlapati,Guillaume Verdon,Trevor McCourt*

Main category: cs.LG

TL;DR: 本文提出了一种基于全晶体管的概率计算机，这种计算机可以在硬件级别实现强大的去噪模型。系统级分析表明，基于这种架构的设备在使用少量能量的情况下，可以在简单的图像基准测试中与GPU达到性能持平。


<details>
  <summary>Details</summary>
Motivation: 现有的专门化随机计算机的提案由于其有限的建模技术和不适用的硬件而未能得到广泛应用。因此，本文提出了一个基于全晶体管的概率计算架构，以改善这些不足。

Method: 提出了一种新的基于全晶体管的概率计算机架构，这种架构可以在硬件级别实现先进的去噪模型，并通过系统级分析验证其性能和能效。

Result: 模拟结果显示，基于此架构的设备在简单的图像处理基准测试中可以与GPU竞争，同时使用的能量仅为其亿分之一。

Conclusion: 该工作展示了一种先进的概率计算架构，它通过先进的硬件技术提高了性能和能效。

Abstract: The proliferation of probabilistic AI has promoted proposals for specialized
stochastic computers. Despite promising efficiency gains, these proposals have
failed to gain traction because they rely on fundamentally limited modeling
techniques and exotic, unscalable hardware. In this work, we address these
shortcomings by proposing an all-transistor probabilistic computer that
implements powerful denoising models at the hardware level. A system-level
analysis indicates that devices based on our architecture could achieve
performance parity with GPUs on a simple image benchmark using approximately
10,000 times less energy.

</details>


### [140] [Diffusion Adaptive Text Embedding for Text-to-Image Diffusion Models](https://arxiv.org/abs/2510.23974)
*Byeonghu Na,Minsang Park,Gyuwon Sim,Donghyeok Shin,HeeSun Bae,Mina Kang,Se Jung Kwon,Wanmo Kang,Il-Chul Moon*

Main category: cs.LG

TL;DR: DATE提出了一个动态更新文本嵌入的方法，以提高文本和生成图像之间的对齐能力，而不需要额外的模型训练。实验证明了DATE在多概念生成和基于文本的图像编辑等任务上的优越性。


<details>
  <summary>Details</summary>
Motivation: 现有的文本到图像生成模型由于使用固定的文本嵌入，限制了生成过程的适应性，从而影响了文本和生成图像的对齐。因此，提出DATE来解决这个问题，通过动态更新文本嵌入，提高生成图像的质量和文本指导能力。

Method: DATE通过在每个扩散时间步更新文本嵌入来改进对齐，方法是根据中间干扰的数据优化文本嵌入，以提高文本和图像之间的对齐和偏好。这些更新基于一个具体实现的优化问题和导出的更新规则，能够自适应地调整文本条件以最佳地适应逆向扩散图像。

Result: DATE在多个任务上展示了比固定文本嵌入更好的文本-图像对齐效果，包括多概念生成和基于文本的图像编辑任务。

Conclusion: DATE通过动态调整文本嵌入，提高了文本条件和逆向扩散图像之间的对齐，同时保持生成模型的能力。这表明动态调整文本嵌入可以显著改善生成图像的质量，而无需额外的训练。

Abstract: Text-to-image diffusion models rely on text embeddings from a pre-trained
text encoder, but these embeddings remain fixed across all diffusion timesteps,
limiting their adaptability to the generative process. We propose Diffusion
Adaptive Text Embedding (DATE), which dynamically updates text embeddings at
each diffusion timestep based on intermediate perturbed data. We formulate an
optimization problem and derive an update rule that refines the text embeddings
at each sampling step to improve alignment and preference between the mean
predicted image and the text. This allows DATE to dynamically adapts the text
conditions to the reverse-diffused images throughout diffusion sampling without
requiring additional model training. Through theoretical analysis and empirical
results, we show that DATE maintains the generative capability of the model
while providing superior text-image alignment over fixed text embeddings across
various tasks, including multi-concept generation and text-guided image
editing. Our code is available at https://github.com/aailab-kaist/DATE.

</details>


### [141] [Synergistic Neural Forecasting of Air Pollution with Stochastic Sampling](https://arxiv.org/abs/2510.23977)
*Yohan Abeysinghe,Muhammad Akhtar Munir,Sanoojan Baliah,Ron Sarafian,Fahad Shahbaz Khan,Yinon Rudich,Salman Khan*

Main category: cs.LG

TL;DR: SynCast 是一种高分辨率的神经预报模型，通过集成气象和空气质量数据来改善颗粒物浓度预测，特别是在极端污染事件中表现优异。该模型基于区域适应的变压器骨干和扩散增强的随机细化模块构建，能够更准确地捕捉颗粒物浓度突增的非线性动态。它利用了标准化的ERA5和CAMS数据集，并对传统损失函数进行了改进，提高了在极端条件下的预报精度。这项研究为下一代空气质量早期预警系统提供了一个可扩展的基础，并有助于气候-健康风险的缓解。


<details>
  <summary>Details</summary>
Motivation: 尽管现有模型在预测空气污染方面已经有所进展，但在罕见但危险的污染事件预测上仍然不足。因此，研究的动机在于开发一种能够更精确预报包括极端污染事件在内的颗粒物质浓度的模型。

Method: 提出了基于分级变压器和扩散增强随机细化模块的SynCast模型，该模型利用标准化的ERA5和CAMS数据集，以提高预测的准确性和鲁棒性。此外，模型采用了改进的损失函数以更好地捕捉分布尾部的概率，从而更准确地模拟罕见污染事件。

Result: 与现有方法相比，SynCast模型在预测颗粒物质浓度方面（特别是PM1、PM2.5和PM10）在极端条件下表现出色。它不仅没有牺牲全球范围内的预报准确性，还显著改善了高暴露地区的预报性能。

Conclusion: SynCast模型为下一代空气质量早期预警系统提供了坚实的理论基础和技术支持。它将在气候-健康风险的缓解工作中发挥重要作用，尤其是在易受空气污染影响的地区。

Abstract: Air pollution remains a leading global health and environmental risk,
particularly in regions vulnerable to episodic air pollution spikes due to
wildfires, urban haze and dust storms. Accurate forecasting of particulate
matter (PM) concentrations is essential to enable timely public health warnings
and interventions, yet existing models often underestimate rare but hazardous
pollution events. Here, we present SynCast, a high-resolution neural
forecasting model that integrates meteorological and air composition data to
improve predictions of both average and extreme pollution levels. Built on a
regionally adapted transformer backbone and enhanced with a diffusion-based
stochastic refinement module, SynCast captures the nonlinear dynamics driving
PM spikes more accurately than existing approaches. Leveraging on harmonized
ERA5 and CAMS datasets, our model shows substantial gains in forecasting
fidelity across multiple PM variables (PM$_1$, PM$_{2.5}$, PM$_{10}$),
especially under extreme conditions. We demonstrate that conventional loss
functions underrepresent distributional tails (rare pollution events) and show
that SynCast, guided by domain-aware objectives and extreme value theory,
significantly enhances performance in highly impacted regions without
compromising global accuracy. This approach provides a scalable foundation for
next-generation air quality early warning systems and supports climate-health
risk mitigation in vulnerable regions.

</details>


### [142] [Predicting Barge Tow Size on Inland Waterways Using Vessel Trajectory Derived Features: Proof of Concept](https://arxiv.org/abs/2510.23994)
*Geoffery Agorku,Sarah Hernandez,Hayley Hames,Cade Wagner*

Main category: cs.LG

TL;DR: 本研究提出了一种使用AIS数据和机器学习来预测内陆水道中拖曳的驳船数量的新方法。该方法包括从卫星图像中手动注释驳船实例，使用时空匹配过程将其与AIS船舶轨迹匹配，并生成30个AIS特征来训练和测试模型。在6种回归模型中，泊松回归模型表现出最佳性能，平均绝对误差为1.92。结果表明，与驳船数量最相关的特征包括船舶操纵性指标，如航向熵、速度变化和行程长度。


<details>
  <summary>Details</summary>
Motivation: 精确、实时估计内陆水道中拖曳的驳船数量是由于驳船的非自推性质以及现有监控系统的局限性所带来的关键挑战。现有的系统难以提供足够的准确性，因此需要一种新的解决方案来改善这一状况。本研究应运而生，旨在开发一种基于AIS数据和机器学习的方法，以解决这一实际问题。

Method: 本研究通过从卫星图像中手动注释驳船实例，并利用时空匹配程序将其与AIS船舶轨迹匹配，生成了30个丰富的AIS特征。然后，通过递归特征消除过程选出关键特征，最后训练并评估了6种回归模型，以确定最佳预测方法。

Result: 在所测试的六种回归模型中，泊松回归模型表现最佳，平均绝对误差为1.92 barges，证明了AIS数据和机器学习预测驳船数量的潜力。重要特征包括船舶的操纵性指标（如航向熵、速度变化和行程长度）。

Conclusion: 本研究表明，基于AIS数据训练的机器学习方法在预测内陆水道中拖曳的驳船数量方面具有很好的精确度与适用性。未来的研究将探索该方法在其他河流不同操作与环境条件下的泛化能力。

Abstract: Accurate, real-time estimation of barge quantity on inland waterways remains
a critical challenge due to the non-self-propelled nature of barges and the
limitations of existing monitoring systems. This study introduces a novel
method to use Automatic Identification System (AIS) vessel tracking data to
predict the number of barges in tow using Machine Learning (ML). To train and
test the model, barge instances were manually annotated from satellite scenes
across the Lower Mississippi River. Labeled images were matched to AIS vessel
tracks using a spatiotemporal matching procedure. A comprehensive set of 30
AIS-derived features capturing vessel geometry, dynamic movement, and
trajectory patterns were created and evaluated using Recursive Feature
Elimination (RFE) to identify the most predictive variables. Six regression
models, including ensemble, kernel-based, and generalized linear approaches,
were trained and evaluated. The Poisson Regressor model yielded the best
performance, achieving a Mean Absolute Error (MAE) of 1.92 barges using 12 of
the 30 features. The feature importance analysis revealed that metrics
capturing vessel maneuverability such as course entropy, speed variability and
trip length were most predictive of barge count. The proposed approach provides
a scalable, readily implementable method for enhancing Maritime Domain
Awareness (MDA), with strong potential applications in lock scheduling, port
management, and freight planning. Future work will expand the proof of concept
presented here to explore model transferability to other inland rivers with
differing operational and environmental conditions.

</details>


### [143] [Training-Free Safe Text Embedding Guidance for Text-to-Image Diffusion Models](https://arxiv.org/abs/2510.24012)
*Byeonghu Na,Mina Kang,Jiseok Kwak,Minsang Park,Jiwoo Shin,SeJoon Jun,Gayoung Lee,Jin-Hwa Kim,Il-Chul Moon*

Main category: cs.LG

TL;DR: 本文提出了STG方法，无需额外训练即可提高扩散模型的安全性，通过指导文本嵌入来生成更安全的输出。实验表明STG在移除不安全内容的同时能够保持输入指令的核心语义意图，优于训练基线方法。


<details>
  <summary>Details</summary>
Motivation: 当前文本到图像生成模型存在的问题是生成不适宜或有偏见的内容，本文的动机是如何在不进行额外训练的情况下提高这些模型的安全性。

Method: 本文提出了安全文本嵌入指导（STG）方法，通过基于预期的最终去噪图像评估的安全函数调整文本嵌入，以生成更安全的输出。这种方法不需要对模型进行额外的训练。

Result: 实验结果表明，STG在去除不适宜内容（如裸体、暴力等）方面表现优于既有训练和非训练基线方法。

Conclusion: STG通过调整文本嵌入提高了文本到图像生成模型的安全性，既不需要额外训练又能在保持核心语义意图的同时移除不安全内容。

Abstract: Text-to-image models have recently made significant advances in generating
realistic and semantically coherent images, driven by advanced diffusion models
and large-scale web-crawled datasets. However, these datasets often contain
inappropriate or biased content, raising concerns about the generation of
harmful outputs when provided with malicious text prompts. We propose Safe Text
embedding Guidance (STG), a training-free approach to improve the safety of
diffusion models by guiding the text embeddings during sampling. STG adjusts
the text embeddings based on a safety function evaluated on the expected final
denoised image, allowing the model to generate safer outputs without additional
training. Theoretically, we show that STG aligns the underlying model
distribution with safety constraints, thereby achieving safer outputs while
minimally affecting generation quality. Experiments on various safety
scenarios, including nudity, violence, and artist-style removal, show that STG
consistently outperforms both training-based and training-free baselines in
removing unsafe content while preserving the core semantic intent of input
prompts. Our code is available at https://github.com/aailab-kaist/STG.

</details>


### [144] [Efficient Global-Local Fusion Sampling for Physics-Informed Neural Networks](https://arxiv.org/abs/2510.24026)
*Jiaqi Luo,Shixin Xu,Zhouwang Yang*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: The accuracy of Physics-Informed Neural Networks (PINNs) critically depends
on the placement of collocation points, as the PDE loss is approximated through
sampling over the solution domain. Global sampling ensures stability by
covering the entire domain but requires many samples and is computationally
expensive, whereas local sampling improves efficiency by focusing on
high-residual regions but may neglect well-learned areas, reducing robustness.
We propose a Global-Local Fusion (GLF) Sampling Strategy that combines the
strengths of both approaches. Specifically, new collocation points are
generated by perturbing training points with Gaussian noise scaled inversely to
the residual, thereby concentrating samples in difficult regions while
preserving exploration. To further reduce computational overhead, a lightweight
linear surrogate is introduced to approximate the global residual-based
distribution, achieving similar effectiveness at a fraction of the cost.
Together, these components, residual-adaptive sampling and residual-based
approximation, preserve the stability of global methods while retaining the
efficiency of local refinement. Extensive experiments on benchmark PDEs
demonstrate that GLF consistently improves both accuracy and efficiency
compared with global and local sampling strategies. This study provides a
practical and scalable framework for enhancing the reliability and efficiency
of PINNs in solving complex and high-dimensional PDEs.

</details>


### [145] [Spatio-temporal Multivariate Time Series Forecast with Chosen Variables](https://arxiv.org/abs/2510.24027)
*Zibo Liu,Zhe Jiang,Zelin Xu,Tingsong Xiao,Yupu Zhang,Zhengkun Xiao,Haibo Wang,Shigang Chen*

Main category: cs.LG

TL;DR: 本文提出了一种优化时空多变量时间序列预测（STMF）模型的方法，该方法能够选择输入中的m个变量以最大化预测准确性，并通过量化屏蔽逐步剪枝非相关信息、优先级重播和动态外推机制来实现。实验结果表明，这种方法在五个真实数据集上比最新的基准方法有更优的准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 当前的STMF模型需要预先确定输入变量，而如何选择这些变量以提高预测准确性的问题尚未被研究。本文解决这个问题，提出了一种选择最优变量的方法以最大化预测准确性。

Method: 本文提出了一种新的框架，通过蒙面变量参数剪枝、优先级变量参数重播和动态外推机制来优化变量选择和模型效能，具体包括：1.通过量化屏蔽逐步剪枝非相关信息；2.优先级重播低损失的过去样本以保持学习到的知识，提高模型稳定性；3.动态外推机制通过可学习的空间嵌入和技术信息传播选定输入变量的信息到其他变量。

Result: 实验结果显示，本文所提出的方法在五个真实数据集上显著优于最新的基准方法，显示了联合变量选择和模型优化的有效性。在准确性和效率方面都取得了更优的结果。

Conclusion: 本文填补了一项空白，提出了一个能够优化时空多变量时间序列预测模型的方法，通过选择输入变量以最大化预测准确性，实验结果佐证了方法的有效性。

Abstract: Spatio-Temporal Multivariate time series Forecast (STMF) uses the time series
of $n$ spatially distributed variables in a period of recent past to forecast
their values in a period of near future. It has important applications in
spatio-temporal sensing forecast such as road traffic prediction and air
pollution prediction. Recent papers have addressed a practical problem of
missing variables in the model input, which arises in the sensing applications
where the number $m$ of sensors is far less than the number $n$ of locations to
be monitored, due to budget constraints. We observe that the state of the art
assumes that the $m$ variables (i.e., locations with sensors) in the model
input are pre-determined and the important problem of how to choose the $m$
variables in the input has never been studied. This paper fills the gap by
studying a new problem of STMF with chosen variables, which optimally selects
$m$-out-of-$n$ variables for the model input in order to maximize the forecast
accuracy. We propose a unified framework that jointly performs variable
selection and model optimization for both forecast accuracy and model
efficiency. It consists of three novel technical components: (1) masked
variable-parameter pruning, which progressively prunes less informative
variables and attention parameters through quantile-based masking; (2)
prioritized variable-parameter replay, which replays low-loss past samples to
preserve learned knowledge for model stability; (3) dynamic extrapolation
mechanism, which propagates information from variables selected for the input
to all other variables via learnable spatial embeddings and adjacency
information. Experiments on five real-world datasets show that our work
significantly outperforms the state-of-the-art baselines in both accuracy and
efficiency, demonstrating the effectiveness of joint variable selection and
model optimization.

</details>


### [146] [GraphNet: A Large-Scale Computational Graph Dataset for Tensor Compiler Research](https://arxiv.org/abs/2510.24035)
*Xinqi Li,Yiqun Liu,Shan Jiang,Enrong Zheng,Huaijin Zheng,Wenhao Dai,Haodong Deng,Dianhai Yu,Yanjun Ma*

Main category: cs.LG

TL;DR: GraphNet 是一个包含2.7K个真实世界深度学习计算图的数据集，包括丰富的元数据和六种主要任务类别，用于评估张量编译器的性能。我们提出了一个新的基准度量 Speedup Score S(t) 来综合考虑速度提升和执行正确性，并扩展为 Error-aware Speedup Score ES(t) 以帮助识别性能瓶颈。我们在计算机视觉和自然语言处理样本上进行了基准测试，展示了GraphNet的实用性。


<details>
  <summary>Details</summary>
Motivation: 现有的一些计算图数据集可能无法全面评估张量编译器的优化能力，而GraphNet 能提供更为全面的评估，不仅考虑速度提升，还考虑执行正确性，进而帮助开发人员识别性能瓶颈，提升编译器质量。

Method: 通过引入一个新的基准度量 Speedup Score S(t)，它考虑了执行时间和正确性。并进一步提出了Error-aware Speedup Score ES(t)，该度量进一步考虑了错误信息。我们还展示了基于GraphNet的数据集如何应用于评估不同框架下的默认张量编译器的性能。

Result: 通过在计算机视觉和自然语言处理领域进行的测试，我们展示了如何使用GraphNet通过Speedup Score S(t)和Error-aware Speedup Score ES(t)来评估现有编译器的性能。测试结果表明GraphNet对于改进张量编译器具有潜力。

Conclusion: GraphNet提供了一个真实世界深度学习计算图数据集，帮助评估张量编译器的优化能力。Speedup Score S(t) 和 Error-aware Speedup Score ES(t) 提供了评估性能新策略，对未来发展深度学习编译器优化具有重要作用。

Abstract: We introduce GraphNet, a dataset of 2.7K real-world deep learning
computational graphs with rich metadata, spanning six major task categories
across multiple deep learning frameworks. To evaluate tensor compiler
performance on these samples, we propose the benchmark metric Speedup Score
S(t), which jointly considers runtime speedup and execution correctness under
tunable tolerance levels, offering a reliable measure of general optimization
capability. Furthermore, we extend S(t) to the Error-aware Speedup Score ES(t),
which incorporates error information and helps compiler developers identify key
performance bottlenecks. In this report, we benchmark the default tensor
compilers, CINN for PaddlePaddle and TorchInductor for PyTorch, on computer
vision (CV) and natural language processing (NLP) samples to demonstrate the
practicality of GraphNet. The full construction pipeline with graph extraction
and compiler evaluation tools is available at
https://github.com/PaddlePaddle/GraphNet .

</details>


### [147] [Geometric Algorithms for Neural Combinatorial Optimization with Constraints](https://arxiv.org/abs/2510.24039)
*Nikolaos Karalias,Akbar Rafiey,Yifei Xu,Zhishang Luo,Behrooz Tahmasebi,Connie Jiang,Stefanie Jegelka*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Self-Supervised Learning (SSL) for Combinatorial Optimization (CO) is an
emerging paradigm for solving combinatorial problems using neural networks. In
this paper, we address a central challenge of SSL for CO: solving problems with
discrete constraints. We design an end-to-end differentiable framework that
enables us to solve discrete constrained optimization problems with neural
networks. Concretely, we leverage algorithmic techniques from the literature on
convex geometry and Carath\'eodory's theorem to decompose neural network
outputs into convex combinations of polytope corners that correspond to
feasible sets. This decomposition-based approach enables self-supervised
training but also ensures efficient quality-preserving rounding of the neural
net output into feasible solutions. Extensive experiments in
cardinality-constrained optimization show that our approach can consistently
outperform neural baselines. We further provide worked-out examples of how our
method can be applied beyond cardinality-constrained problems to a diverse set
of combinatorial optimization tasks, including finding independent sets in
graphs, and solving matroid-constrained problems.

</details>


### [148] [Localized Kernel Projection Outlyingness: A Two-Stage Approach for Multi-Modal Outlier Detection](https://arxiv.org/abs/2510.24043)
*Akira Tamamori*

Main category: cs.LG

TL;DR: Two-Stage LKPLO 是一种新颖的多阶段离群点检测框架，它结合了三个关键的概念：损失基的离群点度量、全局核主成分分析阶段和后续的局部聚类阶段。该框架在含有复杂结构的数据集上取得了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 传统的基于投影的方法依赖固定的统计度量并假设单一数据结构，这限制了它们在处理复杂数据结构上的能力。本研究的动机是开发一种能够克服这些局限性的新型框架，适用于更广泛的数据类型。

Method: 介绍了 Two-Stage LKPLO 框架，其在全局核化PCA之后进行局部聚类分析，采用损失基度量以适应不同数据结构。具体包括提出SVM似损失函数等灵活、自适应的损失函数，形成全局线性化以及处理多峰数据的能力。实验表明，该框架在多种数据集上表现优异。

Result: 在10个基准数据集上进行的5折交叉验证实验表明，Two-Stage LKPLO 显著优于现有的方法，特别是在处理多簇结构和高复杂度数据集时。

Conclusion: 该工作提出了一种高效的离群点检测框架，证实了混合多阶段架构在离群点检测中的重要性。

Abstract: This paper presents Two-Stage LKPLO, a novel multi-stage outlier detection
framework that overcomes the coexisting limitations of conventional
projection-based methods: their reliance on a fixed statistical metric and
their assumption of a single data structure. Our framework uniquely synthesizes
three key concepts: (1) a generalized loss-based outlyingness measure (PLO)
that replaces the fixed metric with flexible, adaptive loss functions like our
proposed SVM-like loss; (2) a global kernel PCA stage to linearize non-linear
data structures; and (3) a subsequent local clustering stage to handle
multi-modal distributions. Comprehensive 5-fold cross-validation experiments on
10 benchmark datasets, with automated hyperparameter optimization, demonstrate
that Two-Stage LKPLO achieves state-of-the-art performance. It significantly
outperforms strong baselines on datasets with challenging structures where
existing methods fail, most notably on multi-cluster data (Optdigits) and
complex, high-dimensional data (Arrhythmia). Furthermore, an ablation study
empirically confirms that the synergistic combination of both the kernelization
and localization stages is indispensable for its superior performance. This
work contributes a powerful new tool for a significant class of outlier
detection problems and underscores the importance of hybrid, multi-stage
architectures.

</details>


### [149] [Mitigating Negative Transfer via Reducing Environmental Disagreement](https://arxiv.org/abs/2510.24044)
*Hui Sun,Zheng Xie,Hao-Yuan He,Ming Li*

Main category: cs.LG

TL;DR: 本文提出了RED方法，用于减少跨域差异中的环境分歧，从而缓解负向迁移并提高模型性能。


<details>
  <summary>Details</summary>
Motivation: 研究指出，过依赖非因果环境特征是导致负向迁移的关键因素，并提出将域划分成因果共因和特定于环境的非因果特征来解决这一问题的动机。

Method: RED通过对抗性训练特定域的环境特征提取器来区分每个样本的因果共因和特定于环境的非因果特征，然后估计和减少基于特定域的非因果环境特征的环境分歧。

Result: 实验表明，RED方法能有效减轻负向迁移，取得最先进的性能。

Conclusion: RED方法通过减少环境分歧成功缓解了负向迁移问题，证明了在跨领域适应中区分因果共因和非因果环境特征的重要性。

Abstract: Unsupervised Domain Adaptation~(UDA) focuses on transferring knowledge from a
labeled source domain to an unlabeled target domain, addressing the challenge
of \emph{domain shift}. Significant domain shifts hinder effective knowledge
transfer, leading to \emph{negative transfer} and deteriorating model
performance. Therefore, mitigating negative transfer is essential. This study
revisits negative transfer through the lens of causally disentangled learning,
emphasizing cross-domain discriminative disagreement on non-causal
environmental features as a critical factor. Our theoretical analysis reveals
that overreliance on non-causal environmental features as the environment
evolves can cause discriminative disagreements~(termed \emph{environmental
disagreement}), thereby resulting in negative transfer. To address this, we
propose Reducing Environmental Disagreement~(RED), which disentangles each
sample into domain-invariant causal features and domain-specific non-causal
environmental features via adversarially training domain-specific environmental
feature extractors in the opposite domains. Subsequently, RED estimates and
reduces environmental disagreement based on domain-specific non-causal
environmental features. Experimental results confirm that RED effectively
mitigates negative transfer and achieves state-of-the-art performance.

</details>


### [150] [Causal-Aware Generative Adversarial Networks with Reinforcement Learning](https://arxiv.org/abs/2510.24046)
*Tu Anh Hoang Nguyen,Dang Nguyen,Tri-Nhan Vo,Thuc Duy Le,Sunil Gupta*

Main category: cs.LG

TL;DR: 介绍了一种新的生成框架CA-GAN，用于生成能够保留因果关系、数据实用性和隐私保护的高质量合成数据集。CA-GAN结合了因果图提取和条件WGAN-GP，并使用强化学习目标来保证生成数据的因果意识。实验表明，该方法在14个表格数据集上优于六个SOTA方法。


<details>
  <summary>Details</summary>
Motivation: 当前的基于GAN的数据生成方法在捕获复杂的因果关系、保持数据实用性和提供企业部署所需的隐私保证方面存在挑战。因此，开发一种新的框架来生成既能保护隐私又能保持实用性的高质量合成数据对于数据工程至关重要。

Method: CA-GAN提出了一种两步方法：首先提取因果图来学习数据流形中的因果关系，然后利用一个特制的条件WGAN-GP根据因果图的节点结构生成样本。生成器训练使用一个新的基于强化学习的目标，保证生成的和真实数据的因果图一致性。

Result: 实验在14个表格数据集上进行，结果表明CA-GAN在因果保护、实用性和隐私保护方面均优于六个最先进的方法，证明了其补充现有技术的能力。

Conclusion: CA-GAN为数据工程师提供了一种实用的、高性能的解决方案，可用于创建高质量的、符合隐私要求的合成数据集，以支持基准测试数据库系统、加速软件开发和促进安全的数据驱动研究。

Abstract: The utility of tabular data for tasks ranging from model training to
large-scale data analysis is often constrained by privacy concerns or
regulatory hurdles. While existing data generation methods, particularly those
based on Generative Adversarial Networks (GANs), have shown promise, they
frequently struggle with capturing complex causal relationship, maintaining
data utility, and providing provable privacy guarantees suitable for enterprise
deployment. We introduce CA-GAN, a novel generative framework specifically
engineered to address these challenges for real-world tabular datasets. CA-GAN
utilizes a two-step approach: causal graph extraction to learn a robust,
comprehensive causal relationship in the data's manifold, followed by a custom
Conditional WGAN-GP (Wasserstein GAN with Gradient Penalty) that operates
exclusively as per the structure of nodes in the causal graph. More
importantly, the generator is trained with a new Reinforcement Learning-based
objective that aligns the causal graphs constructed from real and fake data,
ensuring the causal awareness in both training and sampling phases. We
demonstrate CA-GAN superiority over six SOTA methods across 14 tabular
datasets. Our evaluations, focused on core data engineering metrics: causal
preservation, utility preservation, and privacy preservation. Our method offers
a practical, high-performance solution for data engineers seeking to create
high-quality, privacy-compliant synthetic datasets to benchmark database
systems, accelerate software development, and facilitate secure data-driven
research.

</details>


### [151] [Learning from History: A Retrieval-Augmented Framework for Spatiotemporal Prediction](https://arxiv.org/abs/2510.24049)
*Hao Jia,Penghao Zhao,Hao Wu,Yuan Gao,Yangyu Tao,Bin Cui*

Main category: cs.LG

TL;DR: 提出了一种新的Retrieval-Augmented Prediction (RAP) 框架，通过结合深度学习模型和历史数据，解决了深度学习模型在长期预测中物理不一致的问题，提高了预测的准确性和物理合理性。


<details>
  <summary>Details</summary>
Motivation: 解决深度学习模型长期预测中物理不一致和误差累积的问题，提高预测的物理真实性。

Method: 介绍了一种新的Retrieval-Augmented Prediction (RAP) 框架，该框架利用历史数据中类似的演化例子来作为系统局部动态的非参数估计，并使用这种估计来指导模型的预测，以产生更合理的预测结果。这种方法使用了一个双流架构，其中历史数据提供的参考目标作为条件输入，提供动态指导。

Result: 在气象学、湍流和火灾模拟的广泛基准测试中，RAP不仅超越了现有的最佳方法，而且显著地超过了仅基于模拟的数据预测的基础线。更重要的是，RAP有效抑制了长期预测中的误差扩散，生成了更加物理合理的预测结果。

Conclusion: 提出的RAP框架通过结合深度学习模型和历史数据，解决了深度学习模型长期预测中的物理不一致和误差累积问题，从而提高了预测的准确性和物理合理性。

Abstract: Accurate and long-term spatiotemporal prediction for complex physical systems
remains a fundamental challenge in scientific computing. While deep learning
models, as powerful parametric approximators, have shown remarkable success,
they suffer from a critical limitation: the accumulation of errors during
long-term autoregressive rollouts often leads to physically implausible
artifacts. This deficiency arises from their purely parametric nature, which
struggles to capture the full constraints of a system's intrinsic dynamics. To
address this, we introduce a novel \textbf{Retrieval-Augmented Prediction
(RAP)} framework, a hybrid paradigm that synergizes the predictive power of
deep networks with the grounded truth of historical data. The core philosophy
of RAP is to leverage historical evolutionary exemplars as a non-parametric
estimate of the system's local dynamics. For any given state, RAP efficiently
retrieves the most similar historical analog from a large-scale database. The
true future evolution of this analog then serves as a \textbf{reference
target}. Critically, this target is not a hard constraint in the loss function
but rather a powerful conditional input to a specialized dual-stream
architecture. It provides strong \textbf{dynamic guidance}, steering the
model's predictions towards physically viable trajectories. In extensive
benchmarks across meteorology, turbulence, and fire simulation, RAP not only
surpasses state-of-the-art methods but also significantly outperforms a strong
\textbf{analog-only forecasting baseline}. More importantly, RAP generates
predictions that are more physically realistic by effectively suppressing error
divergence in long-term rollouts.

</details>


### [152] [Low-N Protein Activity Optimization with FolDE](https://arxiv.org/abs/2510.24053)
*Jacob B. Roberts,Catherine R. Ji,Isaac Donnell,Thomas D. Young,Allison N. Pearson,Graham A. Hudson,Leah S. Keiser,Mia Wesselkamper,Peter H. Winegar,Janik Ludwig,Sarah H. Klass,Isha V. Sheth,Ezechinyere C. Ukabiala,Maria C. T. Astolfi,Benjamin Eysenbach,Jay D. Keasling*

Main category: cs.LG

TL;DR: FolDE是一种改进的ALDE方法，通过自然起始和常量撒谎批次选择器提高蛋白质优化效率，可以找到更多高活性突变体。


<details>
  <summary>Details</summary>
Motivation: 传统蛋白质优化方法成本高，依赖于大量突变体的构建和测量。现有的ALDE方法在选择最佳突变体时面临局限性，导致训练数据同质化，预测模型不准确。因此，本研究提出了FolDE方法，以解决上述问题，提高蛋白质优化效率。

Method: FolDE通过自然起始和常量撒谎批次选择器来改进ALDE方法。自然起始通过结合蛋白质语言模型输出和有限的活性测量来提高活性预测。常量撒谎批次选择器则优化了批次多样性，但在研究中的影响相对较小。

Result: 在针对20个蛋白质目标的模拟测试中，FolDE找到了比最佳基线ALDE方法多23%的前10%突变体（p=0.005），且找到前1%突变体的可能性高55%。

Conclusion: FolDE提升了蛋白质优化的效率，且其完整工作流程可作为开源软件使用，为所有实验室提供了高效蛋白质优化的途径。

Abstract: Proteins are traditionally optimized through the costly construction and
measurement of many mutants. Active Learning-assisted Directed Evolution (ALDE)
alleviates that cost by predicting the best improvements and iteratively
testing mutants to inform predictions. However, existing ALDE methods face a
critical limitation: selecting the highest-predicted mutants in each round
yields homogeneous training data insufficient for accurate prediction models in
subsequent rounds. Here we present FolDE, an ALDE method designed to maximize
end-of-campaign success. In simulations across 20 protein targets, FolDE
discovers 23% more top 10% mutants than the best baseline ALDE method (p=0.005)
and is 55% more likely to find top 1% mutants. FolDE achieves this primarily
through naturalness-based warm-starting, which augments limited activity
measurements with protein language model outputs to improve activity
prediction. We also introduce a constant-liar batch selector, which improves
batch diversity; this is important in multi-mutation campaigns but had limited
effect in our benchmarks. The complete workflow is freely available as
open-source software, making efficient protein optimization accessible to any
laboratory.

</details>


### [153] [What do vision-language models see in the context? Investigating multimodal in-context learning](https://arxiv.org/abs/2510.24331)
*Gabriel O. dos Santos,Esther Colombini,Sandra Avila*

Main category: cs.LG

TL;DR: 这项工作研究了视觉语言模型（VLMs）中的in-context learning (ICL)，揭示了训练策略、架构选择以及提示设计对multimodal ICL的影响，并分析了在ICL过程中注意力模式的变化，发现当前VLMs在多模态整合方面存在局限性，主要依赖文本线索而非视觉信息。


<details>
  <summary>Details</summary>
Motivation: 尽管in-context learning（ICL）在大型语言模型中被广泛研究，但在视觉语言模型中的效果仍然未被充分探索。当前工作旨在系统地研究VLMs中的ICL能力，特别是在提示设计、架构选择和训练策略方面的效果如何，以及如何通过增加上下文示范来改善注意力模式。此外，研究还探讨了instruction tuning对于缓解VLM模型过分依赖上下文样本的作用。

Method: 测试了七种模型，涵盖四种架构，并在三个图像描述基准上进行评价。同时，对注意力模式随上下文样本数量增加的变化进行了分析。

Result: 发现训练在图像-文本交错数据上的ICL性能得到提升，但对视觉和文本信息的有效集成无显著贡献。Instruction tuning有助于提升指令遵循能力，但可能减少对上下文示范的依赖，表明指令对齐和上下文自适应之间存在权衡。进一步注意到，当前VLMs主要关注文本线索，无法有效利用视觉信息，这表明多模态整合能力有限。

Conclusion: 这些发现揭示了当前VLMs在ICL能力方面存在的关键限制，并阐明了如何增强其通过多模态上下文示例进行学习的能力。

Abstract: In-context learning (ICL) enables Large Language Models (LLMs) to learn tasks
from demonstration examples without parameter updates. Although it has been
extensively studied in LLMs, its effectiveness in Vision-Language Models (VLMs)
remains underexplored. In this work, we present a systematic study of ICL in
VLMs, evaluating seven models spanning four architectures on three image
captioning benchmarks. We analyze how prompt design, architectural choices, and
training strategies influence multimodal ICL. To our knowledge, we are the
first to analyze how attention patterns in VLMs vary with an increasing number
of in-context demonstrations. Our results reveal that training on imag-text
interleaved data enhances ICL performance but does not imply effective
integration of visual and textual information from demonstration examples. In
contrast, instruction tuning improves instruction-following but can reduce
reliance on in-context demonstrations, suggesting a trade-off between
instruction alignment and in-context adaptation. Attention analyses further
show that current VLMs primarily focus on textual cues and fail to leverage
visual information, suggesting a limited capacity for multimodal integration.
These findings highlight key limitations in the ICL abilities of current VLMs
and provide insights for enhancing their ability to learn from multimodal
in-context examples.

</details>


### [154] [Learning Parameterized Skills from Demonstrations](https://arxiv.org/abs/2510.24095)
*Vedant Gupta,Haotian Fu,Calvin Luo,Yiding Jiang,George Konidaris*

Main category: cs.LG

TL;DR: DEPS 是一种从专家演示中发现参数化技能的端到端算法。该方法通过学习参数化技能与选择适当离散技能和连续参数的元策略，解决了潜在变量模型中的退化问题，并证明从多任务专家演示中学习参数化技能可以提高对未见过任务的泛化能力。DEPS 在 LIBERO 和 MetaWorld 基准测试中表现出优于多任务和技能学习基线的方法，并能够发现如物体抓取技能等可解释的参数化技能。


<details>
  <summary>Details</summary>
Motivation: 为了从专家演示中发现可泛化的参数化技能，解决潜在变量模型中的退化问题，并提高对未见过任务的泛化能力。

Method: DEPS 结合了时间变分推断和信息理论正则化方法，学习参数化技能以及选择适当离散技能和连续参数的元策略。

Result: DEPS 方法在 LIBERO 和 MetaWorld 基准测试中取得了比多任务和技能学习基线更好的结果，并且可以发现如物体抓取技能等可解释的参数化技能。

Conclusion: DEPS 提供了一种有效的从专家演示中发现参数化技能的新方法，具有广泛的应用前景。

Abstract: We present DEPS, an end-to-end algorithm for discovering parameterized skills
from expert demonstrations. Our method learns parameterized skill policies
jointly with a meta-policy that selects the appropriate discrete skill and
continuous parameters at each timestep. Using a combination of temporal
variational inference and information-theoretic regularization methods, we
address the challenge of degeneracy common in latent variable models, ensuring
that the learned skills are temporally extended, semantically meaningful, and
adaptable. We empirically show that learning parameterized skills from
multitask expert demonstrations significantly improves generalization to unseen
tasks. Our method outperforms multitask as well as skill learning baselines on
both LIBERO and MetaWorld benchmarks. We also demonstrate that DEPS discovers
interpretable parameterized skills, such as an object grasping skill whose
continuous arguments define the grasp location.

</details>


### [155] [Local Performance vs. Out-of-Distribution Generalization: An Empirical Analysis of Personalized Federated Learning in Heterogeneous Data Environments](https://arxiv.org/abs/2510.24503)
*Mortesa Hussaini,Jan Theiß,Anthony Stein*

Main category: cs.LG

TL;DR: 本文提出了FedAvg的改良版Federated Learning with Individualized Updates (FLIU)，通过在算法中添加一个具有自适应个人化因子的简单个性化步骤，旨在解决联邦学习中客户端漂移和泛化能力不足的问题。评估是在MNIST和CIFAR-10数据集的不同分布条件下进行的，包括基准同分布（IID）和病态非同分布（non-IID），以及采用狄利克雷分布（Dirichlet distribution）创建的新测试环境来测试算法在复杂数据异质性下的表现。


<details>
  <summary>Details</summary>
Motivation: 联邦学习在处理异构数据环境中的问题，例如客户端漂移和泛化能力不足，通过引入FedAvg的改良版FLIU来提升联邦学习的整体性能和稳定性。

Method: 方法包括了对联邦学习各个阶段的详细评估，以及通过添加个性化步骤来改进FedAvg算法，开发出FLIU。该步骤通过一个自适应的个性化因子来调整局部模型的更新。评估采用MNIST和CIFAR-10数据集，涵盖了不同分布条件下的表现测试。

Result: FLIU在多种分布条件下展示了良好的效果，特别是在病理性的非同分布环境中，其性能优于标准FedAvg，证明了其对复杂数据异质性的适应性。

Conclusion: 本研究通过详细评估不同数据分布环境下的联邦学习算法，证明了FLIU在处理异构数据环境中的有效性和泛化能力。

Abstract: In the context of Federated Learning with heterogeneous data environments,
local models tend to converge to their own local model optima during local
training steps, deviating from the overall data distributions. Aggregation of
these local updates, e.g., with FedAvg, often does not align with the global
model optimum (client drift), resulting in an update that is suboptimal for
most clients. Personalized Federated Learning approaches address this challenge
by exclusively focusing on the average local performances of clients' models on
their own data distribution. Generalization to out-of-distribution samples,
which is a substantial benefit of FedAvg and represents a significant component
of robustness, appears to be inadequately incorporated into the assessment and
evaluation processes. This study involves a thorough evaluation of Federated
Learning approaches, encompassing both their local performance and their
generalization capabilities. Therefore, we examine different stages within a
single communication round to enable a more nuanced understanding of the
considered metrics. Furthermore, we propose and incorporate a modified approach
of FedAvg, designated as Federated Learning with Individualized Updates (FLIU),
extending the algorithm by a straightforward individualization step with an
adaptive personalization factor. We evaluate and compare the approaches
empirically using MNIST and CIFAR-10 under various distributional conditions,
including benchmark IID and pathological non-IID, as well as additional novel
test environments with Dirichlet distribution specifically developed to stress
the algorithms on complex data heterogeneity.

</details>


### [156] [Graph-Guided Concept Selection for Efficient Retrieval-Augmented Generation](https://arxiv.org/abs/2510.24120)
*Ziyu Liu,Yijing Liu,Jianfei Yuan,Minzhi Yan,Le Yue,Honghui Xiong,Yi Yang*

Main category: cs.LG

TL;DR: 提出了Graph-Guided Concept Selection (G2ConS) 方法，通过选择关键文档片段和建立无依赖大型语言模型的概念图，来提升知识图构建的效率并提高检索效果和问答质量。实验表明其在成本、检索有效性和问答质量上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 在特定领域（如生物医学、法律和政治学），基于大型语言模型的问答系统通过文本片段推理会增加高昂的调用成本。因此，需要减少LLM调用的数量来降低成本的同时保证检索的准确性和效率。

Method: 提出了一种基于区块选择的方法和无依赖大型语言模型的概念图，通过选择具有代表性的文本片段减少知识图构建成本，同时使用这个概念图填补由区块选择带来的知识空缺，无需额外的LLM调用成本。

Result: 实验结果表明，所提出的G2ConS方法在知识图构建成本、检索效果和问答质量上都优于其他基准方法。

Conclusion: G2ConS方法不仅能够显著降低知识图构建成本，还能够提升检索和问答任务的效果。

Abstract: Graph-based RAG constructs a knowledge graph (KG) from text chunks to enhance
retrieval in Large Language Model (LLM)-based question answering. It is
especially beneficial in domains such as biomedicine, law, and political
science, where effective retrieval often involves multi-hop reasoning over
proprietary documents. However, these methods demand numerous LLM calls to
extract entities and relations from text chunks, incurring prohibitive costs at
scale. Through a carefully designed ablation study, we observe that certain
words (termed concepts) and their associated documents are more important.
Based on this insight, we propose Graph-Guided Concept Selection (G2ConS). Its
core comprises a chunk selection method and an LLM-independent concept graph.
The former selects salient document chunks to reduce KG construction costs; the
latter closes knowledge gaps introduced by chunk selection at zero cost.
Evaluations on multiple real-world datasets show that G2ConS outperforms all
baselines in construction cost, retrieval effectiveness, and answering quality.

</details>


### [157] [Causal Convolutional Neural Networks as Finite Impulse Response Filters](https://arxiv.org/abs/2510.24125)
*Kiran Bacsa,Wei Liu,Xudong Jian,Huangbin Liang,Eleni Chatzi*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: This study investigates the behavior of Causal Convolutional Neural Networks
(CNNs) with quasi-linear activation functions when applied to time-series data
characterized by multimodal frequency content. We demonstrate that, once
trained, such networks exhibit properties analogous to Finite Impulse Response
(FIR) filters, particularly when the convolutional kernels are of extended
length exceeding those typically employed in standard CNN architectures. Causal
CNNs are shown to capture spectral features both implicitly and explicitly,
offering enhanced interpretability for tasks involving dynamic systems.
Leveraging the associative property of convolution, we further show that the
entire network can be reduced to an equivalent single-layer filter resembling
an FIR filter optimized via least-squares criteria. This equivalence yields new
insights into the spectral learning behavior of CNNs trained on signals with
sparse frequency content. The approach is validated on both simulated beam
dynamics and real-world bridge vibration datasets, underlining its relevance
for modeling and identifying physical systems governed by dynamic responses.

</details>


### [158] [Fixed Point Neural Acceleration and Inverse Surrogate Model for Battery Parameter Identification](https://arxiv.org/abs/2510.24135)
*Hojin Cheon,Hyeongseok Seo,Jihun Jeon,Wooju Lee,Dohyun Jeong,Hongseok Kim*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: The rapid expansion of electric vehicles has intensified the need for
accurate and efficient diagnosis of lithium-ion batteries. Parameter
identification of electrochemical battery models is widely recognized as a
powerful method for battery health assessment. However, conventional
metaheuristic approaches suffer from high computational cost and slow
convergence, and recent machine learning methods are limited by their reliance
on constant current data, which may not be available in practice. To overcome
these challenges, we propose deep learning-based framework for parameter
identification of electrochemical battery models. The proposed framework
combines a neural surrogate model of the single particle model with electrolyte
(NeuralSPMe) and a deep learning-based fixed-point iteration method. NeuralSPMe
is trained on realistic EV load profiles to accurately predict lithium
concentration dynamics under dynamic operating conditions while a parameter
update network (PUNet) performs fixed-point iterative updates to significantly
reduce both the evaluation time per sample and the overall number of iterations
required for convergence. Experimental evaluations demonstrate that the
proposed framework accelerates the parameter identification by more than 2000
times, achieves superior sample efficiency and more than 10 times higher
accuracy compared to conventional metaheuristic algorithms, particularly under
dynamic load scenarios encountered in practical applications.

</details>


### [159] [Identifiable learning of dissipative dynamics](https://arxiv.org/abs/2510.24160)
*Aiqing Zhu,Beatrice W. Soh,Grigorios A. Pavliotis,Qianxiao Li*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Complex dissipative systems appear across science and engineering, from
polymers and active matter to learning algorithms. These systems operate far
from equilibrium, where energy dissipation and time irreversibility are key to
their behavior, but are difficult to quantify from data. Learning accurate and
interpretable models of such dynamics remains a major challenge: the models
must be expressive enough to describe diverse processes, yet constrained enough
to remain physically meaningful and mathematically identifiable. Here, we
introduce I-OnsagerNet, a neural framework that learns dissipative stochastic
dynamics directly from trajectories while ensuring both interpretability and
uniqueness. I-OnsagerNet extends the Onsager principle to guarantee that the
learned potential is obtained from the stationary density and that the drift
decomposes cleanly into time-reversible and time-irreversible components, as
dictated by the Helmholtz decomposition. Our approach enables us to calculate
the entropy production and to quantify irreversibility, offering a principled
way to detect and quantify deviations from equilibrium. Applications to polymer
stretching in elongational flow and to stochastic gradient Langevin dynamics
reveal new insights, including super-linear scaling of barrier heights and
sub-linear scaling of entropy production rates with the strain rate, and the
suppression of irreversibility with increasing batch size. I-OnsagerNet thus
establishes a general, data-driven framework for discovering and interpreting
non-equilibrium dynamics.

</details>


### [160] [EddyFormer: Accelerated Neural Simulations of Three-Dimensional Turbulence at Scale](https://arxiv.org/abs/2510.24173)
*Yiheng Du,Aditi S. Krishnapriyan*

Main category: cs.LG

TL;DR: 提出了一种名为EddyFormer的基于Transformer的光谱元方法架构，用于大规模湍流模拟，结合了光谱方法的准确性与注意力机制的可扩展性。EddyFormer在256^3分辨率下实现了直接数值模拟级别的精度，速度提高了30倍。在更大规模的新领域中也表现出很好的泛化能力，并在复杂物理条件下准确再现了湍流动力学。


<details>
  <summary>Details</summary>
Motivation: 现有技术通过直接数值模拟完全解析大型湍流从计算上来说是不可行的，因此提出了数据驱动的机器学习替代方案。EddyFormer既具有光谱方法的精度，又具有注意力机制的可扩展性，提供了更好的计算效率和准确性。

Method: 引入了光谱元方法标记，将流体分解为网格尺度和次网格尺度组件，结合了局部和全局特征的捕捉。创建了一个新的三维同性湍流数据集，训练EddyFormer以获得直接数值模拟级别的精度，展现出良好的泛化能力。同时，在多样化的湍流测试套件（The Well）上进行测试，以确保模型的全面准确性和稳定性。

Result: EddyFormer在256^3分辨率下实现了直接数值模拟级别的精度，速度提高了30倍。此外，它还能保持在比训练数据集大4倍的未见领域中的准确度，并在一系列物理条件下准确再现了复杂的动力学。EddyFormer通过在各种湍流测试中取得成功，超过了以前的机器学习模型。

Conclusion: EddyFormer作为一种基于Transformer的新型架构，成功地解决了大规模湍流模拟中的准确性与可扩展性挑战，提供了高效的计算方法，适用于广泛的物理条件下的流量预测和分析。

Abstract: Computationally resolving turbulence remains a central challenge in fluid
dynamics due to its multi-scale interactions. Fully resolving large-scale
turbulence through direct numerical simulation (DNS) is computationally
prohibitive, motivating data-driven machine learning alternatives. In this
work, we propose EddyFormer, a Transformer-based spectral-element (SEM)
architecture for large-scale turbulence simulation that combines the accuracy
of spectral methods with the scalability of the attention mechanism. We
introduce an SEM tokenization that decomposes the flow into grid-scale and
subgrid-scale components, enabling capture of both local and global features.
We create a new three-dimensional isotropic turbulence dataset and train
EddyFormer to achieves DNS-level accuracy at 256^3 resolution, providing a 30x
speedup over DNS. When applied to unseen domains up to 4x larger than in
training, EddyFormer preserves accuracy on physics-invariant metrics-energy
spectra, correlation functions, and structure functions-showing domain
generalization. On The Well benchmark suite of diverse turbulent flows,
EddyFormer resolves cases where prior ML models fail to converge, accurately
reproducing complex dynamics across a wide range of physical conditions.

</details>


### [161] [V-SAT: Video Subtitle Annotation Tool](https://arxiv.org/abs/2510.24180)
*Arpita Kundu,Joyita Chakraborty,Anindita Desarkar,Aritra Sen,Srushti Anil Patil,Vishwanathan Raman*

Main category: cs.LG

TL;DR: 提出了V-SAT（视频字幕注释工具），这是一个统一的框架，结合了大语言模型、视觉语言模型、图像处理和自动语音识别，自动检测和纠正放映质量问题，提高了字幕质量，减少了SUBER评分为3.54，语言模式问题的F1分数达到0.80左右，提供了第一个全面的解决方案，确保高质量的字幕注释。


<details>
  <summary>Details</summary>
Motivation: 针对现有字幕生成方法存在的问题，例如同步不良、文本错误或有害的内容，格式不一致，读取速度不合适，无法适应该动态音频-视频上下文，引入了V-SAT来解决这些孤立的特征问题，以减少后期编辑的资源和时间投入。

Method: 结合了大语言模型、视觉语言模型、图像处理和自动语音识别，通过结合音频和视频的上下文提示，自动检测并纠正前景问题，提高字幕质量，达到高质量的字幕输出。

Result: 在解决了所有语言模式问题后的SUBER分数从9.6降至3.54，而图像模式问题的F1分数达到0.80左右，提高了字幕质量，表明了V-SAT的有效性。

Conclusion: 首次提供了全面的解决方案，确保高质量的字幕注释，证明了V-SAT在改善字幕中的综合表现。

Abstract: The surge of audiovisual content on streaming platforms and social media has
heightened the demand for accurate and accessible subtitles. However, existing
subtitle generation methods primarily speech-based transcription or OCR-based
extraction suffer from several shortcomings, including poor synchronization,
incorrect or harmful text, inconsistent formatting, inappropriate reading
speeds, and the inability to adapt to dynamic audio-visual contexts. Current
approaches often address isolated issues, leaving post-editing as a
labor-intensive and time-consuming process. In this paper, we introduce V-SAT
(Video Subtitle Annotation Tool), a unified framework that automatically
detects and corrects a wide range of subtitle quality issues. By combining
Large Language Models(LLMs), Vision-Language Models (VLMs), Image Processing,
and Automatic Speech Recognition (ASR), V-SAT leverages contextual cues from
both audio and video. Subtitle quality improved, with the SUBER score reduced
from 9.6 to 3.54 after resolving all language mode issues and F1-scores of
~0.80 for image mode issues. Human-in-the-loop validation ensures high-quality
results, providing the first comprehensive solution for robust subtitle
annotation.

</details>


### [162] [SPEAR++: Scaling Gradient Inversion via Sparsely-Used Dictionary Learning](https://arxiv.org/abs/2510.24200)
*Alexander Bakarsky,Dimitar I. Dimitrov,Maximilian Baader,Martin Vechev*

Main category: cs.LG

TL;DR: 本文提出了一种新型的SPEAR++攻击方法，该方法使用稀疏字典学习技术优化了SPEAR攻击，使其在处理更大的批量数据时变得更为实际且有效。同时，SPEAR++保留了SPEAR的所有优点，如对抗差分隐私噪声和FedAvg聚合的鲁棒性。 


<details>
  <summary>Details</summary>
Motivation: 传统的梯度反转攻击在处理实际数据时面临挑战，而SPEAR攻击虽在理论上有所突破，但在实际应用中受限于指数级的时间复杂度。为此，研究者提出了一种新的攻击方法SPEAR++。 

Method: 使用来自稀疏字典学习（Sparsely-Used Dictionary Learning）的技术来解决线性层与ReLU激活函数的梯度反转问题，使之可以在较大的批量数据上有效运行。 

Result: 新的SPEAR++攻击方法在大型批量数据上可有效运行，并且保持了SPEAR的所有优点，例如在差分隐私噪声和FedAvg聚合情景下的鲁棒性。 

Conclusion: 通过应用稀疏字典学习技术，SPEAR++成功克服了SPEAR在实际应用中的限制，提供了更强大的攻击手段以测试联邦学习系统的隐私保护能力。 

Abstract: Federated Learning has seen an increased deployment in real-world scenarios
recently, as it enables the distributed training of machine learning models
without explicit data sharing between individual clients. Yet, the introduction
of the so-called gradient inversion attacks has fundamentally challenged its
privacy-preserving properties. Unfortunately, as these attacks mostly rely on
direct data optimization without any formal guarantees, the vulnerability of
real-world systems remains in dispute and requires tedious testing for each new
federated deployment. To overcome these issues, recently the SPEAR attack was
introduced, which is based on a theoretical analysis of the gradients of linear
layers with ReLU activations. While SPEAR is an important theoretical
breakthrough, the attack's practicality was severely limited by its exponential
runtime in the batch size b. In this work, we fill this gap by applying
State-of-the-Art techniques from Sparsely-Used Dictionary Learning to make the
problem of gradient inversion on linear layers with ReLU activations tractable.
Our experiments demonstrate that our new attack, SPEAR++, retains all desirable
properties of SPEAR, such as robustness to DP noise and FedAvg aggregation,
while being applicable to 10x bigger batch sizes.

</details>


### [163] [Unlocking Out-of-Distribution Generalization in Dynamics through Physics-Guided Augmentation](https://arxiv.org/abs/2510.24216)
*Fan Xu,Hao Wu,Kun Wang,Nan Wang,Qingsong Wen,Xian Wu,Wei Gong,Xibin Zhao*

Main category: cs.LG

TL;DR: 我们提出了SPARK，一种基于物理的增强插件，它通过物理参数的自动编码器重建来生成新的训练样本，并将其与增强傅里叶图ODE结合，提高了模型在分布变化和数据稀缺情况下的性能。


<details>
  <summary>Details</summary>
Motivation: 传统数值方法的高计算成本和现代数据驱动方法的数据稀缺和分布变化问题是动力系统建模的限制。为了克服这些问题，提出了物理引导的增强方法SPARK。

Method: SPARK利用重建自动编码器将物理参数集成到物理丰富的离散状态字典中，生成新的训练样本，并与增强傅里叶图ODE结合，用于下游预测。

Result: 在多个基准测试中，SPARK在分布变化和数据稀缺情况下显著优于最先进的基线方法。

Conclusion: 实验结果证明了基于物理的增强方法SPARK的有效性。

Abstract: In dynamical system modeling, traditional numerical methods are limited by
high computational costs, while modern data-driven approaches struggle with
data scarcity and distribution shifts. To address these fundamental
limitations, we first propose SPARK, a physics-guided quantitative augmentation
plugin. Specifically, SPARK utilizes a reconstruction autoencoder to integrate
physical parameters into a physics-rich discrete state dictionary. This state
dictionary then acts as a structured dictionary of physical states, enabling
the creation of new, physically-plausible training samples via principled
interpolation in the latent space. Further, for downstream prediction, these
augmented representations are seamlessly integrated with a Fourier-enhanced
Graph ODE, a combination designed to robustly model the enriched data
distribution while capturing long-term temporal dependencies. Extensive
experiments on diverse benchmarks demonstrate that SPARK significantly
outperforms state-of-the-art baselines, particularly in challenging
out-of-distribution scenarios and data-scarce regimes, proving the efficacy of
our physics-guided augmentation paradigm.

</details>


### [164] [Closing Gaps: An Imputation Analysis of ICU Vital Signs](https://arxiv.org/abs/2510.24217)
*Alisher Turubayev,Anna Shopova,Fabian Lange,Mahmut Kamalak,Paul Mattes,Victoria Ayvasky,Bert Arnrich,Bjarne Pfitzner,Robin P. van de Water*

Main category: cs.LG

TL;DR: 本文通过比较15种插补方法和4种删失方法在ICU数据集上的表现，为提高临床预测模型性能提供了指导和基准比较基础，促进了机器学习方法在临床实践中的应用。


<details>
  <summary>Details</summary>
Motivation: 重症监护病房（ICU）数据中存在大量的缺失值，这限制了机器学习（ML）在临床预测中的应用。为了提高临床预测模型的性能，需要选择最佳的插补技术来填补缺失数据。当前仍存在一些降低预测准确度的手动插补方法，因此需要比较现有的插补方法，以鉴别出最佳实践。

Method: 本文引入了一种可扩展且可重复使用的基准，用来在主要的ICU数据集上评估15种插补方法和4种删失方法的表现，帮助研究者选择最优的插补技术。

Result: 研究表明，不同的插补方法在不同的数据集上表现出不同性能，这展示了选择最合适插补方法的重要性。这些结果能够帮助研究者提高临床预测模型的准确性。

Conclusion: 本研究提供了详细的插补方法比较基准，有助于提升机器学习在临床预测中的应用，促进了模型从实验室到临床的转化。

Abstract: As more Intensive Care Unit (ICU) data becomes available, the interest in
developing clinical prediction models to improve healthcare protocols
increases. However, the lack of data quality still hinders clinical prediction
using Machine Learning (ML). Many vital sign measurements, such as heart rate,
contain sizeable missing segments, leaving gaps in the data that could
negatively impact prediction performance. Previous works have introduced
numerous time-series imputation techniques. Nevertheless, more comprehensive
work is needed to compare a representative set of methods for imputing ICU
vital signs and determine the best practice. In reality, ad-hoc imputation
techniques that could decrease prediction accuracy, like zero imputation, are
still used. In this work, we compare established imputation techniques to guide
researchers in improving the performance of clinical prediction models by
selecting the most accurate imputation technique. We introduce an extensible
and reusable benchmark with currently 15 imputation and 4 amputation methods,
created for benchmarking on major ICU datasets. We hope to provide a
comparative basis and facilitate further ML development to bring more models
into clinical practice.

</details>


### [165] [PRIVET: Privacy Metric Based on Extreme Value Theory](https://arxiv.org/abs/2510.24233)
*Antoine Szatkownik,Aurélien Decelle,Beatriz Seoane,Nicolas Bereux,Léo Planche,Guillaume Charpiat,Burak Yelmen,Flora Jay,Cyril Furtlehner*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Deep generative models are often trained on sensitive data, such as genetic
sequences, health data, or more broadly, any copyrighted, licensed or protected
content. This raises critical concerns around privacy-preserving synthetic
data, and more specifically around privacy leakage, an issue closely tied to
overfitting. Existing methods almost exclusively rely on global criteria to
estimate the risk of privacy failure associated to a model, offering only
quantitative non interpretable insights. The absence of rigorous evaluation
methods for data privacy at the sample-level may hinder the practical
deployment of synthetic data in real-world applications. Using extreme value
statistics on nearest-neighbor distances, we propose PRIVET, a generic
sample-based, modality-agnostic algorithm that assigns an individual privacy
leak score to each synthetic sample. We empirically demonstrate that PRIVET
reliably detects instances of memorization and privacy leakage across diverse
data modalities, including settings with very high dimensionality, limited
sample sizes such as genetic data and even under underfitting regimes. We
compare our method to existing approaches under controlled settings and show
its advantage in providing both dataset level and sample level assessments
through qualitative and quantitative outputs. Additionally, our analysis
reveals limitations in existing computer vision embeddings to yield
perceptually meaningful distances when identifying near-duplicate samples.

</details>


### [166] [Sparse Optimistic Information Directed Sampling](https://arxiv.org/abs/2510.24234)
*Ludovic Schwartz,Hamish Flynn,Gergely Neu*

Main category: cs.LG

TL;DR: 研究提出了一种新的算法Sparse Optimistic Information Directed Sampling (SOIDS)，可以在数据丰富和数据贫乏的环境中同时实现最优的最坏情况遗憾。通过使用时间依赖的学习率，SOIDS能够优化信息和遗憾之间的平衡。实验结果表明SOIDS具有良好的性能。


<details>
  <summary>Details</summary>
Motivation: 目前大多数算法在数据丰富的情况下能够实现最优遗憾，在数据贫乏的情况下则表现出次优的遗憾。本文提出SOIDS，意图在没有贝叶斯假设的情况下实现同时在数据丰富和数据贫乏两种环境下均最优的遗憾。

Method: 利用时间依赖的学习率，SOIDS算法可以实现对信息和遗憾的优化平衡。这种新的分析方法进一步扩展了IDS的理论保证，提供了一种新的优化算法。研究通过实验验证了SOIDS的有效性。

Result: SOIDS算法成功实现了在数据丰富和数据贫乏两种环境下的理想遗憾优化，其效果通过实验得到了验证，实现了最优的最坏情况遗憾。这一结果扩展了IDS的理论基础，提供了在贝叶斯假设之外新的理论支持。

Conclusion: 通过建立SOIDS算法，研究实现了在高维度稀疏线性贝叶斯问题中同时在数据丰富和数据贫乏两种环境下的最优遗憾优化。这一成果对优化和决策制定领域是重要的贡献。

Abstract: Many high-dimensional online decision-making problems can be modeled as
stochastic sparse linear bandits. Most existing algorithms are designed to
achieve optimal worst-case regret in either the data-rich regime, where
polynomial depen- dence on the ambient dimension is unavoidable, or the
data-poor regime, where dimension-independence is possible at the cost of worse
dependence on the num- ber of rounds. In contrast, the sparse Information
Directed Sampling (IDS) algo- rithm satisfies a Bayesian regret bound that has
the optimal rate in both regimes simultaneously. In this work, we explore the
use of Sparse Optimistic Informa- tion Directed Sampling (SOIDS) to achieve the
same adaptivity in the worst-case setting, without Bayesian assumptions.
Through a novel analysis that enables the use of a time-dependent learning
rate, we show that SOIDS can optimally balance information and regret. Our
results extend the theoretical guarantees of IDS, pro- viding the first
algorithm that simultaneously achieves optimal worst-case regret in both the
data-rich and data-poor regimes. We empirically demonstrate the good
performance of SOIDS.

</details>


### [167] [PaTaRM: Bridging Pairwise and Pointwise Signals via Preference-Aware Task-Adaptive Reward Modeling](https://arxiv.org/abs/2510.24235)
*Ai Jian,Jingqing Ruan,Xing Ma,Dailin Li,QianLin Zhou,Ke Zeng,Xunliang Cai*

Main category: cs.LG

TL;DR: 提出了一种新的奖励模型PaTaRM，结合了偏好意识奖励机制和动态评分标准适应系统，旨在通过相对偏好信息提高奖励模型的鲁棒性和泛化能力，同时减少标签需求，提高了RLHF的效果和效率


<details>
  <summary>Details</summary>
Motivation: 现有的奖励模型在直接监督学习时存在局限性，包括生成型模型缺乏可解释性，点对点方法需要复杂的配对策略和昂贵的标签成本。需要一种结合两者的优点，更高效、更泛化的模型

Method: PaTaRM通过相对偏好信息构建点对点训练信号，同时采用任务适应性评分系统生成细粒度评价标准

Result: 实验证明，PaTaRM在RewardBench和RMBench上平均相对提高了4.7%，在下游RLHF任务上平均提高了13.6%。这表明PaTaRM是有效的，且具有鲁棒性

Conclusion: PaTaRM提供了一种新的框架，结合相对偏好信息与动态评分标准适应，使奖励模型在RLHF中更高效、更泛化、更有解释性

Abstract: Reward models (RMs) are central to reinforcement learning from human feedback
(RLHF), providing the critical supervision signals that align large language
models (LLMs) with human preferences. While generative reward models (GRMs)
offer greater interpretability than traditional scalar RMs, current training
paradigms remain limited. Pair-wise methods rely on binary good-versus-bad
labels, which cause mismatches for point-wise inference and necessitate complex
pairing strategies for effective application in RLHF. On the other hand,
point-wise methods require more elaborate absolute labeling with rubric-driven
criteria, resulting in poor adaptability and high annotation costs. In this
work, we propose the Preference-Aware Task-Adaptive Reward Model (PaTaRM), a
unified framework that integrates a preference-aware reward (PAR) mechanism
with dynamic rubric adaptation. PaTaRM leverages relative preference
information from pairwise data to construct robust point-wise training signals,
eliminating the need for explicit point-wise labels. Simultaneously, it employs
a task-adaptive rubric system that flexibly generates evaluation criteria for
both global task consistency and instance-specific fine-grained reasoning. This
design enables efficient, generalizable, and interpretable reward modeling for
RLHF. Extensive experiments show that PaTaRM achieves an average relative
improvement of 4.7% on RewardBench and RMBench across Qwen3-8B and Qwen3-14B
models. Furthermore, PaTaRM boosts downstream RLHF performance, with an average
improvement of 13.6% across IFEval and InFoBench benchmarks, confirming its
effectiveness and robustness. Our code is available at
https://github.com/JaneEyre0530/PaTaRM.

</details>


### [168] [Temporal Knowledge Graph Hyperedge Forecasting: Exploring Entity-to-Category Link Prediction](https://arxiv.org/abs/2510.24240)
*Edward Markai,Sina Molavipour*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Temporal Knowledge Graphs have emerged as a powerful way of not only modeling
static relationships between entities but also the dynamics of how relations
evolve over time. As these informational structures can be used to store
information from a real-world setting, such as a news flow, predicting future
graph components to a certain extent equates predicting real-world events. Most
of the research in this field focuses on embedding-based methods, often
leveraging convolutional neural net architectures. These solutions act as black
boxes, limiting insight. In this paper, we explore an extension to an
established rule-based framework, TLogic, that yields a high accuracy in
combination with explainable predictions. This offers transparency and allows
the end-user to critically evaluate the rules applied at the end of the
prediction stage. The new rule format incorporates entity category as a key
component with the purpose of limiting rule application only to relevant
entities. When categories are unknown for building the graph, we propose a
data-driven method to generate them with an LLM-based approach. Additionally,
we investigate the choice of aggregation method for scores of retrieved
entities when performing category prediction.

</details>


### [169] [SALS: Sparse Attention in Latent Space for KV cache Compression](https://arxiv.org/abs/2510.24273)
*Junlin Mu,Hantao Huang,Jihang Zhang,Minghui Yu,Tao Wang,Yidong Li*

Main category: cs.LG

TL;DR: 提出了一种名为Sparse Attention in Latent Space (SALS) 的框架，通过在隐空间中进行低秩投影和稀疏标记选择，以克服大型语言模型中的KV缓存压缩和速度瓶颈问题。该方法在多个大型模型和任务上显示出优异的性能，相比FlashAttention2在缓存压缩和注意力操作加速方面有显著优势，同时维持了较高的准确性。


<details>
  <summary>Details</summary>
Motivation: 为了应对大型语言模型中KV缓存大小和内存带宽需求高的挑战，同时保持准确性，提出了一种新的压缩策略。该策略克服了传统的低秩压缩在现代LLMs中应用时造成的降准问题，尤其是在使用RoPE机制时。通过新的方法，在不影响模型表现的前提下，进一步减小内存使用并加速推理过程。

Method: 提出了Sparse Attention in Latent Space（SALS）框架，该框架通过低秩投影将KV缓存压缩进紧凑的隐空间，并使用RoPE免查询-键交互进行稀疏标记选择。这种方法通过重建重要标记的关键值，避免了完整KV缓存重建的开销，从而实现KV缓存压缩和注意力操作加速。

Result: 实验结果显示，SALS在LLaMA2-7b-chat和Mistral-7b上取得了优异的结果，尤其是在序列长度4K的情况下，与FlashAttention2相比，缓存压缩比为6.4，注意力操作加速比为5.7。在4K和32K序列长度情况下，与GPT-fast相比，整体吞吐量性能分别提高了1.4倍和4.5倍，同时保持了较高的准确性。

Conclusion: SALS通过在隐空间中进行低秩投影和RoPE免查询-键交互，有效实现了KV缓存的压缩和注意力操作的加速，提高了大型语言模型的内存效率和推理速度，同时保持了高的准确率。

Abstract: Large Language Models capable of handling extended contexts are in high
demand, yet their inference remains challenging due to substantial Key-Value
cache size and high memory bandwidth requirements. Previous research has
demonstrated that KV cache exhibits low-rank characteristics within the hidden
dimension, suggesting the potential for effective compression. However, due to
the widely adopted Rotary Position Embedding mechanism in modern LLMs, naive
low-rank compression suffers severe accuracy degradation or creates a new speed
bottleneck, as the low-rank cache must first be reconstructed in order to apply
RoPE. In this paper, we introduce two key insights: first, the application of
RoPE to the key vectors increases their variance, which in turn results in a
higher rank; second, after the key vectors are transformed into the latent
space, they largely maintain their representation across most layers. Based on
these insights, we propose the Sparse Attention in Latent Space framework. SALS
projects the KV cache into a compact latent space via low-rank projection, and
performs sparse token selection using RoPE-free query-key interactions in this
space. By reconstructing only a small subset of important tokens, it avoids the
overhead of full KV cache reconstruction. We comprehensively evaluate SALS on
various tasks using two large-scale models: LLaMA2-7b-chat and Mistral-7b, and
additionally verify its scalability on the RULER-128k benchmark with
LLaMA3.1-8B-Instruct. Experimental results demonstrate that SALS achieves SOTA
performance by maintaining competitive accuracy. Under different settings, SALS
achieves 6.4-fold KV cache compression and 5.7-fold speed-up in the attention
operator compared to FlashAttention2 on the 4K sequence. For the end-to-end
throughput performance, we achieves 1.4-fold and 4.5-fold improvement compared
to GPT-fast on 4k and 32K sequences, respectively.

</details>


### [170] [EDC: Equation Discovery for Classification](https://arxiv.org/abs/2510.24310)
*Guus Toussaint,Arno Knobbe*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Equation Discovery techniques have shown considerable success in regression
tasks, where they are used to discover concise and interpretable models
(\textit{Symbolic Regression}). In this paper, we propose a new ED-based binary
classification framework. Our proposed method EDC finds analytical functions of
manageable size that specify the location and shape of the decision boundary.
In extensive experiments on artificial and real-life data, we demonstrate how
EDC is able to discover both the structure of the target equation as well as
the value of its parameters, outperforming the current state-of-the-art
ED-based classification methods in binary classification and achieving
performance comparable to the state of the art in binary classification. We
suggest a grammar of modest complexity that appears to work well on the tested
datasets but argue that the exact grammar -- and thus the complexity of the
models -- is configurable, and especially domain-specific expressions can be
included in the pattern language, where that is required. The presented grammar
consists of a series of summands (additive terms) that include linear,
quadratic and exponential terms, as well as products of two features (producing
hyperbolic curves ideal for capturing XOR-like dependencies). The experiments
demonstrate that this grammar allows fairly flexible decision boundaries while
not so rich to cause overfitting.

</details>


### [171] [Transformers can do Bayesian Clustering](https://arxiv.org/abs/2510.24318)
*Prajit Bhaskaran,Tom Viering*

Main category: cs.LG

TL;DR: 本文提出了一种基于Transformer的模型Cluster-PFN，用于无监督的贝叶斯聚类，该模型能够处理包含缺失值的数据集，并在真实数据集上表现优于传统方法。Cluster-PFN在估计聚类数量的准确性上超越了手工模型选择方法，并且在速度上也更快。


<details>
  <summary>Details</summary>
Motivation: 传统贝叶斯聚类方法计算量大，难以处理大规模数据集。此外，实际数据集通常包含缺失值，简单的数据填补方法忽略了这种不确定性，可能导致次优结果。因此，迫切需要一种既能够处理缺失值，又具备高效率和准确性的聚类方法。

Method: 本文提出的Cluster-PFN模型基于Transformer架构，并通过从有限高斯混合模型生成的合成数据集进行训练，从而学会了估计关于聚类数量和聚类分配的后验分布。该模型能在复杂先验中进行训练，包括具有缺失数据的情况。

Result: Cluster-PFN在估计聚类数量上的准确性上超越了AIC、BIC和变分推断等传统模型选择方法，并且其聚类质量与变分推断相当，但速度要快得多。在真实基因组数据集上，Cluster-PFN在高缺失值的数据上表现优于基于填补的基本线方法。

Conclusion: 这些结果说明了Cluster-PFN能够提供可扩展和灵活的贝叶斯聚类方法。

Abstract: Bayesian clustering accounts for uncertainty but is computationally demanding
at scale. Furthermore, real-world datasets often contain missing values, and
simple imputation ignores the associated uncertainty, resulting in suboptimal
results. We present Cluster-PFN, a Transformer-based model that extends
Prior-Data Fitted Networks (PFNs) to unsupervised Bayesian clustering. Trained
entirely on synthetic datasets generated from a finite Gaussian Mixture Model
(GMM) prior, Cluster-PFN learns to estimate the posterior distribution over
both the number of clusters and the cluster assignments. Our method estimates
the number of clusters more accurately than handcrafted model selection
procedures such as AIC, BIC and Variational Inference (VI), and achieves
clustering quality competitive with VI while being orders of magnitude faster.
Cluster-PFN can be trained on complex priors that include missing data,
outperforming imputation-based baselines on real-world genomic datasets, at
high missingness. These results show that the Cluster-PFN can provide scalable
and flexible Bayesian clustering.

</details>


### [172] [Perception Learning: A Formal Separation of Sensory Representation Learning from Decision Learning](https://arxiv.org/abs/2510.24356)
*Suman Sanyal*

Main category: cs.LG

TL;DR: 我们引入了感知学习（PeL）范式，该范式通过任务无关的信号来优化智能体的感官接口，使其稳定且具信息量，并不受下游决策学习的影响。我们还提供了用于评估感知质量的任务无关的评价指标。


<details>
  <summary>Details</summary>
Motivation: 传统方法通常将感知学习与决策学习耦合在一起，这限制了感知学习的表现。为此，我们希望将感知从决策中解耦出来，使其能够独立于任务进行优化，从而获得更好的感知性能。

Method: PeL范式通过任务无关的信号来直接优化感知接口，而不需要下游任务的反馈。我们定义了一系列感知属性，并提出了一种新的优化方法，这种方法可以保持足够的不变性，从而与贝叶斯风险梯度正交。另外，我们还提供了一套任务无关的评价指标来评估感知质量。 

Result: 我们的方法可以独立地优化感知接口，从而提高感知质量和鲁棒性。同时，通过任务无关的评价指标，我们可以有效地评估感知接口的性能。

Conclusion: PeL范式为优化感知接口提供了一种新的途径，它可以独立地优化感知，而不需要知道具体任务的细节能。

Abstract: We introduce Perception Learning (PeL), a paradigm that optimizes an agent's
sensory interface $f_\phi:\mathcal{X}\to\mathcal{Z}$ using task-agnostic
signals, decoupled from downstream decision learning
$g_\theta:\mathcal{Z}\to\mathcal{Y}$. PeL directly targets label-free
perceptual properties, such as stability to nuisances, informativeness without
collapse, and controlled geometry, assessed via objective
representation-invariant metrics. We formalize the separation of perception and
decision, define perceptual properties independent of objectives or
reparameterizations, and prove that PeL updates preserving sufficient
invariants are orthogonal to Bayes task-risk gradients. Additionally, we
provide a suite of task-agnostic evaluation metrics to certify perceptual
quality.

</details>


### [173] [Filtering instances and rejecting predictions to obtain reliable models in healthcare](https://arxiv.org/abs/2510.24368)
*Maria Gabriela Valeriano,David Kohan Marzagão,Alfredo Montelongo,Carlos Roberto Veiga Kiffer,Natan Katz,Ana Carolina Lorena*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Machine Learning (ML) models are widely used in high-stakes domains such as
healthcare, where the reliability of predictions is critical. However, these
models often fail to account for uncertainty, providing predictions even with
low confidence. This work proposes a novel two-step data-centric approach to
enhance the performance of ML models by improving data quality and filtering
low-confidence predictions. The first step involves leveraging Instance
Hardness (IH) to filter problematic instances during training, thereby refining
the dataset. The second step introduces a confidence-based rejection mechanism
during inference, ensuring that only reliable predictions are retained. We
evaluate our approach using three real-world healthcare datasets, demonstrating
its effectiveness at improving model reliability while balancing predictive
performance and rejection rate. Additionally, we use alternative criteria -
influence values for filtering and uncertainty for rejection - as baselines to
evaluate the efficiency of the proposed method. The results demonstrate that
integrating IH filtering with confidence-based rejection effectively enhances
model performance while preserving a large proportion of instances. This
approach provides a practical method for deploying ML systems in
safety-critical applications.

</details>


### [174] [A Comprehensive Evaluation Framework for Synthetic Trip Data Generation in Public Transport](https://arxiv.org/abs/2510.24375)
*Yuanyuan Wu,Zhenlin Qin,Zhenliang Ma*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Synthetic data offers a promising solution to the privacy and accessibility
challenges of using smart card data in public transport research. Despite rapid
progress in generative modeling, there is limited attention to comprehensive
evaluation, leaving unclear how reliable, safe, and useful synthetic data truly
are. Existing evaluations remain fragmented, typically limited to
population-level representativeness or record-level privacy, without
considering group-level variations or task-specific utility. To address this
gap, we propose a Representativeness-Privacy-Utility (RPU) framework that
systematically evaluates synthetic trip data across three complementary
dimensions and three hierarchical levels (record, group, population). The
framework integrates a consistent set of metrics to quantify similarity,
disclosure risk, and practical usefulness, enabling transparent and balanced
assessment of synthetic data quality. We apply the framework to benchmark
twelve representative generation methods, spanning conventional statistical
models, deep generative networks, and privacy-enhanced variants. Results show
that synthetic data do not inherently guarantee privacy and there is no
"one-size-fits-all" model, the trade-off between privacy and
representativeness/utility is obvious. Conditional Tabular generative
adversarial network (CTGAN) provide the most balanced trade-off and is
suggested for practical applications. The RPU framework provides a systematic
and reproducible basis for researchers and practitioners to compare synthetic
data generation techniques and select appropriate methods in public transport
applications.

</details>


### [175] [Fill in the Blanks: Accelerating Q-Learning with a Handful of Demonstrations in Sparse Reward Settings](https://arxiv.org/abs/2510.24432)
*Seyed Mahdi Basiri Azad,Joschka Boedecker*

Main category: cs.LG

TL;DR: 本文提出了一种在稀疏奖励环境中，通过使用少量成功的演示来初始化价值函数，从而提高强化学习效率的方法。该方法减轻了探索负担并提高了样本效率。实验结果表明该方法加速了收敛速度，并优于标准基准线方法，即便是使用最少的或次优的演示数据也能取得好结果。


<details>
  <summary>Details</summary>
Motivation: 稀疏奖励环境下的强化学习具有挑战性，因为缺乏详细的反馈信息。为了解决该问题，该文旨在利用少量成功的演示数据来改进强化学习过程，从而提升学习效率和减少探索期。

Method: 方法主要分为两个阶段：初始化阶段，通过离线计算成功的演示数据的价值估计作为在线学习的目标；优化阶段，通过在线互动进一步优化这些预估值，形成混合的离线到在线的学习机制。

Result: 在此种稀疏奖励的环境下，所提出的方法能够显著加速收敛速度并超越标准基准技术，即使是在演示数据很少或次优的情况下也表现出色。

Conclusion: 该研究的结果展示了通过带有价值函数初始化的成功演示数据来提升稀疏奖励环境下强化学习性能的潜力，这种方法可能为解决类似问题提供新的方案。

Abstract: Reinforcement learning (RL) in sparse-reward environments remains a
significant challenge due to the lack of informative feedback. We propose a
simple yet effective method that uses a small number of successful
demonstrations to initialize the value function of an RL agent. By precomputing
value estimates from offline demonstrations and using them as targets for early
learning, our approach provides the agent with a useful prior over promising
actions. The agent then refines these estimates through standard online
interaction. This hybrid offline-to-online paradigm significantly reduces the
exploration burden and improves sample efficiency in sparse-reward settings.
Experiments on benchmark tasks demonstrate that our method accelerates
convergence and outperforms standard baselines, even with minimal or suboptimal
demonstration data.

</details>


### [176] [Methodology for Comparing Machine Learning Algorithms for Survival Analysis](https://arxiv.org/abs/2510.24473)
*Lucas Buk Cardoso,Simone Aldrey Angelo,Yasmin Pacheco Gil Bonilha,Fernando Maia,Adeylson Guimarães Ribeiro,Maria Paula Curado,Gisele Aparecida Fernandes,Vanderlei Cunha Parro,Flávio Almeida de Magalhães Cipparrone,Alexandre Dias Porto Chiavegatto Filho,Tatiana Natasha Toporcov*

Main category: cs.LG

TL;DR: 本研究对比分析了六种机器学习模型在生存分析中的表现，其中XGBoost-AFT模型表现最佳，显示出MLSA在提高生存预测和辅助决策方面的能力和适用性。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于评估不同机器学习模型在生存分析中的预测性能，特别是在处理删失数据时的表现，希望找到适用于提高生存预测准确性的模型来辅助临床决策。

Method: 研究比较了六种机器学习模型：随机生存森林、梯度提升生存分析、生存SVM、XGBoost-Cox、XGBoost-AFT和LightGBM，评估了它们的性能。通过不同采样器进行超参数优化，并使用多个指标来评估模型性能，还包括与分类算法预测的比较，以及使用SHAP和置换重要性来解释预测因子的作用。

Result: XGBoost-AFT模型在经过对比后显示出了最好的性能（C-Index = 0.7618; IPCW = 0.7532），其他表现不错的模型包括GBSA和RSF。结果表明MLSA有潜在的用于改善生存预测和辅助决策的适用性。

Conclusion: 该研究证明了XGBoost-AFT等MLSA模型在预测具有删失数据集的生存时间上的优势，并证明了它们的应用潜力，特别是在提高癌症患者生存预测准确性和提供基于数据驱动的医疗决策支持方面。

Abstract: This study presents a comparative methodological analysis of six machine
learning models for survival analysis (MLSA). Using data from nearly 45,000
colorectal cancer patients in the Hospital-Based Cancer Registries of S\~ao
Paulo, we evaluated Random Survival Forest (RSF), Gradient Boosting for
Survival Analysis (GBSA), Survival SVM (SSVM), XGBoost-Cox (XGB-Cox),
XGBoost-AFT (XGB-AFT), and LightGBM (LGBM), capable of predicting survival
considering censored data. Hyperparameter optimization was performed with
different samplers, and model performance was assessed using the Concordance
Index (C-Index), C-Index IPCW, time-dependent AUC, and Integrated Brier Score
(IBS). Survival curves produced by the models were compared with predictions
from classification algorithms, and predictor interpretation was conducted
using SHAP and permutation importance. XGB-AFT achieved the best performance
(C-Index = 0.7618; IPCW = 0.7532), followed by GBSA and RSF. The results
highlight the potential and applicability of MLSA to improve survival
prediction and support decision making.

</details>


### [177] [MIMIC-Sepsis: A Curated Benchmark for Modeling and Learning from Sepsis Trajectories in the ICU](https://arxiv.org/abs/2510.24500)
*Yong Huang,Zhongqi Yang,Amir Rahmani*

Main category: cs.LG

TL;DR: MIMIC-Sepsis 是一个基于 MIMIC-IV 数据库构建的脓毒症队列和基准框架，用于支持脓毒症轨迹的可重复建模。研究表明，包含治疗变量可以显著提高模型性能，尤其是在Transformer架构中。


<details>
  <summary>Details</summary>
Motivation: 当前关于脓毒症的研究依赖陈旧的数据集，不可复现的预处理流程以及临床干预措施有限的覆盖面。本文提出了 MIMIC-Sepsis，一个基于 MIMIC-IV 数据库建立的脓毒症队列和基准框架，以支持脓毒症轨迹的可重复建模的研究需要。

Method: 描述了一个透明的预处理流程，基于 Sepsis-3 标准，包含结构化的插补策略和治疗变量，以及发布相关的基准任务，包括早期死亡率预测，住院时间估计和休克发生分类。研究中采用的治疗变量包括血管加压药，液体，机械通气和抗生素等。

Result: 实验证明，纳入治疗变量可以显著提高模型的性能，尤其是在Transformer架构下的效果更为显著。

Conclusion: MIMIC-Sepsis 作为一个具有研究和临界照护模型评估强大功能的平台，在重症研究中具有极大的价值。

Abstract: Sepsis is a leading cause of mortality in intensive care units (ICUs), yet
existing research often relies on outdated datasets, non-reproducible
preprocessing pipelines, and limited coverage of clinical interventions. We
introduce MIMIC-Sepsis, a curated cohort and benchmark framework derived from
the MIMIC-IV database, designed to support reproducible modeling of sepsis
trajectories. Our cohort includes 35,239 ICU patients with time-aligned
clinical variables and standardized treatment data, including vasopressors,
fluids, mechanical ventilation and antibiotics. We describe a transparent
preprocessing pipeline-based on Sepsis-3 criteria, structured imputation
strategies, and treatment inclusion-and release it alongside benchmark tasks
focused on early mortality prediction, length-of-stay estimation, and shock
onset classification. Empirical results demonstrate that incorporating
treatment variables substantially improves model performance, particularly for
Transformer-based architectures. MIMIC-Sepsis serves as a robust platform for
evaluating predictive and sequential models in critical care research.

</details>


### [178] [LoRA-DA: Data-Aware Initialization for Low-Rank Adaptation via Asymptotic Analysis](https://arxiv.org/abs/2510.24561)
*Qingyue Zhang,Chang Chu,Tianren Peng,Qi Li,Xiangyang Luo,Zhihao Jiang,Shao-Lun Huang*

Main category: cs.LG

TL;DR: 提出了一种基于渐近分析的LoRA初始化理论框架，称为LoRA-DA，该框架通过解决一个包含偏差和方差项的优化问题，实现数据感知的LoRA初始化，实验表明LoRA-DA在多个基准测试中提高了最终的准确性，具有更快更稳定的收敛性和更好的秩稳定性，且初始化开销小


<details>
  <summary>Details</summary>
Motivation: 现有LoRA初始化方法缺乏严格的理论基础或依赖于限制性假设，性能不理想。为了提供一个数据感知的LoRA初始化方法，构建了一个理论框架，解决现有方法的这些局限性

Method: 提出了LoRA初始化的理论框架，根据渐近分析，从一个优化目标出发，该目标旨在最小化从Scratch模型中调整后的参数与目标模型参数之间的差异的期望值。通过分析参数差异的空间性质，提出了一个考虑了参数距离偏差和样品不确定性方差的优化问题，解决该问题可以找到最优的LoRA初始化策略。基于该框架，开发了LoRA-DA算法，该算法可以从小样本集中估计优化问题中的项，并找到最优的LoRA初始化

Result: LoRA-DA在多个基准测试中提高了最终的准确性，展示了更快、更稳定的收敛性，并且具有良好的秩稳定性，且只有很小的初始化开销

Conclusion: 提出的LoRA-DA方法可以解决现有LoRA初始化方法的不足，提供一种更加有效的初始化策略，实验结果验证了该方法的有效性

Abstract: With the widespread adoption of LLMs, LoRA has become a dominant method for
PEFT, and its initialization methods have attracted increasing attention.
However, existing methods have notable limitations: many methods do not
incorporate target-domain data, while gradient-based methods exploit data only
at a shallow level by relying on one-step gradient decomposition, which remains
unsatisfactory due to the weak empirical performance of the one-step
fine-tuning model that serves as their basis, as well as the fact that these
methods either lack a rigorous theoretical foundation or depend heavily on
restrictive isotropic assumptions. In this paper, we establish a theoretical
framework for data-aware LoRA initialization based on asymptotic analysis.
Starting from a general optimization objective that minimizes the expectation
of the parameter discrepancy between the fine-tuned and target models, we
derive an optimization problem with two components: a bias term, which is
related to the parameter distance between the fine-tuned and target models, and
is approximated using a Fisher-gradient formulation to preserve anisotropy; and
a variance term, which accounts for the uncertainty introduced by sampling
stochasticity through the Fisher information. By solving this problem, we
obtain an optimal initialization strategy for LoRA. Building on this
theoretical framework, we develop an efficient algorithm, LoRA-DA, which
estimates the terms in the optimization problem from a small set of target
domain samples and obtains the optimal LoRA initialization. Empirical results
across multiple benchmarks demonstrate that LoRA-DA consistently improves final
accuracy over existing initialization methods. Additional studies show faster,
more stable convergence, robustness across ranks, and only a small
initialization overhead for LoRA-DA. The source code will be released upon
publication.

</details>


### [179] [DistDF: Time-Series Forecasting Needs Joint-Distribution Wasserstein Alignment](https://arxiv.org/abs/2510.24574)
*Hao Wang,Licheng Pan,Yuan Lu,Zhixuan Chu,Xiaoxi Li,Shuting He,Zhichao Chen,Haoxuan Li,Qingsong Wen,Zhouchen Lin*

Main category: cs.LG

TL;DR: 本文提出了DistDF方法，通过最小化预测分布和标签分布之间的差异来改进时间序列预测模型的训练，特别是当标签序列具有自相关性时表现更佳。实验结果显示DistDF比传统的直接预测方法有更优的效果，并达到了最新的预测性能水平。


<details>
  <summary>Details</summary>
Motivation: 传统直接预测方法在处理具有自相关性的标签序列时会出现偏差，因此需要一种新的方法来解决这一问题。本文提出了DistDF以应对在时间序列预测中出现的这一挑战。

Method: 提出了一种新的联合分布Wasserstein差异度量方法，该方法能够从有限的时间序列观察中估计条件差异，并且该差异度量可以被有效地从经验样本中估计，从而能够与基于梯度的训练过程无缝结合。

Result: DistDF在广泛的实验中提高了不同的预测模型的性能，并且在时间序列预测中达到了最新的技术水平。具体的代码地址为https://anonymous.4open.science/r/DistDF-F66B。

Conclusion: 本文提出了DistDF，作为一种新的时间序列预测方法，它通过最小化预测分布和标签分布之间的差异，能够有效地提升预测性能，尤其是在具有自相关性的标签序列中表现更佳。

Abstract: Training time-series forecast models requires aligning the conditional
distribution of model forecasts with that of the label sequence. The standard
direct forecast (DF) approach resorts to minimize the conditional negative
log-likelihood of the label sequence, typically estimated using the mean
squared error. However, this estimation proves to be biased in the presence of
label autocorrelation. In this paper, we propose DistDF, which achieves
alignment by alternatively minimizing a discrepancy between the conditional
forecast and label distributions. Because conditional discrepancies are
difficult to estimate from finite time-series observations, we introduce a
newly proposed joint-distribution Wasserstein discrepancy for time-series
forecasting, which provably upper bounds the conditional discrepancy of
interest. This discrepancy admits tractable, differentiable estimation from
empirical samples and integrates seamlessly with gradient-based training.
Extensive experiments show that DistDF improves the performance diverse
forecast models and achieves the state-of-the-art forecasting performance. Code
is available at https://anonymous.4open.science/r/DistDF-F66B.

</details>


### [180] [Physics-Informed Extreme Learning Machine (PIELM): Opportunities and Challenges](https://arxiv.org/abs/2510.24577)
*He Yang,Fei Ren,Hai-Sui Yu,Xiaohui Chen,Pei-Zhi Zhuang*

Main category: cs.LG

TL;DR: 本文总结并回顾了物理信息增强的极学习机(PIELM)的发展，并讨论了其在解决偏微分方程等问题上的挑战与机遇，旨在构建更为稳健、可解释且通用的PIELM框架，应用于科学研究与工程领域。


<details>
  <summary>Details</summary>
Motivation: 物理信息增强的极学习机(PIELM)近年来发展迅速，具有高效计算及准确性，但目前缺乏对该领域的总结或综述，因此作者希望通过此论文展示他们的视角与经验，并探讨该研究方向所面临的挑战和机遇，推动构建更为稳健、可解释且通用的PIELM框架，应用于科学研究与工程领域。

Method: 该论文并不是一个具体的研究论文，而是作为一个总结或综述来展示现有的方法和挑战，因此没有特定的方法论。主要有对现有PIELM方法的回顾、分析以及提出未来研究方向的建议。

Result: 该论文指出了PIELM方法在解决含有陡峭梯度、非线性、高频行为、硬约束、不确定性和多物理耦合偏微分方程等问题上的能力，同时也识别出了一些亟待解决的问题，如提高模型的稳健性、可解释性及泛化能力等。

Conclusion: 尽管PIELM在解决复杂偏微分方程问题上表现出色，但仍有许多挑战需要克服，包括提高模型的稳健性、可解释性和泛化能力。这些问题为研究人员提供了开发更加通用、稳健且可解释的PIELM框架的机会。

Abstract: We are very delighted to see the fast development of physics-informed extreme
learning machine (PIELM) in recent years for higher computation efficiency and
accuracy in physics-informed machine learning. As a summary or review on PIELM
is currently not available, we would like to take this opportunity to show our
perspective and experience for this promising research direction. We can see
many efforts are made to solve PDEs with sharp gradients, nonlinearities,
high-frequency behavior, hard constraints, uncertainty, multiphysics coupling.
Despite the success, many urgent challenges remain to be tackled, which also
provides us opportunities to develop more robust, interpretable, and
generalizable PIELM frameworks with applications in science and engineering.

</details>


### [181] [Symbolic Snapshot Ensembles](https://arxiv.org/abs/2510.24633)
*Mingyue Liu,Andrew Cropper*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Inductive logic programming (ILP) is a form of logical machine learning. Most
ILP algorithms learn a single hypothesis from a single training run. Ensemble
methods train an ILP algorithm multiple times to learn multiple hypotheses. In
this paper, we train an ILP algorithm only once and save intermediate
hypotheses. We then combine the hypotheses using a minimum description length
weighting scheme. Our experiments on multiple benchmarks, including game
playing and visual reasoning, show that our approach improves predictive
accuracy by 4% with less than 1% computational overhead.

</details>


### [182] [Causal Ordering for Structure Learning From Time Series](https://arxiv.org/abs/2510.24639)
*Pedro P. Sanchez,Damian Machlanski,Steven McDonagh,Sotirios A. Tsaftaris*

Main category: cs.LG

TL;DR: 本文提出了一种名为DOTS的新型时间因果关系发现方法，该方法通过结合多个有效的因果顺序，有效恢复了潜在有向无环图的传递闭包，从而提高了时间序列数据中因果关系发现的准确性和效率。实验证明该方法优于目前的最优基线方法。


<details>
  <summary>Details</summary>
Motivation: 当前时间序列因果关系发现方法通常依赖单一的因果顺序，这限制了模型的表达能力，并且随着变量数和时间点的增长，识别真实因果关系变得复杂。本文旨在解决这个问题，通过结合多个因果顺序，提高因果关系发现的准确性和效率。

Method: 提出了一种新的方法DOTS，它利用基于扩散的因果发现方法处理时间序列数据，通过整合多个有效的因果顺序，有效恢复潜在有向无环图的传递闭包，从而解决单一因果顺序方法的误导性问题。

Result: DOTS方法在合成数据集和真实世界数据集上进行了广泛的实验，结果表明其在时间因果发现任务上优于目前的最优基线方法。具体而言，在合成数据集上，DOTS方法提高了窗口图F-1得分，从基线方法的0.63提高到了0.81；在CausalTime基准数据集上，虽然基线方法在单个数据集上表现最好，但DOTS实现了最高的平均总结图F-1得分，并且将运行时间相对图优化方法减半。这些结果表明，DOTS是一种可扩展且准确的解决时间因果发现的方法。

Conclusion: DOTS方法通过结合多个因果顺序，解决了单一因果顺序方法表达能力不足的问题，从而提高了时间序列数据中因果关系发现的准确性和效率。实验验证证明，该方法在准确性和效率上优于当前最优方法，是时间因果发现的一个有前途的工具。

Abstract: Predicting causal structure from time series data is crucial for
understanding complex phenomena in physiology, brain connectivity, climate
dynamics, and socio-economic behaviour. Causal discovery in time series is
hindered by the combinatorial complexity of identifying true causal
relationships, especially as the number of variables and time points grow. A
common approach to simplify the task is the so-called ordering-based methods.
Traditional ordering methods inherently limit the representational capacity of
the resulting model. In this work, we fix this issue by leveraging multiple
valid causal orderings, instead of a single one as standard practice. We
propose DOTS (Diffusion Ordered Temporal Structure), using diffusion-based
causal discovery for temporal data. By integrating multiple orderings, DOTS
effectively recovers the transitive closure of the underlying directed acyclic
graph, mitigating spurious artifacts inherent in single-ordering approaches. We
formalise the problem under standard assumptions such as stationarity and the
additive noise model, and leverage score matching with diffusion processes to
enable efficient Hessian estimation. Extensive experiments validate the
approach. Empirical evaluations on synthetic and real-world datasets
demonstrate that DOTS outperforms state-of-the-art baselines, offering a
scalable and robust approach to temporal causal discovery. On synthetic
benchmarks ($d{=}\!3-\!6$ variables, $T{=}200\!-\!5{,}000$ samples), DOTS
improves mean window-graph $F1$ from $0.63$ (best baseline) to $0.81$. On the
CausalTime real-world benchmark ($d{=}20\!-\!36$), while baselines remain the
best on individual datasets, DOTS attains the highest average summary-graph
$F1$ while halving runtime relative to graph-optimisation methods. These
results establish DOTS as a scalable and accurate solution for temporal causal
discovery.

</details>


### [183] [The Cost of Robustness: Tighter Bounds on Parameter Complexity for Robust Memorization in ReLU Nets](https://arxiv.org/abs/2510.24643)
*Yujun Kim,Chaewon Moon,Chulhee Yun*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: We study the parameter complexity of robust memorization for $\mathrm{ReLU}$
networks: the number of parameters required to interpolate any given dataset
with $\epsilon$-separation between differently labeled points, while ensuring
predictions remain consistent within a $\mu$-ball around each training sample.
We establish upper and lower bounds on the parameter count as a function of the
robustness ratio $\rho = \mu / \epsilon$. Unlike prior work, we provide a
fine-grained analysis across the entire range $\rho \in (0,1)$ and obtain
tighter upper and lower bounds that improve upon existing results. Our findings
reveal that the parameter complexity of robust memorization matches that of
non-robust memorization when $\rho$ is small, but grows with increasing $\rho$.

</details>


### [184] [Pearl: A Foundation Model for Placing Every Atom in the Right Location](https://arxiv.org/abs/2510.24670)
*Genesis Research Team,Alejandro Dobles,Nina Jovic,Kenneth Leidal,Pranav Murugan,David C. Williams,Drausin Wulsin,Nate Gruver,Christina X. Ji,Korrawat Pruegsanusak,Gianluca Scarpellini,Ansh Sharma,Wojciech Swiderski,Andrea Bootsma,Richard Strong Bowen,Charlotte Chen,Jamin Chen,Marc André Dämgen,Roy Tal Dew,Benjamin DiFrancesco,J. D. Fishman,Alla Ivanova,Zach Kagin,David Li-Bland,Zuli Liu,Igor Morozov,Jeffrey Ouyang-Zhang,Frank C. Pickard IV,Kushal S. Shah,Ben Shor,Gabriel Monteiro da Silva,Maxx Tessmer,Carl Tilbury,Cyr Vetcher,Daniel Zeng,Maruan Al-Shedivat,Aleksandra Faust,Evan N. Feinberg,Michael V. LeVine,Matteus Pan*

Main category: cs.LG

TL;DR: Pearl是一个用于蛋白质-配体共折叠的基础模型，相较于现有方法，它在生成准确的、物理上有效的构象上表现出显著的改进。在关键评估指标（RMSD < 2 Å）上，Pearl在公开的基准测试中分别超越了AlphaFold 3和其他开源基准14.5%和14.2%。在口袋条件共折叠领域，Pearl在严格的RMSD < 1 Å阈值下，对现实世界的药物靶标表现出3.6倍的改进。与训练时使用的合成数据集大小直接相关，Pearl模型的性能有所提高。


<details>
  <summary>Details</summary>
Motivation: 计算药物发现中精确预测蛋白质-配体复合物的三维结构依然是一项重大挑战，限制了治疗设计的速度和成功率。尽管深度学习方法在结构预测工具方面表现出色，但受限于实验数据稀少、非高效的架构、物理上无效的构象以及在推理时利用辅助信息的不足。为了克服这些问题，Pearl模型被引入，用以改进蛋白质-配体共折叠的能力。

Method: Pearl以三种关键创新来克服现存障碍：(1) 大规模合成数据的训练配方去解决数据稀缺问题；(2) 包含SO(3)等变扩散模块的架构，可以内在地尊重三维旋转对称性，提高泛化能力和样本效率；（3）可控的推理，包括支持蛋白质和非聚合成分的通用多链模板系统以及无条件/有条件模式。

Result: Pearl在蛋白质-配体共折叠上建立了新的性能标杆。在生成准确的和物理上有效的构象这一关键指标上，Pearl在公开基准测试中分别比AlphaFold 3及其他开源基准模型提高14.5%和14.2%。在口袋条件共折叠领域，Pearl在更为严格的RMSD < 1 Å阈值下，对现实世界的药物靶标表现出3.6倍的性能提升。另外，模型性能直接与训练中使用的合成数据集大小相关。

Conclusion: Pearl模型通过引入大规模合成数据、SO(3)等变扩散模块及可控推理，显著提高了蛋白质-配体共折叠的准确性和物理有效性，展现了在实际应用中的潜在价值。

Abstract: Accurately predicting the three-dimensional structures of protein-ligand
complexes remains a fundamental challenge in computational drug discovery that
limits the pace and success of therapeutic design. Deep learning methods have
recently shown strong potential as structural prediction tools, achieving
promising accuracy across diverse biomolecular systems. However, their
performance and utility are constrained by scarce experimental data,
inefficient architectures, physically invalid poses, and the limited ability to
exploit auxiliary information available at inference. To address these issues,
we introduce Pearl (Placing Every Atom in the Right Location), a foundation
model for protein-ligand cofolding at scale. Pearl addresses these challenges
with three key innovations: (1) training recipes that include large-scale
synthetic data to overcome data scarcity; (2) architectures that incorporate an
SO(3)-equivariant diffusion module to inherently respect 3D rotational
symmetries, improving generalization and sample efficiency, and (3)
controllable inference, including a generalized multi-chain templating system
supporting both protein and non-polymeric components as well as dual
unconditional/conditional modes. Pearl establishes a new state-of-the-art
performance in protein-ligand cofolding. On the key metric of generating
accurate (RMSD < 2 \r{A}) and physically valid poses, Pearl surpasses AlphaFold
3 and other open source baselines on the public Runs N' Poses and PoseBusters
benchmarks, delivering 14.5% and 14.2% improvements, respectively, over the
next best model. In the pocket-conditional cofolding regime, Pearl delivers
$3.6\times$ improvement on a proprietary set of challenging, real-world drug
targets at the more rigorous RMSD < 1 \r{A} threshold. Finally, we demonstrate
that model performance correlates directly with synthetic dataset size used in
training.

</details>


### [185] [Eigenfunction Extraction for Ordered Representation Learning](https://arxiv.org/abs/2510.24672)
*Burak Varıcı,Che-Ping Tsai,Ritabrata Ray,Nicholas M. Boffi,Pradeep Ravikumar*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Recent advances in representation learning reveal that widely used
objectives, such as contrastive and non-contrastive, implicitly perform
spectral decomposition of a contextual kernel, induced by the relationship
between inputs and their contexts. Yet, these methods recover only the linear
span of top eigenfunctions of the kernel, whereas exact spectral decomposition
is essential for understanding feature ordering and importance. In this work,
we propose a general framework to extract ordered and identifiable
eigenfunctions, based on modular building blocks designed to satisfy key
desiderata, including compatibility with the contextual kernel and scalability
to modern settings. We then show how two main methodological paradigms,
low-rank approximation and Rayleigh quotient optimization, align with this
framework for eigenfunction extraction. Finally, we validate our approach on
synthetic kernels and demonstrate on real-world image datasets that the
recovered eigenvalues act as effective importance scores for feature selection,
enabling principled efficiency-accuracy tradeoffs via adaptive-dimensional
representations.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [186] [Flexible Intelligent Layered Metasurfaces for Downlink Multi-user MISO Communications](https://arxiv.org/abs/2510.24190)
*Hong Niu,Jiancheng An,Chau Yuen*

Main category: cs.IT

TL;DR: 提出了一种柔性智能层状超表面（FILM）架构，通过采用两个形状可控制的柔性超表面层来解决传统智能超表面（SIM）所需层数多且传输系数矩阵难以动态调整的问题。FILM能减少所需的层数，同时保持信号处理性能。在两层FILM辅助的多用户多输入单输出（MU-MISO）系统中，通过使用交替优化（AO）方法解决了信道拟合问题，实验证明FILM架构能够显著提升和改善性能指标。



<details>
  <summary>Details</summary>
Motivation: 传统智能超表面(SIM)需要较多的层数来保证信号处理能力，导致实践中的功率衰减严重。为了解决这个问题，研究团队提出了一种新型柔性智能层状超表面（FILM）架构，旨在减少所需的层数，同时保持信号处理性能。


Method: 开发了一个由两个形状可控制的柔性超表面层组成的FILM辅助的多用户多输入单输出（MU-MISO）系统，并通过求解非凸问题来减少由FILM引起的信道与目标信道之间的差异，使用了交替优化（AO）方法进行解决。


Result: 实验表明，FILM架构在总速率上有了超过200%的提高，并在误码率（BER）上相对于传统的七层SIM有超过7 dB的增强。


Conclusion: 研究展示了一种新的FILM架构，成功降低了智能超表面所需的层数，并通过解决非凸信道拟合问题和使用贝尔法斯特优化方法显著改善了信号处理性能。


Abstract: Stacked intelligent metasurfaces (SIMs) have recently gained attention as a
paradigm for wave-domain signal processing with reduced reliance on costly
radio-frequency (RF) chains. However, conventional SIMs rely on uniform
inter-layer spacing and require deep stacking to ensure processing capability,
resulting in severe power attenuation in practice. To address this issue, we
propose a flexible intelligent layered metasurface (FILM) architecture
consisting of two shape-controllable flexible metasurface layers. By replacing
rigid metasurfaces with flexible ones in both layers, the transmission
coefficient matrix can be dynamically adjusted, significantly decreasing the
number of required layers while maintaining signal processing performance.
Firstly, we develop a two-layer FILM-assisted multi-user multiple-input
single-output (MU-MISO) system, wherein we formulate a channel fitting problem
aimed at reducing the difference between the FILM-induced and target channels.
Then, we solve this non-convex problem by employing an alternating optimization
(AO) method, featuring closed-form phase shift updates and a gradient
descent-based shape optimization. Furthermore, we analyze the upper bound on
sum-rate and the complexity of computation to provide insights into design
trade-offs. Finally, simulation results demonstrated that the proposed
transmissive FILM architecture achieves over 200\% improvement in sum-rate and
more than 7 dB bit-error rate (BER) gain compared to the conventional
seven-layer SIMs.

</details>


### [187] [What Can Be Recovered Under Sparse Adversarial Corruption? Assumption-Free Theory for Linear Measurements](https://arxiv.org/abs/2510.24215)
*Vishal Halder,Alexandre Reiffers-Masson,Abdeldjalil Aïssa-El-Bey,Gugan Thoppe*

Main category: cs.IT

TL;DR: 该论文探讨了在存在未知稀疏扰动的情况下，从已知矩阵A及其线性变换的结果y中恢复原向量x*的问题。主要结果表明，最优情况下可以恢复的是x*加上某个特定的零空间向量。并提供了一种通过最小化l0范数的构造性方法来恢复该集合的途径。


<details>
  <summary>Details</summary>
Motivation: 研究在未知稀疏扰动存在的背景下，如何从观察值m{y}中获得尽可能多的关于原向量m{x*}的信息

Method: 通过定义一个特殊的投影矩阵m{U}，该矩阵是在任意m{A}和m{x*}的情况下唯一表示了删除2q行后的所有子矩阵行空间的交集的投影矩阵。进而证明每个最小化m{y} - m{A} x的m{x}的m{l0}范数的解都在x*+m{ker(U)}中，提供了一种构造性的恢复方法。

Result: 表明在未知稀疏扰动的情况下，最佳恢复效果是能恢复到x*加上某个特定零空间向量的集合。找到这个集合的一个算法是通过最小化m{l0}范数实现的。

Conclusion: 在给定m{y}和稀疏度m{q}而未知m{e}的情况下，最优的解形式是x*加上通过最小化l0范数所确定的核空间。
这一发现为重建未知向量的可能范围提供了一个界限，且给出了该范围的构造方法。

Abstract: Let \(\bm{A} \in \mathbb{R}^{m \times n}\) be an arbitrary, known matrix and
\(\bm{e}\) a \(q\)-sparse adversarial vector. Given \(\bm{y} = \bm{A} x^* +
\bm{e}\) and \(q\), we seek the smallest set containing \(x^*\)-hence the one
conveying maximal information about \(x^*\)-that is uniformly recoverable from
\(\bm{y}\) without knowing \(\bm{e}\). While exact recovery of \(x^*\) via
strong (and often impractical) structural assumptions on \(\bm{A}\) or \(x^*\)
(for example, restricted isometry, sparsity) is well studied, recoverability
for arbitrary \(\bm{A}\) and \(x^*\) remains open. Our main result shows that
the best that one can hope to recover is \(x^* + \ker(\bm{U})\), where
\(\bm{U}\) is the unique projection matrix onto the intersection of rowspaces
of all possible submatrices of \(\bm{A}\) obtained by deleting \(2q\) rows.
Moreover, we prove that every \(x\) that minimizes the \(\ell\_0\)-norm of
\(\bm{y} - \bm{A} x\) lies in \(x^* + \ker(\bm{U})\), which then gives a
constructive approach to recover this set.

</details>


### [188] [Joint Active and Passive Beamforming with Sensing-Assisted Discrete Phase Shifts for Dual-RIS ISAC Systems](https://arxiv.org/abs/2510.24480)
*Qing Xue,Yun Lan,Jiajia Guo,Qianbin Chen,Shaodan Ma*

Main category: cs.IT

TL;DR: 论文提出了一种利用半被动双可重构智能表面（RIS）辅助的集成传感和通信（ISAC）系统来解决6G需求下的最大最小用户信号与干扰加噪声比（SINR）问题，通过联合有源和无源波束成形提高系统性能并确保用户公平性。提出了一种交替优化算法，并通过仿真验证了其性能优越性。


<details>
  <summary>Details</summary>
Motivation: 为了解决6G系统中的最大最小用户SINR问题，提高系统性能并保证用户公平性，论文提出了一种半被动双可重构智能表面辅助的集成传感和通信系统方案。通过联合有源和无源波束成形技术，提高系统整体效能。同时，利用双RIS简化了解决该问题的过程，并通过有效的交替优化算法解决相关优化问题。

Method: 论文提出了一套联合利用双RIS进行用户角度估计和优化波束成形的策略。特别地，采用了半定义松弛和二分法来解决发射波束成形优化次问题。对于RIS的离散相移，应用了基于传感的技术来限制优化搜索空间，并对于不同大小的RIS提出了两种不同的低复杂度搜索策略。

Result: 通过数值模拟，论文展示了所提出算法在性能上接近理想的连续相移基准，优于传统离散相移优化算法，并且在单RIS系统上表现出显著的性能提升。

Conclusion: 研究结果表明，文中提出的算法能够有效解决半被动双RIS辅助ISAC系统中的最大最小用户SINR问题，其性能优越，并能提升整体系统效率，为设计高效、公平的6G通信系统提供了新的思路和技术支持。

Abstract: Targeting the requirements of 6G, this paper investigates a semi-passive
dual-reconfigurable intelligent surface (RIS)-assisted integrated sensing and
communication (ISAC) system, tackling the max-min user
signal-to-interference-plus-noise ratio (SINR) problem via joint active and
passive beamforming to enhance system performance and ensure user fairness.
Addressing this challenge, we first utilize dual RISs for user angle estimation
to simplify the solution process of the formulated problem, an efficient
alternating optimization algorithm is then developed. Specifically,
semi-definite relaxation and the bisection method are employed to solve the
transmit beamforming optimization subproblem. For the RIS discrete phase
shifts, a sensing-assisted approach is adopted to constrain the optimization
search space, with two distinct low-complexity search strategies introduced for
different RIS sizes. Numerical simulation results demonstrate that the proposed
algorithm achieves performance close to the ideal continuous phase shift
benchmark, outperforms conventional discrete phase shift optimization
algorithms, and exhibits a significant improvement over single-RIS systems.

</details>


### [189] [Feedback Lunch: Deep Feedback Codes for Wiretap Channels](https://arxiv.org/abs/2510.16620)
*Yingyao Zhou,Natasha Devroye,Onur Günlü*

Main category: cs.IT

TL;DR: 本文研究了具有输出反馈的高斯窃听信道，通过结合通用哈希函数和基于反馈学习的编码，在保证信息可靠传输的同时实现秘密速率。发现反馈使合法用户能够协商共享的秘密密钥，克服了窃听者的安全优势，提出了一种基于传感辅助的安全通信代码设计方法。


<details>
  <summary>Details</summary>
Motivation: 考虑保密容量为零的反向退化窃听信道，在没有信道反馈的情况下，提出了一种带有信道输出反馈的种子模块化代码设计，以实现正的秘密速率。主要研究了通信可靠性和信息泄露之间的权衡，以克服窃听者的安全优势，并为下一代集成传感和通信提供代码设计方法。

Method: 结合通用哈希函数和基于反馈学习的编码设计，利用反馈使合法用户之间可以建立共享的秘密密钥，以提高秘密通信的安全性。

Result: 研究表明，通过这种方法能够实现正的秘密速率，同时具有较高的通信可靠性，克服了窃听者的安全优势。

Conclusion: 这项工作展示了在窃听信道中利用反馈实现正秘密速率的潜力，并通过引入传感辅助的安全通信代码设计方法，为未来的集成传感和通信技术铺平了道路。

Abstract: We consider reversely-degraded wiretap channels, for which the secrecy
capacity is zero if there is no channel feedback. This work focuses on a seeded
modular code design for the Gaussian wiretap channel with channel output
feedback, combining universal hash functions for security and learned
feedback-based codes for reliability to achieve positive secrecy rates. We
study the trade-off between communication reliability and information leakage,
illustrating that feedback enables agreeing on a secret key shared between
legitimate parties, overcoming the security advantage of the wiretapper. Our
findings also motivate code designs for sensing-assisted secure communication,
to be used in next-generation integrated sensing and communication methods.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [190] [A Simultaneous ECG-PCG Acquisition System with Real-Time Burst-Adaptive Noise Cancellation](https://arxiv.org/abs/2510.23819)
*Avishka Herath,Malith Jayalath,Kumudu Kaushalya,Sanjana Kapukotuwa,Chathuni Wijegunawardena,Pahan Mendis,Kithmin Wickremasinghe,Duminda Samarasinghe,Wageesha N. Manamperi,Chamira U. S. Edussooriya*

Main category: eess.SY

TL;DR: 提出了一种集成了实时自适应噪声消除管道的端到端系统，该系统同时获取心电图（ECG）和心音图（PCG）信号。在嘈杂的医院环境中，该系统对PCG和ECG信号的信噪比分别提高了37.01 dB和30.32 dB，从而在资源有限的环境中实现了可靠和可访问的心脏筛查方法。


<details>
  <summary>Details</summary>
Motivation: 传统的心音听诊技术因周围噪声干扰及信息有限，诊断难度大。现有的噪声消除技术不具备实时性或计算复杂度过高，不适合便携设备应用。因此，提出一种适用于便携设备的实时噪声消除系统，改善心脏筛查的效果。

Method: 研发了一个包含实时自适应噪声消除管道的端到端系统，该系统能够同时采集心电图（ECG）和心音图（PCG）信号。实验中使用了真实的医院噪声数据集和双模态设备捕捉的记录。

Result: 在医院嘈杂环境中对该系统进行验证，结果显示，对于实验设备采集的PCG和ECG信号，该算法分别实现了37.01 dB和30.32 dB的信噪比提高。实验结果表明，该系统在资源有限的环境中为可靠且可访问的心脏检查提供了有效的支持。

Conclusion: 研究提出一种集成实时自适应噪声消除管道的便携式双模态心音采集及分析系统，在资源有限的环境中显著提高了心音听诊的准确性和可靠性，为医院等环境提供了有效的心脏筛查手段。

Abstract: Cardiac auscultation is an essential clinical skill, requiring excellent
hearing to distinguish subtle differences in timing and pitch of heart sounds.
However, diagnosing solely from these sounds is often challenging due to
interference from surrounding noise, and the information may be limited.
Existing solutions that adaptively cancel external noise are either not
real-time or are computationally intensive, making them unsuitable for
implementation in a portable system. This work proposes an end-to-end system
with a real-time adaptive noise cancellation pipeline integrated into a device
that simultaneously acquires electrocardiogram (ECG) and phonocardiogram (PCG)
signals. The performance of the system is validated using real-world hospital
noise datasets and recordings captured with the dual-modality device. For PCG
and ECG signals recorded from the device in noisy hospital settings, the
proposed algorithms achieved signal-to-noise ratio improvements of 37.01 dB and
30.32 dB, respectively. These results demonstrate the systems effectiveness in
enabling reliable and accessible cardiac screening, including noisy hospital
environments typical of resource-constrained settings.

</details>


### [191] [MDP-based Energy-aware Task Scheduling for Battery-less IoT](https://arxiv.org/abs/2510.23820)
*Shahab Jahanbazi,Mateen Ashraf,Onel L. A. López*

Main category: eess.SY

TL;DR: 本文提出了一种基于马尔可夫决策过程（MDP）的最优阈值调度策略，用于提高无电池物联网设备的长期任务完成率。本文采用MDP框架，处理能量变化，使长期任务完成率最大化。实验结果显示，提出的最优静态阈值基（OSTB）调度策略优于经典的“尽可能晚”策略，任务完成率提高了8.6%，功率故障减少了65%，执行延迟减少了86.29%。


<details>
  <summary>Details</summary>
Motivation: 由于可用能量的随机和时间变化特性，设计最佳任务调度政策变得复杂，提高无电池物联网设备的长期任务完成率是需要解决的基本挑战

Method: 本文采用MDP框架处理能量变化，同时将长期任务完成率最大化。定义了相关的收益函数，深入研究了MDP形式化及其相关最优策略。提出了最优静态阈值基（OSTB）调度策略，以解决优化问题。

Result: 实验结果显示，提出的OSTB策略优于经典的ALAP策略，任务完成率提高了8.6%。功率故障减少了65%，执行延迟减少了86.29%。这些结果示范了其在4.7 mF电容器条件下的有效性

Conclusion: 综上所述，提出的OSTB调度策略成功解决了无电池物联网设备的长期任务完成率挑战，通过在能量变化条件下发现最优调度方案，显著提高任务完成率，减少功率故障和工作延迟

Abstract: Realizing high long-term task completion rates represents a fundamental
challenge in battery-less Internet of Things (IoT) devices powered by ambient
energy harvesting. This difficulty is primarily due to the stochastic and
time-varying characteristics of the available energy, which significantly
complicate the design of optimal task scheduling policies. In this paper, we
consider a battery-less IoT device that must periodically report sensing
measurements to a monitoring center. We adopt the Markov decision process (MDP)
framework to handle energy variability while aiming to maximize the long-term
task completion rate. For this, we first identify its components and then
define two appropriate reward functions. We demonstrate the inherent properties
associated with the MDP formulation and the related optimal policy.
Subsequently, we solve the resulting optimization problem, leading to the
optimal stationary threshold-based (OSTB) scheduling. Simulation results
demonstrate that OSTB outperforms the well-known ``as late as possible'' (ALAP)
scheduling strategy. For instance, an $8.6\%$ increase in the task completion
rate, along with a $65\%$ reduction in power failures and a $86.29\%$ decrease
in execution delays during task execution are registered assuming a $4.7$ mF
capacitor.

</details>


### [192] [Carbon-Aware Optimal Power Flow with Data-Driven Carbon Emission Tracing](https://arxiv.org/abs/2510.23877)
*Zhentong Shao,Nanpeng Yu*

Main category: eess.SY

TL;DR: 本文提出了一种考虑碳排放的最优潮流（OPF）框架，该框架通过数据驱动的碳追踪技术，能够快速估计电力负荷节点的碳排放量。通过开发发电机到负荷的碳排放分配系数，将平均和边际碳排放的公式集成到直流OPF模型中作为线性约束。所提出的碳感知OPF模型允许市场运营商在优化能源调度的同时减少温室气体排放。通过对IEEE测试系统的模拟，证实了该方法的准确性和计算效率，突显了其实时碳感知系统操作的应用潜力。


<details>
  <summary>Details</summary>
Motivation: 量化电网中的位置碳排放对于实施有效的减排策略以服务于依赖电力的客户至关重要。因此，需要一个能够快速准确估计电力负荷造成的碳排放量的框架。通过本研究，可以实现碳排放的精确追踪和优化调度，从而减少温室气体排放，促进绿色能源的使用。

Method: 该方法基于数据驱动的碳追踪技术，通过生成发电机到负荷的碳排放分布因子，提出了一个与DC OPF模型兼容的集成框架。该框架能够在优化能源调度时，同时将碳排放的约束条件考虑进去。

Result: 通过在IEEE测试系统上的模拟，该方法证明了其在解决碳排放追踪与优化调度问题中的高效性和准确性。模拟结果表明，该方法能够准确地计算碳排放量，并且计算效率高，适用于实时的碳感知系统操作。

Conclusion: 本文提出了一种新的碳感知OPF方法，通过将碳排放数据整合到现有的优化调度框架中，使得市场运营者可以在减少碳排放的同时优化能源调度，有助于实现更低的碳足迹和更可持续的能源使用。

Abstract: Quantifying locational carbon emissions in power grids is crucial for
implementing effective carbon reduction strategies for customers relying on
electricity. This paper presents a carbon-aware optimal power flow (OPF)
framework that incorporates data-driven carbon tracing, enabling rapid
estimation of nodal carbon emissions from electric loads. By developing
generator-to-load carbon emission distribution factors through data-driven
technique, the analytical formulas for both average and marginal carbon
emissions can be derived and integrated seamlessly into DC OPF models as linear
constraints. The proposed carbon-aware OPF model enables market operators to
optimize energy dispatch while reducing greenhouse gas emissions. Simulations
on IEEE test systems confirm the accuracy and computational efficiency of the
proposed approach, highlighting its applicability for real-time carbon-aware
system operations.

</details>


### [193] [Modeling and Scheduling of Fusion Patterns in Autonomous Driving Systems (Extended Version)](https://arxiv.org/abs/2510.23895)
*Hoora Sobhani,Hyoseung Kim*

Main category: eess.SY

TL;DR: 提出了一种针对自动驾驶系统中数据融合任务的系统框架，该框架能够分析和优化多种数据融合模式的性能。使用整数线性规划方法优化实时性能指标，并生成可以直接应用于真实平台的确定性离线调度。实验表明，该框架超越了现有工作的范围，实现了显著的性能改进。


<details>
  <summary>Details</summary>
Motivation: 现有数据融合任务的调度方法过于简化，无法捕捉到实际自动驾驶系统中数据融合的多样性模式。从而导致了对于真实应用场景的性能优化不足。因此，我们需要一种能够全面分析并优化这些模式的框架。

Method: 采用了整数线性规划方法（ILP）来处理不同类型的融合任务，包括基于定时触发、等待所有任务完成和即时融合。该方法可以优化多个实时性能指标，同时生成可以直接用于真实平台的离线调度。

Result: 相比于现有工作，该框架能够处理更复杂的融合模式，实现了更佳的性能表现。通过真实案例和随机生成的 DAG 数据集测试，该方法在反应时间、时间差距、信息年龄和响应时间等指标上取得了显著提升。

Conclusion: 这一框架为自动驾驶系统中的数据融合任务提供了一种新的优化方法，能够更真实地反映和优化实际场景中的性能。

Abstract: In Autonomous Driving Systems (ADS), Directed Acyclic Graphs (DAGs) are
widely used to model complex data dependencies and inter-task communication.
However, existing DAG scheduling approaches oversimplify data fusion tasks by
assuming fixed triggering mechanisms, failing to capture the diverse fusion
patterns found in real-world ADS software stacks. In this paper, we propose a
systematic framework for analyzing various fusion patterns and their
performance implications in ADS. Our framework models three distinct fusion
task types: timer-triggered, wait-for-all, and immediate fusion, which
comprehensively represent real-world fusion behaviors. Our Integer Linear
Programming (ILP)-based approach enables an optimization of multiple real-time
performance metrics, including reaction time, time disparity, age of
information, and response time, while generating deterministic offline
schedules directly applicable to real platforms. Evaluation using real-world
ADS case studies, Raspberry Pi implementation, and randomly generated DAGs
demonstrates that our framework handles diverse fusion patterns beyond the
scope of existing work, and achieves substantial performance improvements in
comparable scenarios.

</details>


### [194] [Dynamical Modeling of Temperature and Smoke Evolution in a Thermal-Runaway Event of a Large-Format Lithium-ion Battery in a Mine Tunnel](https://arxiv.org/abs/2510.23910)
*Khadija Omar Said,Yukta Pareek,Satadru Dey,Ashish Ranjan Kumar*

Main category: eess.SY

TL;DR: 大型锂离子电池（LIB）为地下矿山设备提供有效的能量存储解决方案，但不当使用可能导致热失控（TR），释放有毒和易燃气体。为研究TR的传播，构建了简化的动态模型，这些模型能够合理地复制温度和烟雾的变化趋势，并与实际数据高度一致。


<details>
  <summary>Details</summary>
Motivation: 避免LIB不当使用导致的热失控风险，需要研究其传播机制。但是，直接实验可能成本高昂且危险，因此使用简化的动态模型来模拟热失控过程。

Method: 利用降阶模型（ROM）在简化的框架内构建动态模型，用于模拟热失控事件。该模型能够有效地复制温度和烟雾的变化趋势。

Result: 该模型与实际数据高度一致，成功地复制了温度和烟雾的变化趋势，证明了其在研究热失控事件中的有效性。

Conclusion: 提出的简化动态模型能够有效模拟矿用大型锂离子电池热失控过程，为研究和预防类似事故提供了可靠的方法。

Abstract: Large-format lithium-ion batteries (LIBs) provide effective energy storage
solutions for high-power equipment used in underground mining operations. They
have high Columbic efficiency and minimal heat and emission footprints.
However, improper use of LIBs, accidents, or other factors may increase the
probability of thermal runaway (TR), a rapid combustion reaction that
discharges toxic and flammable substances. Several such incidents have been
documented in mines. Since repeatable TR experiments to uncover the
transient-state propagation of TR are expensive and hazardous, high-fidelity
models are usually developed to mimic the impact of these events. They are
resource-intensive and are impractical to develop for many scenarios that could
be observed in a mine. Therefore, dynamic models within a reduced-order
framework were constructed to represent the transient-state combustion event.
Reduced order models (ROMs) reasonably replicate trends in temperature and
smoke, showing strong alignment with the ground-truth dataset.

</details>


### [195] [Sample-based Moving Horizon Estimation](https://arxiv.org/abs/2510.24191)
*Isabelle Krauss,Victor G. Lopez,Matthias A. Müller*

Main category: eess.SY

TL;DR: 本文提出了一种基于样本的移动地平线估计（MHE）方案，适用于用不规则且/或不频繁的测量估计非线性系统当前状态。该方案在样本增量输入/输出-状态稳定性条件下能实现鲁棒全局指数稳定性。对于线性系统，提出了基于样本的可观察性和增量输入/输出-状态稳定性的联系，说明了样本可观察性的条件可应用于验证或设计确保样本MHE鲁棒全局指数稳定的采样策略。并给出了一个模拟示例来展示该方案的有效性。


<details>
  <summary>Details</summary>
Motivation: 现有MHE方案通常假设测量数据是规则且频繁的，未能充分利用不规则和不频繁的测量数据。因此需要开发一种新的MHE方案以适应这样的测量数据。本文的动机在于设计一种基于样本的MHE方法，能够处理不规则且/或不频繁的测量数据。

Method: 通过设计合适的成本函数来适应不规则输出序列，提出了基于样本的MHE方案。同时，给出了该方案在样本增量输入/输出-状态稳定性条件下的鲁棒全局指数稳定性定理。对于线性系统，探讨了样本可观察性与样本增量输入/输出-状态稳定性的联系。最后，通过模拟实验展示了所提方案的有效性。

Result: 本文提出了一个适应于非规则及不频繁测量的基于样本的MHE方案。证明了在样本增量输入/输出-状态稳定性的条件下，该方案能实现鲁棒全局指数稳定性。同时，对于线性系统，揭示了样本可观察性和样本增量输入/输出-状态稳定性之间的联系，同时展示了该方案的效用。

Conclusion: 本文研究提出了一种新的MHE方案，该方案能够处理非规则及不频繁的测量数据，适用于更为广泛的系统状态估计场景。此方案在满足样本增量输入/输出-状态稳定性的前提下，可以实现鲁棒全局指数稳定性。

Abstract: In this paper, we propose a sample-based moving horizon estimation (MHE)
scheme for general nonlinear systems to estimate the current system state using
irregularly and/or infrequently available measurements. The cost function of
the MHE optimization problem is suitably designed to accommodate these
irregular output sequences. We also establish that, under a suitable
sample-based detectability condition known as sample-based incremental
input/output-to-state stability (i-IOSS), the proposed sample-based MHE
achieves robust global exponential stability (RGES). Additionally, for the case
of linear systems, we draw connections between sample-based observability and
sample-based i-IOSS. This demonstrates that previously established conditions
for linear systems to be sample-based observable can be utilized to verify or
design sampling strategies that satisfy the conditions to guarantee RGES of the
sample-based MHE. Finally, the effectiveness of the proposed sample-based MHE
is illustrated through a simulation example.

</details>


### [196] [Survey and Tutorial of Reinforcement Learning Methods in Process Systems Engineering](https://arxiv.org/abs/2510.24272)
*Maximilian Bloor,Max Mowbray,Ehecatl Antonio Del Rio Chanona,Calvin Tsay*

Main category: eess.SY

TL;DR: 本文综述并介绍了强化学习（RL）方法，为过程系统工程（PSE）领域提供教程和应用案例分析，涵盖了价值型、策略型和Actor-Critic方法，并探讨了PSE领域内的特殊技术和新兴方向，提出了未来研究的方向和挑战。


<details>
  <summary>Details</summary>
Motivation: 传统的控制和优化方法在处理复杂和具有不确定性的过程系统工程挑战时常常显得力不从心，强化学习作为一种数据驱动的方法可以很好地解决这一问题，为此本论文进行了综述和介绍，以拓宽PSE领域的研究范围和技术应用。

Method: 本论文首先提供了一个关于强化学习的教程，接着介绍了各种PSE领域的应用案例，最后探讨了特殊技术和新兴方向。覆盖的内容包括价值型、策略型和Actor-Critic方法。

Result: 总结了当前的强化学习算法发展状态，为PSE领域的应用提供了参考，并提出了成功的案例、存在的挑战和未来的研究方向。

Conclusion: 研究表明，强化学习在处理复杂且充满不确定性的过程系统工程问题中潜力巨大，指出此领域的研究具有重要的现实意义和广阔的发展前景。

Abstract: Sequential decision making under uncertainty is central to many Process
Systems Engineering (PSE) challenges, where traditional methods often face
limitations related to controlling and optimizing complex and stochastic
systems. Reinforcement Learning (RL) offers a data-driven approach to derive
control policies for such challenges. This paper presents a survey and tutorial
on RL methods, tailored for the PSE community. We deliver a tutorial on RL,
covering fundamental concepts and key algorithmic families including
value-based, policy-based and actor-critic methods. Subsequently, we survey
existing applications of these RL techniques across various PSE domains, such
as in fed-batch and continuous process control, process optimization, and
supply chains. We conclude with PSE focused discussion of specialized
techniques and emerging directions. By synthesizing the current state of RL
algorithm development and implications for PSE this work identifies successes,
challenges, trends, and outlines avenues for future research at the interface
of these fields.

</details>


### [197] [Mechanism-Guided Residual Lifting and Control Consistent Modeling for Pneumatic Drying Processes](https://arxiv.org/abs/2510.24370)
*Yue Wu*

Main category: eess.SY

TL;DR: 该论文提出了一种统一的混合建模框架，名为'物理引导的残差提升与控制一致修正'，能够更准确地对农业、化工和制药等行业中的气动干燥过程进行建模和控制，实验验证了该框架的有效性


<details>
  <summary>Details</summary>
Motivation: 传统建模方法在同时保证准确性、可解释性和闭环应用性方面存在困难

Method: 提出了一个结合了瞬态机理模型和稳定性约束数据驱动组件的统一混合建模框架，模型在机理层面上统一了水蒸气分压差的动态质量转移，并引入了控制一致扩展的动态模式分解方法来修正残差动力学

Result: 该混合模型在未见测试数据集中的平均绝对误差分别为出口含水量0.016%和出口温度0.015°C，显著提高了预测的准确性

Conclusion: 验证了该框架的有效性和可靠性，证明了其能够有效地应用于气动干燥过程的建模和控制

Abstract: Pneumatic drying processes in industries such as agriculture, chemicals,and
pharmaceuticals are notoriously difficult to model and control due to
multi-source disturbances,coupled stage dynamics, and significant measurement
delays. Traditional modeling paradigms often fail to simultaneously deliver
accuracy, interpretability, and closed-loop applicability. To address this
challenge, this paper introduces a unified hybrid modeling framework, termed
Physics-Guided Residual Lifting with Control-Consistent Correction,which
integrates a transient mechanistic model with a stability-constrained
data-driven component. The framework covers the complete process chain of
drying, transport, and winnowing. On the mechanistic level, the model unifies
mass transfer dynamics using the partial pressure difference of water vapor,
incorporates water activity clamping and latent heat corrections for bound
water, and ensures energy closure with moisture-dependent specific heat. On the
data-driven level,we propose an orthogonal residual learning scheme. It
leverages intermediate states from the mechanistic model as proxy variables to
construct a physics-inspired dictionary, preventing parameter compensation and
overfitting during ridge regression. Furthermore, to ensure suitability for
predictive control, a Control-Consistent Extended Dynamic Mode Decomposition
with stability constraints is employed to learn the residual dynamics, for
which we provide boundedness proofs and stability guarantees. The framework was
validated on 10 industrial batches, comprising 63,000 samples. On unseen test
data, the hybrid model achieved a Mean Absolute Error of 0.016% for outlet
moisture and 0.015 {\deg}C for outlet temperature, with values improving to
0.986 and 0.995, respectively. The resulting prediction residuals exhibit
white-noise characteristics, with significantly reduced spectral energy at low
frequencies.

</details>


### [198] [Development of a Digital Twin for an Electric Vehicle Emulator Modeling, Control, and Experimental Validation](https://arxiv.org/abs/2510.24389)
*Lamine Chalal,Ahmed Rachid*

Main category: eess.SY

TL;DR: 本文提出了一种用于电动汽车（EV）模拟器的数字孪生开发和验证方法，该模拟器能够模拟不同的操作条件下的纵向车辆动态。该方法使用能量宏观表示（EMR）框架，优于传统的图形建模工具，通过能量交互的明确表示和控制结构的系统导出，实现了准确的速度控制。实验结果与模拟结果高度吻合，并提出了一种基于加速度和坡度约束的最大允许车辆质量的计算方法以及用于双向转换器的切换算法，确保了可靠的四象限操作。


<details>
  <summary>Details</summary>
Motivation: 当前电动汽车模拟中存在的问题需要一种新的方法来提高模拟精度和控制设计的复杂性，本文提出了一种基于能量宏观表示（EMR）框架的方法来解决这些问题，它是专门为电动汽车模拟器的验证和能量管理验证而设计的。这个框架提高了效率和准确性，避免了传统建模方法诸如图块图模型的局限性。

Method: 开发了一种数字孪生方法来模拟电动汽车的纵向动态，基于能量宏观表示（EMR）模型建立了系统架构，使用能量流控制方法，并提供了对双向转换器四象限操作的切换算法。

Result: 实验测试显示数值模拟和实验数据高度吻合；基于加速和坡度约束，最大允许车辆质量计算为13.5 kg；开发的双向转换器与切换算法，显示了可靠的四象限操作，验证了该新方法的准确性和效率。

Conclusion: 本文提出的基于EMR框架的方法对于电动汽车的仿真，控制器设计和能量管理的验证，提供了一种可拓展且有效的方法，与传统建模工具相比具有明显的优点。

Abstract: This paper presents the development and validation of a digital twin for a
scaled-down electric vehicle (EV) emulator, designed to replicate longitudinal
vehicle dynamics under diverse operating conditions. The emulator integrates a
separately excited DC motor (SEDCM), a four-quadrant DC-DC converter, a battery
emulator, and a mechanical load emulator. The system models tractive effort,
aerodynamic drag, and gradient resistance using Newton's second law. In
contrast to conventional graphical modeling tools (e.g., block diagrams and
bond graphs), the adopted Energetic Macroscopic Representation (EMR) framework
offers clear advantages by explicitly representing energy interactions and
facilitating the systematic derivation of control structures. A control
strategy developed within this framework governs energy flow across the
powertrain, enabling accurate speed control via armature voltage regulation.
Experimental tests conducted on a Lucas-Nulle test bench show strong
correlation with simulation results. The study also introduces a methodology to
compute the maximum admissible vehicle mass - determined to be 13.5 kg for a
180 W motor operating at 1900 rpm - based on acceleration and slope
constraints. Furthermore, a switching algorithm for the bidirectional converter
ensures reliable four quadrant operation. Overall, the proposed framework
provides a scalable and effective approach for EV emulation, control design,
and energy management validation.

</details>


### [199] [Analyzing Parametric Oscillator Ising Machines through the Kuramoto Lens](https://arxiv.org/abs/2510.24416)
*Nikhat Khan,E. M. H. E. B. Ekanayake,Nicolas Casilli,Cristian Cassella,Luke Theogarajan,Nikhil Shukla*

Main category: eess.SY

TL;DR: 本文开发了一个基于Kuramoto模型的相位描述，用于描述参数振荡器伊辛机，并解释了为什么在参数振荡器中不需要显式的二次谐波驱动。此外，还探讨了准稳态振幅异质性如何影响解决方案的质量，提供了构造伊辛机的新视角。


<details>
  <summary>Details</summary>
Motivation: 当前参数振荡器实现和传统振荡器伊辛机之间的关系尚待深入研究。为了填补这一空白，作者希望开发一个通用的理论框架来指导参数振荡器伊辛机的设计和理解。

Method: 从Stuart-Landau振子模型出发，发展了一个Kuramoto型、规范相位描述的手法，用以阐释参数振荡器伊辛机的工作原理，并以此揭示参数振荡器中相异策略的原理与结果。

Result: 研究结果表明，在参数振荡器中不使用显式的二次谐波驱动是合理的，准稳态振幅异质性对解决质量有潜在的负面影响。本文的分析结果为理解参数振荡器伊辛机提供了一个统一的视角。

Conclusion: 我们的研究为理解参数振荡器伊辛机提供了一个新颖的理论框架，有利于开发更有效的Ising机器设计。

Abstract: Networks of coupled nonlinear oscillators are emerging as powerful physical
platforms for implementing Ising machines. Yet the relationship between
parametric-oscillator implementations and traditional oscillator-based Ising
machines remains underexplored. In this work, we develop a Kuramoto-style,
canonical phase description of parametric oscillator Ising machines by starting
from the Stuart-Landau oscillator model- the canonical normal form near a Hopf
bifurcation, and a natural reduced description for many parametric oscillator
implementations such as the degenerate optical parametric oscillator (DOPO)
among others. The resulting phase dynamics combine the usual phase-difference
coupling observed in the standard Kuramoto model along with an intrinsic phase
sum term that is generated when conjugate coupling is considered. Moreover, our
formulation helps explain why explicit second-harmonic driving is unnecessary
in parametric oscillators and also reveals how quasi-steady amplitude
heterogeneity scales the original strength of the spin interaction with
potentially adverse impacts on the solution quality. Our work helps develop a
unifying view of the oscillator-based approach to designing Ising machines.

</details>
