<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 23]
- [cs.LG](#cs.LG) [Total: 1]
- [cs.AI](#cs.AI) [Total: 4]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [GeoPredict: Leveraging Predictive Kinematics and 3D Gaussian Geometry for Precise VLA Manipulation](https://arxiv.org/abs/2512.16811)
*Jingjing Qian,Boyao Han,Chen Shi,Lei Xiao,Long Yang,Shaoshuai Shi,Li Jiang*

Main category: cs.CV

TL;DR: GeoPredict是一个几何感知的视觉-语言-动作（VLA）框架，通过预测机器人手臂的3D关键点轨迹和未来工作空间几何，提升在需要精确3D推理的任务中的性能。


<details>
  <summary>Details</summary>
Motivation: 现有VLA模型在机器人操作中表现出良好的泛化能力，但大多是反应式和二维中心化的，在需要精确三维推理的任务中不可靠。因此需要一种能够处理三维几何信息的增强框架。

Method: GeoPredict引入了两个预测模块：1) 轨迹级模块，编码运动历史并预测机器人手臂的多步3D关键点轨迹；2) 预测性3D高斯几何模块，预测工作空间几何，并沿未来关键点轨迹进行跟踪引导的精炼。这些模块仅在训练时通过基于深度的渲染作为监督，推理时只需要轻量级的额外查询令牌，无需任何3D解码。

Result: 在RoboCasa Human-50、LIBERO和真实世界操作任务上的实验表明，GeoPredict始终优于强大的VLA基线，特别是在几何密集和空间要求高的场景中。

Conclusion: GeoPredict通过集成几何和运动先验，显著提升了VLA模型在需要精确3D推理的机器人操作任务中的性能，同时在推理时保持了较低的计算开销。

Abstract: Vision-Language-Action (VLA) models achieve strong generalization in robotic manipulation but remain largely reactive and 2D-centric, making them unreliable in tasks that require precise 3D reasoning. We propose GeoPredict, a geometry-aware VLA framework that augments a continuous-action policy with predictive kinematic and geometric priors. GeoPredict introduces a trajectory-level module that encodes motion history and predicts multi-step 3D keypoint trajectories of robot arms, and a predictive 3D Gaussian geometry module that forecasts workspace geometry with track-guided refinement along future keypoint trajectories. These predictive modules serve exclusively as training-time supervision through depth-based rendering, while inference requires only lightweight additional query tokens without invoking any 3D decoding. Experiments on RoboCasa Human-50, LIBERO, and real-world manipulation tasks show that GeoPredict consistently outperforms strong VLA baselines, especially in geometry-intensive and spatially demanding scenarios.

</details>


### [2] [Next-Generation License Plate Detection and Recognition System using YOLOv8](https://arxiv.org/abs/2512.16826)
*Arslan Amin,Rafia Mumtaz,Muhammad Jawad Bashir,Syed Mohammad Hassan Zaidi*

Main category: cs.CV

TL;DR: 本研究通过对比YOLOv8不同变体在车牌识别(LPR)和字符识别任务上的性能，提出了一种优化的车牌检测与识别流水线方案。


<details>
  <summary>Details</summary>
Motivation: 交通管理和车辆监控中，车牌的高效检测与识别至关重要，但现有方法在多样化环境下的实时准确率仍不理想，阻碍智慧交通系统的实际应用。

Method: 采用两个独立数据集分别训练YOLOv8 Nano（用于车牌定位）和YOLOv8 Small（用于字符识别），并创新设计了基于x轴坐标的字符排序方法来重构车牌序列。

Result: YOLOv8 Nano在车牌定位任务上达到0.964精确率和0.918 mAP50；YOLOv8 Small在字符识别任务上达到0.92精确率和0.91 mAP50。提出的组合方案兼顾了计算效率与识别准确率。

Conclusion: 提出的YOLOv8 Nano+YOLOv8 Small优化流水线为边缘设备部署提供了高效准确的解决方案，有助于推进智慧城市基础设施的实际应用。

Abstract: In the evolving landscape of traffic management and vehicle surveillance, efficient license plate detection and recognition are indispensable. Historically, many methodologies have tackled this challenge, but consistent real-time accuracy, especially in diverse environments, remains elusive. This study examines the performance of YOLOv8 variants on License Plate Recognition (LPR) and Character Recognition tasks, crucial for advancing Intelligent Transportation Systems. Two distinct datasets were employed for training and evaluation, yielding notable findings. The YOLOv8 Nano variant demonstrated a precision of 0.964 and mAP50 of 0.918 on the LPR task, while the YOLOv8 Small variant exhibited a precision of 0.92 and mAP50 of 0.91 on the Character Recognition task. A custom method for character sequencing was introduced, effectively sequencing the detected characters based on their x-axis positions. An optimized pipeline, utilizing YOLOv8 Nano for LPR and YOLOv8 Small for Character Recognition, is proposed. This configuration not only maintains computational efficiency but also ensures high accuracy, establishing a robust foundation for future real-world deployments on edge devices within Intelligent Transportation Systems. This effort marks a significant stride towards the development of smarter and more efficient urban infrastructures.

</details>


### [3] [Radiology Report Generation with Layer-Wise Anatomical Attention](https://arxiv.org/abs/2512.16841)
*Emmanuel D. Muñiz-De-León,Jorge A. Rosales-de-Golferichs,Ana S. Muñoz-Rodríguez,Alejandro I. Trejo-Castro,Eduardo de Avila-Armenta,Antonio Martínez-Torteya*

Main category: cs.CV

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Automatic radiology report generation is a promising application of multimodal deep learning, aiming to reduce reporting workload and improve consistency. However, current state-of-the-art (SOTA) systems - such as Multimodal AI for Radiology Applications (MAIRA-2) and Medical Pathways Language Model-Multimodal (MedPaLM-M) - depend on large-scale multimodal training, clinical metadata, and multiple imaging views, making them resource-intensive and inaccessible for most settings. We introduce a compact image-to-text architecture that generates the Findings section of chest X-ray reports from a single frontal image. The model combines a frozen Self-Distillation with No Labels v3 (DINOv3) Vision Transformer (ViT) encoder with a Generative Pre-trained Transformer 2 (GPT-2) decoder enhanced by layer-wise anatomical attention. This mechanism integrates lung and heart segmentation masks through hierarchical Gaussian smoothing, biasing attention toward clinically relevant regions without adding trainable parameters. Evaluated on the official Medical Information Mart for Intensive Care-Chest X-ray (MIMIC-CXR) dataset using Chest Radiograph Expert (CheXpert) and Radiology Graph (RadGraph) metrics, our approach achieved substantial gains: CheXpert Macro-F1 for five key pathologies increased by 168% (0.083 -> 0.238) and Micro-F1 by 146% (0.137 -> 0.337), while broader performance across 14 observations improved by 86% (0.170 -> 0.316). Structural coherence also improved, with RadGraph F1 rising by 9.7%. Despite its small size and purely image-conditioned design, the model demonstrates that decoder-level anatomical guidance improves spatial grounding and enhances coherence in clinically relevant regions. The source code is publicly available at: https://github.com/devMuniz02/UDEM-CXR-Reporting-Thesis-2025.

</details>


### [4] [OPENTOUCH: Bringing Full-Hand Touch to Real-World Interaction](https://arxiv.org/abs/2512.16842)
*Yuxin Ray Song,Jinzhou Li,Rao Fu,Devin Murphy,Kaichen Zhou,Rishi Shiv,Yaqi Li,Haoyu Xiong,Crystal Elaine Owens,Yilun Du,Yiyue Luo,Xianyi Cheng,Antonio Torralba,Wojciech Matusik,Paul Pu Liang*

Main category: cs.CV

TL;DR: 该研究提出了首个野外环境下的自我中心视角全手触觉数据集OpenTouch，包含5.1小时的同步视频-触觉-姿态数据和2,900个带详细文本标注的剪辑片段。通过该数据集，研究者建立了检索和分类基准，展示了触觉信号在抓握理解、跨模态对齐和视频检索方面的价值。


<details>
  <summary>Details</summary>
Motivation: 人类手是与物理世界交互的主要界面，但当前的自我中心视角感知很少能准确识别接触的时间、位置和力度。缺乏鲁棒的穿戴式触觉传感器，也没有野外环境下的一人称视频与全手接触数据的对齐数据集，这限制了视觉感知与物理交互的连接。

Method: 创建OpenTouch数据集，包含同步采集的视频、触觉和手部姿态数据。开发触觉信号采集系统，确保数据质量和时间同步。基于该数据集构建检索和分类基准任务，包括视频到触觉的检索、触觉信号分类等。

Result: 研究发现触觉信号能为抓握理解提供紧凑而有力的线索，能增强跨模态对齐效果，并且能从野外环境的视频查询中可靠地检索出相关触觉信息。数据集包含5.1小时的同步数据和2,900个带标注的剪辑。

Conclusion: 通过发布这个标注好的视觉-触觉-姿态数据集和相关基准任务，研究旨在推动多模态自我中心感知、具身学习和接触丰富的机器人操作技术的发展，弥合视觉感知与物理交互之间的鸿沟。

Abstract: The human hand is our primary interface to the physical world, yet egocentric perception rarely knows when, where, or how forcefully it makes contact. Robust wearable tactile sensors are scarce, and no existing in-the-wild datasets align first-person video with full-hand touch. To bridge the gap between visual perception and physical interaction, we present OpenTouch, the first in-the-wild egocentric full-hand tactile dataset, containing 5.1 hours of synchronized video-touch-pose data and 2,900 curated clips with detailed text annotations. Using OpenTouch, we introduce retrieval and classification benchmarks that probe how touch grounds perception and action. We show that tactile signals provide a compact yet powerful cue for grasp understanding, strengthen cross-modal alignment, and can be reliably retrieved from in-the-wild video queries. By releasing this annotated vision-touch-pose dataset and benchmark, we aim to advance multimodal egocentric perception, embodied learning, and contact-rich robotic manipulation.

</details>


### [5] [GenEval 2: Addressing Benchmark Drift in Text-to-Image Evaluation](https://arxiv.org/abs/2512.16853)
*Amita Kamath,Kai-Wei Chang,Ranjay Krishna,Luke Zettlemoyer,Yushi Hu,Marjan Ghazvininejad*

Main category: cs.CV

TL;DR: 论文指出T2I模型自动评估存在基准漂移问题，显示GenEval基准已显著偏离人类判断，提出新基准GenEval 2和改进评估方法Soft-TIFA来解决此问题。


<details>
  <summary>Details</summary>
Motivation: 当前文本到图像模型评估面临挑战：需要合适的评估模型和测试提示。静态基准会随时间推移出现漂移问题，即评估标准无法跟上新模型能力发展，导致评估结果逐渐偏离人类判断。以GenEval为例，该问题已造成高达17.7%的绝对误差，说明基准已经饱和，急需新的解决方案。

Method: 1) 分析并验证GenEval基准在实际应用中的漂移现象；2) 进行大规模人工研究验证基准饱和程度；3) 提出新基准GenEval 2，具有更好的基础视觉概念覆盖和更高的组合复杂度；4) 开发Soft-TIFA评估方法，通过组合视觉基元的判断来提高评估质量。

Result: 研究表明GenEval基准已严重漂移，绝对误差达17.7%，验证了基准饱和假设。提出的GenEval 2基准对当前模型更具挑战性，Soft-TIFA评估方法相比VQAScore等整体性评估方法能更好地与人类判断保持一致，且更不容易随时间漂移。

Conclusion: 基准漂移是T2I模型评估中的重要问题，需要持续的审计和改进。GenEval 2和Soft-TIFA为解决当前评估问题提供了有效方案，但避免基准漂移仍具挑战性，强调了对T2I及相关自动模型评估基准持续优化的重要性。

Abstract: Automating Text-to-Image (T2I) model evaluation is challenging; a judge model must be used to score correctness, and test prompts must be selected to be challenging for current T2I models but not the judge. We argue that satisfying these constraints can lead to benchmark drift over time, where the static benchmark judges fail to keep up with newer model capabilities. We show that benchmark drift is a significant problem for GenEval, one of the most popular T2I benchmarks. Although GenEval was well-aligned with human judgment at the time of its release, it has drifted far from human judgment over time -- resulting in an absolute error of as much as 17.7% for current models. This level of drift strongly suggests that GenEval has been saturated for some time, as we verify via a large-scale human study. To help fill this benchmarking gap, we introduce a new benchmark, GenEval 2, with improved coverage of primitive visual concepts and higher degrees of compositionality, which we show is more challenging for current models. We also introduce Soft-TIFA, an evaluation method for GenEval 2 that combines judgments for visual primitives, which we show is more well-aligned with human judgment and argue is less likely to drift from human-alignment over time (as compared to more holistic judges such as VQAScore). Although we hope GenEval 2 will provide a strong benchmark for many years, avoiding benchmark drift is far from guaranteed and our work, more generally, highlights the importance of continual audits and improvement for T2I and related automated model evaluation benchmarks.

</details>


### [6] [RePlan: Reasoning-guided Region Planning for Complex Instruction-based Image Editing](https://arxiv.org/abs/2512.16864)
*Tianyuan Qu,Lei Ke,Xiaohang Zhan,Longxiang Tang,Yuqi Liu,Bohao Peng,Bei Yu,Dong Yu,Jiaya Jia*

Main category: cs.CV

TL;DR: 本文提出了RePlan框架解决指令-视觉复杂度问题，通过区域对齐规划分解复杂指令并明确关联到目标区域，然后使用无需训练的注意力区域注入机制进行并行多区域编辑。


<details>
  <summary>Details</summary>
Motivation: 现有的基于指令的图像编辑模型在处理复杂的自然语言指令和混乱/模糊场景时表现不佳，需要一个新的框架来提升对复杂指令的精确理解和多区域编辑能力。

Method: RePlan采用规划-执行框架，包括：1）视觉语言规划器通过逐步推理分解指令并显式地将其定位到目标区域；2）扩散编辑器使用无需训练的注意力区域注入机制进行并行多区域编辑；3）使用GRPO强化学习提升规划能力。

Result: 在聚焦细粒度定位和知识密集型编辑的IV-Edit基准测试中，RePlan在复杂指令视觉设置下始终优于强大的基线模型，显著提高了区域精确度和总体保真度。

Conclusion: RePlan框架通过解耦规划和执行，解决了指令-视觉复杂度问题，使得在复杂指令和混乱场景下实现精确的多区域编辑成为可能，无需大规模数据集训练也能取得优异性能。

Abstract: Instruction-based image editing enables natural-language control over visual modifications, yet existing models falter under Instruction-Visual Complexity (IV-Complexity), where intricate instructions meet cluttered or ambiguous scenes. We introduce RePlan (Region-aligned Planning), a plan-then-execute framework that couples a vision-language planner with a diffusion editor. The planner decomposes instructions via step-by-step reasoning and explicitly grounds them to target regions; the editor then applies changes using a training-free attention-region injection mechanism, enabling precise, parallel multi-region edits without iterative inpainting. To strengthen planning, we apply GRPO-based reinforcement learning using 1K instruction-only examples, yielding substantial gains in reasoning fidelity and format reliability. We further present IV-Edit, a benchmark focused on fine-grained grounding and knowledge-intensive edits. Across IV-Complex settings, RePlan consistently outperforms strong baselines trained on far larger datasets, improving regional precision and overall fidelity. Our project page: https://replan-iv-edit.github.io

</details>


### [7] [Pixel Seal: Adversarial-only training for invisible image and video watermarking](https://arxiv.org/abs/2512.16874)
*Tomáš Souček,Pierre Fernandez,Hady Elsahar,Sylvestre-Alvise Rebuffi,Valeriu Lacatusu,Tuan Tran,Tom Sander,Alexandre Mourachko*

Main category: cs.CV

TL;DR: Pixel Seal是一种新型不可见水印方法，提出对抗性训练、三阶段训练计划和高分辨率适应，在图像和视频水印方面达到最先进水平。


<details>
  <summary>Details</summary>
Motivation: 现有方法在平衡水印鲁棒性和真正不可感知性方面存在困难，主要问题包括依赖失真的感知损失、优化不稳定以及扩展到高分辨率时性能下降。

Method: 1) 提出仅使用对抗性训练范式，消除不可靠的像素级不可感知损失；2) 引入三阶段训练计划，通过解耦鲁棒性和不可感知性来稳定收敛；3) 通过JND衰减和训练时推理模拟进行高分辨率适应。

Result: 在各种图像类型和变换上评估显示，Pixel Seal在鲁棒性和不可感知性方面明显优于最先进方法，能通过时间水印池化高效适应视频场景。

Conclusion: Pixel Seal为现实世界图像和视频场景中的可靠来源追溯提供了一个实用、可扩展的解决方案。

Abstract: Invisible watermarking is essential for tracing the provenance of digital content. However, training state-of-the-art models remains notoriously difficult, with current approaches often struggling to balance robustness against true imperceptibility. This work introduces Pixel Seal, which sets a new state-of-the-art for image and video watermarking. We first identify three fundamental issues of existing methods: (i) the reliance on proxy perceptual losses such as MSE and LPIPS that fail to mimic human perception and result in visible watermark artifacts; (ii) the optimization instability caused by conflicting objectives, which necessitates exhaustive hyperparameter tuning; and (iii) reduced robustness and imperceptibility of watermarks when scaling models to high-resolution images and videos. To overcome these issues, we first propose an adversarial-only training paradigm that eliminates unreliable pixel-wise imperceptibility losses. Second, we introduce a three-stage training schedule that stabilizes convergence by decoupling robustness and imperceptibility. Third, we address the resolution gap via high-resolution adaptation, employing JND-based attenuation and training-time inference simulation to eliminate upscaling artifacts. We thoroughly evaluate the robustness and imperceptibility of Pixel Seal on different image types and across a wide range of transformations, and show clear improvements over the state-of-the-art. We finally demonstrate that the model efficiently adapts to video via temporal watermark pooling, positioning Pixel Seal as a practical and scalable solution for reliable provenance in real-world image and video settings.

</details>


### [8] [Memory-Enhanced SAM3 for Occlusion-Robust Surgical Instrument Segmentation](https://arxiv.org/abs/2512.16880)
*Valay Bundele,Mehran Hosseinzadeh,Hendrik P. A. Lensch*

Main category: cs.CV

TL;DR: 提出ReMeDI-SAM3，这是SAM3的训练免费内存增强扩展，通过三个组件解决其在手术场景中的限制：相关性感知内存过滤、分段插值方案和基于特征的重新识别模块，显著提升手术器械分割性能。


<details>
  <summary>Details</summary>
Motivation: 内窥镜视频中的手术器械分割对计算机辅助干预至关重要，但因遮挡、快速运动、镜面伪影等问题而具有挑战性。SAM3虽提供强大的时空框架，但在手术场景中性能受限，主要问题是内存更新不够区分、固定内存容量以及遮挡后身份恢复能力弱。

Method: ReMeDI-SAM3包含三个部分：(1)使用遮挡感知内存存储遮挡前帧的相关性过滤；(2)通过分段插值方案扩展实际内存容量；(3)基于特征的重新识别模块结合时序投票，实现遮挡后可靠的身份识别。该方法无需训练，直接增强SAM3。

Result: 在EndoVis17和EndoVis18数据集上以零样本设置评估，相较原始SAM3分别获得了约7%和16%的绝对mcIoU提升，性能甚至超过先前的基于训练的方法。

Conclusion: ReMeDI-SAM3有效解决了SAM3在手术场景中的关键限制，通过增强的内存机制改善了遮挡处理能力，为内窥镜手术中的实时器械分割提供了更强大的解决方案。

Abstract: Accurate surgical instrument segmentation in endoscopic videos is crucial for computer-assisted interventions, yet remains challenging due to frequent occlusions, rapid motion, specular artefacts, and long-term instrument re-entry. While SAM3 provides a powerful spatio-temporal framework for video object segmentation, its performance in surgical scenes is limited by indiscriminate memory updates, fixed memory capacity, and weak identity recovery after occlusions. We propose ReMeDI-SAM3, a training-free memory-enhanced extension of SAM3, that addresses these limitations through three components: (i) relevance-aware memory filtering with a dedicated occlusion-aware memory for storing pre-occlusion frames, (ii) a piecewise interpolation scheme that expands the effective memory capacity, and (iii) a feature-based re-identification module with temporal voting for reliable post-occlusion identity disambiguation. Together, these components mitigate error accumulation and enable reliable recovery after occlusions. Evaluations on EndoVis17 and EndoVis18 under a zero-shot setting show absolute mcIoU improvements of around 7% and 16%, respectively, over vanilla SAM3, outperforming even prior training-based approaches. Project page: https://valaybundele.github.io/remedi-sam3/.

</details>


### [9] [M-PhyGs: Multi-Material Object Dynamics from Video](https://arxiv.org/abs/2512.16885)
*Norika Wada,Kohei Yamashita,Ryo Kawahara,Ko Nishino*

Main category: cs.CV

TL;DR: 这篇论文提出了M-PhyGs方法，用于从视频中估计花朵等多材料自然物体的材料成分和物理参数。


<details>
  <summary>Details</summary>
Motivation: 现有的物理参数估计方法假设物体是单一材料、形状简单或需要预学习动力学，无法处理复杂的多材料自然物体。

Method: 使用级联的3D和2D损失函数，结合时间小批量处理，从自然场景视频中联合分割材料并恢复其连续介质力学参数。

Result: 在Phlowers数据集上的实验验证了M-PhyGs方法及其组件的准确性和有效性。

Conclusion: M-PhyGs能够准确估计复杂自然物体的多材料物理参数，为现实世界物体的物理理解提供了新方法。

Abstract: Knowledge of the physical material properties governing the dynamics of a real-world object becomes necessary to accurately anticipate its response to unseen interactions. Existing methods for estimating such physical material parameters from visual data assume homogeneous single-material objects, pre-learned dynamics, or simplistic topologies. Real-world objects, however, are often complex in material composition and geometry lying outside the realm of these assumptions. In this paper, we particularly focus on flowers as a representative common object. We introduce Multi-material Physical Gaussians (M-PhyGs) to estimate the material composition and parameters of such multi-material complex natural objects from video. From a short video captured in a natural setting, M-PhyGs jointly segments the object into similar materials and recovers their continuum mechanical parameters while accounting for gravity. M-PhyGs achieves this efficiently with newly introduced cascaded 3D and 2D losses, and by leveraging temporal mini-batching. We introduce a dataset, Phlowers, of people interacting with flowers as a novel platform to evaluate the accuracy of this challenging task of multi-material physical parameter estimation. Experimental results on Phlowers dataset demonstrate the accuracy and effectiveness of M-PhyGs and its components.

</details>


### [10] [LinkedOut: Linking World Knowledge Representation Out of Video LLM for Next-Generation Video Recommendation](https://arxiv.org/abs/2512.16891)
*Haichao Zhang,Yao Lu,Lichen Wang,Yunzhe Li,Daiwei Chen,Yunpeng Xu,Yun Fu*

Main category: cs.CV

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Video Large Language Models (VLLMs) unlock world-knowledge-aware video understanding through pretraining on internet-scale data and have already shown promise on tasks such as movie analysis and video question answering. However, deploying VLLMs for downstream tasks such as video recommendation remains challenging, since real systems require multi-video inputs, lightweight backbones, low-latency sequential inference, and rapid response. In practice, (1) decode-only generation yields high latency for sequential inference, (2) typical interfaces do not support multi-video inputs, and (3) constraining outputs to language discards fine-grained visual details that matter for downstream vision tasks. We argue that these limitations stem from the absence of a representation that preserves pixel-level detail while leveraging world knowledge. We present LinkedOut, a representation that extracts VLLM world knowledge directly from video to enable fast inference, supports multi-video histories, and removes the language bottleneck. LinkedOut extracts semantically grounded, knowledge-aware tokens from raw frames using VLLMs, guided by promptable queries and optional auxiliary modalities. We introduce a cross-layer knowledge fusion MoE that selects the appropriate level of abstraction from the rich VLLM features, enabling personalized, interpretable, and low-latency recommendation. To our knowledge, LinkedOut is the first VLLM-based video recommendation method that operates on raw frames without handcrafted labels, achieving state-of-the-art results on standard benchmarks. Interpretability studies and ablations confirm the benefits of layer diversity and layer-wise fusion, pointing to a practical path that fully leverages VLLM world-knowledge priors and visual reasoning for downstream vision tasks such as recommendation.

</details>


### [11] [FlashPortrait: 6x Faster Infinite Portrait Animation with Adaptive Latent Prediction](https://arxiv.org/abs/2512.16900)
*Shuyuan Tu,Yueming Pan,Yinming Huang,Xintong Han,Zhen Xing,Qi Dai,Kai Qiu,Chong Luo,Zuxuan Wu*

Main category: cs.CV

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Current diffusion-based acceleration methods for long-portrait animation struggle to ensure identity (ID) consistency. This paper presents FlashPortrait, an end-to-end video diffusion transformer capable of synthesizing ID-preserving, infinite-length videos while achieving up to 6x acceleration in inference speed. In particular, FlashPortrait begins by computing the identity-agnostic facial expression features with an off-the-shelf extractor. It then introduces a Normalized Facial Expression Block to align facial features with diffusion latents by normalizing them with their respective means and variances, thereby improving identity stability in facial modeling. During inference, FlashPortrait adopts a dynamic sliding-window scheme with weighted blending in overlapping areas, ensuring smooth transitions and ID consistency in long animations. In each context window, based on the latent variation rate at particular timesteps and the derivative magnitude ratio among diffusion layers, FlashPortrait utilizes higher-order latent derivatives at the current timestep to directly predict latents at future timesteps, thereby skipping several denoising steps and achieving 6x speed acceleration. Experiments on benchmarks show the effectiveness of FlashPortrait both qualitatively and quantitatively.

</details>


### [12] [VIVA: VLM-Guided Instruction-Based Video Editing with Reward Optimization](https://arxiv.org/abs/2512.16906)
*Xiaoyan Cong,Haotian Yang,Angtian Wang,Yizhi Wang,Yiding Yang,Canyu Zhang,Chongyang Ma*

Main category: cs.CV

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Instruction-based video editing aims to modify an input video according to a natural-language instruction while preserving content fidelity and temporal coherence. However, existing diffusion-based approaches are often trained on paired data of simple editing operations, which fundamentally limits their ability to generalize to diverse and complex, real-world instructions. To address this generalization gap, we propose VIVA, a scalable framework for instruction-based video editing that leverages VLM-guided encoding and reward optimization. First, we introduce a VLM-based instructor that encodes the textual instruction, the first frame of the source video, and an optional reference image into visually-grounded instruction representations, providing fine-grained spatial and semantic context for the diffusion transformer backbone. Second, we propose a post-training stage, Edit-GRPO, which adapts Group Relative Policy Optimization to the domain of video editing, directly optimizing the model for instruction-faithful, content-preserving, and aesthetically pleasing edits using relative rewards. Furthermore, we propose a data construction pipeline designed to synthetically generate diverse, high-fidelity paired video-instruction data of basic editing operations. Extensive experiments show that VIVA achieves superior instruction following, generalization, and editing quality over state-of-the-art methods. Website: https://viva-paper.github.io

</details>


### [13] [Flowing from Reasoning to Motion: Learning 3D Hand Trajectory Prediction from Egocentric Human Interaction Videos](https://arxiv.org/abs/2512.16907)
*Mingfei Chen,Yifan Wang,Zhengqin Li,Homanga Bharadhwaj,Yujin Chen,Chuan Qin,Ziyi Kou,Yuan Tian,Eric Whitmire,Rajinder Sodhi,Hrvoje Benko,Eli Shlizerman,Yue Liu*

Main category: cs.CV

TL;DR: 提出了EgoMAN数据集和模型，解决现有3D手部轨迹预测工作中数据与语义监督解耦、推理与动作联系弱的问题。


<details>
  <summary>Details</summary>
Motivation: 现有3D手部轨迹预测研究受限于数据集将运动与语义监督解耦，且模型对推理与动作的关联较弱，需要更好的数据集和推理-运动耦合框架。

Method: 构建EgoMAN数据集（22万条6自由度轨迹和300万结构化QA对）；提出EgoMAN模型，通过轨迹标记接口连接视觉-语言推理与运动生成，采用渐进式训练对齐推理与运动动力学。

Result: 该方法能够生成准确且具有阶段感知的轨迹，在真实场景中表现出良好的泛化能力。

Conclusion: 通过结合大型具身化数据集和推理到运动的框架，该方法在3D手部轨迹预测任务上取得了具有语义理解和泛化能力的成果。

Abstract: Prior works on 3D hand trajectory prediction are constrained by datasets that decouple motion from semantic supervision and by models that weakly link reasoning and action. To address these, we first present the EgoMAN dataset, a large-scale egocentric dataset for interaction stage-aware 3D hand trajectory prediction with 219K 6DoF trajectories and 3M structured QA pairs for semantic, spatial, and motion reasoning. We then introduce the EgoMAN model, a reasoning-to-motion framework that links vision-language reasoning and motion generation via a trajectory-token interface. Trained progressively to align reasoning with motion dynamics, our approach yields accurate and stage-aware trajectories with generalization across real-world scenes.

</details>


### [14] [SceneDiff: A Benchmark and Method for Multiview Object Change Detection](https://arxiv.org/abs/2512.16908)
*Yuqun Wu,Chih-hao Lin,Henry Che,Aditi Tiwari,Chuhang Zou,Shenlong Wang,Derek Hoiem*

Main category: cs.CV

TL;DR: 该论文提出了第一个多视角变化检测基准SceneDiff Benchmark，并提出了无训练的SceneDiff方法，用于检测不同时间拍摄的同一场景中物体的添加、移除或移动。


<details>
  <summary>Details</summary>
Motivation: 识别场景中物体的变化对机器人整理、施工进度监控等应用很重要，但不同拍摄视角的差异会导致物体被错误检测为变化。

Method: 提出的SceneDiff方法利用预训练的3D、分割和图像编码模型，无需训练即可进行多视角对象变化检测。方法包括3D对齐捕获场景、提取物体区域、比较空间和语义特征来检测变化。

Result: 在多视角和双视角基准测试中，该方法比现有方法有显著提升（相对AP分别提高94%和37.4%）。

Conclusion: 论文提出了首个多视角变化检测基准和有效的无训练方法，在多个基准测试中表现出色，基准和数据代码将公开发布。

Abstract: We investigate the problem of identifying objects that have been added, removed, or moved between a pair of captures (images or videos) of the same scene at different times. Detecting such changes is important for many applications, such as robotic tidying or construction progress and safety monitoring. A major challenge is that varying viewpoints can cause objects to falsely appear changed. We introduce SceneDiff Benchmark, the first multiview change detection benchmark with object instance annotations, comprising 350 diverse video pairs with thousands of changed objects. We also introduce the SceneDiff method, a new training-free approach for multiview object change detection that leverages pretrained 3D, segmentation, and image encoding models to robustly predict across multiple benchmarks. Our method aligns the captures in 3D, extracts object regions, and compares spatial and semantic region features to detect changes. Experiments on multi-view and two-view benchmarks demonstrate that our method outperforms existing approaches by large margins (94% and 37.4% relative AP improvements). The benchmark and code will be publicly released.

</details>


### [15] [MomaGraph: State-Aware Unified Scene Graphs with Vision-Language Model for Embodied Task Planning](https://arxiv.org/abs/2512.16909)
*Yuanchen Ju,Yongyuan Liang,Yen-Jen Wang,Nandiraju Gireesh,Yuanliang Ju,Seungjae Lee,Qiao Gu,Elvis Hsieh,Furong Huang,Koushil Sreenath*

Main category: cs.CV

TL;DR: MomaGraph提出了一种用于具身智能体的统一场景表示方法，集成了空间-功能关系和部件级交互元素；配套的数据集MomaGraph-Scenes和评测基准MomaGraph-Bench支持该表示的开发；在此基础上构建的7B视觉语言模型MomaGraph-R1在零样本任务规划和场景理解方面实现了SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有的家庭移动机械臂系统在导航和操作任务中所需的多维度场景表示存在不足：传统方法往往将空间与功能关系分离、将场景视为静态快照而忽略物体状态和时序更新，且未能捕捉与当前任务最相关的信息。这些限制促使研究者开发更紧凑、语义丰富且能整合多种关系类型的场景表示方法。

Method: 论文提出了三个核心贡献：1）MomaGraph——一种集成了空间-功能关系和部件级交互元素的统一场景表示；2）MomaGraph-Scenes——首个大规模、细粒度标注的任务驱动家庭场景图数据集；3）MomaGraph-R1——基于7B参数视觉语言模型，通过在MomaGraph-Scenes上强化学习训练，采用“Graph-then-Plan”框架进行零样本任务规划和场景图预测。

Result: MomaGraph-R1在MomaGraph-Bench基准测试中达到71.6%的准确率，超过最佳基线11.4%，在开源模型中取得最佳性能。该模型在公共基准测试上展现出良好泛化能力，并能有效迁移到真实机器人实验中。

Conclusion: MomaGraph框架通过结合统一的场景表示、专用数据集和评测基准，以及先进的视觉语言模型，显著提升了具身智能体在家庭环境中的场景理解和任务规划能力。实验结果验证了该方法的有效性和优越性，为未来具身AI系统的发展提供了重要基础。

Abstract: Mobile manipulators in households must both navigate and manipulate. This requires a compact, semantically rich scene representation that captures where objects are, how they function, and which parts are actionable. Scene graphs are a natural choice, yet prior work often separates spatial and functional relations, treats scenes as static snapshots without object states or temporal updates, and overlooks information most relevant for accomplishing the current task. To address these limitations, we introduce MomaGraph, a unified scene representation for embodied agents that integrates spatial-functional relationships and part-level interactive elements. However, advancing such a representation requires both suitable data and rigorous evaluation, which have been largely missing. We thus contribute MomaGraph-Scenes, the first large-scale dataset of richly annotated, task-driven scene graphs in household environments, along with MomaGraph-Bench, a systematic evaluation suite spanning six reasoning capabilities from high-level planning to fine-grained scene understanding. Built upon this foundation, we further develop MomaGraph-R1, a 7B vision-language model trained with reinforcement learning on MomaGraph-Scenes. MomaGraph-R1 predicts task-oriented scene graphs and serves as a zero-shot task planner under a Graph-then-Plan framework. Extensive experiments demonstrate that our model achieves state-of-the-art results among open-source models, reaching 71.6% accuracy on the benchmark (+11.4% over the best baseline), while generalizing across public benchmarks and transferring effectively to real-robot experiments.

</details>


### [16] [SFTok: Bridging the Performance Gap in Discrete Tokenizers](https://arxiv.org/abs/2512.16910)
*Qihang Rao,Borui Zhang,Wenzhao Zheng,Jie Zhou,Jiwen Lu*

Main category: cs.CV

TL;DR: 论文提出名为SFTok的离散分词器，通过多步迭代机制提高图像重建质量。该模型整合自强制引导视觉重建和去偏拟合训练策略，有效解决了训练-推理不一致的问题。在高压缩比率下，SFTok在ImageNet实现最优重建质量，在类到图像生成任务中表现卓越。


<details>
  <summary>Details</summary>
Motivation: 近年来多模态模型中，图像分词对高分辨率图像生成至关重要。离散分词器虽与自回归范式自然对齐，但在性能上仍落后于连续分词器，限制了其在多模态系统中的采用。为弥补这一差距，论文旨在设计更优的离散分词器以提升图像重建质量。

Method: 提出的SFTok采用多步迭代机制，其中结合了自强制引导视觉重建和去偏拟合训练策略。通过这种方式，有效解决了多步过程中训练与推理不一致的问题，从而显著改善图像重建。

Result: 在每张图像仅用64个token的高压缩率下，SFTok在ImageNet上达到了最优重建质量（rFID=1.21），并在类到图像生成任务中表现优秀（gFID=2.29）。这表明SFTok超越了现有离散分词器的性能。

Conclusion: SFTok通过创新的多步迭代设计和训练策略，显著提升了离散图像分词器的性能，在高压缩率下实现了领先的图像重建质量，为多模态生成模型提供了更有效的图像编码方案。

Abstract: Recent advances in multimodal models highlight the pivotal role of image tokenization in high-resolution image generation. By compressing images into compact latent representations, tokenizers enable generative models to operate in lower-dimensional spaces, thereby improving computational efficiency and reducing complexity. Discrete tokenizers naturally align with the autoregressive paradigm but still lag behind continuous ones, limiting their adoption in multimodal systems. To address this, we propose \textbf{SFTok}, a discrete tokenizer that incorporates a multi-step iterative mechanism for precise reconstruction. By integrating \textbf{self-forcing guided visual reconstruction} and \textbf{debias-and-fitting training strategy}, SFTok resolves the training-inference inconsistency in multi-step process, significantly enhancing image reconstruction quality. At a high compression rate of only 64 tokens per image, SFTok achieves state-of-the-art reconstruction quality on ImageNet (rFID = 1.21) and demonstrates exceptional performance in class-to-image generation tasks (gFID = 2.29).

</details>


### [17] [StereoPilot: Learning Unified and Efficient Stereo Conversion via Generative Priors](https://arxiv.org/abs/2512.16915)
*Guibao Shen,Yihua Du,Wenhang Ge,Jing He,Chirui Chang,Donghao Zhou,Zhen Yang,Luozhou Wang,Xin Tao,Ying-Cong Chen*

Main category: cs.CV

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: The rapid growth of stereoscopic displays, including VR headsets and 3D cinemas, has led to increasing demand for high-quality stereo video content. However, producing 3D videos remains costly and complex, while automatic Monocular-to-Stereo conversion is hindered by the limitations of the multi-stage ``Depth-Warp-Inpaint'' (DWI) pipeline. This paradigm suffers from error propagation, depth ambiguity, and format inconsistency between parallel and converged stereo configurations. To address these challenges, we introduce UniStereo, the first large-scale unified dataset for stereo video conversion, covering both stereo formats to enable fair benchmarking and robust model training. Building upon this dataset, we propose StereoPilot, an efficient feed-forward model that directly synthesizes the target view without relying on explicit depth maps or iterative diffusion sampling. Equipped with a learnable domain switcher and a cycle consistency loss, StereoPilot adapts seamlessly to different stereo formats and achieves improved consistency. Extensive experiments demonstrate that StereoPilot significantly outperforms state-of-the-art methods in both visual fidelity and computational efficiency. Project page: https://hit-perfect.github.io/StereoPilot/.

</details>


### [18] [AdaTooler-V: Adaptive Tool-Use for Images and Videos](https://arxiv.org/abs/2512.16918)
*Chaoyang Wang,Kaituo Feng,Dongyang Chen,Zhongyu Wang,Zhixun Li,Sicheng Gao,Meng Meng,Xu Zhou,Manyuan Zhang,Yuzhang Shang,Xiangyu Yue*

Main category: cs.CV

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Recent advances have shown that multimodal large language models (MLLMs) benefit from multimodal interleaved chain-of-thought (CoT) with vision tool interactions. However, existing open-source models often exhibit blind tool-use reasoning patterns, invoking vision tools even when they are unnecessary, which significantly increases inference overhead and degrades model performance. To this end, we propose AdaTooler-V, an MLLM that performs adaptive tool-use by determining whether a visual problem truly requires tools. First, we introduce AT-GRPO, a reinforcement learning algorithm that adaptively adjusts reward scales based on the Tool Benefit Score of each sample, encouraging the model to invoke tools only when they provide genuine improvements. Moreover, we construct two datasets to support training: AdaTooler-V-CoT-100k for SFT cold start and AdaTooler-V-300k for RL with verifiable rewards across single-image, multi-image, and video data. Experiments across twelve benchmarks demonstrate the strong reasoning capability of AdaTooler-V, outperforming existing methods in diverse visual reasoning tasks. Notably, AdaTooler-V-7B achieves an accuracy of 89.8\% on the high-resolution benchmark V*, surpassing the commercial proprietary model GPT-4o and Gemini 1.5 Pro. All code, models, and data are released.

</details>


### [19] [EasyV2V: A High-quality Instruction-based Video Editing Framework](https://arxiv.org/abs/2512.16920)
*Jinjie Mai,Chaoyang Wang,Guocheng Gordon Qian,Willi Menapace,Sergey Tulyakov,Bernard Ghanem,Peter Wonka,Ashkan Mirzaei*

Main category: cs.CV

TL;DR: EasyV2V 是一个用于指令视频编辑的简单高效框架，通过数据构建、模型简化和控制统一三个方面提升性能。


<details>
  <summary>Details</summary>
Motivation: 当前视频编辑研究相对较少，面临一致性、控制和泛化等挑战，而图像编辑已相当成熟，需要探索适用于视频编辑的有效方法。

Method: 1）数据方面：组合现有专家方法与快速逆变换构建多样化视频对，通过单帧监督和仿射运动将图像编辑对提升为视频，挖掘密集标注片段，并添加转换监督。2）模型方面：观察到预训练文生视频模型具备编辑能力，采用简单序列拼接和轻量LoRA微调。3）控制方面：统一时空控制通过单一掩码机制，支持可选参考图像。

Result: EasyV2V 在各种输入模式下（如视频+文本、视频+掩码+文本、视频+掩码+参考图像+文本）均实现了最先进的视频编辑效果，超越了同期系统和商业系统。

Conclusion: 该框架通过系统的设计空间探索（数据、架构、控制），提供了一个简单而强大的视频编辑解决方案，显著推进了指令式视频编辑技术的发展。

Abstract: While image editing has advanced rapidly, video editing remains less explored, facing challenges in consistency, control, and generalization. We study the design space of data, architecture, and control, and introduce \emph{EasyV2V}, a simple and effective framework for instruction-based video editing. On the data side, we compose existing experts with fast inverses to build diverse video pairs, lift image edit pairs into videos via single-frame supervision and pseudo pairs with shared affine motion, mine dense-captioned clips for video pairs, and add transition supervision to teach how edits unfold. On the model side, we observe that pretrained text-to-video models possess editing capability, motivating a simplified design. Simple sequence concatenation for conditioning with light LoRA fine-tuning suffices to train a strong model. For control, we unify spatiotemporal control via a single mask mechanism and support optional reference images. Overall, EasyV2V works with flexible inputs, e.g., video+text, video+mask+text, video+mask+reference+text, and achieves state-of-the-art video editing results, surpassing concurrent and commercial systems. Project page: https://snap-research.github.io/easyv2v/

</details>


### [20] [Differences That Matter: Auditing Models for Capability Gap Discovery and Rectification](https://arxiv.org/abs/2512.16921)
*Qihao Liu,Chengzhi Mao,Yaojie Liu,Alan Yuille,Wen-Sheng Chu*

Main category: cs.CV

TL;DR: AuditDM是一个自动发现和修复多模态大语言模型失败模式的框架，通过强化学习训练审计器生成具有挑战性的问题和反事实图像，揭示模型弱点并提供无标注的修复数据


<details>
  <summary>Details</summary>
Motivation: 传统的多模态大语言模型评估方法缺乏可解释性，且往往无法充分揭示模型间的显著能力差距，需要一个更有效的模型审计和改进方法

Method: AuditDM通过强化学习将MLLM微调为审计器，生成能够最大化目标模型间分歧的挑战性问题和反事实图像，发现多样化的可解释失败案例

Result: 应用在Gemma-3和PaliGemma-2等先进模型上，发现了20多种不同的失败类型，基于这些发现的微调在16个基准测试上持续提升所有模型性能，并使3B模型超越其28B版本

Conclusion: 随着数据扩展收益递减，有针对性的模型审计为模型诊断和改进提供了有效途径，AuditDM框架通过主动发现失败模式实现显著的模型性能提升

Abstract: Conventional evaluation methods for multimodal LLMs (MLLMs) lack interpretability and are often insufficient to fully disclose significant capability gaps across models. To address this, we introduce AuditDM, an automated framework that actively discovers and rectifies MLLM failure modes by auditing their divergence. AuditDM fine-tunes an MLLM as an auditor via reinforcement learning to generate challenging questions and counterfactual images that maximize disagreement among target models. Once trained, the auditor uncovers diverse, interpretable exemplars that reveal model weaknesses and serve as annotation-free data for rectification. When applied to SoTA models like Gemma-3 and PaliGemma-2, AuditDM discovers more than 20 distinct failure types. Fine-tuning on these discoveries consistently improves all models across 16 benchmarks, and enables a 3B model to surpass its 28B counterpart. Our results suggest that as data scaling hits diminishing returns, targeted model auditing offers an effective path to model diagnosis and improvement.

</details>


### [21] [Next-Embedding Prediction Makes Strong Vision Learners](https://arxiv.org/abs/2512.16922)
*Sihan Xu,Ziqiao Ma,Wenhao Chai,Xuweiyi Chen,Weiyang Jin,Joyce Chai,Saining Xie,Stella X. Yu*

Main category: cs.CV

TL;DR: 本研究提出NEPA方法，通过预测未来图像块嵌入进行自监督学习，无需像素重建、离散化或对比损失，在ImageNet和ADE20K上表现优异


<details>
  <summary>Details</summary>
Motivation: 受自然语言生成预训练成功的启发，探索是否同样的原则可以应用于视觉自监督学习，从学习表示转向学习模型

Method: 使用因果掩码和停止梯度技术，训练Transformer模型基于历史图像块嵌入预测未来嵌入（NEPA方法），仅使用这一单一学习目标

Result: ViT-B在ImageNet-1K上达到83.8% top-1准确率，ViT-L达到85.3%，在ADE20K语义分割任务上也有效迁移

Conclusion: 基于嵌入的生成预训练为视觉自监督学习提供了简单、可扩展且可能模态无关的替代方案

Abstract: Inspired by the success of generative pretraining in natural language, we ask whether the same principles can yield strong self-supervised visual learners. Instead of training models to output features for downstream use, we train them to generate embeddings to perform predictive tasks directly. This work explores such a shift from learning representations to learning models. Specifically, models learn to predict future patch embeddings conditioned on past ones, using causal masking and stop gradient, which we refer to as Next-Embedding Predictive Autoregression (NEPA). We demonstrate that a simple Transformer pretrained on ImageNet-1k with next embedding prediction as its sole learning objective is effective - no pixel reconstruction, discrete tokens, contrastive loss, or task-specific heads. This formulation retains architectural simplicity and scalability, without requiring additional design complexity. NEPA achieves strong results across tasks, attaining 83.8% and 85.3% top-1 accuracy on ImageNet-1K with ViT-B and ViT-L backbones after fine-tuning, and transferring effectively to semantic segmentation on ADE20K. We believe generative pretraining from embeddings provides a simple, scalable, and potentially modality-agnostic alternative to visual self-supervised learning.

</details>


### [22] [Generative Refocusing: Flexible Defocus Control from a Single Image](https://arxiv.org/abs/2512.16923)
*Chun-Wei Tuan Mu,Jia-Bin Huang,Yu-Lun Liu*

Main category: cs.CV

TL;DR: 本文提出了一种两步式生成式重对焦方法，通过DeblurNet恢复全焦图像和BokehNet生成可控散景，采用半监督训练结合合成数据和真实散景图像，取得优越性能并支持文本引导调整和自定义光圈形状。


<details>
  <summary>Details</summary>
Motivation: 景深控制在摄影中至关重要，但单图像重对焦仍然困难，现有方法需要全焦输入、依赖仿真器合成数据且光圈控制有限，存在显著缺陷。

Method: 提出生成式重对焦方法，包含两大步骤：DeblurNet从各种输入恢复全焦图像，BokehNet生成可控散景；采用半监督训练策略，结合合成配对数据与非配对的真实散景图像，利用EXIF元数据捕捉真实光学特性。

Result: 实验结果表明，该方法在离焦去模糊、散景合成和重对焦基准测试中均取得最优性能。

Conclusion: 提出的生成式重对焦方法能够有效解决单图像重对焦难题，通过半监督训练结合真实光学特性，实现高质量的散景生成和灵活的控制能力，支持文本引导调整和自定义光圈形状。

Abstract: Depth-of-field control is essential in photography, but getting the perfect focus often takes several tries or special equipment. Single-image refocusing is still difficult. It involves recovering sharp content and creating realistic bokeh. Current methods have significant drawbacks. They need all-in-focus inputs, depend on synthetic data from simulators, and have limited control over aperture. We introduce Generative Refocusing, a two-step process that uses DeblurNet to recover all-in-focus images from various inputs and BokehNet for creating controllable bokeh. Our main innovation is semi-supervised training. This method combines synthetic paired data with unpaired real bokeh images, using EXIF metadata to capture real optical characteristics beyond what simulators can provide. Our experiments show we achieve top performance in defocus deblurring, bokeh synthesis, and refocusing benchmarks. Additionally, our Generative Refocusing allows text-guided adjustments and custom aperture shapes.

</details>


### [23] [The World is Your Canvas: Painting Promptable Events with Reference Images, Trajectories, and Text](https://arxiv.org/abs/2512.16924)
*Hanlin Wang,Hao Ouyang,Qiuyu Wang,Yue Yu,Yihao Meng,Wen Wang,Ka Leong Cheng,Shuailei Ma,Qingyan Bai,Yixuan Li,Cheng Chen,Yanhong Zeng,Xing Zhu,Yujun Shen,Qifeng Chen*

Main category: cs.CV

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: We present WorldCanvas, a framework for promptable world events that enables rich, user-directed simulation by combining text, trajectories, and reference images. Unlike text-only approaches and existing trajectory-controlled image-to-video methods, our multimodal approach combines trajectories -- encoding motion, timing, and visibility -- with natural language for semantic intent and reference images for visual grounding of object identity, enabling the generation of coherent, controllable events that include multi-agent interactions, object entry/exit, reference-guided appearance and counterintuitive events. The resulting videos demonstrate not only temporal coherence but also emergent consistency, preserving object identity and scene despite temporary disappearance. By supporting expressive world events generation, WorldCanvas advances world models from passive predictors to interactive, user-shaped simulators. Our project page is available at: https://worldcanvas.github.io/.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [24] [Training Together, Diagnosing Better: Federated Learning for Collagen VI-Related Dystrophies](https://arxiv.org/abs/2512.16876)
*Astrid Brull,Sara Aguti,Véronique Bolduc,Ying Hu,Daniel M. Jimenez-Gutierrez,Enrique Zuazua,Joaquin Del-Rio,Oleksii Sliusarenko,Haiyan Zhou,Francesco Muntoni,Carsten G. Bönnemann,Xabi Uribe-Etxebarria*

Main category: cs.LG

TL;DR: 本研究利用联邦学习技术，通过Sherpa.ai平台整合多国医疗机构的胶原VI免疫荧光显微图像数据，显著提升了罕见病胶原VI相关性肌营养不良症（COL6-RD）的诊断准确性。


<details>
  <summary>Details</summary>
Motivation: 罕见病诊断面临数据稀缺且分散的挑战，跨机构数据共享存在隐私和监管障碍。本研究旨在通过联邦学习解决这些障碍，实现多中心协作的机器学习模型训练，提高COL6-RD的诊断准确性和泛化能力。

Method: 采用Sherpa.ai联邦学习平台，整合两家国际机构的患者源性成纤维细胞培养的胶原VI免疫荧光显微图像数据。通过分布式训练构建机器学习模型，将胶原VI患者图像分为三个主要致病机制组：外显子跳跃、甘氨酸替代和假外显子插入。

Result: 联邦学习模型在COL6-RD分类任务中获得了0.82的F1分数，显著优于单一机构模型（0.57-0.75）。

Conclusion: 联邦学习技术能够有效克服数据隐私和监管障碍，显著提升罕见病诊断模型的准确性和泛化能力。该方法不仅支持更精确的诊断，还有助于解读意义不确定的变异，指导测序策略以识别新的致病变异。

Abstract: The application of Machine Learning (ML) to the diagnosis of rare diseases, such as collagen VI-related dystrophies (COL6-RD), is fundamentally limited by the scarcity and fragmentation of available data. Attempts to expand sampling across hospitals, institutions, or countries with differing regulations face severe privacy, regulatory, and logistical obstacles that are often difficult to overcome. The Federated Learning (FL) provides a promising solution by enabling collaborative model training across decentralized datasets while keeping patient data local and private. Here, we report a novel global FL initiative using the Sherpa.ai FL platform, which leverages FL across distributed datasets in two international organizations for the diagnosis of COL6-RD, using collagen VI immunofluorescence microscopy images from patient-derived fibroblast cultures. Our solution resulted in an ML model capable of classifying collagen VI patient images into the three primary pathogenic mechanism groups associated with COL6-RD: exon skipping, glycine substitution, and pseudoexon insertion. This new approach achieved an F1-score of 0.82, outperforming single-organization models (0.57-0.75). These results demonstrate that FL substantially improves diagnostic utility and generalizability compared to isolated institutional models. Beyond enabling more accurate diagnosis, we anticipate that this approach will support the interpretation of variants of uncertain significance and guide the prioritization of sequencing strategies to identify novel pathogenic variants.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [25] [TOGGLE: Temporal Logic-Guided Large Language Model Compression for Edge](https://arxiv.org/abs/2512.16855)
*Khurram Khalil,Khaza Anuarul Hoque*

Main category: cs.AI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Large Language Models (LLMs) deliver exceptional performance across natural language tasks but demand substantial computational resources, limiting their deployment on resource-constrained edge devices. Existing compression techniques, such as quantization and pruning, often degrade critical linguistic properties and lack formal guarantees for preserving model behavior. We propose Temporal Logic-Guided Large Language Model Compression (TOGGLE), a novel framework that leverages Signal Temporal Logic (STL) to formally specify and enforce linguistic properties during compression. TOGGLE employs an STL robustness-guided Bayesian optimization to systematically explore layer-wise quantization and pruning configurations, generating compressed models that formally satisfy specified linguistic constraints without retraining or fine-tuning. Evaluating TOGGLE on four LLM architectures (GPT-2, DeepSeek-V2 7B, LLaMA 3 8B, and Mistral 7B), we achieve up to 3.3x reduction in computational costs (FLOPs) and up to a 68.8% reduction in model size while satisfying all linguistic properties. TOGGLE represents the first integration of formal methods into LLM compression, enabling efficient, verifiable deployment of LLMs on edge hardware.

</details>


### [26] [Distributional AGI Safety](https://arxiv.org/abs/2512.16856)
*Nenad Tomašev,Matija Franklin,Julian Jacobs,Sébastien Krier,Simon Osindero*

Main category: cs.AI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: AI safety and alignment research has predominantly been focused on methods for safeguarding individual AI systems, resting on the assumption of an eventual emergence of a monolithic Artificial General Intelligence (AGI). The alternative AGI emergence hypothesis, where general capability levels are first manifested through coordination in groups of sub-AGI individual agents with complementary skills and affordances, has received far less attention. Here we argue that this patchwork AGI hypothesis needs to be given serious consideration, and should inform the development of corresponding safeguards and mitigations. The rapid deployment of advanced AI agents with tool-use capabilities and the ability to communicate and coordinate makes this an urgent safety consideration. We therefore propose a framework for distributional AGI safety that moves beyond evaluating and aligning individual agents. This framework centers on the design and implementation of virtual agentic sandbox economies (impermeable or semi-permeable), where agent-to-agent transactions are governed by robust market mechanisms, coupled with appropriate auditability, reputation management, and oversight to mitigate collective risks.

</details>


### [27] [The Social Responsibility Stack: A Control-Theoretic Architecture for Governing Socio-Technical AI](https://arxiv.org/abs/2512.16873)
*Otman A. Basir*

Main category: cs.AI

TL;DR: 本文提出了一个“社会责任栈（SRS）”框架，旨在将社会价值嵌入AI系统的工程实现中，形成一个六层结构，将责任建模为对复杂系统的闭环监督控制问题。


<details>
  <summary>Details</summary>
Motivation: 当前负责任AI和治理工作多停留在原则层面，缺乏能在系统全生命周期内强制执行的技术机制，因此需要一种能将规范目标转化为可操作工程控制的方法。

Method: 引入社会责任栈（SRS）六层架构框架，采用基于约束的统一形式化方法，整合设计时保障机制和运行时监测与治理过程的监督控制闭环，并通过安全包络和反馈解释来实施。

Result: 通过临床决策支持、协作自动驾驶汽车和公共部门系统等案例研究，展示了SRS如何将规范目标转化为可操作的工程和运营控制，实现对公平性、自主性、认知负担和解释质量的持续监测与执行。

Conclusion: 该框架在伦理、控制理论和AI治理之间架起了桥梁，为建设可问责、自适应、可审计的社会技术与AI系统提供了实践基础。

Abstract: Artificial intelligence systems are increasingly deployed in domains that shape human behaviour, institutional decision-making, and societal outcomes. Existing responsible AI and governance efforts provide important normative principles but often lack enforceable engineering mechanisms that operate throughout the system lifecycle. This paper introduces the Social Responsibility Stack (SRS), a six-layer architectural framework that embeds societal values into AI systems as explicit constraints, safeguards, behavioural interfaces, auditing mechanisms, and governance processes. SRS models responsibility as a closed-loop supervisory control problem over socio-technical systems, integrating design-time safeguards with runtime monitoring and institutional oversight. We develop a unified constraint-based formulation, introduce safety-envelope and feedback interpretations, and show how fairness, autonomy, cognitive burden, and explanation quality can be continuously monitored and enforced. Case studies in clinical decision support, cooperative autonomous vehicles, and public-sector systems illustrate how SRS translates normative objectives into actionable engineering and operational controls. The framework bridges ethics, control theory, and AI governance, providing a practical foundation for accountable, adaptive, and auditable socio-technical AI systems.

</details>


### [28] [Generative Adversarial Reasoner: Enhancing LLM Reasoning with Adversarial Reinforcement Learning](https://arxiv.org/abs/2512.16917)
*Qihao Liu,Luoxin Ye,Wufei Ma,Yu-Cheng Chou,Alan Yuille*

Main category: cs.AI

TL;DR: 论文提出了生成对抗推理器框架，通过对抗强化学习联合训练推理模型和判别器来改进数学推理质量。


<details>
  <summary>Details</summary>
Motivation: 大语言模型虽然在数学推理方面表现突出，但仍然存在过程错误，如计算错误、逻辑脆弱和表面合理但无效的推理步骤。现有方法难以提供密集且校准良好的步骤级奖励信号。

Method: 提出了生成对抗推理器框架，包含推理模型和判别器，通过对抗强化学习进行联合训练。使用计算高效的审查调度将推理链分割为逻辑完整的切片，判别器评估每个切片的合理性并提供结构化反馈。推理模型奖励逻辑一致的步骤和正确答案，判别器奖励正确检测错误。该方法产生密集、校准良好的步骤级奖励信号，补充稀疏的精确匹配信号。

Result: 在多个数学基准测试上，该方法在标准的强化学习后训练基础上取得了持续的提升。在AIME24上，将DeepSeek-R1-Distill-Qwen-7B从54.0提高到61.3（+7.3），将DeepSeek-R1-Distill-Llama-8B从43.7提高到53.7（+10.0）。模块化判别器还支持灵活的奖励塑造，可用于教师蒸馏、偏好对齐和基于数学证明的推理等目标。

Conclusion: 提出的生成对抗推理器框架通过对抗强化学习和密集步骤级奖励信号，提高了大语言模型的推理质量和样本效率。模块化判别器设计为多种推理任务的灵活奖励塑造提供了可能。

Abstract: Large language models (LLMs) with explicit reasoning capabilities excel at mathematical reasoning yet still commit process errors, such as incorrect calculations, brittle logic, and superficially plausible but invalid steps. In this paper, we introduce Generative Adversarial Reasoner, an on-policy joint training framework designed to enhance reasoning by co-evolving an LLM reasoner and an LLM-based discriminator through adversarial reinforcement learning. A compute-efficient review schedule partitions each reasoning chain into logically complete slices of comparable length, and the discriminator evaluates each slice's soundness with concise, structured justifications. Learning couples complementary signals: the LLM reasoner is rewarded for logically consistent steps that yield correct answers, while the discriminator earns rewards for correctly detecting errors or distinguishing traces in the reasoning process. This produces dense, well-calibrated, on-policy step-level rewards that supplement sparse exact-match signals, improving credit assignment, increasing sample efficiency, and enhancing overall reasoning quality of LLMs. Across various mathematical benchmarks, the method delivers consistent gains over strong baselines with standard RL post-training. Specifically, on AIME24, we improve DeepSeek-R1-Distill-Qwen-7B from 54.0 to 61.3 (+7.3) and DeepSeek-R1-Distill-Llama-8B from 43.7 to 53.7 (+10.0). The modular discriminator also enables flexible reward shaping for objectives such as teacher distillation, preference alignment, and mathematical proof-based reasoning.

</details>
